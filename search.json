[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "The RBioc Book",
    "section": "",
    "text": "Preface",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#who-is-this-book-for",
    "href": "index.html#who-is-this-book-for",
    "title": "The RBioc Book",
    "section": "Who is this book for?",
    "text": "Who is this book for?\n\nPeople who want to learn data science\nPeople who want to teach data science\nPeople who want to learn how to teach data science\nPeople who want to learn how to learn data science",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#why-this-book",
    "href": "index.html#why-this-book",
    "title": "The RBioc Book",
    "section": "Why this book?",
    "text": "Why this book?\nThis book is a collection of resources for learning R and Bioconductor. It is meant to be largely self-directed, but for those looking to teach data science, it can also be used as a guide for structuring a course. Material is a bit variable in terms of difficulty, prerequisites, and format which is a reflection of the organic creation of the material.\nStudents are encouraged to work with others to learn the material. Instructors are encouraged to use the material to create a course that is tailored to the needs of their students and to spend lots of time in 1:1 and small groups to support students in their learning. See below for additional thoughts on adult learning and how it relates to this material.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#adult-learners",
    "href": "index.html#adult-learners",
    "title": "The RBioc Book",
    "section": "Adult learners",
    "text": "Adult learners\nAdult Learning Theory, also known as Andragogy, is the concept and practice of designing, developing, and delivering instructional experiences for adult learners. It is based on the belief that adults learn differently than children, and thus, require distinct approaches to engage, motivate, and retain information (Center 2016). The term was first introduced by Malcolm Knowles, an American educator who is known for his work in adult education (Knowles, Holton, and Swanson 2005).\nOne of the fundamental principles of Adult Learning Theory is that adults are self-directed learners. This means that we prefer to take control of our own learning process and set personal goals for themselves. We are motivated by our desire to solve problems or gain knowledge to improve our lives (see Figure 1). As a result, educational content for adults should be relevant and applicable to real-life situations. Furthermore, adult learners should be given opportunities to actively engage in the learning process by making choices, setting goals, and evaluating their progress.\n\n\n\n\n\nFigure 1: Why do adults choose to learn something?\n\n\nAnother key aspect of Adult Learning Theory is the role of experience. We bring a wealth of experience to the learning process, which serves as a resource for new learning. We often have well-established beliefs, values, and mental models that can influence our willingness to accept new ideas and concepts. Therefore, it is essential to acknowledge and respect our shared and unique past experiences and create an environment where we all feel comfortable sharing our perspectives.\nTo effectively learn as a group of adult learners, it is crucial to establish a collaborative learning environment that promotes open communication and fosters trust among participants. We all appreciate and strive for a respectful and supportive atmosphere where we can express our opinions without fear of judgment. Instructors should help facilitate discussions, encourage peer-to-peer interactions, and incorporate group activities and collaboration to capitalize on the collective knowledge of participants.\nAdditionally, adult learners often have multiple responsibilities outside of the learning environment, such as work and family commitments. As a result, we require flexible learning opportunities that accommodate busy schedules. Offering a variety of instructional formats, such as online modules, self-paced learning, or evening classes, can help ensure that adult learners have access to education despite any time constraints.\nAdult learners benefit from a learner-centered approach that focuses on the individual needs, preferences, and interests of each participant can greatly enhance the overall learning experience. In addition, we tend to be more intrinsically motivated to learn when we have a sense of autonomy and can practice and experiment (see Figure 2) with new concepts in a safe environment.\n\n\n\n\n\n\n\nFigure 2: How to stay stuck in data science (or anything). The “Read-Do” loop tends to deliver the best results. Too much reading between doing can be somewhat effective. Reading and simply copy-paste is probably the least effective. When working through material, experiment. Try to break things. Incorporate your own experience or applications whenever possible.\n\n\n\n\nUnderstanding Adult Learning Theory and its principles can significantly enhance the effectiveness of teaching and learning as adults. By respecting our autonomy, acknowledging our experiences, creating a supportive learning environment, offering flexible learning opportunities, and utilizing diverse teaching methods, we can better cater to the unique needs and preferences of adult learners.\nIn practice, that means that we will will not be prescriptive in our approach to teaching data science. We will not tell you what to do, but rather we will provide you with a variety of options and you can choose what works best for you. We will also provide you with a variety of resources and you can choose where to focus your time. Given that we cannot possibly cover everything, we will provide you with a framework for learning and you can fill in the gaps as you see fit. A key component of our success as adult learners is to gain the confidence to ask questions and problem-solve on our own.\n\n\n\n\nCenter, Pew Research. 2016. “Lifelong Learning and Technology.” Pew Research Center: Internet, Science & Tech. https://www.pewresearch.org/internet/2016/03/22/lifelong-learning-and-technology/.\n\n\nKnowles, Malcolm S., Elwood F. Holton, and Richard A. Swanson. 2005. The Adult Learner: The Definitive Classic in Adult Education and Human Resource Development. 6th ed. Amsterdam ; Boston: Elsevier.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "intro.html",
    "href": "intro.html",
    "title": "\n1  Introducing R and RStudio\n",
    "section": "",
    "text": "1.1 Introduction\nIn this chapter, we will discuss the basics of R and RStudio, two essential tools in genomics data analysis. We will cover the advantages of using R and RStudio, how to set up RStudio, and the different panels of the RStudio interface.",
    "crumbs": [
      "Introduction",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introducing R and RStudio</span>"
    ]
  },
  {
    "objectID": "intro.html#questions",
    "href": "intro.html#questions",
    "title": "\n1  Introducing R and RStudio\n",
    "section": "Questions",
    "text": "Questions\n\nWhat is R?\nWhy use R?\nWhy not use R?\nWhy use RStudio and how does it differ from R?",
    "crumbs": [
      "Introduction",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introducing R and RStudio</span>"
    ]
  },
  {
    "objectID": "intro.html#learning-objectives",
    "href": "intro.html#learning-objectives",
    "title": "\n1  Introducing R and RStudio\n",
    "section": "Learning Objectives",
    "text": "Learning Objectives\n\nKnow advantages of analyzing data in R\nKnow advantages of using RStudio\nBe able to start RStudio on your computer\nIdentify the panels of the RStudio interface\nBe able to customize the RStudio layout",
    "crumbs": [
      "Introduction",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introducing R and RStudio</span>"
    ]
  },
  {
    "objectID": "intro.html#what-is-r",
    "href": "intro.html#what-is-r",
    "title": "\n1  Introducing R and RStudio\n",
    "section": "\n1.2 What is R?",
    "text": "1.2 What is R?\n[R](https://en.wikipedia.org/wiki/R_(programming_language) is a programming language and software environment designed for statistical computing and graphics. It is widely used by statisticians, data scientists, and researchers for data analysis and visualization. R is an open-source language, which means it is free to use, modify, and distribute. Over the years, R has become particularly popular in the fields of genomics and bioinformatics, owing to its extensive libraries and powerful data manipulation capabilities.\nThe R language is a dialect of the S language, which was developed in the 1970s at Bell Laboratories. The first version of R was written by Robert Gentleman and Ross Ihaka and released in 1995 (see this slide deck for Ross Ihaka’s take on R’s history). Since then, R has been continuously developed by the R Core Team, a group of statisticians and computer scientists. The R Core Team releases a new version of R every year.\n\n\n\n\n\n\n\nFigure 1.1: Google trends showing the popularity of R over time based on Google searches",
    "crumbs": [
      "Introduction",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introducing R and RStudio</span>"
    ]
  },
  {
    "objectID": "intro.html#why-use-r",
    "href": "intro.html#why-use-r",
    "title": "\n1  Introducing R and RStudio\n",
    "section": "\n1.3 Why use R?",
    "text": "1.3 Why use R?\nThere are several reasons why R is a popular choice for data analysis, particularly in genomics and bioinformatics. These include:\n\n\nOpen-source: R is free to use and has a large community of developers who contribute to its growth and development. What is “open-source”?\n\n\nExtensive libraries: There are thousands of R packages available for a wide range of tasks, including specialized packages for genomics and bioinformatics. These libraries have been extensively tested and ara available for free.\n\nData manipulation: R has powerful data manipulation capabilities, making it easy (or at least possible) to clean, process, and analyze large datasets.\n\nGraphics and visualization: R has excellent tools for creating high-quality graphics and visualizations that can be customized to meet the specific needs of your analysis. In most cases, graphics produced by R are publication-quality.\n\nReproducible research: R enables you to create reproducible research by recording your analysis in a script, which can be easily shared and executed by others. In addition, R does not have a meaningful graphical user interface (GUI), which renders analysis in R much more reproducible than tools that rely on GUI interactions.\n\nCross-platform: R runs on Windows, Mac, and Linux (as well as more obscure systems).\n\nInteroperability with other languages: R can interfact with FORTRAN, C, and many other languages.\n\nScalability: R is useful for small and large projects.\n\nI can develop code for analysis on my Mac laptop. I can then install the same code on our 20k core cluster and run it in parallel on 100 samples, monitor the process, and then update a database (for example) with R when complete.",
    "crumbs": [
      "Introduction",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introducing R and RStudio</span>"
    ]
  },
  {
    "objectID": "intro.html#why-not-use-r",
    "href": "intro.html#why-not-use-r",
    "title": "\n1  Introducing R and RStudio\n",
    "section": "\n1.4 Why not use R?",
    "text": "1.4 Why not use R?\n\nR cannot do everything.\nR is not always the “best” tool for the job.\nR will not hold your hand. Often, it will slap your hand instead.\nThe documentation can be opaque (but there is documentation).\nR can drive you crazy (on a good day) or age you prematurely (on a bad one).\nFinding the right package to do the job you want to do can be challenging; worse, some contributed packages are unreliable.]{}\nR does not have a meaningfully useful graphical user interface (GUI).",
    "crumbs": [
      "Introduction",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introducing R and RStudio</span>"
    ]
  },
  {
    "objectID": "intro.html#r-license-and-the-open-source-ideal",
    "href": "intro.html#r-license-and-the-open-source-ideal",
    "title": "\n1  Introducing R and RStudio\n",
    "section": "\n1.5 R License and the Open Source Ideal",
    "text": "1.5 R License and the Open Source Ideal\nR is free (yes, totally free!) and distributed under GNU license. In particular, this license allows one to:\n\nDownload the source code\nModify the source code to your heart’s content\nDistribute the modified source code and even charge money for it, but you must distribute the modified source code under the original GNU license]{}\n\nThis license means that R will always be available, will always be open source, and can grow organically without constraint.",
    "crumbs": [
      "Introduction",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introducing R and RStudio</span>"
    ]
  },
  {
    "objectID": "intro.html#rstudio",
    "href": "intro.html#rstudio",
    "title": "\n1  Introducing R and RStudio\n",
    "section": "\n1.6 RStudio",
    "text": "1.6 RStudio\nRStudio is an integrated development environment (IDE) for R. It provides a graphical user interface (GUI) for R, making it easier to write and execute R code. RStudio also provides several other useful features, including a built-in console, syntax-highlighting editor, and tools for plotting, history, debugging, workspace management, and workspace viewing. RStudio is available in both free and commercial editions; the commercial edition provides some additional features, including support for multiple sessions and enhanced debugging\n\n1.6.1 Getting started with RStudio\nTo get started with RStudio, you first need to install both R and RStudio on your computer. Follow these steps:\n\nDownload and install R from the official R website.\nDownload and install RStudio from the official RStudio website.\nLaunch RStudio. You should see the RStudio interface with four panels.\n\n1.6.2 The RStudio Interface\nRStudio’s interface consists of four panels (see Figure 1.2):\n\n\nConsole\n\nThis panel displays the R console, where you can enter and execute R commands directly. The console also shows the output of your code, error messages, and other information.\n\n\n\nSource\n\nThis panel is where you write and edit your R scripts. You can create new scripts, open existing ones, and run your code from this panel.\n\n\n\nEnvironment\n\nThis panel displays your current workspace, including all variables, data objects, and functions that you have created or loaded in your R session.\n\n\n\nPlots, Packages, Help, and Viewer\n\nThese panels display plots, installed packages, help files, and web content, respectively.\n\n\n\n\n\n\n\n\nFigure 1.2: The RStudio interface. In this layout, the source pane is in the upper left, the console is in the lower left, the environment panel is in the top right and the viewer/help/files panel is in the bottom right.\n\n\n\n\n\n\n\n\nDo I need to use RStudio?\n\n\n\nNo. You can use R without RStudio. However, RStudio makes it easier to write and execute R code, and it provides several useful features that are not available in the basic R console. Note that the only part of RStudio that is actually interacting with R directly is the console. The other panels are simply providing a GUI that enhances the user experience.\n\n\n\n\n\n\n\n\nCustomizing the RStudio Interface\n\n\n\nYou can customize the layout of RStudio to suit your preferences. To do so, go to Tools &gt; Global Options &gt; Appearance. Here, you can change the theme, font size, and panel layout. You can also resize the panels as needed to gain screen real estate (see Figure 1.3).\n\n\n\n\n\n\n\nFigure 1.3: Dealing with limited screen real estate can be a challenge, particularly when you want to open another window to, for example, view a web page. You can resize the panes by sliding the center divider (red arrows) or by clicking on the minimize/maximize buttons (see blue arrow).\n\n\nIn summary, R and RStudio are powerful tools for genomics data analysis. By understanding the advantages of using R and RStudio and familiarizing yourself with the RStudio interface, you can efficiently analyze and visualize your data. In the following chapters, we will delve deeper into the functionality of R, Bioconductor, and various statistical methods to help you gain a comprehensive understanding of genomics data analysis.",
    "crumbs": [
      "Introduction",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introducing R and RStudio</span>"
    ]
  },
  {
    "objectID": "r_intro_mechanics.html",
    "href": "r_intro_mechanics.html",
    "title": "\n2  R mechanics\n",
    "section": "",
    "text": "2.1 Learning objectives",
    "crumbs": [
      "Introduction",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>R mechanics</span>"
    ]
  },
  {
    "objectID": "r_intro_mechanics.html#learning-objectives",
    "href": "r_intro_mechanics.html#learning-objectives",
    "title": "\n2  R mechanics\n",
    "section": "",
    "text": "Be able to start R and RStudio\nLearn to interact with the R console\nKnow the difference between expressions and assignment\nRecognize valid and invalid R names\nKnow how to access the R help system\nKnow how to assign values to variables, find what is in R memory, and remove values from R memory",
    "crumbs": [
      "Introduction",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>R mechanics</span>"
    ]
  },
  {
    "objectID": "r_intro_mechanics.html#installing-r",
    "href": "r_intro_mechanics.html#installing-r",
    "title": "\n2  R mechanics\n",
    "section": "\n2.2 Installing R",
    "text": "2.2 Installing R\nR is available for Windows, Mac, and Linux. To install R, go to the Comprehensive R Archive Network (CRAN). Click on the download link for your operating system and follow the instructions.",
    "crumbs": [
      "Introduction",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>R mechanics</span>"
    ]
  },
  {
    "objectID": "r_intro_mechanics.html#installing-rstudio",
    "href": "r_intro_mechanics.html#installing-rstudio",
    "title": "\n2  R mechanics\n",
    "section": "\n2.3 Installing RStudio",
    "text": "2.3 Installing RStudio\nRStudio is an Integrated Development Environment (IDE) for R. It is available for Windows, Mac, and Linux. To install RStudio, go to the RStudio download page. Click on the download link for your operating system and follow the instructions.",
    "crumbs": [
      "Introduction",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>R mechanics</span>"
    ]
  },
  {
    "objectID": "r_intro_mechanics.html#starting-r",
    "href": "r_intro_mechanics.html#starting-r",
    "title": "\n2  R mechanics\n",
    "section": "\n2.4 Starting R",
    "text": "2.4 Starting R\nHow to start R depends a bit on the operating system (Mac, Windows, Linux) and interface. In this course, we will largely be using an Integrated Development Environment (IDE) called RStudio, but there is nothing to prohibit using R at the command line or in some other interface (and there are a few).",
    "crumbs": [
      "Introduction",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>R mechanics</span>"
    ]
  },
  {
    "objectID": "r_intro_mechanics.html#rstudio-a-quick-tour",
    "href": "r_intro_mechanics.html#rstudio-a-quick-tour",
    "title": "\n2  R mechanics\n",
    "section": "\n2.5 RStudio: A Quick Tour",
    "text": "2.5 RStudio: A Quick Tour\nThe RStudio interface has multiple panes. All of these panes are simply for convenience except the “Console” panel, typically in the lower left corner (by default). The console pane contains the running R interface. If you choose to run R outside RStudio, the interaction will be identical to working in the console pane. This is useful to keep in mind as some environments, such as a computer cluster, encourage using R without RStudio.\n\nPanes\nOptions\nHelp\nEnvironment, History, and Files",
    "crumbs": [
      "Introduction",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>R mechanics</span>"
    ]
  },
  {
    "objectID": "r_intro_mechanics.html#interacting-with-r",
    "href": "r_intro_mechanics.html#interacting-with-r",
    "title": "\n2  R mechanics\n",
    "section": "\n2.6 Interacting with R",
    "text": "2.6 Interacting with R\nThe only meaningful way of interacting with R is by typing into the R console. At the most basic level, anything that we type at the command line will fall into one of two categories:\n\n\nAssignments\n\nx = 1\ny &lt;- 2\n\n\n\nExpressions\n\n1 + pi + sin(42)\n\n[1] 3.225071\n\n\n\n\nThe assignment type is obvious because either the The &lt;- or = are used. Note that when we type expressions, R will return a result. In this case, the result of R evaluating 1 + pi + sin(42) is 3.2250711.\nThe standard R prompt is a “&gt;” sign. When present, R is waiting for the next expression or assignment. If a line is not a complete R command, R will continue the next line with a “+”. For example, typing the fillowing with a “Return” after the second “+” will result in R giving back a “+” on the next line, a prompt to keep typing.\n\n1 + pi +\nsin(3.7)\n\n[1] 3.611757\n\n\nR can be used as a glorified calculator by using R expressions. Mathematical operations include:\n\nAddition: +\n\nSubtraction: -\n\nMultiplication: *\n\nDivision: /\n\nExponentiation: ^\n\nModulo: %%\n\n\nThe ^ operator raises the number to its left to the power of the number to its right: for example 3^2 is 9. The modulo returns the remainder of the division of the number to the left by the number on its right, for example 5 modulo 3 or 5 %% 3 is 2.\n\n2.6.1 Expressions\n\n5 + 2\n28 %% 3\n3^2\n5 + 4 * 4 + 4 ^ 4 / 10\n\nNote that R follows order-of-operations and groupings based on parentheses.\n\n5 + 4 / 9\n(5 + 4) / 9\n\n\n2.6.2 Assignment\nWhile using R as a calculator is interesting, to do useful and interesting things, we need to assign values to objects. To create objects, we need to give it a name followed by the assignment operator &lt;- (or, entirely equivalently, =) and the value we want to give it:\n\nweight_kg &lt;- 55 \n\n&lt;- is the assignment operator. Assigns values on the right to objects on the left, it is like an arrow that points from the value to the object. Using an = is equivalent (in nearly all cases). Learn to use &lt;- as it is good programming practice.\nObjects can be given any name such as x, current_temperature, or subject_id (see below). You want your object names to be explicit and not too long. They cannot start with a number (2x is not valid but x2 is). R is case sensitive (e.g., weight_kg is different from Weight_kg). There are some names that cannot be used because they represent the names of fundamental functions in R (e.g., if, else, for, see here for a complete list). In general, even if it’s allowed, it’s best to not use other function names, which we’ll get into shortly (e.g., c, T, mean, data, df, weights). When in doubt, check the help to see if the name is already in use. It’s also best to avoid dots (.) within a variable name as in my.dataset. It is also recommended to use nouns for variable names, and verbs for function names.\nWhen assigning a value to an object, R does not print anything. You can force to print the value by typing the name:\n\nweight_kg\n\n[1] 55\n\n\nNow that R has weight_kg in memory, which R refers to as the “global environment”, we can do arithmetic with it. For instance, we may want to convert this weight in pounds (weight in pounds is 2.2 times the weight in kg).\n\n2.2 * weight_kg\n\n[1] 121\n\n\nWe can also change a variable’s value by assigning it a new one:\n\nweight_kg &lt;- 57.5\n2.2 * weight_kg\n\n[1] 126.5\n\n\nThis means that assigning a value to one variable does not change the values of other variables. For example, let’s store the animal’s weight in pounds in a variable.\n\nweight_lb &lt;- 2.2 * weight_kg\n\nand then change weight_kg to 100.\n\nweight_kg &lt;- 100\n\nWhat do you think is the current content of the object weight_lb, 126.5 or 220?\nYou can see what objects (variables) are stored by viewing the Environment tab in Rstudio. You can also use the ls() function. You can remove objects (variables) with the rm() function. You can do this one at a time or remove several objects at once. You can also use the little broom button in your environment pane to remove everything from your environment.\n\nls()\nrm(weight_lb, weight_kg)\nls()\n\nWhat happens when you type the following, now?\n\nweight_lb # oops! you should get an error because weight_lb no longer exists!",
    "crumbs": [
      "Introduction",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>R mechanics</span>"
    ]
  },
  {
    "objectID": "r_intro_mechanics.html#rules-for-names-in-r",
    "href": "r_intro_mechanics.html#rules-for-names-in-r",
    "title": "\n2  R mechanics\n",
    "section": "\n2.7 Rules for Names in R",
    "text": "2.7 Rules for Names in R\nR allows users to assign names to objects such as variables, functions, and even dimensions of data. However, these names must follow a few rules.\n\nNames may contain any combination of letters, numbers, underscore, and “.”\nNames may not start with numbers, underscore.\nR names are case-sensitive.\n\nExamples of valid R names include:\npi\nx\ncamelCaps\nmy_stuff\nMY_Stuff\nthis.is.the.name.of.the.man\nABC123\nabc1234asdf\n.hi",
    "crumbs": [
      "Introduction",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>R mechanics</span>"
    ]
  },
  {
    "objectID": "r_intro_mechanics.html#resources-for-getting-help",
    "href": "r_intro_mechanics.html#resources-for-getting-help",
    "title": "\n2  R mechanics\n",
    "section": "\n2.8 Resources for Getting Help",
    "text": "2.8 Resources for Getting Help\nThere is extensive built-in help and documentation within R. A separate page contains a collection of additional resources.\nIf the name of the function or object on which help is sought is known, the following approaches with the name of the function or object will be helpful. For a concrete example, examine the help for the print method.\n\nhelp(print)\nhelp('print')\n?print\n\nIf the name of the function or object on which help is sought is not known, the following from within R will be helpful.\n\nhelp.search('microarray')\nRSiteSearch('microarray')\napropos('histogram')\n\nThere are also tons of online resources that Google will include in searches if online searching feels more appropriate.\nI strongly recommend using help(\"newfunction\"\") for all functions that are new or unfamiliar to you.\nThere are also many open and free resources and reference guides for R.\n\n\nQuick-R: a quick online reference for data input, basic statistics and plots\nR reference card PDF by Tom Short\nRstudio cheatsheets",
    "crumbs": [
      "Introduction",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>R mechanics</span>"
    ]
  },
  {
    "objectID": "r_basics.html",
    "href": "r_basics.html",
    "title": "\n3  Up and Running with R\n",
    "section": "",
    "text": "3.1 The R User Interface\nThe RStudio interface is simple. You type R code into the bottom line of the RStudio console pane and then click Enter to run it. The code you type is called a command, because it will command your computer to do something for you. The line you type it into is called the command line.\nWhen you type a command at the prompt and hit Enter, your computer executes the command and shows you the results. Then RStudio displays a fresh prompt for your next command. For example, if you type 1 + 1 and hit Enter, RStudio will display:\nYou’ll notice that a [1] appears next to your result. R is just letting you know that this line begins with the first value in your result. Some commands return more than one value, and their results may fill up multiple lines. For example, the command 100:130 returns 31 values; it creates a sequence of integers from 100 to 130. Notice that new bracketed numbers appear at the start of the second and third lines of output. These numbers just mean that the second line begins with the 14th value in the result, and the third line begins with the 25th value. You can mostly ignore the numbers that appear in brackets:\nIf you type an incomplete command and press Enter, R will display a + prompt, which means R is waiting for you to type the rest of your command. Either finish the command or hit Escape to start over:\nIf you type a command that R doesn’t recognize, R will return an error message. If you ever see an error message, don’t panic. R is just telling you that your computer couldn’t understand or do what you asked it to do. You can then try a different command at the next prompt:\nOnce you get the hang of the command line, you can easily do anything in R that you would do with a calculator. For example, you could do some basic arithmetic:\n2 * 3   \n\n[1] 6\n\n4 - 1   \n\n[1] 3\n\n# this obeys order-of-operations\n6 / (4 - 1)   \n\n[1] 2",
    "crumbs": [
      "Introduction",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Up and Running with R</span>"
    ]
  },
  {
    "objectID": "r_basics.html#the-r-user-interface",
    "href": "r_basics.html#the-r-user-interface",
    "title": "\n3  Up and Running with R\n",
    "section": "",
    "text": "Figure 3.1: Your computer does your bidding when you type R commands at the prompt in the bottom line of the console pane. Don’t forget to hit the Enter key. When you first open RStudio, the console appears in the pane on your left, but you can change this with File &gt; Tools &gt; Global Options in the menu bar.\n\n\n\n&gt; 1 + 1\n[1] 2\n&gt;\n\n&gt; 100:130\n [1] 100 101 102 103 104 105 106 107 108 109 110 111 112\n[14] 113 114 115 116 117 118 119 120 121 122 123 124 125\n[25] 126 127 128 129 130\n\n\n\n\n\n\nTip\n\n\n\nThe colon operator (:) returns every integer between two integers. It is an easy way to create a sequence of numbers.\n\n\n\n\n\n\n\n\nWhen do we compile?\n\n\n\nIn some languages, like C, Java, and FORTRAN, you have to compile your human-readable code into machine-readable code (often 1s and 0s) before you can run it. If you’ve programmed in such a language before, you may wonder whether you have to compile your R code before you can use it. The answer is no. R is a dynamic programming language, which means R automatically interprets your code as you run it.\n\n\n\n&gt; 5 -\n+\n+ 1\n[1] 4\n\n&gt; 3 % 5\nError: unexpected input in \"3 % 5\"\n&gt;\n\n\n\n\n\n\nTip\n\n\n\nWhenever you get an error message in R, consider googling the error message. You’ll often find that someone else has had the same problem and has posted a solution online. Simply cutting-and-pasting the error message into a search engine will often work\n\n\n\n\n\n\n\n\n\n\nTip\n\n\n\nR treats the hashtag character, #, in a special way; R will not run anything that follows a hashtag on a line. This makes hashtags very useful for adding comments and annotations to your code. Humans will be able to read the comments, but your computer will pass over them. The hashtag is known as the commenting symbol in R.\n\n\n\n\n\n\n\n\nCancelling commands\n\n\n\nSome R commands may take a long time to run. You can cancel a command once it has begun by pressing ctrl + c or by clicking the “stop sign” if it is available in Rstudio. Note that it may also take R a long time to cancel the command.\n\n\n\n3.1.1 An exercise\nThat’s the basic interface for executing R code in RStudio. Think you have it? If so, try doing these simple tasks. If you execute everything correctly, you should end up with the same number that you started with:\n\nChoose any number and add 2 to it.\nMultiply the result by 3.\nSubtract 6 from the answer.\nDivide what you get by 3.\n\n\n10 + 2\n\n[1] 12\n\n12 * 3\n\n[1] 36\n\n36 - 6\n\n[1] 30\n\n30 / 3\n\n[1] 10",
    "crumbs": [
      "Introduction",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Up and Running with R</span>"
    ]
  },
  {
    "objectID": "r_basics.html#objects",
    "href": "r_basics.html#objects",
    "title": "\n3  Up and Running with R\n",
    "section": "\n3.2 Objects",
    "text": "3.2 Objects\nNow that you know how to use R, let’s use it to make a virtual die. The : operator from a couple of pages ago gives you a nice way to create a group of numbers from one to six. The : operator returns its results as a vector (we are going to work with vectors in more detail), a one-dimensional set of numbers:\n1:6\n## 1 2 3 4 5 6\nThat’s all there is to how a virtual die looks! But you are not done yet. Running 1:6 generated a vector of numbers for you to see, but it didn’t save that vector anywhere for later use. If we want to use those numbers again, we’ll have to ask your computer to save them somewhere. You can do that by creating an R object.\nR lets you save data by storing it inside an R object. What is an object? Just a name that you can use to call up stored data. For example, you can save data into an object like a or b. Wherever R encounters the object, it will replace it with the data saved inside, like so:\n\na &lt;- 1\na\n\n[1] 1\n\n\n\na + 2\n\n[1] 3\n\n\n\n\n\n\n\n\nWhat just happened?\n\n\n\n\nTo create an R object, choose a name and then use the less-than symbol, &lt;, followed by a minus sign, -, to save data into it. This combination looks like an arrow, &lt;-. R will make an object, give it your name, and store in it whatever follows the arrow. So a &lt;- 1 stores 1 in an object named a.\nWhen you ask R what’s in a, R tells you on the next line.\nYou can use your object in new R commands, too. Since a previously stored the value of 1, you’re now adding 1 to 2.\n\n\n\n\n\n\n\n\n\nAssignment vs expressions\n\n\n\nEverything that you type into the R console can be assigned to one of two categories:\n\nAssignments\nExpressions\n\nAn expression is a command that tells R to do something. For example, 1 + 2 is an expression that tells R to add 1 and 2. When you type an expression into the R console, R will evaluate the expression and return the result. For example, if you type 1 + 2 into the R console, R will return 3. Expressions can have “side effects” but they don’t explicitly result in anything being added to R memory.\n\n5 + 2\n\n[1] 7\n\n28 %% 3\n\n[1] 1\n\n3^2\n\n[1] 9\n\n5 + 4 * 4 + 4 ^ 4 / 10\n\n[1] 46.6\n\n\nWhile using R as a calculator is interesting, to do useful and interesting things, we need to assign values to objects. To create objects, we need to give it a name followed by the assignment operator &lt;- (or, entirely equivalently, =) and the value we want to give it:\n\nweight_kg &lt;- 55\n\n\n\nSo, for another example, the following code would create an object named die that contains the numbers one through six. To see what is stored in an object, just type the object’s name by itself:\n\ndie &lt;- 1:6\ndie\n\n[1] 1 2 3 4 5 6\n\n\nWhen you create an object, the object will appear in the environment pane of RStudio, as shown in Figure 3.2. This pane will show you all of the objects you’ve created since opening RStudio.\n\n\n\n\n\nFigure 3.2: Assignment creates an object in the environment pane.\n\n\n\n\n\nYou can name an object in R almost anything you want, but there are a few rules. First, a name cannot start with a number. Second, a name cannot use some special symbols, like ^, !, $, @, +, -, /, or *:\n\n\nGood names\nNames that cause errors\n\n\n\na\n1trial\n\n\nb\n$\n\n\nFOO\n^mean\n\n\nmy_var\n2nd\n\n\n.day\n!bad\n\n\n\n\n\n\n\n\n\nCapitalization matters\n\n\n\nR is case-sensitive, so name and Name will refer to different objects:\n&gt; Name = 0\n&gt; Name + 1\n[1] 1\n&gt; name + 1\nError: object 'name' not found\nThe error above is a common one!\n\n\nFinally, R will overwrite any previous information stored in an object without asking you for permission. So, it is a good idea to not use names that are already taken:\n\nmy_number &lt;- 1\nmy_number \n\n[1] 1\n\n\n\nmy_number &lt;- 999\nmy_number\n\n[1] 999\n\n\nYou can see which object names you have already used with the function ls:\nls()\nYour environment will contain different names than mine, because you have probably created different objects.\nYou can also see which names you have used by examining RStudio’s environment pane.\nWe now have a virtual die that is stored in the computer’s memory and which has a name that we can use to refer to it. You can access it whenever you like by typing the word die.\nSo what can you do with this die? Quite a lot. R will replace an object with its contents whenever the object’s name appears in a command. So, for example, you can do all sorts of math with the die. Math isn’t so helpful for rolling dice, but manipulating sets of numbers will be your stock and trade as a data scientist. So let’s take a look at how to do that:\n\ndie - 1\n\n[1] 0 1 2 3 4 5\n\ndie / 2\n\n[1] 0.5 1.0 1.5 2.0 2.5 3.0\n\ndie * die\n\n[1]  1  4  9 16 25 36\n\n\nR uses element-wise execution when working with a vector like die. When you manipulate a set of numbers, R will apply the same operation to each element in the set. So for example, when you run die - 1, R subtracts one from each element of die.\nWhen you use two or more vectors in an operation, R will line up the vectors and perform a sequence of individual operations. For example, when you run die * die, R lines up the two die vectors and then multiplies the first element of vector 1 by the first element of vector 2. R then multiplies the second element of vector 1 by the second element of vector 2, and so on, until every element has been multiplied. The result will be a new vector the same length as the first two {Figure 3.3}.\n\n\n\n\n\nFigure 3.3: “When R performs element-wise execution, it matches up vectors and then manipulates each pair of elements independently.”\n\n\nIf you give R two vectors of unequal lengths, R will repeat the shorter vector until it is as long as the longer vector, and then do the math, as shown in Figure 3.4. This isn’t a permanent change–the shorter vector will be its original size after R does the math. If the length of the short vector does not divide evenly into the length of the long vector, R will return a warning message. This behavior is known as vector recycling, and it helps R do element-wise operations:\n\n1:2\n\n[1] 1 2\n\n1:4\n\n[1] 1 2 3 4\n\ndie\n\n[1] 1 2 3 4 5 6\n\ndie + 1:2\n\n[1] 2 4 4 6 6 8\n\ndie + 1:4\n\nWarning in die + 1:4: longer object length is not a multiple of shorter object\nlength\n\n\n[1] 2 4 6 8 6 8\n\n\n\n\n\n\n\nFigure 3.4: “R will repeat a short vector to do element-wise operations with two vectors of uneven lengths.”\n\n\nElement-wise operations are a very useful feature in R because they manipulate groups of values in an orderly way. When you start working with data sets, element-wise operations will ensure that values from one observation or case are only paired with values from the same observation or case. Element-wise operations also make it easier to write your own programs and functions in R.\n\n\n\n\n\n\nElement-wise operations are not matrix operations\n\n\n\nIt is important to know that operations with vectors are not the same that you might expect if you are expecting R to perform “matrix” operations. R can do inner multiplication with the %*% operator and outer multiplication with the %o% operator:\n# Inner product (1*1 + 2*2 + 3*3 + 4*4 + 5*5 + 6*6)\ndie %*% die\n# Outer product\ndie %o% die\n\n\nNow that you can do math with your die object, let’s look at how you could “roll” it. Rolling your die will require something more sophisticated than basic arithmetic; you’ll need to randomly select one of the die’s values. And for that, you will need a function.",
    "crumbs": [
      "Introduction",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Up and Running with R</span>"
    ]
  },
  {
    "objectID": "r_basics.html#functions",
    "href": "r_basics.html#functions",
    "title": "\n3  Up and Running with R\n",
    "section": "\n3.3 Functions",
    "text": "3.3 Functions\nR has many functions and puts them all at our disposal. We can use functions to do simple and sophisticated tasks. For example, we can round a number with the round function, or calculate its factorial with the factorial function. Using a function is pretty simple. Just write the name of the function and then the data you want the function to operate on in parentheses:\n\nround(3.1415)\n\n[1] 3\n\nfactorial(3)\n\n[1] 6\n\n\nThe data that you pass into the function is called the function’s argument. The argument can be raw data, an R object, or even the results of another R function. In this last case, R will work from the innermost function to the outermost Figure 3.5.\n\nmean(1:6)\n\n[1] 3.5\n\nmean(die)\n\n[1] 3.5\n\nround(mean(die))\n\n[1] 4\n\n\n\n\n\n\n\nFigure 3.5: “When you link functions together, R will resolve them from the innermost operation to the outermost. Here R first looks up die, then calculates the mean of one through six, then rounds the mean.”\n\n\nReturning to our die, we can use the sample function to randomly select one of the die’s values; in other words, the sample function can simulate rolling the die.\nThe sample function takes two arguments: a vector named x and a number named size. sample will return size elements from the vector:\n\nsample(x = 1:4, size = 2)\n\n[1] 3 2\n\n\nTo roll your die and get a number back, set x to die and sample one element from it. You’ll get a new (maybe different) number each time you roll it:\n\nsample(x = die, size = 1)\n\n[1] 2\n\nsample(x = die, size = 1)\n\n[1] 6\n\nsample(x = die, size = 1)\n\n[1] 6\n\n\nMany R functions take multiple arguments that help them do their job. You can give a function as many arguments as you like as long as you separate each argument with a comma.\nYou may have noticed that I set die and 1 equal to the names of the arguments in sample, x and size. Every argument in every R function has a name. You can specify which data should be assigned to which argument by setting a name equal to data, as in the preceding code. This becomes important as you begin to pass multiple arguments to the same function; names help you avoid passing the wrong data to the wrong argument. However, using names is optional. You will notice that R users do not often use the name of the first argument in a function. So you might see the previous code written as:\n\nsample(die, size = 1)\n\n[1] 4\n\n\nOften, the name of the first argument is not very descriptive, and it is usually obvious what the first piece of data refers to anyways.\nBut how do you know which argument names to use? If you try to use a name that a function does not expect, you will likely get an error:\nround(3.1415, corners = 2)\n## Error in round(3.1415, corners = 2) : unused argument(s) (corners = 2)\nIf you’re not sure which names to use with a function, you can look up the function’s arguments with args. To do this, place the name of the function in the parentheses behind args. For example, you can see that the round function takes two arguments, one named x and one named digits:\n\nargs(round)\n\nfunction (x, digits = 0, ...) \nNULL\n\n\nDid you notice that args shows that the digits argument of round is already set to 0? Frequently, an R function will take optional arguments like digits. These arguments are considered optional because they come with a default value. You can pass a new value to an optional argument if you want, and R will use the default value if you do not. For example, round will round your number to 0 digits past the decimal point by default. To override the default, supply your own value for digits:\n\nround(3.1415)\n\n[1] 3\n\nround(3.1415, digits = 2)\n\n[1] 3.14\n\n# pi happens to be a built-in value in R\npi\n\n[1] 3.141593\n\nround(pi)\n\n[1] 3\n\n\nYou should write out the names of each argument after the first one or two when you call a function with multiple arguments. Why? First, this will help you and others understand your code. It is usually obvious which argument your first input refers to (and sometimes the second input as well). However, you’d need a large memory to remember the third and fourth arguments of every R function. Second, and more importantly, writing out argument names prevents errors.\nIf you do not write out the names of your arguments, R will match your values to the arguments in your function by order. For example, in the following code, the first value, die, will be matched to the first argument of sample, which is named x. The next value, 1, will be matched to the next argument, size:\n\nsample(die, 1)\n\n[1] 1\n\n\nAs you provide more arguments, it becomes more likely that your order and R’s order may not align. As a result, values may get passed to the wrong argument. Argument names prevent this. R will always match a value to its argument name, no matter where it appears in the order of arguments:\n\nsample(size = 1, x = die)\n\n[1] 3\n\n\n\n3.3.1 Sample with Replacement\nIf you set size = 2, you can almost simulate a pair of dice. Before we run that code, think for a minute why that might be the case. sample will return two numbers, one for each die:\n\nsample(die, size = 2)\n\n[1] 3 1\n\n\nI said this “almost” works because this method does something funny. If you use it many times, you’ll notice that the second die never has the same value as the first die, which means you’ll never roll something like a pair of threes or snake eyes. What is going on?\nBy default, sample builds a sample without replacement. To see what this means, imagine that sample places all of the values of die in a jar or urn. Then imagine that sample reaches into the jar and pulls out values one by one to build its sample. Once a value has been drawn from the jar, sample sets it aside. The value doesn’t go back into the jar, so it cannot be drawn again. So if sample selects a six on its first draw, it will not be able to select a six on the second draw; six is no longer in the jar to be selected. Although sample creates its sample electronically, it follows this seemingly physical behavior.\nOne side effect of this behavior is that each draw depends on the draws that come before it. In the real world, however, when you roll a pair of dice, each die is independent of the other. If the first die comes up six, it does not prevent the second die from coming up six. In fact, it doesn’t influence the second die in any way whatsoever. You can recreate this behavior in sample by adding the argument replace = TRUE:\n\nsample(die, size = 2, replace = TRUE)\n\n[1] 1 3\n\n\nThe argument replace = TRUE causes sample to sample with replacement. Our jar example provides a good way to understand the difference between sampling with replacement and without. When sample uses replacement, it draws a value from the jar and records the value. Then it puts the value back into the jar. In other words, sample replaces each value after each draw. As a result, sample may select the same value on the second draw. Each value has a chance of being selected each time. It is as if every draw were the first draw.\nSampling with replacement is an easy way to create independent random samples. Each value in your sample will be a sample of size one that is independent of the other values. This is the correct way to simulate a pair of dice:\n\nsample(die, size = 2, replace = TRUE)\n\n[1] 5 4\n\n\nCongratulate yourself; you’ve just run your first simulation in R! You now have a method for simulating the result of rolling a pair of dice. If you want to add up the dice, you can feed your result straight into the sum function:\n\ndice &lt;- sample(die, size = 2, replace = TRUE)\ndice\n\n[1] 4 4\n\nsum(dice)\n\n[1] 8\n\n\nWhat would happen if you call dice multiple times? Would R generate a new pair of dice values each time? Let’s give it a try:\n\ndice\n\n[1] 4 4\n\ndice\n\n[1] 4 4\n\ndice\n\n[1] 4 4\n\n\nThe name dice refers to a vector of two numbers. Calling more than once does not change the favlue. Each time you call dice, R will show you the result of that one time you called sample and saved the output to dice. R won’t rerun sample(die, 2, replace = TRUE) to create a new roll of the dice. Once you save a set of results to an R object, those results do not change.\nHowever, it would be convenient to have an object that can re-roll the dice whenever you call it. You can make such an object by writing your own R function.",
    "crumbs": [
      "Introduction",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Up and Running with R</span>"
    ]
  },
  {
    "objectID": "r_basics.html#write-functions",
    "href": "r_basics.html#write-functions",
    "title": "\n3  Up and Running with R\n",
    "section": "\n3.4 Writing Your Own Functions",
    "text": "3.4 Writing Your Own Functions\nTo recap, you already have working R code that simulates rolling a pair of dice:\n\ndie &lt;- 1:6\ndice &lt;- sample(die, size = 2, replace = TRUE)\nsum(dice)\n\n[1] 7\n\n\nYou can retype this code into the console anytime you want to re-roll your dice. However, this is an awkward way to work with the code. It would be easier to use your code if you wrapped it into its own function, which is exactly what we’ll do now. We’re going to write a function named roll that you can use to roll your virtual dice. When you’re finished, the function will work like this: each time you call roll(), R will return the sum of rolling two dice:\nroll()\n## 8 \n\nroll()\n## 3\n\nroll()\n## 7\nFunctions may seem mysterious or fancy, but they are just another type of R object. Instead of containing data, they contain code. This code is stored in a special format that makes it easy to reuse the code in new situations. You can write your own functions by recreating this format.\n\n3.4.1 The Function Constructor\nEvery function in R has three basic parts: a name, a body of code, and a set of arguments. To make your own function, you need to replicate these parts and store them in an R object, which you can do with the function function. To do this, call function() and follow it with a pair of braces, {}:\n\nmy_function &lt;- function() {}\n\nThis function, as written, doesn’t do anything (yet). However, it is a valid function. You can call it by typing its name followed by an open and closed parenthesis:\n\nmy_function()\n\nNULL\n\n\nfunction will build a function out of whatever R code you place between the braces. For example, you can turn your dice code into a function by calling:\n\nroll &lt;- function() {\n  die &lt;- 1:6\n  dice &lt;- sample(die, size = 2, replace = TRUE)\n  sum(dice)\n}\n\n\n\n\n\n\n\nIndentation and readability\n\n\n\nNotice each line of code between the braces is indented. This makes the code easier to read but has no impact on how the code runs. R ignores spaces and line breaks and executes one complete expression at a time. Note that in other languages like python, spacing is extremely important and part of the language.\n\n\nJust hit the Enter key between each line after the first brace, {. R will wait for you to type the last brace, }, before it responds.\nDon’t forget to save the output of function to an R object. This object will become your new function. To use it, write the object’s name followed by an open and closed parenthesis:\n\nroll()\n\n[1] 9\n\n\nYou can think of the parentheses as the “trigger” that causes R to run the function. If you type in a function’s name without the parentheses, R will show you the code that is stored inside the function. If you type in the name with the parentheses, R will run that code:\n\nroll\n\nfunction() {\n  die &lt;- 1:6\n  dice &lt;- sample(die, size = 2, replace = TRUE)\n  sum(dice)\n}\n\nroll()\n\n[1] 2\n\n\nThe code that you place inside your function is known as the body of the function. When you run a function in R, R will execute all of the code in the body and then return the result of the last line of code. If the last line of code doesn’t return a value, neither will your function, so you want to ensure that your final line of code returns a value. One way to check this is to think about what would happen if you ran the body of code line by line in the command line. Would R display a result after the last line, or would it not?\nHere’s some code that would display a result:\ndice\n1 + 1\nsqrt(2)\nAnd here’s some code that would not:\ndice &lt;- sample(die, size = 2, replace = TRUE)\ntwo &lt;- 1 + 1\na &lt;- sqrt(2)\nAgain, this is just showing the distinction between expressions and assignments.",
    "crumbs": [
      "Introduction",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Up and Running with R</span>"
    ]
  },
  {
    "objectID": "r_basics.html#arguments",
    "href": "r_basics.html#arguments",
    "title": "\n3  Up and Running with R\n",
    "section": "\n3.5 Arguments",
    "text": "3.5 Arguments\nWhat if we removed one line of code from our function and changed the name die to bones (just a name–don’t think of it as important), like this?\n\nroll2 &lt;- function() {\n  dice &lt;- sample(bones, size = 2, replace = TRUE)\n  sum(dice)\n}\n\nNow I’ll get an error when I run the function. The function needs the object bones to do its job, but there is no object named bones to be found (you can check by typing ls() which will show you the names in the environment, or memory).\nroll2()\n## Error in sample(bones, size = 2, replace = TRUE) : \n##   object 'bones' not found\nYou can supply bones when you call roll2 if you make bones an argument of the function. To do this, put the name bones in the parentheses that follow function when you define roll2:\n\nroll2 &lt;- function(bones) {\n  dice &lt;- sample(bones, size = 2, replace = TRUE)\n  sum(dice)\n}\n\nNow roll2 will work as long as you supply bones when you call the function. You can take advantage of this to roll different types of dice each time you call roll2.\nRemember, we’re rolling pairs of dice:\n\nroll2(bones = 1:4)\n\n[1] 7\n\nroll2(bones = 1:6)\n\n[1] 2\n\nroll2(1:20)\n\n[1] 37\n\n\nNotice that roll2 will still give an error if you do not supply a value for the bones argument when you call roll2:\nroll2()\n## Error in sample(bones, size = 2, replace = TRUE) : \n##   argument \"bones\" is missing, with no default\nYou can prevent this error by giving the bones argument a default value. To do this, set bones equal to a value when you define roll2:\n\nroll2 &lt;- function(bones = 1:6) {\n  dice &lt;- sample(bones, size = 2, replace = TRUE)\n  sum(dice)\n}\n\nNow you can supply a new value for bones if you like, and roll2 will use the default if you do not:\n\nroll2()\n\n[1] 4\n\n\nYou can give your functions as many arguments as you like. Just list their names, separated by commas, in the parentheses that follow function. When the function is run, R will replace each argument name in the function body with the value that the user supplies for the argument. If the user does not supply a value, R will replace the argument name with the argument’s default value (if you defined one).\nTo summarize, function helps you construct your own R functions. You create a body of code for your function to run by writing code between the braces that follow function. You create arguments for your function to use by supplying their names in the parentheses that follow function. Finally, you give your function a name by saving its output to an R object, as shown in Figure 3.6.\nOnce you’ve created your function, R will treat it like every other function in R. Think about how useful this is. Have you ever tried to create a new Excel option and add it to Microsoft’s menu bar? Or a new slide animation and add it to Powerpoint’s options? When you work with a programming language, you can do these types of things. As you learn to program in R, you will be able to create new, customized, reproducible tools for yourself whenever you like.\n\n\n\n\n\nFigure 3.6: “Every function in R has the same parts, and you can use function to create these parts. Assign the result to a name, so you can call the function later.”",
    "crumbs": [
      "Introduction",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Up and Running with R</span>"
    ]
  },
  {
    "objectID": "r_basics.html#scripts",
    "href": "r_basics.html#scripts",
    "title": "\n3  Up and Running with R\n",
    "section": "\n3.6 Scripts",
    "text": "3.6 Scripts\nScripts are code that are saved for later reuse or editing. An R script is just a plain text file that you save R code in. You can open an R script in RStudio by going to File &gt; New File &gt; R script in the menu bar. RStudio will then open a fresh script above your console pane, as shown in Figure 3.7.\nI strongly encourage you to write and edit all of your R code in a script before you run it in the console. Why? This habit creates a reproducible record of your work. When you’re finished for the day, you can save your script and then use it to rerun your entire analysis the next day. Scripts are also very handy for editing and proofreading your code, and they make a nice copy of your work to share with others. To save a script, click the scripts pane, and then go to File &gt; Save As in the menu bar.\n\n\n\n\n\nFigure 3.7: “When you open an R Script (File &gt; New File &gt; R Script in the menu bar), RStudio creates a fourth pane (or puts a new tab in the existing pane) above the console where you can write and edit your code.”\n\n\nRStudio comes with many built-in features that make it easy to work with scripts. First, you can automatically execute a line of code in a script by clicking the Run button at the top of the editor panel.\nR will run whichever line of code your cursor is on. If you have a whole section highlighted, R will run the highlighted code. Alternatively, you can run the entire script by clicking the Source button. Don’t like clicking buttons? You can use Control + Return as a shortcut for the Run button. On Macs, that would be Command + Return.\n\n\n\nIf you’re not convinced about scripts, you soon will be. It becomes a pain to write multi-line code in the console’s single-line command line. Let’s avoid that headache and open your first script now before we move to the next chapter.\n\n\n\n\n\n\nTip\n\n\n\nExtract function\nRStudio comes with a tool that can help you build functions. To use it, highlight the lines of code in your R script that you want to turn into a function. Then click Code &gt; Extract Function in the menu bar. RStudio will ask you for a function name to use and then wrap your code in a function call. It will scan the code for undefined variables and use these as arguments.\nYou may want to double-check RStudio’s work. It assumes that your code is correct, so if it does something surprising, you may have a problem in your code.",
    "crumbs": [
      "Introduction",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Up and Running with R</span>"
    ]
  },
  {
    "objectID": "r_basics.html#summary",
    "href": "r_basics.html#summary",
    "title": "\n3  Up and Running with R\n",
    "section": "\n3.7 Summary",
    "text": "3.7 Summary\nWe’ve covered a lot of ground already. You now have a virtual die stored in your computer’s memory, as well as your own R function that rolls a pair of dice. You’ve also begun speaking the R language.\nThe two most important components of the R language are objects, which store data, and functions, which manipulate data. R also uses a host of operators like +, -, *, /, and &lt;- to do basic tasks. As a data scientist, you will use R objects to store data in your computer’s memory, and you will use functions to automate tasks and do complicated calculations.",
    "crumbs": [
      "Introduction",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Up and Running with R</span>"
    ]
  },
  {
    "objectID": "packages_and_dice.html",
    "href": "packages_and_dice.html",
    "title": "\n4  Packages and more dice\n",
    "section": "",
    "text": "4.1 Packages\nR is a powerful language for data science and programming, allowing beginners and experts alike to manipulate, analyze, and visualize data effectively. One of the most appealing features of R is its extensive library of packages, which are essential tools for expanding its capabilities and streamlining the coding process.\nAn R package is a collection of reusable functions, datasets, and compiled code created by other users and developers to extend the functionality of the base R language. These packages cover a wide range of applications, such as data manipulation, statistical analysis, machine learning, and data visualization. By utilizing existing R packages, you can leverage the expertise of others and save time by avoiding the need to create custom functions from scratch.\nUsing others’ R packages is incredibly beneficial as it allows you to take advantage of the collective knowledge of the R community. Developers often create packages to address specific challenges, optimize performance, or implement popular algorithms or methodologies. By incorporating these packages into your projects, you can enhance your productivity, reduce development time, and ensure that you are using well-tested and reliable code.",
    "crumbs": [
      "Introduction",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Packages and more dice</span>"
    ]
  },
  {
    "objectID": "packages_and_dice.html#packages",
    "href": "packages_and_dice.html#packages",
    "title": "\n4  Packages and more dice\n",
    "section": "",
    "text": "4.1.1 install.packages\nTo install an R package, you can use the install.packages() function in the R console or script. For example, to install the popular data manipulation package “dplyr,” simply type install.packages(“dplyr”). This command will download the package from the Comprehensive R Archive Network (CRAN) and install it on your local machine. Keep in mind that you only need to install a package once, unless you want to update it to a newer version.\nIn our case, we want to install the ggplot2 package.\n\ninstall.packages('ggplot2')\n\n\n4.1.2 library\nAfter installing an R package, you will need to load it into your R session before using its functions. To load a package, use the library() function followed by the package name, such as library(dplyr). Loading a package makes its functions and datasets available for use in your current R session. Note that you need to load a package every time you start a new R session.\n\nlibrary(ggplot2)\n\nNow, the functionality of the ggplot2 package is available in our R session.\n\n\n\n\n\n\nInstalling vs loading packages\n\n\n\nThe main thing to remember is that you only need to install a package once, but you need to load it with library each time you wish to use it in a new R session. R will unload all of its packages each time you close RStudio.\n\n\n\n4.1.3 Finding R packages\nFinding useful R packages can be done in several ways. First, browsing CRAN (https://cran.r-project.org/) and Bioconductor (more later, https://bioconductor.org) are an excellent starting points, as they host thousands of packages categorized by topic. Additionally, online forums like Stack Overflow and R-bloggers can provide valuable recommendations based on user experiences. Social media platforms such as Twitter, where developers and data scientists often share new packages and updates, can also be a helpful resource. Finally, don’t forget to ask your colleagues or fellow R users for their favorite packages, as they may have insights on which ones best suit your specific needs.",
    "crumbs": [
      "Introduction",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Packages and more dice</span>"
    ]
  },
  {
    "objectID": "packages_and_dice.html#are-our-dice-fair",
    "href": "packages_and_dice.html#are-our-dice-fair",
    "title": "\n4  Packages and more dice\n",
    "section": "\n4.2 Are our dice fair?",
    "text": "4.2 Are our dice fair?\nWell, let’s review our code.\n\nroll2 &lt;- function(bones = 1:6) {\n  dice = sample(bones, size = 2, replace = TRUE)\n  sum(dice)\n}\n\nIf our dice are fair, then each number should show up equally. What does the sum look like with our two dice?\n\n\n\n\n\nFigure 4.1: In an ideal world, a histogram of the results would look like this\n\n\nRead the help page for replicate (i.e., help(\"replicate\")). In short, it suggests that we can repeat our dice rolling as many times as we like and replicate will return a vector of the sums for each roll.\n\nrolls = replicate(n = 100, roll2())\n\nWhat does rolls look like?\n\nhead(rolls)\n\n[1] 8 3 5 7 4 6\n\nlength(rolls)\n\n[1] 100\n\nmean(rolls)\n\n[1] 6.92\n\nsummary(rolls)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n   2.00    6.00    7.00    6.92    9.00   12.00 \n\n\nThis looks like it roughly agrees with our sketched out ideal histogram in Figure 4.1. However, now that we’ve loaded the qplot function from the ggplot2 package, we can make a histogram of the data themselves.\n\nqplot(rolls, binwidth=1)\n\nWarning: `qplot()` was deprecated in ggplot2 3.4.0.\n\n\n\n\n\n\n\nFigure 4.2: Histogram of the sums from 100 rolls of our fair dice\n\n\n\n\nHow does your histogram look (and yours will be different from mine since we are sampling random values)? Is it what you expect?\nWhat happens to our histogram as we increase the number of replicates?\n\nrolls = replicate(n = 100000, roll2())\nqplot(rolls, binwidth=1)\n\n\n\n\n\n\nFigure 4.3: Histogram with 100000 rolls much more closely approximates the pyramidal shape we anticipated",
    "crumbs": [
      "Introduction",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Packages and more dice</span>"
    ]
  },
  {
    "objectID": "packages_and_dice.html#bonus-exercise",
    "href": "packages_and_dice.html#bonus-exercise",
    "title": "\n4  Packages and more dice\n",
    "section": "\n4.3 Bonus exercise",
    "text": "4.3 Bonus exercise\nHow would you change the roll2 function to weight the dice?",
    "crumbs": [
      "Introduction",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Packages and more dice</span>"
    ]
  },
  {
    "objectID": "reading_and_writing.html",
    "href": "reading_and_writing.html",
    "title": "\n5  Reading and writing data files\n",
    "section": "",
    "text": "5.1 Introduction\nIn this chapter, we will discuss how to read and write data files in R. Data files are essential for storing and sharing data across different platforms and applications. R provides a variety of functions and packages to read and write data files in different formats, such as text files, CSV files, Excel files. By mastering these functions, you can efficiently import and export data in R, enabling you to perform data analysis and visualization tasks effectively.",
    "crumbs": [
      "Introduction",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Reading and writing data files</span>"
    ]
  },
  {
    "objectID": "reading_and_writing.html#csv-files",
    "href": "reading_and_writing.html#csv-files",
    "title": "\n5  Reading and writing data files\n",
    "section": "\n5.2 CSV files",
    "text": "5.2 CSV files\nComma-Separated Values (CSV) files are a common file format for storing tabular data. They consist of rows and columns, with each row representing a record and each column representing a variable or attribute. CSV files are widely used for data storage and exchange due to their simplicity and compatibility with various software applications. In R, you can read and write CSV files using the read.csv() and write.csv() functions, respectively. A commonly used alternative is to use the readr package, which provides faster and more user-friendly functions for reading and writing CSV files.\n\n5.2.1 Writing a CSV file\nSince we are going to use the readr package, we need to install it first. You can install the readr package using the following command:\n\ninstall.packages(\"readr\")\n\nOnce the package is installed, you can load it into your R session using the library() function:\n\nlibrary(readr)\n\nSince we don’t have a CSV file sitting around, let’s create a simple data frame to write to a CSV file. Here’s an example data frame:\n\ndf &lt;- data.frame(\n  id = c(1, 2, 3, 4, 5),\n  name = c(\"Alice\", \"Bob\", \"Charlie\", \"David\", \"Eve\"),\n  age = c(25, 30, 35, 40, 45)\n)\n\nNow, you can write this data frame to a CSV file using the write_csv() function from the readr package. Here’s how you can do it:\n\nwrite_csv(df, \"data.csv\")\n\nYou can check the current working directory to see if the CSV file was created successfully. If you want to specify a different directory or file path, you can provide the full path in the write_csv() function.\n\n# see what the current working directory is\ngetwd()\n\n[1] \"/Users/seandavis/Documents/git/RBiocBook\"\n\n# and check to see that the file was created\ndir(pattern = \"data.csv\")\n\n[1] \"data.csv\"\n\n\n\n5.2.2 Reading a CSV file\nNow that we have a CSV file, let’s read it back into R using the read_csv() function from the readr package. Here’s how you can do it:\n\ndf2 &lt;- read_csv(\"data.csv\")\n\nRows: 5 Columns: 3\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (1): name\ndbl (2): id, age\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nYou can check the structure of the data frame df2 to verify that the data was read correctly:\n\ndf2\n\n# A tibble: 5 × 3\n     id name      age\n  &lt;dbl&gt; &lt;chr&gt;   &lt;dbl&gt;\n1     1 Alice      25\n2     2 Bob        30\n3     3 Charlie    35\n4     4 David      40\n5     5 Eve        45\n\n\nThe readr package can read CSV files with various delimiters, headers, and data types, making it a versatile tool for handling tabular data in R. It can also read CSV files directly from web locations like so:\n\ndf3 &lt;- read_csv(\"https://data.cdc.gov/resource/pwn4-m3yp.csv\")\n\nRows: 1000 Columns: 10\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr  (1): state\ndbl  (6): tot_cases, new_cases, tot_deaths, new_deaths, new_historic_cases, ...\ndttm (3): date_updated, start_date, end_date\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nThe dataset that you just downloaded is described here: Covid-19 data from CDC",
    "crumbs": [
      "Introduction",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Reading and writing data files</span>"
    ]
  },
  {
    "objectID": "reading_and_writing.html#excel-files",
    "href": "reading_and_writing.html#excel-files",
    "title": "\n5  Reading and writing data files\n",
    "section": "\n5.3 Excel files",
    "text": "5.3 Excel files\nMicrosoft Excel files are another common file format for storing tabular data. Excel files can contain multiple sheets, formulas, and formatting options, making them a popular choice for data storage and analysis. In R, you can read and write Excel files using the readxl package. This package provides functions to import and export data from Excel files, enabling you to work with Excel data in R.\n\n5.3.1 Reading an Excel file\nTo read an Excel file in R, you need to install and load the readxl package. You can install the readxl package using the following command:\n\ninstall.packages(\"readxl\")\n\nOnce the package is installed, you can load it into your R session using the library() function:\n\nlibrary(readxl)\n\nNow, you can read an Excel file using the read_excel() function from the readxl package. We don’t have an excel file available, so let’s download one from the internet. Here’s an example:\n\ndownload.file('https://www.w3resource.com/python-exercises/pandas/excel/SaleData.xlsx', 'SaleData.xlsx')\n\nNow, you can read the Excel file into R using the read_excel() function:\n\ndf_excel &lt;- read_excel(\"SaleData.xlsx\")\n\nYou can check the structure of the data frame df_excel to verify that the data was read correctly:\n\ndf_excel\n\n# A tibble: 45 × 8\n   OrderDate           Region  Manager SalesMan  Item  Units Unit_price Sale_amt\n   &lt;dttm&gt;              &lt;chr&gt;   &lt;chr&gt;   &lt;chr&gt;     &lt;chr&gt; &lt;dbl&gt;      &lt;dbl&gt;    &lt;dbl&gt;\n 1 2018-01-06 00:00:00 East    Martha  Alexander Tele…    95       1198   113810\n 2 2018-01-23 00:00:00 Central Hermann Shelli    Home…    50        500    25000\n 3 2018-02-09 00:00:00 Central Hermann Luis      Tele…    36       1198    43128\n 4 2018-02-26 00:00:00 Central Timothy David     Cell…    27        225     6075\n 5 2018-03-15 00:00:00 West    Timothy Stephen   Tele…    56       1198    67088\n 6 2018-04-01 00:00:00 East    Martha  Alexander Home…    60        500    30000\n 7 2018-04-18 00:00:00 Central Martha  Steven    Tele…    75       1198    89850\n 8 2018-05-05 00:00:00 Central Hermann Luis      Tele…    90       1198   107820\n 9 2018-05-22 00:00:00 West    Douglas Michael   Tele…    32       1198    38336\n10 2018-06-08 00:00:00 East    Martha  Alexander Home…    60        500    30000\n# ℹ 35 more rows\n\n\nThe readxl package provides various options to read Excel files with multiple sheets, specific ranges, and data types, making it a versatile tool for handling Excel data in R.\n\n5.3.2 Writing an Excel file\nTo write an Excel file in R, you can use the write_xlsx() function from the writexl package. You can install the writexl package using the following command:\n\ninstall.packages(\"writexl\")\n\nOnce the package is installed, you can load it into your R session using the library() function:\n\nlibrary(writexl)\n\nThe write_xlsx() function allows you to write a data frame to an Excel file. Here’s an example:\n\nwrite_xlsx(df, \"data.xlsx\")\n\nYou can check the current working directory to see if the Excel file was created successfully. If you want to specify a different directory or file path, you can provide the full path in the write_xlsx() function.\n\n# see what the current working directory is\ngetwd()\n\n[1] \"/Users/seandavis/Documents/git/RBiocBook\"\n\n# and check to see that the file was created\ndir(pattern = \"data.xlsx\")\n\n[1] \"data.xlsx\"",
    "crumbs": [
      "Introduction",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Reading and writing data files</span>"
    ]
  },
  {
    "objectID": "reading_and_writing.html#additional-options",
    "href": "reading_and_writing.html#additional-options",
    "title": "\n5  Reading and writing data files\n",
    "section": "\n5.4 Additional options",
    "text": "5.4 Additional options\n\nGoogle Sheets: You can read and write data from Google Sheets using the googlesheets4 package. This package provides functions to interact with Google Sheets, enabling you to import and export data from Google Sheets to R.\nJSON files: You can read and write JSON files using the jsonlite package. This package provides functions to convert R objects to JSON format and vice versa, enabling you to work with JSON data in R.\nDatabase files: You can read and write data from database files using the DBI and RSQLite packages. These packages provide functions to interact with various database systems, enabling you to import and export data from databases to R.",
    "crumbs": [
      "Introduction",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Reading and writing data files</span>"
    ]
  },
  {
    "objectID": "dataviz.html",
    "href": "dataviz.html",
    "title": "6  Data Visualization with ggplot2",
    "section": "",
    "text": "Start with this worked example to get a feel for the ggplot2 package.\n\nhttps://rkabacoff.github.io/datavis/IntroGGPLOT.html\n\nThen, for more detail, I refer you to this excellent ggplot2 tutorial.\nFinally, for more R graphics inspiration, see the R Graph Gallery.",
    "crumbs": [
      "Introduction",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Data Visualization with ggplot2</span>"
    ]
  },
  {
    "objectID": "data_structures_overview.html",
    "href": "data_structures_overview.html",
    "title": "R Data Structures",
    "section": "",
    "text": "Chapter overview\nAs you progress through these chapters, practice the examples and exercises provided, engage in discussion, and collaborate with your peers to deepen your understanding of R data structures. This solid foundation will serve as the basis for more advanced data manipulation, analysis, and visualization techniques in R.",
    "crumbs": [
      "R Data Structures"
    ]
  },
  {
    "objectID": "data_structures_overview.html#chapter-overview",
    "href": "data_structures_overview.html#chapter-overview",
    "title": "R Data Structures",
    "section": "",
    "text": "Vectors : In this chapter, we will introduce you to the simplest data structure in R, the vector. We will cover how to create, access, and manipulate vectors, as well as discuss their unique properties and limitations.\n\nMatrices\n\nNext, we will explore matrices, which are two-dimensional data structures that extend vectors. You will learn how to create, access, and manipulate matrices, and understand their usefulness in mathematical operations and data organization.\n\n\n\nLists\n\nThe third chapter will focus on lists, a versatile data structure that can store elements of different types and sizes. We will discuss how to create, access, and modify lists, and demonstrate their flexibility in handling complex data structures.\n\n\n\nData.frames\n\nFinally, we will examine data.frames, a widely-used data structure for organizing and manipulating tabular data. You will learn how to create, access, and manipulate data.frames, and understand their advantages over other data structures for data analysis tasks.\n\n\n\nArrays\n\nWhile we will not focus directly on the array data type, which are multidimensional data structures that extend matrices, they are very similar to matrices, but with a third dimension.",
    "crumbs": [
      "R Data Structures"
    ]
  },
  {
    "objectID": "vectors.html",
    "href": "vectors.html",
    "title": "\n7  Vectors\n",
    "section": "",
    "text": "7.1 What is a Vector?\nA vector is the simplest and most basic data structure in R. It is a one-dimensional, ordered collection of elements, where all the elements are of the same data type. Vectors can store various types of data, such as numeric, character, or logical values. Figure 7.1 shows a pictorial representation of three vector examples.\nIn this chapter, we will provide a comprehensive overview of vectors, including how to create, access, and manipulate them. We will also discuss some unique properties and rules associated with vectors, and explore their applications in data analysis tasks.\nIn R, even a single value is a vector with length=1.\nz = 1\nz\n\n[1] 1\n\nlength(z)\n\n[1] 1\nIn the code above, we “assigned” the value 1 to the variable named z. Typing z by itself is an “expression” that returns a result which is, in this case, the value that we just assigned. The length method takes an R object and returns the R length. There are numerous ways of asking R about what an object represents, and length is one of them.\nVectors can contain numbers, strings (character data), or logical values (TRUE and FALSE) or other “atomic” data types Table 7.1. Vectors cannot contain a mix of types! We will introduce another data structure, the R list for situations when we need to store a mix of base R data types.",
    "crumbs": [
      "R Data Structures",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Vectors</span>"
    ]
  },
  {
    "objectID": "vectors.html#what-is-a-vector",
    "href": "vectors.html#what-is-a-vector",
    "title": "\n7  Vectors\n",
    "section": "",
    "text": "Figure 7.1: “Pictorial representation of three vector examples. The first vector is a numeric vector. The second is a ‘logical’ vector. The third is a character vector. Vectors also have indices and, optionally, names.”\n\n\n\n\n\n\n\n\n\n\n\nData type\nStores\n\n\n\nnumeric\nfloating point numbers\n\n\ninteger\nintegers\n\n\ncomplex\ncomplex numbers\n\n\nfactor\ncategorical data\n\n\ncharacter\nstrings\n\n\nlogical\nTRUE or FALSE\n\n\nNA\nmissing\n\n\nNULL\nempty\n\n\nfunction\nfunction type\n\n\n\n\n\nTable 7.1: Atomic (simplest) data types in R.",
    "crumbs": [
      "R Data Structures",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Vectors</span>"
    ]
  },
  {
    "objectID": "vectors.html#creating-vectors",
    "href": "vectors.html#creating-vectors",
    "title": "\n7  Vectors\n",
    "section": "\n7.2 Creating vectors",
    "text": "7.2 Creating vectors\nCharacter vectors (also sometimes called “string” vectors) are entered with each value surrounded by single or double quotes; either is acceptable, but they must match. They are always displayed by R with double quotes. Here are some examples of creating vectors:\n\n# examples of vectors\nc('hello','world')\n\n[1] \"hello\" \"world\"\n\nc(1,3,4,5,1,2)\n\n[1] 1 3 4 5 1 2\n\nc(1.12341e7,78234.126)\n\n[1] 11234100.00    78234.13\n\nc(TRUE,FALSE,TRUE,TRUE)\n\n[1]  TRUE FALSE  TRUE  TRUE\n\n# note how in the next case the TRUE is converted to \"TRUE\"\n# with quotes around it.\nc(TRUE,'hello')\n\n[1] \"TRUE\"  \"hello\"\n\n\nWe can also create vectors as “regular sequences” of numbers. For example:\n\n# create a vector of integers from 1 to 10\nx = 1:10\n# and backwards\nx = 10:1\n\nThe seq function can create more flexible regular sequences.\n\n# create a vector of numbers from 1 to 4 skipping by 0.3\ny = seq(1,4,0.3)\n\nAnd creating a new vector by concatenating existing vectors is possible, as well.\n\n# create a sequence by concatenating two other sequences\nz = c(y,x)\nz\n\n [1]  1.0  1.3  1.6  1.9  2.2  2.5  2.8  3.1  3.4  3.7  4.0 10.0  9.0  8.0  7.0\n[16]  6.0  5.0  4.0  3.0  2.0  1.0",
    "crumbs": [
      "R Data Structures",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Vectors</span>"
    ]
  },
  {
    "objectID": "vectors.html#vector-operations",
    "href": "vectors.html#vector-operations",
    "title": "\n7  Vectors\n",
    "section": "\n7.3 Vector Operations",
    "text": "7.3 Vector Operations\nOperations on a single vector are typically done element-by-element. For example, we can add 2 to a vector, 2 is added to each element of the vector and a new vector of the same length is returned.\n\nx = 1:10\nx + 2\n\n [1]  3  4  5  6  7  8  9 10 11 12\n\n\nIf the operation involves two vectors, the following rules apply. If the vectors are the same length: R simply applies the operation to each pair of elements.\n\nx + x\n\n [1]  2  4  6  8 10 12 14 16 18 20\n\n\nIf the vectors are different lengths, but one length a multiple of the other, R reuses the shorter vector as needed.\n\nx = 1:10\ny = c(1,2)\nx * y\n\n [1]  1  4  3  8  5 12  7 16  9 20\n\n\nIf the vectors are different lengths, but one length not a multiple of the other, R reuses the shorter vector as needed and delivers a warning.\n\nx = 1:10\ny = c(2,3,4)\nx * y\n\nWarning in x * y: longer object length is not a multiple of shorter object\nlength\n\n\n [1]  2  6 12  8 15 24 14 24 36 20\n\n\nTypical operations include multiplication (“*”), addition, subtraction, division, exponentiation (“^”), but many operations in R operate on vectors and are then called “vectorized”.\nBe aware of the recycling rule when working with vectors of different lengths, as it may lead to unexpected results if you’re not careful.",
    "crumbs": [
      "R Data Structures",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Vectors</span>"
    ]
  },
  {
    "objectID": "vectors.html#logical-vectors",
    "href": "vectors.html#logical-vectors",
    "title": "\n7  Vectors\n",
    "section": "\n7.4 Logical Vectors",
    "text": "7.4 Logical Vectors\nLogical vectors are vectors composed on only the values TRUE and FALSE. Note the all-upper-case and no quotation marks.\n\na = c(TRUE,FALSE,TRUE)\n\n# we can also create a logical vector from a numeric vector\n# 0 = false, everything else is 1\nb = c(1,0,217)\nd = as.logical(b)\nd\n\n[1]  TRUE FALSE  TRUE\n\n# test if a and d are the same at every element\nall.equal(a,d)\n\n[1] TRUE\n\n# We can also convert from logical to numeric\nas.numeric(a)\n\n[1] 1 0 1\n\n\n\n7.4.1 Logical Operators\nSome operators like &lt;, &gt;, ==, &gt;=, &lt;=, != can be used to create logical vectors.\n\n# create a numeric vector\nx = 1:10\n# testing whether x &gt; 5 creates a logical vector\nx &gt; 5\n\n [1] FALSE FALSE FALSE FALSE FALSE  TRUE  TRUE  TRUE  TRUE  TRUE\n\nx &lt;= 5\n\n [1]  TRUE  TRUE  TRUE  TRUE  TRUE FALSE FALSE FALSE FALSE FALSE\n\nx != 5\n\n [1]  TRUE  TRUE  TRUE  TRUE FALSE  TRUE  TRUE  TRUE  TRUE  TRUE\n\nx == 5\n\n [1] FALSE FALSE FALSE FALSE  TRUE FALSE FALSE FALSE FALSE FALSE\n\n\nWe can also assign the results to a variable:\n\ny = (x == 5)\ny\n\n [1] FALSE FALSE FALSE FALSE  TRUE FALSE FALSE FALSE FALSE FALSE",
    "crumbs": [
      "R Data Structures",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Vectors</span>"
    ]
  },
  {
    "objectID": "vectors.html#indexing-vectors",
    "href": "vectors.html#indexing-vectors",
    "title": "\n7  Vectors\n",
    "section": "\n7.5 Indexing Vectors",
    "text": "7.5 Indexing Vectors\nIn R, an index is used to refer to a specific element or set of elements in an vector (or other data structure). [R uses [ and ] to perform indexing, although other approaches to getting subsets of larger data structures are common in R.\n\nx = seq(0,1,0.1)\n# create a new vector from the 4th element of x\nx[4]\n\n[1] 0.3\n\n\nWe can even use other vectors to perform the “indexing”.\n\nx[c(3,5,6)]\n\n[1] 0.2 0.4 0.5\n\ny = 3:6\nx[y]\n\n[1] 0.2 0.3 0.4 0.5\n\n\nCombining the concept of indexing with the concept of logical vectors results in a very power combination.\n\n# use help('rnorm') to figure out what is happening next\nmyvec = rnorm(10)\n\n# create logical vector that is TRUE where myvec is &gt;0.25\ngt1 = (myvec &gt; 0.25)\nsum(gt1)\n\n[1] 5\n\n# and use our logical vector to create a vector of myvec values that are &gt;0.25\nmyvec[gt1]\n\n[1] 0.9920401 0.4123509 0.4837873 0.9089888 0.2869754\n\n# or &lt;=0.25 using the logical \"not\" operator, \"!\"\nmyvec[!gt1]\n\n[1]  0.2332562 -0.7828833 -1.2242465 -1.2165037 -0.2254581\n\n# shorter, one line approach\nmyvec[myvec &gt; 0.25]\n\n[1] 0.9920401 0.4123509 0.4837873 0.9089888 0.2869754",
    "crumbs": [
      "R Data Structures",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Vectors</span>"
    ]
  },
  {
    "objectID": "vectors.html#named-vectors",
    "href": "vectors.html#named-vectors",
    "title": "\n7  Vectors\n",
    "section": "\n7.6 Named Vectors",
    "text": "7.6 Named Vectors\nNamed vectors are vectors with labels or names assigned to their elements. These names can be used to access and manipulate the elements in a more meaningful way.\nTo create a named vector, use the names() function:\n\nfruit_prices &lt;- c(0.5, 0.75, 1.25)\nnames(fruit_prices) &lt;- c(\"apple\", \"banana\", \"cherry\")\nprint(fruit_prices)\n\n apple banana cherry \n  0.50   0.75   1.25 \n\n\nYou can also access and modify elements using their names:\n\nbanana_price &lt;- fruit_prices[\"banana\"]\nprint(banana_price)\n\nbanana \n  0.75 \n\nfruit_prices[\"apple\"] &lt;- 0.6\nprint(fruit_prices)\n\n apple banana cherry \n  0.60   0.75   1.25",
    "crumbs": [
      "R Data Structures",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Vectors</span>"
    ]
  },
  {
    "objectID": "vectors.html#character-vectors-a.k.a.-strings",
    "href": "vectors.html#character-vectors-a.k.a.-strings",
    "title": "\n7  Vectors\n",
    "section": "\n7.7 Character Vectors, A.K.A. Strings",
    "text": "7.7 Character Vectors, A.K.A. Strings\nR uses the paste function to concatenate strings.\n\npaste(\"abc\",\"def\")\n\n[1] \"abc def\"\n\npaste(\"abc\",\"def\",sep=\"THISSEP\")\n\n[1] \"abcTHISSEPdef\"\n\npaste0(\"abc\",\"def\")\n\n[1] \"abcdef\"\n\n## [1] \"abcdef\"\npaste(c(\"X\",\"Y\"),1:10)\n\n [1] \"X 1\"  \"Y 2\"  \"X 3\"  \"Y 4\"  \"X 5\"  \"Y 6\"  \"X 7\"  \"Y 8\"  \"X 9\"  \"Y 10\"\n\npaste(c(\"X\",\"Y\"),1:10,sep=\"_\")\n\n [1] \"X_1\"  \"Y_2\"  \"X_3\"  \"Y_4\"  \"X_5\"  \"Y_6\"  \"X_7\"  \"Y_8\"  \"X_9\"  \"Y_10\"\n\n\nWe can count the number of characters in a string.\n\nnchar('abc')\n\n[1] 3\n\nnchar(c('abc','d',123456))\n\n[1] 3 1 6\n\n\nPulling out parts of strings is also sometimes useful.\n\nsubstr('This is a good sentence.',start=10,stop=15)\n\n[1] \" good \"\n\n\nAnother common operation is to replace something in a string with something (a find-and-replace).\n\nsub('This','That','This is a good sentence.')\n\n[1] \"That is a good sentence.\"\n\n\nWhen we want to find all strings that match some other string, we can use grep, or “grab regular expression”.\n\ngrep('bcd',c('abcdef','abcd','bcde','cdef','defg'))\n\n[1] 1 2 3\n\ngrep('bcd',c('abcdef','abcd','bcde','cdef','defg'),value=TRUE)\n\n[1] \"abcdef\" \"abcd\"   \"bcde\"  \n\n\nRead about the grepl function (?grepl). Use that function to return a logical vector (TRUE/FALSE) for each entry above with an a in it.",
    "crumbs": [
      "R Data Structures",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Vectors</span>"
    ]
  },
  {
    "objectID": "vectors.html#missing-values-aka-na",
    "href": "vectors.html#missing-values-aka-na",
    "title": "\n7  Vectors\n",
    "section": "\n7.8 Missing Values, AKA “NA”",
    "text": "7.8 Missing Values, AKA “NA”\nR has a special value, “NA”, that represents a “missing” value, or Not Available, in a vector or other data structure. Here, we just create a vector to experiment.\n\nx = 1:5\nx\n\n[1] 1 2 3 4 5\n\nlength(x)\n\n[1] 5\n\n\n\nis.na(x)\n\n[1] FALSE FALSE FALSE FALSE FALSE\n\nx[2] = NA\nx\n\n[1]  1 NA  3  4  5\n\n\nThe length of x is unchanged, but there is one value that is marked as “missing” by virtue of being NA.\n\nlength(x)\n\n[1] 5\n\nis.na(x)\n\n[1] FALSE  TRUE FALSE FALSE FALSE\n\n\nWe can remove NA values by using indexing. In the following, is.na(x) returns a logical vector the length of x. The ! is the logical NOT operator and converts TRUE to FALSE and vice-versa.\n\nx[!is.na(x)]\n\n[1] 1 3 4 5",
    "crumbs": [
      "R Data Structures",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Vectors</span>"
    ]
  },
  {
    "objectID": "vectors.html#exercises",
    "href": "vectors.html#exercises",
    "title": "\n7  Vectors\n",
    "section": "\n7.9 Exercises",
    "text": "7.9 Exercises\n\n\nCreate a numeric vector called temperatures containing the following values: 72, 75, 78, 81, 76, 73.\n\nShow answertemperatures &lt;- c(72, 75, 78, 81, 76, 73, 93)\n\n\n\n\nCreate a character vector called days containing the following values: “Monday”, “Tuesday”, “Wednesday”, “Thursday”, “Friday”, “Saturday”, “Sunday”.\n\nShow answerdays &lt;- c(\"Monday\", \"Tuesday\", \"Wednesday\", \"Thursday\", \"Friday\", \"Saturday\", \"Sunday\")\n\n\n\n\nCalculate the average temperature for the week and store it in a variable called average_temperature.\n\nShow answeraverage_temperature &lt;- mean(temperatures)\n\n\n\n\nCreate a named vector called weekly_temperatures, where the names are the days of the week and the values are the temperatures from the temperatures vector.\n\nShow answerweekly_temperatures &lt;- temperatures\nnames(weekly_temperatures) &lt;- days\n\n\n\n\nCreate a numeric vector called ages containing the following values: 25, 30, 35, 40, 45, 50, 55, 60.\n\nShow answerages &lt;- c(25, 30, 35, 40, 45, 50, 55, 60)\n\n\n\n\nCreate a logical vector called is_adult by checking if the elements in the ages vector are greater than or equal to 18.\n\nShow answeris_adult &lt;- ages &gt;= 18\n\n\n\n\nCalculate the sum and product of the ages vector.\n\nShow answersum_ages &lt;- sum(ages)\nproduct_ages &lt;- prod(ages)\n\n\n\n\nExtract the ages greater than or equal to 40 from the ages vector and store them in a variable called older_ages.\n\nShow answerolder_ages &lt;- ages[ages &gt;= 40]",
    "crumbs": [
      "R Data Structures",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Vectors</span>"
    ]
  },
  {
    "objectID": "matrices.html",
    "href": "matrices.html",
    "title": "\n8  Matrices\n",
    "section": "",
    "text": "8.1 Creating a matrix\nThere are many ways to create a matrix in R. One of the simplest is to use the matrix() function. In the code below, we’ll create a matrix from a vector from 1:16.\nmat1 &lt;- matrix(1:16,nrow=4)\nmat1\n\n     [,1] [,2] [,3] [,4]\n[1,]    1    5    9   13\n[2,]    2    6   10   14\n[3,]    3    7   11   15\n[4,]    4    8   12   16\nThe same is possible, but specifying that the matrix be “filled” by row.\nmat1 &lt;- matrix(1:16,nrow=4,byrow = TRUE)\nmat1\n\n     [,1] [,2] [,3] [,4]\n[1,]    1    2    3    4\n[2,]    5    6    7    8\n[3,]    9   10   11   12\n[4,]   13   14   15   16\nNotice the subtle difference in the order that the numbers go into the matrix.\nWe can also build a matrix from parts by “binding” vectors together:\nx &lt;- 1:10 \ny &lt;- rnorm(10)\nEach of the vectors above is of length 10 and both are “numeric”, so we can make them into a matrix. Using rbind binds rows (r) into a matrix.\nmat &lt;- rbind(x,y)\nmat\n\n        [,1]      [,2]      [,3]       [,4]       [,5]       [,6]       [,7]\nx  1.0000000  2.000000 3.0000000  4.0000000  5.0000000  6.0000000  7.0000000\ny -0.2260241 -1.094141 0.1714483 -0.2555133 -0.1925184 -0.7936755 -0.8252991\n        [,8]      [,9]     [,10]\nx  8.0000000 9.0000000 10.000000\ny -0.8556162 0.7268338 -1.243649\nThe alternative to rbind is cbind that binds columns (c) together.\nmat &lt;- cbind(x,y)\nmat\n\n       x          y\n [1,]  1 -0.2260241\n [2,]  2 -1.0941408\n [3,]  3  0.1714483\n [4,]  4 -0.2555133\n [5,]  5 -0.1925184\n [6,]  6 -0.7936755\n [7,]  7 -0.8252991\n [8,]  8 -0.8556162\n [9,]  9  0.7268338\n[10,] 10 -1.2436490\nInspecting the names associated with rows and columns is often useful, particularly if the names have human meaning.\nrownames(mat)\n\nNULL\n\ncolnames(mat)\n\n[1] \"x\" \"y\"\nWe can also change the names of the matrix by assigning valid names to the columns or rows.\ncolnames(mat) = c('apples','oranges')\ncolnames(mat)\n\n[1] \"apples\"  \"oranges\"\n\nmat\n\n      apples    oranges\n [1,]      1 -0.2260241\n [2,]      2 -1.0941408\n [3,]      3  0.1714483\n [4,]      4 -0.2555133\n [5,]      5 -0.1925184\n [6,]      6 -0.7936755\n [7,]      7 -0.8252991\n [8,]      8 -0.8556162\n [9,]      9  0.7268338\n[10,]     10 -1.2436490\nMatrices have dimensions.\ndim(mat)\n\n[1] 10  2\n\nnrow(mat)\n\n[1] 10\n\nncol(mat)\n\n[1] 2",
    "crumbs": [
      "R Data Structures",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Matrices</span>"
    ]
  },
  {
    "objectID": "matrices.html#accessing-elements-of-a-matrix",
    "href": "matrices.html#accessing-elements-of-a-matrix",
    "title": "\n8  Matrices\n",
    "section": "\n8.2 Accessing elements of a matrix",
    "text": "8.2 Accessing elements of a matrix\nIndexing for matrices works as for vectors except that we now need to include both the row and column (in that order). We can access elements of a matrix using the square bracket [ indexing method. Elements can be accessed as var[r, c]. Here, r and c are vectors describing the elements of the matrix to select.\n\n\n\n\n\n\nImportant\n\n\n\nThe indices in R start with one, meaning that the first element of a vector or the first row/column of a matrix is indexed as one.\nThis is different from some other programming languages, such as Python, which use zero-based indexing, meaning that the first element of a vector or the first row/column of a matrix is indexed as zero.\nIt is important to be aware of this difference when working with data in R, especially if you are coming from a programming background that uses zero-based indexing. Using the wrong index can lead to unexpected results or errors in your code.\n\n\n\n# The 2nd element of the 1st row of mat\nmat[1,2]\n\n   oranges \n-0.2260241 \n\n# The first ROW of mat\nmat[1,]\n\n    apples    oranges \n 1.0000000 -0.2260241 \n\n# The first COLUMN of mat\nmat[,1]\n\n [1]  1  2  3  4  5  6  7  8  9 10\n\n# and all elements of mat that are &gt; 4; note no comma\nmat[mat&gt;4]\n\n[1]  5  6  7  8  9 10\n\n## [1]  5  6  7  8  9 10\n\n\n\n\n\n\n\nCaution\n\n\n\nNote that in the last case, there is no “,”, so R treats the matrix as a long vector (length=20). This is convenient, sometimes, but it can also be a source of error, as some code may “work” but be doing something unexpected.\n\n\nWe can also use indexing to exclude a row or column by prefixing the selection with a - sign.\n\nmat[,-1]       # remove first column\n\n [1] -0.2260241 -1.0941408  0.1714483 -0.2555133 -0.1925184 -0.7936755\n [7] -0.8252991 -0.8556162  0.7268338 -1.2436490\n\nmat[-c(1:5),]  # remove first five rows\n\n     apples    oranges\n[1,]      6 -0.7936755\n[2,]      7 -0.8252991\n[3,]      8 -0.8556162\n[4,]      9  0.7268338\n[5,]     10 -1.2436490",
    "crumbs": [
      "R Data Structures",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Matrices</span>"
    ]
  },
  {
    "objectID": "matrices.html#changing-values-in-a-matrix",
    "href": "matrices.html#changing-values-in-a-matrix",
    "title": "\n8  Matrices\n",
    "section": "\n8.3 Changing values in a matrix",
    "text": "8.3 Changing values in a matrix\nWe can create a matrix filled with random values drawn from a normal distribution for our work below.\n\nm = matrix(rnorm(20),nrow=10)\nsummary(m)\n\n       V1                V2         \n Min.   :-1.2254   Min.   :-1.4417  \n 1st Qu.:-0.6030   1st Qu.:-1.0352  \n Median : 0.3881   Median :-0.1091  \n Mean   : 0.2752   Mean   : 0.1471  \n 3rd Qu.: 1.1030   3rd Qu.: 0.9386  \n Max.   : 1.6500   Max.   : 2.8153  \n\n\nMultiplication and division works similarly to vectors. When multiplying by a vector, for example, the values of the vector are reused. In the simplest case, let’s multiply the matrix by a constant (vector of length 1).\n\n# multiply all values in the matrix by 20\nm2 = m*20\nsummary(m2)\n\n       V1                V2         \n Min.   :-24.508   Min.   :-28.834  \n 1st Qu.:-12.059   1st Qu.:-20.703  \n Median :  7.761   Median : -2.181  \n Mean   :  5.505   Mean   :  2.943  \n 3rd Qu.: 22.059   3rd Qu.: 18.772  \n Max.   : 33.000   Max.   : 56.306  \n\n\nBy combining subsetting with assignment, we can make changes to just part of a matrix.\n\n# and add 100 to the first column of m\nm2[,1] = m2[,1] + 100\n# summarize m\nsummary(m2)\n\n       V1               V2         \n Min.   : 75.49   Min.   :-28.834  \n 1st Qu.: 87.94   1st Qu.:-20.703  \n Median :107.76   Median : -2.181  \n Mean   :105.50   Mean   :  2.943  \n 3rd Qu.:122.06   3rd Qu.: 18.772  \n Max.   :133.00   Max.   : 56.306  \n\n\nA somewhat common transformation for a matrix is to transpose which changes rows to columns. One might need to do this if an assay output from a lab machine puts samples in rows and genes in columns, for example, while in Bioconductor/R, we often want the samples in columns and the genes in rows.\n\nt(m2)\n\n          [,1]      [,2]     [,3]      [,4]      [,5]      [,6]       [,7]\n[1,]  86.90116 113.48913 75.49234 133.00012 108.15383 129.18570 107.368657\n[2,] -27.62233  19.16009 56.30615 -14.23669  17.60645 -28.83382  -3.364782\n           [,8]      [,9]     [,10]\n[1,] 85.4795888  91.05885 124.91609\n[2,] -0.9980966 -22.85878  34.26772",
    "crumbs": [
      "R Data Structures",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Matrices</span>"
    ]
  },
  {
    "objectID": "matrices.html#calculations-on-matrix-rows-and-columns",
    "href": "matrices.html#calculations-on-matrix-rows-and-columns",
    "title": "\n8  Matrices\n",
    "section": "\n8.4 Calculations on matrix rows and columns",
    "text": "8.4 Calculations on matrix rows and columns\nAgain, we just need a matrix to play with. We’ll use rnorm again, but with a slight twist.\n\nm3 = matrix(rnorm(100,5,2),ncol=10) # what does the 5 mean here? And the 2?\n\nSince these data are from a normal distribution, we can look at a row (or column) to see what the mean and standard deviation are.\n\nmean(m3[,1])\n\n[1] 5.80262\n\nsd(m3[,1])\n\n[1] 2.140594\n\n# or a row\nmean(m3[1,])\n\n[1] 4.987707\n\nsd(m3[1,])\n\n[1] 1.03461\n\n\nThere are some useful convenience functions for computing means and sums of data in all of the columns and rows of matrices.\n\ncolMeans(m3)\n\n [1] 5.802620 5.460025 4.821321 4.992476 3.679565 5.079196 4.623018 4.522687\n [9] 4.943004 4.878935\n\nrowMeans(m3)\n\n [1] 4.987707 5.146057 5.552158 5.725254 4.228795 5.374928 4.062281 4.421042\n [9] 5.316340 3.988285\n\nrowSums(m3)\n\n [1] 49.87707 51.46057 55.52158 57.25254 42.28795 53.74928 40.62281 44.21042\n [9] 53.16340 39.88285\n\ncolSums(m3)\n\n [1] 58.02620 54.60025 48.21321 49.92476 36.79565 50.79196 46.23018 45.22687\n [9] 49.43004 48.78935\n\n\nWe can look at the distribution of column means:\n\n# save as a variable\ncmeans = colMeans(m3)\nsummary(cmeans)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  3.680   4.673   4.911   4.880   5.058   5.803 \n\n\nNote that this is centered pretty closely around the selected mean of 5 above.\nHow about the standard deviation? There is not a colSd function, but it turns out that we can easily apply functions that take vectors as input, like sd and “apply” them across either the rows (the first dimension) or columns (the second) dimension.\n\ncsds = apply(m3, 2, sd)\nsummary(csds)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  1.249   1.723   1.985   1.909   2.215   2.327 \n\n\nAgain, take a look at the distribution which is centered quite close to the selected standard deviation when we created our matrix.",
    "crumbs": [
      "R Data Structures",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Matrices</span>"
    ]
  },
  {
    "objectID": "matrices.html#exercises",
    "href": "matrices.html#exercises",
    "title": "\n8  Matrices\n",
    "section": "\n8.5 Exercises",
    "text": "8.5 Exercises\n\n8.5.1 Data preparation\nFor this set of exercises, we are going to rely on a dataset that comes with R. It gives the number of sunspots per month from 1749-1983. The dataset comes as a ts or time series data type which I convert to a matrix using the following code.\nJust run the code as is and focus on the rest of the exercises.\n\ndata(sunspots)\nsunspot_mat &lt;- matrix(as.vector(sunspots),ncol=12,byrow = TRUE)\ncolnames(sunspot_mat) &lt;- as.character(1:12)\nrownames(sunspot_mat) &lt;- as.character(1749:1983)\n\n\n8.5.2 Questions\n\n\nAfter the conversion above, what does sunspot_mat look like? Use functions to find the number of rows, the number of columns, the class, and some basic summary statistics.\n\nShow answerncol(sunspot_mat)\nnrow(sunspot_mat)\ndim(sunspot_mat)\nsummary(sunspot_mat)\nhead(sunspot_mat)\ntail(sunspot_mat)\n\n\n\n\nPractice subsetting the matrix a bit by selecting:\n\nThe first 10 years (rows)\nThe month of July (7th column)\nThe value for July, 1979 using the rowname to do the selection.\n\n\nShow answersunspot_mat[1:10,]\nsunspot_mat[,7]\nsunspot_mat['1979',7]\n\n\n\n\n\n\nThese next few exercises take advantage of the fact that calling a univariate statistical function (one that expects a vector) works for matrices by just making a vector of all the values in the matrix. What is the highest (max) number of sunspots recorded in these data?\n\nShow answermax(sunspot_mat)\n\n\n\n\nAnd the minimum?\n\nShow answermin(sunspot_mat)\n\n\n\n\nAnd the overall mean and median?\n\nShow answermean(sunspot_mat)\nmedian(sunspot_mat)\n\n\n\n\nUse the hist() function to look at the distribution of all the monthly sunspot data.\n\nShow answerhist(sunspot_mat)\n\n\n\n\nRead about the breaks argument to hist() to try to increase the number of breaks in the histogram to increase the resolution slightly. Adjust your hist() and breaks to your liking.\n\nShow answerhist(sunspot_mat, breaks=40)\n\n\n\n\nNow, let’s move on to summarizing the data a bit to learn about the pattern of sunspots varies by month or by year. Examine the dataset again. What do the columns represent? And the rows?\n\nShow answer# just a quick glimpse of the data will give us a sense\nhead(sunspot_mat)\n\n\n\n\nWe’d like to look at the distribution of sunspots by month. How can we do that?\n\nShow answer# the mean of the columns is the mean number of sunspots per month.\ncolMeans(sunspot_mat)\n\n# Another way to write the same thing:\napply(sunspot_mat, 2, mean)\n\n\n\n\nAssign the month summary above to a variable and summarize it to get a sense of the spread over months.\n\nShow answermonthmeans = colMeans(sunspot_mat)\nsummary(monthmeans)\n\n\n\n\nPlay the same game for years to get the per-year mean?\n\nShow answerymeans = rowMeans(sunspot_mat)\nsummary(ymeans)\n\n\n\n\nMake a plot of the yearly means. Do you see a pattern?\n\nShow answerplot(ymeans)\n# or make it clearer\nplot(ymeans, type='l')",
    "crumbs": [
      "R Data Structures",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Matrices</span>"
    ]
  },
  {
    "objectID": "dataframes_intro.html",
    "href": "dataframes_intro.html",
    "title": "\n9  Data Frames\n",
    "section": "",
    "text": "9.1 Learning goals",
    "crumbs": [
      "R Data Structures",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Data Frames</span>"
    ]
  },
  {
    "objectID": "dataframes_intro.html#learning-goals",
    "href": "dataframes_intro.html#learning-goals",
    "title": "\n9  Data Frames\n",
    "section": "",
    "text": "Understand how data.frames are different from matrices.\nKnow a few functions for examing the contents of a data.frame.\nList approaches for subsetting data.frames.\nBe able to load and save tabular data from and to disk.\nShow how to create a data.frames from scratch.",
    "crumbs": [
      "R Data Structures",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Data Frames</span>"
    ]
  },
  {
    "objectID": "dataframes_intro.html#learning-objectives",
    "href": "dataframes_intro.html#learning-objectives",
    "title": "\n9  Data Frames\n",
    "section": "\n9.2 Learning objectives",
    "text": "9.2 Learning objectives\n\nLoad the yeast growth dataset into R using read.csv.\nExamine the contents of the dataset.\nUse subsetting to find genes that may be involved with nutrient metabolism and transport.\nSummarize data measurements by categories.",
    "crumbs": [
      "R Data Structures",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Data Frames</span>"
    ]
  },
  {
    "objectID": "dataframes_intro.html#dataset",
    "href": "dataframes_intro.html#dataset",
    "title": "\n9  Data Frames\n",
    "section": "\n9.3 Dataset",
    "text": "9.3 Dataset\nThe data used here are borrowed directly from the fantastic Bioconnector tutorials and are a cleaned up version of the data from Brauer et al. Coordination of Growth Rate, Cell Cycle, Stress Response, and Metabolic Activity in Yeast (2008) Mol Biol Cell 19:352-367. These data are from a gene expression microarray, and in this paper the authors examine the relationship between growth rate and gene expression in yeast cultures limited by one of six different nutrients (glucose, leucine, ammonium, sulfate, phosphate, uracil). If you give yeast a rich media loaded with nutrients except restrict the supply of a single nutrient, you can control the growth rate to any rate you choose. By starving yeast of specific nutrients you can find genes that:\n\nRaise or lower their expression in response to growth rate. Growth-rate dependent expression patterns can tell us a lot about cell cycle control, and how the cell responds to stress. The authors found that expression of &gt;25% of all yeast genes is linearly correlated with growth rate, independent of the limiting nutrient. They also found that the subset of negatively growth-correlated genes is enriched for peroxisomal functions, and positively correlated genes mainly encode ribosomal functions.\nRespond differently when different nutrients are being limited. If you see particular genes that respond very differently when a nutrient is sharply restricted, these genes might be involved in the transport or metabolism of that specific nutrient.\n\nThe dataset can be downloaded directly from:\n\nbrauer2007_tidy.csv\n\nWe are going to read this dataset into R and then use it as a playground for learning about data.frames.",
    "crumbs": [
      "R Data Structures",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Data Frames</span>"
    ]
  },
  {
    "objectID": "dataframes_intro.html#reading-in-data",
    "href": "dataframes_intro.html#reading-in-data",
    "title": "\n9  Data Frames\n",
    "section": "\n9.4 Reading in data",
    "text": "9.4 Reading in data\nR has many capabilities for reading in data. Many of the functions have names that help us to understand what data format is to be expected. In this case, the filename that we want to read ends in .csv, meaning comma-separated-values. The read.csv() function reads in .csv files. As usual, it is worth reading help('read.csv') to get a better sense of the possible bells-and-whistles.\nThe read.csv() function can read directly from a URL, so we do not need to download the file directly. This dataset is relatively large (about 16MB), so this may take a bit depending on your network connection speed.\n\noptions(width=60)\n\n\nurl = paste0(\n    'https://raw.githubusercontent.com',\n    '/bioconnector/workshops/master/data/brauer2007_tidy.csv'\n)\nydat &lt;- read.csv(url)\n\nOur variable, ydat, now “contains” the downloaded and read data. We can check to see what data type read.csv gave us:\n\nclass(ydat)\n\n[1] \"data.frame\"",
    "crumbs": [
      "R Data Structures",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Data Frames</span>"
    ]
  },
  {
    "objectID": "dataframes_intro.html#inspecting-data.frames",
    "href": "dataframes_intro.html#inspecting-data.frames",
    "title": "\n9  Data Frames\n",
    "section": "\n9.5 Inspecting data.frames",
    "text": "9.5 Inspecting data.frames\nOur ydat variable is a data.frame. As I mentioned, the dataset is fairly large, so we will not be able to look at it all at once on the screen. However, R gives us many tools to inspect a data.frame.\n\nOverviews of content\n\n\nhead() to show first few rows\n\ntail() to show last few rows\n\n\nSize\n\n\ndim() for dimensions (rows, columns)\nnrow()\nncol()\n\nobject.size() for power users interested in the memory used to store an object\n\n\nData and attribute summaries\n\n\ncolnames() to get the names of the columns\n\nrownames() to get the “names” of the rows–may not be present\n\nsummary() to get per-column summaries of the data in the data.frame.\n\n\n\n\nhead(ydat)\n\n  symbol systematic_name nutrient rate expression\n1   SFB2         YNL049C  Glucose 0.05      -0.24\n2   &lt;NA&gt;         YNL095C  Glucose 0.05       0.28\n3   QRI7         YDL104C  Glucose 0.05      -0.02\n4   CFT2         YLR115W  Glucose 0.05      -0.33\n5   SSO2         YMR183C  Glucose 0.05       0.05\n6   PSP2         YML017W  Glucose 0.05      -0.69\n                            bp\n1        ER to Golgi transport\n2   biological process unknown\n3 proteolysis and peptidolysis\n4      mRNA polyadenylylation*\n5              vesicle fusion*\n6   biological process unknown\n                             mf\n1    molecular function unknown\n2    molecular function unknown\n3 metalloendopeptidase activity\n4                   RNA binding\n5              t-SNARE activity\n6    molecular function unknown\n\ntail(ydat)\n\n       symbol systematic_name nutrient rate expression\n198425   DOA1         YKL213C   Uracil  0.3       0.14\n198426   KRE1         YNL322C   Uracil  0.3       0.28\n198427   MTL1         YGR023W   Uracil  0.3       0.27\n198428   KRE9         YJL174W   Uracil  0.3       0.43\n198429   UTH1         YKR042W   Uracil  0.3       0.19\n198430   &lt;NA&gt;         YOL111C   Uracil  0.3       0.04\n                                               bp\n198425    ubiquitin-dependent protein catabolism*\n198426      cell wall organization and biogenesis\n198427      cell wall organization and biogenesis\n198428     cell wall organization and biogenesis*\n198429 mitochondrion organization and biogenesis*\n198430                 biological process unknown\n                                        mf\n198425          molecular function unknown\n198426 structural constituent of cell wall\n198427          molecular function unknown\n198428          molecular function unknown\n198429          molecular function unknown\n198430          molecular function unknown\n\ndim(ydat)\n\n[1] 198430      7\n\nnrow(ydat)\n\n[1] 198430\n\nncol(ydat)\n\n[1] 7\n\ncolnames(ydat)\n\n[1] \"symbol\"          \"systematic_name\" \"nutrient\"       \n[4] \"rate\"            \"expression\"      \"bp\"             \n[7] \"mf\"             \n\nsummary(ydat)\n\n    symbol          systematic_name      nutrient        \n Length:198430      Length:198430      Length:198430     \n Class :character   Class :character   Class :character  \n Mode  :character   Mode  :character   Mode  :character  \n                                                         \n                                                         \n                                                         \n      rate          expression             bp           \n Min.   :0.0500   Min.   :-6.500000   Length:198430     \n 1st Qu.:0.1000   1st Qu.:-0.290000   Class :character  \n Median :0.2000   Median : 0.000000   Mode  :character  \n Mean   :0.1752   Mean   : 0.003367                     \n 3rd Qu.:0.2500   3rd Qu.: 0.290000                     \n Max.   :0.3000   Max.   : 6.640000                     \n      mf           \n Length:198430     \n Class :character  \n Mode  :character  \n                   \n                   \n                   \n\n\nIn RStudio, there is an additional function, View() (note the capital “V”) that opens the first 1000 rows (default) in the RStudio window, akin to a spreadsheet view.\n\nView(ydat)",
    "crumbs": [
      "R Data Structures",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Data Frames</span>"
    ]
  },
  {
    "objectID": "dataframes_intro.html#accessing-variables-columns-and-subsetting",
    "href": "dataframes_intro.html#accessing-variables-columns-and-subsetting",
    "title": "\n9  Data Frames\n",
    "section": "\n9.6 Accessing variables (columns) and subsetting",
    "text": "9.6 Accessing variables (columns) and subsetting\nIn R, data.frames can be subset similarly to other two-dimensional data structures. The [ in R is used to denote subsetting of any kind. When working with two-dimensional data, we need two values inside the [ ] to specify the details. The specification is [rows, columns]. For example, to get the first three rows of ydat, use:\n\nydat[1:3, ]\n\n  symbol systematic_name nutrient rate expression\n1   SFB2         YNL049C  Glucose 0.05      -0.24\n2   &lt;NA&gt;         YNL095C  Glucose 0.05       0.28\n3   QRI7         YDL104C  Glucose 0.05      -0.02\n                            bp\n1        ER to Golgi transport\n2   biological process unknown\n3 proteolysis and peptidolysis\n                             mf\n1    molecular function unknown\n2    molecular function unknown\n3 metalloendopeptidase activity\n\n\nNote how the second number, the columns, is blank. R takes that to mean “all the columns”. Similarly, we can combine rows and columns specification arbitrarily.\n\nydat[1:3, 1:3]\n\n  symbol systematic_name nutrient\n1   SFB2         YNL049C  Glucose\n2   &lt;NA&gt;         YNL095C  Glucose\n3   QRI7         YDL104C  Glucose\n\n\nBecause selecting a single variable, or column, is such a common operation, there are two shortcuts for doing so with data.frames. The first, the $ operator works like so:\n\n# Look at the column names, just to refresh memory\ncolnames(ydat)\n\n[1] \"symbol\"          \"systematic_name\" \"nutrient\"       \n[4] \"rate\"            \"expression\"      \"bp\"             \n[7] \"mf\"             \n\n# Note that I am using \"head\" here to limit the output\nhead(ydat$symbol)\n\n[1] \"SFB2\" NA     \"QRI7\" \"CFT2\" \"SSO2\" \"PSP2\"\n\n# What is the actual length of \"symbol\"?\nlength(ydat$symbol)\n\n[1] 198430\n\n\nThe second is related to the fact that, in R, data.frames are also lists. We subset a list by using [[]] notation. To get the second column of ydat, we can use:\n\nhead(ydat[[2]])\n\n[1] \"YNL049C\" \"YNL095C\" \"YDL104C\" \"YLR115W\" \"YMR183C\"\n[6] \"YML017W\"\n\n\nAlternatively, we can use the column name:\n\nhead(ydat[[\"systematic_name\"]])\n\n[1] \"YNL049C\" \"YNL095C\" \"YDL104C\" \"YLR115W\" \"YMR183C\"\n[6] \"YML017W\"\n\n\n\n9.6.1 Some data exploration\nThere are a couple of columns that include numeric values. Which columns are numeric?\n\nclass(ydat$symbol)\n\n[1] \"character\"\n\nclass(ydat$rate)\n\n[1] \"numeric\"\n\nclass(ydat$expression)\n\n[1] \"numeric\"\n\n\nMake histograms of: - the expression values - the rate values\nWhat does the table() function do? Could you use that to look a the rate column given that that column appears to have repeated values?\nWhat rate corresponds to the most nutrient-starved condition?\n\n9.6.2 More advanced indexing and subsetting\nWe can use, for example, logical values (TRUE/FALSE) to subset data.frames.\n\nhead(ydat[ydat$symbol == 'LEU1', ])\n\n     symbol systematic_name nutrient rate expression   bp\nNA     &lt;NA&gt;            &lt;NA&gt;     &lt;NA&gt;   NA         NA &lt;NA&gt;\nNA.1   &lt;NA&gt;            &lt;NA&gt;     &lt;NA&gt;   NA         NA &lt;NA&gt;\nNA.2   &lt;NA&gt;            &lt;NA&gt;     &lt;NA&gt;   NA         NA &lt;NA&gt;\nNA.3   &lt;NA&gt;            &lt;NA&gt;     &lt;NA&gt;   NA         NA &lt;NA&gt;\nNA.4   &lt;NA&gt;            &lt;NA&gt;     &lt;NA&gt;   NA         NA &lt;NA&gt;\nNA.5   &lt;NA&gt;            &lt;NA&gt;     &lt;NA&gt;   NA         NA &lt;NA&gt;\n       mf\nNA   &lt;NA&gt;\nNA.1 &lt;NA&gt;\nNA.2 &lt;NA&gt;\nNA.3 &lt;NA&gt;\nNA.4 &lt;NA&gt;\nNA.5 &lt;NA&gt;\n\ntail(ydat[ydat$symbol == 'LEU1', ])\n\n         symbol systematic_name nutrient rate expression\nNA.47244   &lt;NA&gt;            &lt;NA&gt;     &lt;NA&gt;   NA         NA\nNA.47245   &lt;NA&gt;            &lt;NA&gt;     &lt;NA&gt;   NA         NA\nNA.47246   &lt;NA&gt;            &lt;NA&gt;     &lt;NA&gt;   NA         NA\nNA.47247   &lt;NA&gt;            &lt;NA&gt;     &lt;NA&gt;   NA         NA\nNA.47248   &lt;NA&gt;            &lt;NA&gt;     &lt;NA&gt;   NA         NA\nNA.47249   &lt;NA&gt;            &lt;NA&gt;     &lt;NA&gt;   NA         NA\n           bp   mf\nNA.47244 &lt;NA&gt; &lt;NA&gt;\nNA.47245 &lt;NA&gt; &lt;NA&gt;\nNA.47246 &lt;NA&gt; &lt;NA&gt;\nNA.47247 &lt;NA&gt; &lt;NA&gt;\nNA.47248 &lt;NA&gt; &lt;NA&gt;\nNA.47249 &lt;NA&gt; &lt;NA&gt;\n\n\nWhat is the problem with this approach? It appears that there are a bunch of NA values. Taking a quick look at the symbol column, we see what the problem.\n\nsummary(ydat$symbol)\n\n   Length     Class      Mode \n   198430 character character \n\n\nUsing the is.na() function, we can make filter further to get down to values of interest.\n\nhead(ydat[ydat$symbol == 'LEU1' & !is.na(ydat$symbol), ])\n\n      symbol systematic_name nutrient rate expression\n1526    LEU1         YGL009C  Glucose 0.05      -1.12\n7043    LEU1         YGL009C  Glucose 0.10      -0.77\n12555   LEU1         YGL009C  Glucose 0.15      -0.67\n18071   LEU1         YGL009C  Glucose 0.20      -0.59\n23603   LEU1         YGL009C  Glucose 0.25      -0.20\n29136   LEU1         YGL009C  Glucose 0.30       0.03\n                        bp\n1526  leucine biosynthesis\n7043  leucine biosynthesis\n12555 leucine biosynthesis\n18071 leucine biosynthesis\n23603 leucine biosynthesis\n29136 leucine biosynthesis\n                                          mf\n1526  3-isopropylmalate dehydratase activity\n7043  3-isopropylmalate dehydratase activity\n12555 3-isopropylmalate dehydratase activity\n18071 3-isopropylmalate dehydratase activity\n23603 3-isopropylmalate dehydratase activity\n29136 3-isopropylmalate dehydratase activity\n\n\nSometimes, looking at the data themselves is not that important. Using dim() is one possibility to look at the number of rows and columns after subsetting.\n\ndim(ydat[ydat$expression &gt; 3, ])\n\n[1] 714   7\n\n\nFind the high expressed genes when leucine-starved. For this task we can also use subset which allows us to treat column names as R variables (no $ needed).\n\nsubset(ydat, nutrient == 'Leucine' & rate == 0.05 & expression &gt; 3)\n\n       symbol systematic_name nutrient rate expression\n133768   QDR2         YIL121W  Leucine 0.05       4.61\n133772   LEU1         YGL009C  Leucine 0.05       3.84\n133858   BAP3         YDR046C  Leucine 0.05       4.29\n135186   &lt;NA&gt;         YPL033C  Leucine 0.05       3.43\n135187   &lt;NA&gt;         YLR267W  Leucine 0.05       3.23\n135288   HXT3         YDR345C  Leucine 0.05       5.16\n135963   TPO2         YGR138C  Leucine 0.05       3.75\n135965   YRO2         YBR054W  Leucine 0.05       4.40\n136102   GPG1         YGL121C  Leucine 0.05       3.08\n136109  HSP42         YDR171W  Leucine 0.05       3.07\n136119   HXT5         YHR096C  Leucine 0.05       4.90\n136151   &lt;NA&gt;         YJL144W  Leucine 0.05       3.06\n136152   MOH1         YBL049W  Leucine 0.05       3.43\n136153   &lt;NA&gt;         YBL048W  Leucine 0.05       3.95\n136189  HSP26         YBR072W  Leucine 0.05       4.86\n136231   NCA3         YJL116C  Leucine 0.05       4.03\n136233   &lt;NA&gt;         YBR116C  Leucine 0.05       3.28\n136486   &lt;NA&gt;         YGR043C  Leucine 0.05       3.07\n137443   ADH2         YMR303C  Leucine 0.05       4.15\n137448   ICL1         YER065C  Leucine 0.05       3.54\n137451   SFC1         YJR095W  Leucine 0.05       3.72\n137569   MLS1         YNL117W  Leucine 0.05       3.76\n                                              bp\n133768                       multidrug transport\n133772                      leucine biosynthesis\n133858                      amino acid transport\n135186                                  meiosis*\n135187                biological process unknown\n135288                          hexose transport\n135963                       polyamine transport\n135965                biological process unknown\n136102                       signal transduction\n136109                       response to stress*\n136119                          hexose transport\n136151                   response to dessication\n136152                biological process unknown\n136153                                      &lt;NA&gt;\n136189                       response to stress*\n136231 mitochondrion organization and biogenesis\n136233                                      &lt;NA&gt;\n136486                biological process unknown\n137443                             fermentation*\n137448                          glyoxylate cycle\n137451                       fumarate transport*\n137569                          glyoxylate cycle\n                                           mf\n133768         multidrug efflux pump activity\n133772 3-isopropylmalate dehydratase activity\n133858        amino acid transporter activity\n135186             molecular function unknown\n135187             molecular function unknown\n135288          glucose transporter activity*\n135963          spermine transporter activity\n135965             molecular function unknown\n136102             signal transducer activity\n136109               unfolded protein binding\n136119          glucose transporter activity*\n136151             molecular function unknown\n136152             molecular function unknown\n136153                                   &lt;NA&gt;\n136189               unfolded protein binding\n136231             molecular function unknown\n136233                                   &lt;NA&gt;\n136486                 transaldolase activity\n137443         alcohol dehydrogenase activity\n137448              isocitrate lyase activity\n137451 succinate:fumarate antiporter activity\n137569               malate synthase activity",
    "crumbs": [
      "R Data Structures",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Data Frames</span>"
    ]
  },
  {
    "objectID": "dataframes_intro.html#aggregating-data",
    "href": "dataframes_intro.html#aggregating-data",
    "title": "\n9  Data Frames\n",
    "section": "\n9.7 Aggregating data",
    "text": "9.7 Aggregating data\nAggregating data, or summarizing by category, is a common way to look for trends or differences in measurements between categories. Use aggregate to find the mean expression by gene symbol.\n\nhead(aggregate(ydat$expression, by=list( ydat$symbol), mean))\n\n  Group.1           x\n1    AAC1  0.52888889\n2    AAC3 -0.21628571\n3   AAD10  0.43833333\n4   AAD14 -0.07166667\n5   AAD16  0.24194444\n6    AAD4 -0.79166667\n\n# or \nhead(aggregate(expression ~ symbol, mean, data=ydat))\n\n  symbol  expression\n1   AAC1  0.52888889\n2   AAC3 -0.21628571\n3  AAD10  0.43833333\n4  AAD14 -0.07166667\n5  AAD16  0.24194444\n6   AAD4 -0.79166667",
    "crumbs": [
      "R Data Structures",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Data Frames</span>"
    ]
  },
  {
    "objectID": "dataframes_intro.html#creating-a-data.frame-from-scratch",
    "href": "dataframes_intro.html#creating-a-data.frame-from-scratch",
    "title": "\n9  Data Frames\n",
    "section": "\n9.8 Creating a data.frame from scratch",
    "text": "9.8 Creating a data.frame from scratch\nSometimes it is useful to combine related data into one object. For example, let’s simulate some data.\n\nsmoker = factor(rep(c(\"smoker\", \"non-smoker\"), each=50))\nsmoker_numeric = as.numeric(smoker)\nx = rnorm(100)\nrisk = x + 2*smoker_numeric\n\nWe have two varibles, risk and smoker that are related. We can make a data.frame out of them:\n\nsmoker_risk = data.frame(smoker = smoker, risk = risk)\nhead(smoker_risk)\n\n  smoker     risk\n1 smoker 2.268207\n2 smoker 5.170728\n3 smoker 4.066137\n4 smoker 3.470311\n5 smoker 3.708060\n6 smoker 4.309701\n\n\nR also has plotting shortcuts that work with data.frames to simplify plotting\n\nplot( risk ~ smoker, data=smoker_risk)",
    "crumbs": [
      "R Data Structures",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Data Frames</span>"
    ]
  },
  {
    "objectID": "dataframes_intro.html#saving-a-data.frame",
    "href": "dataframes_intro.html#saving-a-data.frame",
    "title": "\n9  Data Frames\n",
    "section": "\n9.9 Saving a data.frame",
    "text": "9.9 Saving a data.frame\nOnce we have a data.frame of interest, we may want to save it. The most portable way to save a data.frame is to use one of the write functions. In this case, let’s save the data as a .csv file.\n\nwrite.csv(smoker_risk, \"smoker_risk.csv\")",
    "crumbs": [
      "R Data Structures",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Data Frames</span>"
    ]
  },
  {
    "objectID": "factors.html",
    "href": "factors.html",
    "title": "\n10  Factors\n",
    "section": "",
    "text": "10.1 Factors\nA factor is a special type of vector, normally used to hold a categorical variable–such as smoker/nonsmoker, state of residency, zipcode–in many statistical functions. Such vectors have class “factor”. Factors are primarily used in Analysis of Variance (ANOVA) or other situations when “categories” are needed. When a factor is used as a predictor variable, the corresponding indicator variables are created (more later).\nNote of caution that factors in R often appear to be character vectors when printed, but you will notice that they do not have double quotes around them. They are stored in R as numbers with a key name, so sometimes you will note that the factor behaves like a numeric vector.\n# create the character vector\ncitizen&lt;-c(\"uk\",\"us\",\"no\",\"au\",\"uk\",\"us\",\"us\",\"no\",\"au\") \n\n# convert to factor\ncitizenf&lt;-factor(citizen)                                \ncitizen             \n\n[1] \"uk\" \"us\" \"no\" \"au\" \"uk\" \"us\" \"us\" \"no\" \"au\"\n\ncitizenf\n\n[1] uk us no au uk us us no au\nLevels: au no uk us\n\n# convert factor back to character vector\nas.character(citizenf)\n\n[1] \"uk\" \"us\" \"no\" \"au\" \"uk\" \"us\" \"us\" \"no\" \"au\"\n\n# convert to numeric vector\nas.numeric(citizenf)\n\n[1] 3 4 2 1 3 4 4 2 1\nR stores many data structures as vectors with “attributes” and “class” (just so you have seen this).\nattributes(citizenf)\n\n$levels\n[1] \"au\" \"no\" \"uk\" \"us\"\n\n$class\n[1] \"factor\"\n\nclass(citizenf)\n\n[1] \"factor\"\n\n# note that after unclassing, we can see the \n# underlying numeric structure again\nunclass(citizenf)\n\n[1] 3 4 2 1 3 4 4 2 1\nattr(,\"levels\")\n[1] \"au\" \"no\" \"uk\" \"us\"\nTabulating factors is a useful way to get a sense of the “sample” set available.\ntable(citizenf)\n\ncitizenf\nau no uk us \n 2  2  2  3",
    "crumbs": [
      "R Data Structures",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Factors</span>"
    ]
  },
  {
    "objectID": "eda_overview.html",
    "href": "eda_overview.html",
    "title": "Exploratory data analysis",
    "section": "",
    "text": "Imagine you’re on an adventure, about to embark on a journey into the unknown. You’ve just been handed a treasure map, with the promise of valuable insights waiting to be discovered. This map is your data set, and the journey is exploratory data analysis (EDA).\nAs you begin your exploration, you start by getting a feel for the terrain. You take a broad, bird’s-eye view of the data, examining its structure and dimensions. Are you dealing with a vast landscape or a small, confined area? Are there any missing pieces in the map that you’ll need to account for? Understanding the overall context of your data set is crucial before venturing further.\nWith a sense of the landscape, you now zoom in to identify key landmarks in the data. You might look for unusual patterns, trends, or relationships between variables. As you spot these landmarks, you start asking questions: What’s causing that spike in values? Are these two factors related, or is it just a coincidence? By asking these questions, you’re actively engaging with the data and forming hypotheses that could guide future analysis or experiments.\nAs you continue your journey, you realize that the map alone isn’t enough to fully understand the terrain. You need more tools to bring the data to life. You start visualizing the data using charts, plots, and graphs. These visualizations act as your binoculars, allowing you to see patterns and relationships more clearly. Through them, you can uncover the hidden treasures buried within the data.\nEDA isn’t a linear path from start to finish. As you explore, you’ll find yourself circling back to previous points, refining your questions, and digging deeper. The process is iterative, with each new discovery informing the next. And as you go, you’ll gain a deeper understanding of the data’s underlying structure and potential.\nFinally, after your thorough exploration, you’ll have a solid foundation to build upon. You’ll be better equipped to make informed decisions, test hypotheses, and draw meaningful conclusions. The insights you’ve gained through EDA will serve as a compass, guiding you towards the true value hidden within your data. And with that, you’ve successfully completed your journey through exploratory data analysis.",
    "crumbs": [
      "Exploratory data analysis"
    ]
  },
  {
    "objectID": "dplyr_intro_msleep.html",
    "href": "dplyr_intro_msleep.html",
    "title": "\n11  Introduction to dplyr: mammal sleep dataset\n",
    "section": "",
    "text": "11.1 Learning goals",
    "crumbs": [
      "Exploratory data analysis",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Introduction to dplyr: mammal sleep dataset</span>"
    ]
  },
  {
    "objectID": "dplyr_intro_msleep.html#learning-goals",
    "href": "dplyr_intro_msleep.html#learning-goals",
    "title": "\n11  Introduction to dplyr: mammal sleep dataset\n",
    "section": "",
    "text": "Know that dplyr is just a different approach to manipulating data in data.frames.\nList the commonly used dplyr verbs and how they can be used to manipulate data.frames.\nShow how to aggregate and summarized data using dplyr\n\nKnow what the piping operator, |&gt;, is and how it can be used.",
    "crumbs": [
      "Exploratory data analysis",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Introduction to dplyr: mammal sleep dataset</span>"
    ]
  },
  {
    "objectID": "dplyr_intro_msleep.html#learning-objectives",
    "href": "dplyr_intro_msleep.html#learning-objectives",
    "title": "\n11  Introduction to dplyr: mammal sleep dataset\n",
    "section": "\n11.2 Learning objectives",
    "text": "11.2 Learning objectives\n\nSelect subsets of the mammal sleep dataset.\nReorder the dataset.\nAdd columns to the dataset based on existing columns.\nSummarize the amount of sleep by categorical variables using group_by and summarize.",
    "crumbs": [
      "Exploratory data analysis",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Introduction to dplyr: mammal sleep dataset</span>"
    ]
  },
  {
    "objectID": "dplyr_intro_msleep.html#what-is-dplyr",
    "href": "dplyr_intro_msleep.html#what-is-dplyr",
    "title": "\n11  Introduction to dplyr: mammal sleep dataset\n",
    "section": "\n11.3 What is dplyr?",
    "text": "11.3 What is dplyr?\nThe dplyr package is a specialized package for working with data.frames (and the related tibble) to transform and summarize tabular data with rows and columns. For another explanation of dplyr see the dplyr package vignette: Introduction to dplyr",
    "crumbs": [
      "Exploratory data analysis",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Introduction to dplyr: mammal sleep dataset</span>"
    ]
  },
  {
    "objectID": "dplyr_intro_msleep.html#why-is-dplyr-userful",
    "href": "dplyr_intro_msleep.html#why-is-dplyr-userful",
    "title": "\n11  Introduction to dplyr: mammal sleep dataset\n",
    "section": "\n11.4 Why Is dplyr userful?",
    "text": "11.4 Why Is dplyr userful?\ndplyr contains a set of functions–commonly called the dplyr “verbs”–that perform common data manipulations such as filtering for rows, selecting specific columns, re-ordering rows, adding new columns and summarizing data. In addition, dplyr contains a useful function to perform another common task which is the “split-apply-combine” concept.\nCompared to base functions in R, the functions in dplyr are often easier to work with, are more consistent in the syntax and are targeted for data analysis around data frames, instead of just vectors.",
    "crumbs": [
      "Exploratory data analysis",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Introduction to dplyr: mammal sleep dataset</span>"
    ]
  },
  {
    "objectID": "dplyr_intro_msleep.html#data-mammals-sleep",
    "href": "dplyr_intro_msleep.html#data-mammals-sleep",
    "title": "\n11  Introduction to dplyr: mammal sleep dataset\n",
    "section": "\n11.5 Data: Mammals Sleep",
    "text": "11.5 Data: Mammals Sleep\nThe msleep (mammals sleep) data set contains the sleep times and weights for a set of mammals and is available in the dagdata repository on github. This data set contains 83 rows and 11 variables. The data happen to be available as a dataset in the ggplot2 package. To get access to the msleep dataset, we need to first install the ggplot2 package.\n\ninstall.packages('ggplot2')\n\nThen, we can load the library.\n\nlibrary(ggplot2)\ndata(msleep)\n\nAs with many datasets in R, “help” is available to describe the dataset itself.\n\n?msleep\n\nThe columns are described in the help page, but are included here, also.\n\n\ncolumn name\nDescription\n\n\n\nname\ncommon name\n\n\ngenus\ntaxonomic rank\n\n\nvore\ncarnivore, omnivore or herbivore?\n\n\norder\ntaxonomic rank\n\n\nconservation\nthe conservation status of the mammal\n\n\nsleep_total\ntotal amount of sleep, in hours\n\n\nsleep_rem\nrem sleep, in hours\n\n\nsleep_cycle\nlength of sleep cycle, in hours\n\n\nawake\namount of time spent awake, in hours\n\n\nbrainwt\nbrain weight in kilograms\n\n\nbodywt\nbody weight in kilograms",
    "crumbs": [
      "Exploratory data analysis",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Introduction to dplyr: mammal sleep dataset</span>"
    ]
  },
  {
    "objectID": "dplyr_intro_msleep.html#dplyr-verbs",
    "href": "dplyr_intro_msleep.html#dplyr-verbs",
    "title": "\n11  Introduction to dplyr: mammal sleep dataset\n",
    "section": "\n11.6 dplyr verbs",
    "text": "11.6 dplyr verbs\nThe dplyr verbs are listed here. There are many other functions available in dplyr, but we will focus on just these.\n\n\n\n\n\n\ndplyr verbs\nDescription\n\n\n\nselect()\nselect columns\n\n\nfilter()\nfilter rows\n\n\narrange()\nre-order or arrange rows\n\n\nmutate()\ncreate new columns\n\n\nsummarise()\nsummarise values\n\n\ngroup_by()\nallows for group operations in the “split-apply-combine” concept",
    "crumbs": [
      "Exploratory data analysis",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Introduction to dplyr: mammal sleep dataset</span>"
    ]
  },
  {
    "objectID": "dplyr_intro_msleep.html#using-the-dplyr-verbs",
    "href": "dplyr_intro_msleep.html#using-the-dplyr-verbs",
    "title": "\n11  Introduction to dplyr: mammal sleep dataset\n",
    "section": "\n11.7 Using the dplyr verbs",
    "text": "11.7 Using the dplyr verbs\nThe two most basic functions are select() and filter(), which selects columns and filters rows respectively. What are the equivalent ways to select columns without dplyr? And filtering to include only specific rows?\nBefore proceeding, we need to install the dplyr package:\n\ninstall.packages('dplyr')\n\nAnd then load the library:\n\nlibrary(dplyr)\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\n\n\n11.7.1 Selecting columns: select()\n\nSelect a set of columns such as the name and the sleep_total columns.\n\nsleepData &lt;- select(msleep, name, sleep_total)\nhead(sleepData)\n\n# A tibble: 6 × 2\n  name                       sleep_total\n  &lt;chr&gt;                            &lt;dbl&gt;\n1 Cheetah                           12.1\n2 Owl monkey                        17  \n3 Mountain beaver                   14.4\n4 Greater short-tailed shrew        14.9\n5 Cow                                4  \n6 Three-toed sloth                  14.4\n\n\nTo select all the columns except a specific column, use the “-” (subtraction) operator (also known as negative indexing). For example, to select all columns except name:\n\nhead(select(msleep, -name))\n\n# A tibble: 6 × 10\n  genus      vore  order    conservation sleep_total sleep_rem sleep_cycle awake\n  &lt;chr&gt;      &lt;chr&gt; &lt;chr&gt;    &lt;chr&gt;              &lt;dbl&gt;     &lt;dbl&gt;       &lt;dbl&gt; &lt;dbl&gt;\n1 Acinonyx   carni Carnivo… lc                  12.1      NA        NA      11.9\n2 Aotus      omni  Primates &lt;NA&gt;                17         1.8      NA       7  \n3 Aplodontia herbi Rodentia nt                  14.4       2.4      NA       9.6\n4 Blarina    omni  Soricom… lc                  14.9       2.3       0.133   9.1\n5 Bos        herbi Artioda… domesticated         4         0.7       0.667  20  \n6 Bradypus   herbi Pilosa   &lt;NA&gt;                14.4       2.2       0.767   9.6\n# ℹ 2 more variables: brainwt &lt;dbl&gt;, bodywt &lt;dbl&gt;\n\n\nTo select a range of columns by name, use the “:” operator. Note that dplyr allows us to use the column names without quotes and as “indices” of the columns.\n\nhead(select(msleep, name:order))\n\n# A tibble: 6 × 4\n  name                       genus      vore  order       \n  &lt;chr&gt;                      &lt;chr&gt;      &lt;chr&gt; &lt;chr&gt;       \n1 Cheetah                    Acinonyx   carni Carnivora   \n2 Owl monkey                 Aotus      omni  Primates    \n3 Mountain beaver            Aplodontia herbi Rodentia    \n4 Greater short-tailed shrew Blarina    omni  Soricomorpha\n5 Cow                        Bos        herbi Artiodactyla\n6 Three-toed sloth           Bradypus   herbi Pilosa      \n\n\nTo select all columns that start with the character string “sl”, use the function starts_with().\n\nhead(select(msleep, starts_with(\"sl\")))\n\n# A tibble: 6 × 3\n  sleep_total sleep_rem sleep_cycle\n        &lt;dbl&gt;     &lt;dbl&gt;       &lt;dbl&gt;\n1        12.1      NA        NA    \n2        17         1.8      NA    \n3        14.4       2.4      NA    \n4        14.9       2.3       0.133\n5         4         0.7       0.667\n6        14.4       2.2       0.767\n\n\nSome additional options to select columns based on a specific criteria include:\n\n\nends_with() = Select columns that end with a character string\n\ncontains() = Select columns that contain a character string\n\nmatches() = Select columns that match a regular expression\n\none_of() = Select column names that are from a group of names\n\n11.7.2 Selecting rows: filter()\n\nThe filter() function allows us to filter rows to include only those rows that match the filter. For example, we can filter the rows for mammals that sleep a total of more than 16 hours.\n\nfilter(msleep, sleep_total &gt;= 16)\n\n# A tibble: 8 × 11\n  name    genus vore  order conservation sleep_total sleep_rem sleep_cycle awake\n  &lt;chr&gt;   &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt;              &lt;dbl&gt;     &lt;dbl&gt;       &lt;dbl&gt; &lt;dbl&gt;\n1 Owl mo… Aotus omni  Prim… &lt;NA&gt;                17         1.8      NA       7  \n2 Long-n… Dasy… carni Cing… lc                  17.4       3.1       0.383   6.6\n3 North … Dide… omni  Dide… lc                  18         4.9       0.333   6  \n4 Big br… Epte… inse… Chir… lc                  19.7       3.9       0.117   4.3\n5 Thick-… Lutr… carni Dide… lc                  19.4       6.6      NA       4.6\n6 Little… Myot… inse… Chir… &lt;NA&gt;                19.9       2         0.2     4.1\n7 Giant … Prio… inse… Cing… en                  18.1       6.1      NA       5.9\n8 Arctic… Sper… herbi Rode… lc                  16.6      NA        NA       7.4\n# ℹ 2 more variables: brainwt &lt;dbl&gt;, bodywt &lt;dbl&gt;\n\n\nFilter the rows for mammals that sleep a total of more than 16 hours and have a body weight of greater than 1 kilogram.\n\nfilter(msleep, sleep_total &gt;= 16, bodywt &gt;= 1)\n\n# A tibble: 3 × 11\n  name    genus vore  order conservation sleep_total sleep_rem sleep_cycle awake\n  &lt;chr&gt;   &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt;              &lt;dbl&gt;     &lt;dbl&gt;       &lt;dbl&gt; &lt;dbl&gt;\n1 Long-n… Dasy… carni Cing… lc                  17.4       3.1       0.383   6.6\n2 North … Dide… omni  Dide… lc                  18         4.9       0.333   6  \n3 Giant … Prio… inse… Cing… en                  18.1       6.1      NA       5.9\n# ℹ 2 more variables: brainwt &lt;dbl&gt;, bodywt &lt;dbl&gt;\n\n\nFilter the rows for mammals in the Perissodactyla and Primates taxonomic order. The %in% operator is a logical operator that returns TRUE for values of a vector that are present in a second vector.\n\nfilter(msleep, order %in% c(\"Perissodactyla\", \"Primates\"))\n\n# A tibble: 15 × 11\n   name   genus vore  order conservation sleep_total sleep_rem sleep_cycle awake\n   &lt;chr&gt;  &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt;              &lt;dbl&gt;     &lt;dbl&gt;       &lt;dbl&gt; &lt;dbl&gt;\n 1 Owl m… Aotus omni  Prim… &lt;NA&gt;                17         1.8      NA       7  \n 2 Grivet Cerc… omni  Prim… lc                  10         0.7      NA      14  \n 3 Horse  Equus herbi Peri… domesticated         2.9       0.6       1      21.1\n 4 Donkey Equus herbi Peri… domesticated         3.1       0.4      NA      20.9\n 5 Patas… Eryt… omni  Prim… lc                  10.9       1.1      NA      13.1\n 6 Galago Gala… omni  Prim… &lt;NA&gt;                 9.8       1.1       0.55   14.2\n 7 Human  Homo  omni  Prim… &lt;NA&gt;                 8         1.9       1.5    16  \n 8 Mongo… Lemur herbi Prim… vu                   9.5       0.9      NA      14.5\n 9 Macaq… Maca… omni  Prim… &lt;NA&gt;                10.1       1.2       0.75   13.9\n10 Slow … Nyct… carni Prim… &lt;NA&gt;                11        NA        NA      13  \n11 Chimp… Pan   omni  Prim… &lt;NA&gt;                 9.7       1.4       1.42   14.3\n12 Baboon Papio omni  Prim… &lt;NA&gt;                 9.4       1         0.667  14.6\n13 Potto  Pero… omni  Prim… lc                  11        NA        NA      13  \n14 Squir… Saim… omni  Prim… &lt;NA&gt;                 9.6       1.4      NA      14.4\n15 Brazi… Tapi… herbi Peri… vu                   4.4       1         0.9    19.6\n# ℹ 2 more variables: brainwt &lt;dbl&gt;, bodywt &lt;dbl&gt;\n\n\nYou can use the boolean operators (e.g. &gt;, &lt;, &gt;=, &lt;=, !=, %in%) to create the logical tests.",
    "crumbs": [
      "Exploratory data analysis",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Introduction to dplyr: mammal sleep dataset</span>"
    ]
  },
  {
    "objectID": "dplyr_intro_msleep.html#piping-with",
    "href": "dplyr_intro_msleep.html#piping-with",
    "title": "\n11  Introduction to dplyr: mammal sleep dataset\n",
    "section": "\n11.8 “Piping”” with |>\n",
    "text": "11.8 “Piping”” with |&gt;\n\nIt is not unusual to want to perform a set of operations using dplyr. The pipe operator |&gt; allows us to “pipe” the output from one function into the input of the next. While there is nothing special about how R treats operations that are written in a pipe, the idea of piping is to allow us to read multiple functions operating one after another from left-to-right. Without piping, one would either 1) save each step in set of functions as a temporary variable and then pass that variable along the chain or 2) have to “nest” functions, which can be hard to read.\nHere’s an example we have already used:\n\nhead(select(msleep, name, sleep_total))\n\n# A tibble: 6 × 2\n  name                       sleep_total\n  &lt;chr&gt;                            &lt;dbl&gt;\n1 Cheetah                           12.1\n2 Owl monkey                        17  \n3 Mountain beaver                   14.4\n4 Greater short-tailed shrew        14.9\n5 Cow                                4  \n6 Three-toed sloth                  14.4\n\n\nNow in this case, we will pipe the msleep data frame to the function that will select two columns (name and sleep\\_total) and then pipe the new data frame to the function head(), which will return the head of the new data frame.\n\nmsleep |&gt; \n    select(name, sleep_total) |&gt; \n    head()\n\n# A tibble: 6 × 2\n  name                       sleep_total\n  &lt;chr&gt;                            &lt;dbl&gt;\n1 Cheetah                           12.1\n2 Owl monkey                        17  \n3 Mountain beaver                   14.4\n4 Greater short-tailed shrew        14.9\n5 Cow                                4  \n6 Three-toed sloth                  14.4\n\n\nYou will soon see how useful the pipe operator is when we start to combine many functions.\nNow that you know about the pipe operator (|&gt;), we will use it throughout the rest of this tutorial.\n\n11.8.1 Arrange Or Re-order Rows Using arrange()\n\nTo arrange (or re-order) rows by a particular column, such as the taxonomic order, list the name of the column you want to arrange the rows by:\n\nmsleep |&gt; arrange(order) |&gt; head()\n\n# A tibble: 6 × 11\n  name    genus vore  order conservation sleep_total sleep_rem sleep_cycle awake\n  &lt;chr&gt;   &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt;              &lt;dbl&gt;     &lt;dbl&gt;       &lt;dbl&gt; &lt;dbl&gt;\n1 Tenrec  Tenr… omni  Afro… &lt;NA&gt;                15.6       2.3      NA       8.4\n2 Cow     Bos   herbi Arti… domesticated         4         0.7       0.667  20  \n3 Roe de… Capr… herbi Arti… lc                   3        NA        NA      21  \n4 Goat    Capri herbi Arti… lc                   5.3       0.6      NA      18.7\n5 Giraffe Gira… herbi Arti… cd                   1.9       0.4      NA      22.1\n6 Sheep   Ovis  herbi Arti… domesticated         3.8       0.6      NA      20.2\n# ℹ 2 more variables: brainwt &lt;dbl&gt;, bodywt &lt;dbl&gt;\n\n\nNow we will select three columns from msleep, arrange the rows by the taxonomic order and then arrange the rows by sleep_total. Finally, show the head of the final data frame:\n\nmsleep |&gt; \n    select(name, order, sleep_total) |&gt;\n    arrange(order, sleep_total) |&gt; \n    head()\n\n# A tibble: 6 × 3\n  name     order        sleep_total\n  &lt;chr&gt;    &lt;chr&gt;              &lt;dbl&gt;\n1 Tenrec   Afrosoricida        15.6\n2 Giraffe  Artiodactyla         1.9\n3 Roe deer Artiodactyla         3  \n4 Sheep    Artiodactyla         3.8\n5 Cow      Artiodactyla         4  \n6 Goat     Artiodactyla         5.3\n\n\nSame as above, except here we filter the rows for mammals that sleep for 16 or more hours, instead of showing the head of the final data frame:\n\nmsleep |&gt; \n    select(name, order, sleep_total) |&gt;\n    arrange(order, sleep_total) |&gt; \n    filter(sleep_total &gt;= 16)\n\n# A tibble: 8 × 3\n  name                   order           sleep_total\n  &lt;chr&gt;                  &lt;chr&gt;                 &lt;dbl&gt;\n1 Big brown bat          Chiroptera             19.7\n2 Little brown bat       Chiroptera             19.9\n3 Long-nosed armadillo   Cingulata              17.4\n4 Giant armadillo        Cingulata              18.1\n5 North American Opossum Didelphimorphia        18  \n6 Thick-tailed opposum   Didelphimorphia        19.4\n7 Owl monkey             Primates               17  \n8 Arctic ground squirrel Rodentia               16.6\n\n\nFor something slightly more complicated do the same as above, except arrange the rows in the sleep_total column in a descending order. For this, use the function desc()\n\nmsleep |&gt; \n    select(name, order, sleep_total) |&gt;\n    arrange(order, desc(sleep_total)) |&gt; \n    filter(sleep_total &gt;= 16)\n\n# A tibble: 8 × 3\n  name                   order           sleep_total\n  &lt;chr&gt;                  &lt;chr&gt;                 &lt;dbl&gt;\n1 Little brown bat       Chiroptera             19.9\n2 Big brown bat          Chiroptera             19.7\n3 Giant armadillo        Cingulata              18.1\n4 Long-nosed armadillo   Cingulata              17.4\n5 Thick-tailed opposum   Didelphimorphia        19.4\n6 North American Opossum Didelphimorphia        18  \n7 Owl monkey             Primates               17  \n8 Arctic ground squirrel Rodentia               16.6",
    "crumbs": [
      "Exploratory data analysis",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Introduction to dplyr: mammal sleep dataset</span>"
    ]
  },
  {
    "objectID": "dplyr_intro_msleep.html#create-new-columns-using-mutate",
    "href": "dplyr_intro_msleep.html#create-new-columns-using-mutate",
    "title": "\n11  Introduction to dplyr: mammal sleep dataset\n",
    "section": "\n11.9 Create New Columns Using mutate()\n",
    "text": "11.9 Create New Columns Using mutate()\n\nThe mutate() function will add new columns to the data frame. Create a new column called rem_proportion, which is the ratio of rem sleep to total amount of sleep.\n\nmsleep |&gt; \n    mutate(rem_proportion = sleep_rem / sleep_total) |&gt;\n    head()\n\n# A tibble: 6 × 12\n  name    genus vore  order conservation sleep_total sleep_rem sleep_cycle awake\n  &lt;chr&gt;   &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt;              &lt;dbl&gt;     &lt;dbl&gt;       &lt;dbl&gt; &lt;dbl&gt;\n1 Cheetah Acin… carni Carn… lc                  12.1      NA        NA      11.9\n2 Owl mo… Aotus omni  Prim… &lt;NA&gt;                17         1.8      NA       7  \n3 Mounta… Aplo… herbi Rode… nt                  14.4       2.4      NA       9.6\n4 Greate… Blar… omni  Sori… lc                  14.9       2.3       0.133   9.1\n5 Cow     Bos   herbi Arti… domesticated         4         0.7       0.667  20  \n6 Three-… Brad… herbi Pilo… &lt;NA&gt;                14.4       2.2       0.767   9.6\n# ℹ 3 more variables: brainwt &lt;dbl&gt;, bodywt &lt;dbl&gt;, rem_proportion &lt;dbl&gt;\n\n\nYou can add many new columns using mutate (separated by commas). Here we add a second column called bodywt_grams which is the bodywt column in grams.\n\nmsleep |&gt; \n    mutate(rem_proportion = sleep_rem / sleep_total, \n           bodywt_grams = bodywt * 1000) |&gt;\n    head()\n\n# A tibble: 6 × 13\n  name    genus vore  order conservation sleep_total sleep_rem sleep_cycle awake\n  &lt;chr&gt;   &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt;              &lt;dbl&gt;     &lt;dbl&gt;       &lt;dbl&gt; &lt;dbl&gt;\n1 Cheetah Acin… carni Carn… lc                  12.1      NA        NA      11.9\n2 Owl mo… Aotus omni  Prim… &lt;NA&gt;                17         1.8      NA       7  \n3 Mounta… Aplo… herbi Rode… nt                  14.4       2.4      NA       9.6\n4 Greate… Blar… omni  Sori… lc                  14.9       2.3       0.133   9.1\n5 Cow     Bos   herbi Arti… domesticated         4         0.7       0.667  20  \n6 Three-… Brad… herbi Pilo… &lt;NA&gt;                14.4       2.2       0.767   9.6\n# ℹ 4 more variables: brainwt &lt;dbl&gt;, bodywt &lt;dbl&gt;, rem_proportion &lt;dbl&gt;,\n#   bodywt_grams &lt;dbl&gt;\n\n\nIs there a relationship between rem_proportion and bodywt? How about sleep_total?\n\n11.9.1 Create summaries: summarise()\n\nThe summarise() function will create summary statistics for a given column in the data frame such as finding the mean. For example, to compute the average number of hours of sleep, apply the mean() function to the column sleep_total and call the summary value avg_sleep.\n\nmsleep |&gt; \n    summarise(avg_sleep = mean(sleep_total))\n\n# A tibble: 1 × 1\n  avg_sleep\n      &lt;dbl&gt;\n1      10.4\n\n\nThere are many other summary statistics you could consider such sd(), min(), max(), median(), sum(), n() (returns the length of vector), first() (returns first value in vector), last() (returns last value in vector) and n_distinct() (number of distinct values in vector).\n\nmsleep |&gt; \n    summarise(avg_sleep = mean(sleep_total), \n              min_sleep = min(sleep_total),\n              max_sleep = max(sleep_total),\n              total = n())\n\n# A tibble: 1 × 4\n  avg_sleep min_sleep max_sleep total\n      &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt; &lt;int&gt;\n1      10.4       1.9      19.9    83",
    "crumbs": [
      "Exploratory data analysis",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Introduction to dplyr: mammal sleep dataset</span>"
    ]
  },
  {
    "objectID": "dplyr_intro_msleep.html#grouping-data-group_by",
    "href": "dplyr_intro_msleep.html#grouping-data-group_by",
    "title": "\n11  Introduction to dplyr: mammal sleep dataset\n",
    "section": "\n11.10 Grouping data: group_by()\n",
    "text": "11.10 Grouping data: group_by()\n\nThe group_by() verb is an important function in dplyr. The group_by allows us to use the concept of “split-apply-combine”. We literally want to split the data frame by some variable (e.g. taxonomic order), apply a function to the individual data frames and then combine the output. This approach is similar to the aggregate function from R, but group_by integrates with dplyr.\nLet’s do that: split the msleep data frame by the taxonomic order, then ask for the same summary statistics as above. We expect a set of summary statistics for each taxonomic order.\n\nmsleep |&gt; \n    group_by(order) |&gt;\n    summarise(avg_sleep = mean(sleep_total), \n              min_sleep = min(sleep_total), \n              max_sleep = max(sleep_total),\n              total = n())\n\n# A tibble: 19 × 5\n   order           avg_sleep min_sleep max_sleep total\n   &lt;chr&gt;               &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt; &lt;int&gt;\n 1 Afrosoricida        15.6       15.6      15.6     1\n 2 Artiodactyla         4.52       1.9       9.1     6\n 3 Carnivora           10.1        3.5      15.8    12\n 4 Cetacea              4.5        2.7       5.6     3\n 5 Chiroptera          19.8       19.7      19.9     2\n 6 Cingulata           17.8       17.4      18.1     2\n 7 Didelphimorphia     18.7       18        19.4     2\n 8 Diprotodontia       12.4       11.1      13.7     2\n 9 Erinaceomorpha      10.2       10.1      10.3     2\n10 Hyracoidea           5.67       5.3       6.3     3\n11 Lagomorpha           8.4        8.4       8.4     1\n12 Monotremata          8.6        8.6       8.6     1\n13 Perissodactyla       3.47       2.9       4.4     3\n14 Pilosa              14.4       14.4      14.4     1\n15 Primates            10.5        8        17      12\n16 Proboscidea          3.6        3.3       3.9     2\n17 Rodentia            12.5        7        16.6    22\n18 Scandentia           8.9        8.9       8.9     1\n19 Soricomorpha        11.1        8.4      14.9     5",
    "crumbs": [
      "Exploratory data analysis",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Introduction to dplyr: mammal sleep dataset</span>"
    ]
  },
  {
    "objectID": "eda_and_univariate_brfss.html",
    "href": "eda_and_univariate_brfss.html",
    "title": "\n12  Case Study: Behavioral Risk Factor Surveillance System\n",
    "section": "",
    "text": "12.1 A Case Study on the Behavioral Risk Factor Surveillance System\nThe Behavioral Risk Factor Surveillance System (BRFSS) is a large-scale health survey conducted annually by the Centers for Disease Control and Prevention (CDC) in the United States. The BRFSS collects information on various health-related behaviors, chronic health conditions, and the use of preventive services among the adult population (18 years and older) through telephone interviews. The main goal of the BRFSS is to identify and monitor the prevalence of risk factors associated with chronic diseases, inform public health policies, and evaluate the effectiveness of health promotion and disease prevention programs. The data collected through BRFSS is crucial for understanding the health status and needs of the population, and it serves as a valuable resource for researchers, policy makers, and healthcare professionals in making informed decisions and designing targeted interventions.\nIn this chapter, we will walk through an exploratory data analysis (EDA) of the Behavioral Risk Factor Surveillance System dataset using R. EDA is an important step in the data analysis process, as it helps you to understand your data, identify trends, and detect any anomalies before performing more advanced analyses. We will use various R functions and packages to explore the dataset, with a focus on active learning and hands-on experience.",
    "crumbs": [
      "Exploratory data analysis",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Case Study: Behavioral Risk Factor Surveillance System</span>"
    ]
  },
  {
    "objectID": "eda_and_univariate_brfss.html#loading-the-dataset",
    "href": "eda_and_univariate_brfss.html#loading-the-dataset",
    "title": "\n12  Case Study: Behavioral Risk Factor Surveillance System\n",
    "section": "\n12.2 Loading the Dataset",
    "text": "12.2 Loading the Dataset\nFirst, let’s load the dataset into R. We will use the read.csv() function from the base R package to read the data and store it in a data frame called brfss. Make sure the CSV file is in your working directory, or provide the full path to the file.\nFirst, we need to get the data. Either download the data from THIS LINK or have R do it directly from the command-line (preferred):\n\ndownload.file('https://raw.githubusercontent.com/seandavi/ITR/master/BRFSS-subset.csv',\n              destfile = 'BRFSS-subset.csv')\n\n\n\npath &lt;- file.choose()    # look for BRFSS-subset.csv\n\n\nstopifnot(file.exists(path))\nbrfss &lt;- read.csv(path)",
    "crumbs": [
      "Exploratory data analysis",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Case Study: Behavioral Risk Factor Surveillance System</span>"
    ]
  },
  {
    "objectID": "eda_and_univariate_brfss.html#inspecting-the-data",
    "href": "eda_and_univariate_brfss.html#inspecting-the-data",
    "title": "\n12  Case Study: Behavioral Risk Factor Surveillance System\n",
    "section": "\n12.3 Inspecting the Data",
    "text": "12.3 Inspecting the Data\nOnce the data is loaded, let’s take a look at the first few rows of the dataset using the head() function:\n\nhead(brfss)\n\n  Age   Weight    Sex Height Year\n1  31 48.98798 Female 157.48 1990\n2  57 81.64663 Female 157.48 1990\n3  43 80.28585   Male 177.80 1990\n4  72 70.30682   Male 170.18 1990\n5  31 49.89516 Female 154.94 1990\n6  58 54.43108 Female 154.94 1990\n\n\nThis will display the first six rows of the dataset, allowing you to get a feel for the data structure and variable types.\nNext, let’s check the dimensions of the dataset using the dim() function:\n\ndim(brfss)\n\n[1] 20000     5\n\n\nThis will return the number of rows and columns in the dataset, which is important to know for subsequent analyses.",
    "crumbs": [
      "Exploratory data analysis",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Case Study: Behavioral Risk Factor Surveillance System</span>"
    ]
  },
  {
    "objectID": "eda_and_univariate_brfss.html#summary-statistics",
    "href": "eda_and_univariate_brfss.html#summary-statistics",
    "title": "\n12  Case Study: Behavioral Risk Factor Surveillance System\n",
    "section": "\n12.4 Summary Statistics",
    "text": "12.4 Summary Statistics\nNow that we have a basic understanding of the data structure, let’s calculate some summary statistics. The summary() function in R provides a quick overview of the main statistics for each variable in the dataset:\n\nsummary(brfss)\n\n      Age            Weight           Sex                Height     \n Min.   :18.00   Min.   : 34.93   Length:20000       Min.   :105.0  \n 1st Qu.:36.00   1st Qu.: 61.69   Class :character   1st Qu.:162.6  \n Median :51.00   Median : 72.57   Mode  :character   Median :168.0  \n Mean   :50.99   Mean   : 75.42                      Mean   :169.2  \n 3rd Qu.:65.00   3rd Qu.: 86.18                      3rd Qu.:177.8  \n Max.   :99.00   Max.   :278.96                      Max.   :218.0  \n NA's   :139     NA's   :649                         NA's   :184    \n      Year     \n Min.   :1990  \n 1st Qu.:1990  \n Median :2000  \n Mean   :2000  \n 3rd Qu.:2010  \n Max.   :2010  \n               \n\n\nThis will display the minimum, first quartile, median, mean, third quartile, and maximum for each numeric variable, and the frequency counts for each factor level for categorical variables.",
    "crumbs": [
      "Exploratory data analysis",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Case Study: Behavioral Risk Factor Surveillance System</span>"
    ]
  },
  {
    "objectID": "eda_and_univariate_brfss.html#data-visualization",
    "href": "eda_and_univariate_brfss.html#data-visualization",
    "title": "\n12  Case Study: Behavioral Risk Factor Surveillance System\n",
    "section": "\n12.5 Data Visualization",
    "text": "12.5 Data Visualization\nVisualizing the data can help you identify patterns and trends in the dataset. Let’s start by creating a histogram of the Age variable using the hist() function.\nThis will create a histogram showing the frequency distribution of ages in the dataset. You can customize the appearance of the histogram by adjusting the parameters within the hist() function.\n\nhist(brfss$Age, main = \"Age Distribution\", \n     xlab = \"Age\", col = \"lightblue\")\n\n\n\n\n\n\n\n\n\n\n\n\n\nWhat are the options for a histogram?\n\n\n\nThe hist() function has many options. For example, you can change the number of bins, the color of the bars, the title, and the x-axis label. You can also add a vertical line at the mean or median, or add a normal curve to the histogram. For more information, type ?hist in the R console.\nMore generally, it is important to understand the options available for each function you use. You can do this by reading the documentation for the function, which can be accessed by typing ?function_name or help(\"function_name\")in the R console.\n\n\nNext, let’s create a boxplot to compare the distribution of Weight between males and females. We will use the boxplot() function for this. This will create a boxplot comparing the weight distribution between males and females. You can customize the appearance of the boxplot by adjusting the parameters within the boxplot() function.\n\nboxplot(brfss$Weight ~ brfss$Sex, main = \"Weight Distribution by Sex\", \n        xlab = \"Sex\", ylab = \"Weight\", col = c(\"pink\", \"lightblue\"))",
    "crumbs": [
      "Exploratory data analysis",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Case Study: Behavioral Risk Factor Surveillance System</span>"
    ]
  },
  {
    "objectID": "eda_and_univariate_brfss.html#analyzing-relationships-between-variables",
    "href": "eda_and_univariate_brfss.html#analyzing-relationships-between-variables",
    "title": "\n12  Case Study: Behavioral Risk Factor Surveillance System\n",
    "section": "\n12.6 Analyzing Relationships Between Variables",
    "text": "12.6 Analyzing Relationships Between Variables\nTo further explore the data, let’s investigate the relationship between age and weight using a scatterplot. We will use the plot() function for this:\nThis will create a scatterplot of age and weight, allowing you to visually assess the relationship between these two variables.\n\nplot(brfss$Age, brfss$Weight, main = \"Scatterplot of Age and Weight\", \n     xlab = \"Age\", ylab = \"Weight\", col = \"darkblue\")  \n\n\n\n\n\n\n\nTo quantify the strength of the relationship between age and weight, we can calculate the correlation coefficient using the cor() function:\nThis will return the correlation coefficient between age and weight, which can help you determine whether there is a linear relationship between these variables.\n\ncor(brfss$Age, brfss$Weight)\n\n[1] NA\n\n\nWhy does cor() give a value of NA? What can we do about it? A quick glance at help(\"cor\") will give you the answer.\n\ncor(brfss$Age, brfss$Weight, use = \"complete.obs\")\n\n[1] 0.02699989",
    "crumbs": [
      "Exploratory data analysis",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Case Study: Behavioral Risk Factor Surveillance System</span>"
    ]
  },
  {
    "objectID": "eda_and_univariate_brfss.html#exercises",
    "href": "eda_and_univariate_brfss.html#exercises",
    "title": "\n12  Case Study: Behavioral Risk Factor Surveillance System\n",
    "section": "\n12.7 Exercises",
    "text": "12.7 Exercises\n\n\nWhat is the mean weight in this dataset? How about the median? What is the difference between the two? What does this tell you about the distribution of weights in the dataset?\n\nShow answermean(brfss$Weight, na.rm = TRUE)\n\n[1] 75.42455\n\nShow answermedian(brfss$Weight, na.rm = TRUE)\n\n[1] 72.57478\n\nShow answermean(brfss$Weight, na.rm=TRUE) - median(brfss$Weight, na.rm = TRUE)\n\n[1] 2.849774\n\n\n\n\nGiven the findings about the mean and median in the previous exercise, use the hist() function to create a histogram of the weight distribution in this dataset. How would you describe the shape of this distribution?\n\nShow answerhist(brfss$Weight, xlab=\"Weight (kg)\", breaks = 30)\n\n\n\n\n\n\n\n\n\nUse plot() to examine the relationship between height and weight in this dataset.\n\nShow answerplot(brfss$Height, brfss$Weight)\n\n\n\n\n\n\n\n\n\nWhat is the correlation between height and weight? What does this tell you about the relationship between these two variables?\n\nShow answercor(brfss$Height, brfss$Weight, use = \"complete.obs\")\n\n[1] 0.5140928\n\n\n\n\nCreate a histogram of the height distribution in this dataset. How would you describe the shape of this distribution?\n\nShow answerhist(brfss$Height, xlab=\"Height (cm)\", breaks = 30)",
    "crumbs": [
      "Exploratory data analysis",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Case Study: Behavioral Risk Factor Surveillance System</span>"
    ]
  },
  {
    "objectID": "eda_and_univariate_brfss.html#conclusion",
    "href": "eda_and_univariate_brfss.html#conclusion",
    "title": "\n12  Case Study: Behavioral Risk Factor Surveillance System\n",
    "section": "\n12.8 Conclusion",
    "text": "12.8 Conclusion\nIn this chapter, we have demonstrated how to perform an exploratory data analysis on the Behavioral Risk Factor Surveillance System dataset using R. We covered data loading, inspection, summary statistics, visualization, and the analysis of relationships between variables. By actively engaging with the R code and data, you have gained valuable experience in using R for EDA and are well-equipped to tackle more complex analyses in your future work.\nRemember that EDA is just the beginning of the data analysis process, and further statistical modeling and hypothesis testing will likely be necessary to draw meaningful conclusions from your data. However, EDA is a crucial step in understanding your data and informing your subsequent analyses.",
    "crumbs": [
      "Exploratory data analysis",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Case Study: Behavioral Risk Factor Surveillance System</span>"
    ]
  },
  {
    "objectID": "eda_and_univariate_brfss.html#learn-about-the-data",
    "href": "eda_and_univariate_brfss.html#learn-about-the-data",
    "title": "\n12  Case Study: Behavioral Risk Factor Surveillance System\n",
    "section": "\n12.9 Learn about the data",
    "text": "12.9 Learn about the data\nUsing the data exploration techniques you have seen to explore the brfss dataset.\n\nsummary()\ndim()\ncolnames()\nhead()\ntail()\nclass()\nView()\n\nYou may want to investigate individual columns visually using plotting like hist(). For categorical data, consider using something like table().",
    "crumbs": [
      "Exploratory data analysis",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Case Study: Behavioral Risk Factor Surveillance System</span>"
    ]
  },
  {
    "objectID": "eda_and_univariate_brfss.html#clean-data",
    "href": "eda_and_univariate_brfss.html#clean-data",
    "title": "\n12  Case Study: Behavioral Risk Factor Surveillance System\n",
    "section": "\n12.10 Clean data",
    "text": "12.10 Clean data\nR read Year as an integer value, but it’s really a factor\n\nbrfss$Year &lt;- factor(brfss$Year)",
    "crumbs": [
      "Exploratory data analysis",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Case Study: Behavioral Risk Factor Surveillance System</span>"
    ]
  },
  {
    "objectID": "eda_and_univariate_brfss.html#weight-in-1990-vs.-2010-females",
    "href": "eda_and_univariate_brfss.html#weight-in-1990-vs.-2010-females",
    "title": "\n12  Case Study: Behavioral Risk Factor Surveillance System\n",
    "section": "\n12.11 Weight in 1990 vs. 2010 Females",
    "text": "12.11 Weight in 1990 vs. 2010 Females\n\nCreate a subset of the data\n\n\nbrfssFemale &lt;- brfss[brfss$Sex == \"Female\",]\nsummary(brfssFemale)\n\n      Age            Weight           Sex                Height     \n Min.   :18.00   Min.   : 36.29   Length:12039       Min.   :105.0  \n 1st Qu.:37.00   1st Qu.: 57.61   Class :character   1st Qu.:157.5  \n Median :52.00   Median : 65.77   Mode  :character   Median :163.0  \n Mean   :51.92   Mean   : 69.05                      Mean   :163.3  \n 3rd Qu.:67.00   3rd Qu.: 77.11                      3rd Qu.:168.0  \n Max.   :99.00   Max.   :272.16                      Max.   :200.7  \n NA's   :103     NA's   :560                         NA's   :140    \n   Year     \n 1990:5718  \n 2010:6321  \n            \n            \n            \n            \n            \n\n\n\nVisualize\n\n\nplot(Weight ~ Year, brfssFemale)\n\n\n\n\n\n\n\n\nStatistical test\n\n\nt.test(Weight ~ Year, brfssFemale)\n\n\n    Welch Two Sample t-test\n\ndata:  Weight by Year\nt = -27.133, df = 11079, p-value &lt; 2.2e-16\nalternative hypothesis: true difference in means between group 1990 and group 2010 is not equal to 0\n95 percent confidence interval:\n -8.723607 -7.548102\nsample estimates:\nmean in group 1990 mean in group 2010 \n          64.81838           72.95424",
    "crumbs": [
      "Exploratory data analysis",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Case Study: Behavioral Risk Factor Surveillance System</span>"
    ]
  },
  {
    "objectID": "eda_and_univariate_brfss.html#weight-and-height-in-2010-males",
    "href": "eda_and_univariate_brfss.html#weight-and-height-in-2010-males",
    "title": "\n12  Case Study: Behavioral Risk Factor Surveillance System\n",
    "section": "\n12.12 Weight and height in 2010 Males",
    "text": "12.12 Weight and height in 2010 Males\n\nCreate a subset of the data\n\n\nbrfss2010Male &lt;- subset(brfss,  Year == 2010 & Sex == \"Male\")\nsummary(brfss2010Male)\n\n      Age            Weight           Sex                Height      Year     \n Min.   :18.00   Min.   : 36.29   Length:3679        Min.   :135   1990:   0  \n 1st Qu.:45.00   1st Qu.: 77.11   Class :character   1st Qu.:173   2010:3679  \n Median :57.00   Median : 86.18   Mode  :character   Median :178              \n Mean   :56.25   Mean   : 88.85                      Mean   :178              \n 3rd Qu.:68.00   3rd Qu.: 99.79                      3rd Qu.:183              \n Max.   :99.00   Max.   :278.96                      Max.   :218              \n NA's   :30      NA's   :49                          NA's   :31               \n\n\n\nVisualize the relationship\n\n\nhist(brfss2010Male$Weight)\n\n\n\n\n\n\nhist(brfss2010Male$Height)\n\n\n\n\n\n\nplot(Weight ~ Height, brfss2010Male)\n\n\n\n\n\n\n\n\nFit a linear model (regression)\n\n\nfit &lt;- lm(Weight ~ Height, brfss2010Male)\nfit\n\n\nCall:\nlm(formula = Weight ~ Height, data = brfss2010Male)\n\nCoefficients:\n(Intercept)       Height  \n   -86.8747       0.9873  \n\n\nSummarize as ANOVA table\n\nanova(fit)\n\nAnalysis of Variance Table\n\nResponse: Weight\n            Df  Sum Sq Mean Sq F value    Pr(&gt;F)    \nHeight       1  197664  197664   693.8 &lt; 2.2e-16 ***\nResiduals 3617 1030484     285                      \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\nPlot points, superpose fitted regression line; where am I?\n\n\nplot(Weight ~ Height, brfss2010Male)\nabline(fit, col=\"blue\", lwd=2)\n# Substitute your own weight and height...\npoints(73 * 2.54, 178 / 2.2, col=\"red\", cex=4, pch=20)\n\n\n\n\n\n\n\n\nClass and available ‘methods’\n\n\nclass(fit)                 # 'noun'\nmethods(class=class(fit))  # 'verb'\n\n\nDiagnostics\n\n\nplot(fit)\n# Note that the \"plot\" above does not have a \".lm\"\n# However, R will use \"plot.lm\". Why?\n?plot.lm",
    "crumbs": [
      "Exploratory data analysis",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Case Study: Behavioral Risk Factor Surveillance System</span>"
    ]
  },
  {
    "objectID": "norm.html",
    "href": "norm.html",
    "title": "\n13  Working with distribution functions\n",
    "section": "",
    "text": "13.1 pnorm\nThis function gives the probability function for a normal distribution. If you do not specify the mean and standard deviation, R defaults to standard normal. Figure 13.1\npnorm(q, mean = 0, sd = 1, lower.tail = TRUE, log.p = FALSE)\nThe R help file for pnorm provides the template above. The value you input for q is a value on the x-axis, and the returned value is the area under the distribution curve to the left of that point.\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\nℹ Please use `linewidth` instead.\nThis function gives the probability function for a normal distribution. If you do not specify the mean and standard deviation, R defaults to standard normal.\npnorm(q, mean = 0, sd = 1, lower.tail = TRUE, log.p = FALSE) The R help file for pnorm provides the template above. The value you input for q is a value on the x-axis, and the returned value is the area under the distribution curve to the left of that point.\nThe option lower.tail = TRUE tells R to use the area to the left of the given point. This is the default, so will remain true even without entering it. In order to compute the area to the right of the given point, you can either switch to lower.tail = FALSE, or simply calculate 1-pnorm() instead. This is demonstrated below.\nFigure 13.1: The pnorm function takes a quantile (value on the x-axis) and returns the area under the curve to the left of that value.\n\n\n\n\n\n\n\n\n\nFigure 13.2: The pnorm function takes a quantile (value on the x-axis) and returns the area under the curve to the left of that value.\n\n\n\n\n\n\n\n\n\nFigure 13.3: The pnorm function takes a quantile (value on the x-axis) and returns the area under the curve to the left of that value.\n\n\n\n\n\n\n\n\n\nFigure 13.4: The pnorm function takes a quantile (value on the x-axis) and returns the area under the curve to the left of that value.\nThe option lower.tail = TRUE tells R to use the area to the left of the given point. This is the default, so will remain true even without entering it. In order to compute the area to the right of the given point, you can either switch to lower.tail = FALSE, or simply calculate 1-pnorm() instead.",
    "crumbs": [
      "statististics",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Working with distribution functions</span>"
    ]
  },
  {
    "objectID": "norm.html#dnorm",
    "href": "norm.html#dnorm",
    "title": "\n13  Working with distribution functions\n",
    "section": "\n13.2 dnorm",
    "text": "13.2 dnorm\nThis function calculates the probability density function (PDF) for the normal distribution. It gives the probability density (height of the curve) at a specified value (x).\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 13.5: The dnorm function returns the height of the normal distribution at a given point.\n\n\n\n\n\n\n\n\n\nFigure 13.6: The dnorm function returns the height of the normal distribution at a given point.\n\n\n\n\n\n\n\n\n\nFigure 13.7: The dnorm function returns the height of the normal distribution at a given point.",
    "crumbs": [
      "statististics",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Working with distribution functions</span>"
    ]
  },
  {
    "objectID": "norm.html#qnorm",
    "href": "norm.html#qnorm",
    "title": "\n13  Working with distribution functions\n",
    "section": "\n13.3 qnorm",
    "text": "13.3 qnorm\nThis function calculates the quantiles of the normal distribution. It returns the value (x) corresponding to a specified probability (p). It is the inverse of thepnorm function.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 13.8: The qnorm function is the inverse of the pnorm function in that it takes a probability and gives the quantile.\n\n\n\n\n\n\n\n\n\nFigure 13.9: The qnorm function is the inverse of the pnorm function in that it takes a probability and gives the quantile.\n\n\n\n\n\n\n\n\n\n\n\nFigure 13.10: The qnorm function is the inverse of the pnorm function in that it takes a probability and gives the quantile.\n\n\n\n\n\n\n\n\n\nFigure 13.11: The qnorm function is the inverse of the pnorm function in that it takes a probability and gives the quantile.\n\n\n\n\n\n\n\n\n\n\n\nFigure 13.12: The qnorm function is the inverse of the pnorm function in that it takes a probability and gives the quantile.",
    "crumbs": [
      "statististics",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Working with distribution functions</span>"
    ]
  },
  {
    "objectID": "norm.html#rnorm",
    "href": "norm.html#rnorm",
    "title": "\n13  Working with distribution functions\n",
    "section": "\n13.4 rnorm",
    "text": "13.4 rnorm\n\n\n\n\nprint(r1)\n\n\n\n\n\n\nFigure 13.13: The rnorm function takes a number of samples and returns a vector of random numbers from the normal distribution (with mean=0, sd=1 as defaults)",
    "crumbs": [
      "statististics",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Working with distribution functions</span>"
    ]
  },
  {
    "objectID": "norm.html#iq-scores",
    "href": "norm.html#iq-scores",
    "title": "\n13  Working with distribution functions\n",
    "section": "\n13.5 IQ scores",
    "text": "13.5 IQ scores\nNormal Distribution and its Application with IQ\nThe normal distribution, also known as the Gaussian distribution, is a continuous probability distribution characterized by its bell-shaped curve. It is defined by two parameters: the mean (µ) and the standard deviation (σ). The mean represents the central tendency of the distribution, while the standard deviation represents the dispersion or spread of the data.\nThe IQ scores are an excellent example of the normal distribution, as they are designed to follow this distribution pattern. The mean IQ score is set at 100, and the standard deviation is set at 15. This means that the majority of the population (about 68%) have an IQ score between 85 and 115, while 95% of the population have an IQ score between 70 and 130.\n\n\nWhat is the probability of having an IQ score between 85 and 115?\n\nShow answerpnorm(115, mean = 100, sd = 15) - pnorm(85, mean = 100, sd = 15)\n\n\n\n\nWhat is the 90th percentile of the IQ scores?\n\nShow answerqnorm(0.9, mean = 100, sd = 15)\n\n\n\n\nWhat is the probability of having an IQ score above 130?\n\nShow answer1 - pnorm(130, mean = 100, sd = 15)\n\n\n\n\nWhat is the probability of having an IQ score below 70?\n\nShow answerpnorm(70, mean = 100, sd = 15)",
    "crumbs": [
      "statististics",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Working with distribution functions</span>"
    ]
  },
  {
    "objectID": "t-stats-and-tests.html",
    "href": "t-stats-and-tests.html",
    "title": "\n14  The t-statistic and t-distribution\n",
    "section": "",
    "text": "14.1 Background\nThe t-test is a statistical hypothesis test that is commonly used when the data are normally distributed (follow a normal distribution) if the value of the population standard deviation were known. When the population standard deviation is not known and is replaced by an estimate based no the data, the test statistic follows a Student’s t distribution.\nT-tests are handy hypothesis tests in statistics when you want to compare means. You can compare a sample mean to a hypothesized or target value using a one-sample t-test. You can compare the means of two groups with a two-sample t-test. If you have two groups with paired observations (e.g., before and after measurements), use the paired t-test.\nA t-test looks at the t-statistic, the t-distribution values, and the degrees of freedom to determine the statistical significance. To conduct a test with three or more means, we would use an analysis of variance.\nThe distriubution that the t-statistic follows was described in a famous paper (Student 1908) by “Student”, a pseudonym for William Sealy Gosset.",
    "crumbs": [
      "statististics",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>The t-statistic and t-distribution</span>"
    ]
  },
  {
    "objectID": "t-stats-and-tests.html#the-z-score-and-probability",
    "href": "t-stats-and-tests.html#the-z-score-and-probability",
    "title": "\n14  The t-statistic and t-distribution\n",
    "section": "\n14.2 The Z-score and probability",
    "text": "14.2 The Z-score and probability\nBefore talking about the t-distribution and t-scores, lets review the Z-score, its relation to the normal distribution, and probability.\nThe Z-score is defined as:\n\\[Z = \\frac{x - \\mu}{\\sigma} \\tag{14.1}\\]\nwhere \\(\\mu\\) is a the population mean from which \\(x\\) is drawn and \\(\\sigma\\) is the population standard deviation (taken as known, not estimated from the data).\nThe probability of observing a \\(Z\\) score of \\(z\\) or greater can be calculated by \\(pnorm(z,\\mu,\\sigma)\\).\nFor example, let’s assume that our “population” is known and it truly has a mean 0 and standard deviation 1. If we have observations drawn from that population, we can assign a probability of seeing that observation by random chance under the assumption that the null hypothesis is TRUE.\n\nzscore = seq(-5,5,1)\n\nFor each value of zscore, let’s calculate the p-value and put the results in a data.frame.\n\ndf = data.frame(\n    zscore = zscore,\n    pval   = pnorm(zscore, 0, 1)\n)\ndf\n\n   zscore         pval\n1      -5 2.866516e-07\n2      -4 3.167124e-05\n3      -3 1.349898e-03\n4      -2 2.275013e-02\n5      -1 1.586553e-01\n6       0 5.000000e-01\n7       1 8.413447e-01\n8       2 9.772499e-01\n9       3 9.986501e-01\n10      4 9.999683e-01\n11      5 9.999997e-01\n\n\nWhy is the p-value of something 5 population standard deviations away from the mean (zscore=5) nearly 1 in this calculation? What is the default for pnorm with respect to being one-sided or two-sided?\nLet’s plot the values of probability vs z-score:\n\nplot(df$zscore, df$pval, type='b')\n\n\n\n\n\n\n\nThis plot is the empirical cumulative density function (cdf) for our data. How can we use it? If we know the z-score, we can look up the probability of observing that value. Since we have constructed our experiment to follow the standard normal distribution, this cdf also represents the cdf of the standard normal distribution.\n\n14.2.1 Small diversion: two-sided pnorm function\nThe pnorm function returns the “one-sided” probability of having a value at least as extreme as the observed \\(x\\) and uses the “lower” tail by default. Let’s create a function that computes two-sided p-values.\n\nTake the absolute value of x\nCompute pnorm with lower.tail=FALSE so we get lower p-values with larger values of \\(x\\).\nSince we want to include both tails, we need to multiply the area (probability) returned by pnorm by 2.\n\n\ntwosidedpnorm = function(x,mu=0,sd=1) {\n    2*pnorm(abs(x),mu,sd,lower.tail=FALSE)\n}\n\nAnd we can test this to see how likely it is to be 2 or 3 standard deviations from the mean:\n\ntwosidedpnorm(2)\n\n[1] 0.04550026\n\ntwosidedpnorm(3)\n\n[1] 0.002699796",
    "crumbs": [
      "statististics",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>The t-statistic and t-distribution</span>"
    ]
  },
  {
    "objectID": "t-stats-and-tests.html#the-t-distribution",
    "href": "t-stats-and-tests.html#the-t-distribution",
    "title": "\n14  The t-statistic and t-distribution\n",
    "section": "\n14.3 The t-distribution",
    "text": "14.3 The t-distribution\nWe spent time above working with z-scores and probability. An important aspect of working with the normal distribution is that we MUST assume that we know the standard deviation. Remember that the Z-score is defined as:\n\\[Z = \\frac{x - \\mu}{\\sigma}\\]\nThe formula for the population standard deviation is:\n\\[\\sigma = \\sqrt{\\frac{1}{N}\\sum_{i=1}^{N}({xi - \\mu)^2}} \\tag{14.2}\\]\nIn general, the population standard deviation is taken as “known” as we did above.\nIf we do not but only have a sample from the population, instead of using the Z-score, we use the t-score defined as:\n\\[t = \\frac{x - \\bar{x}}{s} \\tag{14.3}\\]\nThis looks quite similar to the formula for Z-score, but here we have to estimate the standard deviation, \\(s\\) from the data. The formula for \\(s\\) is:\n\\[s = \\sqrt{\\frac{1}{N-1}\\sum_{i=1}^{N}({x_{i} - \\bar{x})^2}} \\tag{14.4}\\]\nSince we are estimating the standard deviation from the data, this leads to extra variability that shows up as “fatter tails” for smaller sample sizes than for larger sample sizes. We can see this by comparing the t-distribution for various numbers of degrees of freedom (sample sizes).\nWe can look at the effect of sample size on the distributions graphically by looking at the densities for 3, 5, 10, 20 degrees of freedom and the normal distribution:\n\nlibrary(dplyr)\nlibrary(ggplot2)\nt_values = seq(-6,6,0.01)\ndf = data.frame(\n    value = t_values,\n    t_3   = dt(t_values,3),\n    t_6   = dt(t_values,6),\n    t_10  = dt(t_values,10),\n    t_20  = dt(t_values,20),\n    Normal= dnorm(t_values)\n) |&gt;\n    tidyr::gather(\"Distribution\", \"density\", -value)\nggplot(df, aes(x=value, y=density, color=Distribution)) + \n    geom_line()\n\n\n\n\n\n\nFigure 14.1: t-distributions for various degrees of freedom. Note that the tails are fatter for smaller degrees of freedom, which is a result of estimating the standard deviation from the data.\n\n\n\n\nThe dt and dnorm functions give the density of the distributions for each point.\n\ndf2 = df |&gt; \n    group_by(Distribution) |&gt;\n    arrange(value) |&gt; \n    mutate(cdf=cumsum(density))\nggplot(df2, aes(x=value, y=cdf, color=Distribution)) + \n    geom_line()\n\n\n\n\n\n\n\n\n14.3.1 p-values based on Z vs t\nWhen we have a “sample” of data and want to compute the statistical significance of the difference of the mean from the population mean, we calculate the standard deviation of the sample means (standard error).\n\\[z = \\frac{x - \\mu}{\\sigma/\\sqrt{n}}\\]\nLet’s look at the relationship between the p-values of Z (from the normal distribution) vs t for a sample of data.\n\nset.seed(5432)\nsamp = rnorm(5,mean = 0.5)\nz = sqrt(length(samp)) * mean(samp) #simplifying assumption (sigma=1, mu=0)\n\nAnd the p-value if we assume we know the standard deviation:\n\npnorm(z, lower.tail = FALSE)\n\n[1] 0.02428316\n\n\nIn reality, we don’t know the standard deviation, so we have to estimate it from the data. We can do this by calculating the sample standard deviation:\n\nts = sqrt(length(samp)) * mean(samp) / sd(samp)\npnorm(ts, lower.tail = FALSE)\n\n[1] 0.0167297\n\npt(ts,df = length(samp)-1, lower.tail = FALSE)\n\n[1] 0.0503001\n\n\n\n14.3.2 Experiment\nWhen sampling from a normal distribution, we often calculate p-values to test hypotheses or determine the statistical significance of our results. The p-value represents the probability of obtaining a test statistic as extreme or more extreme than the one observed, under the null hypothesis.\nIn a typical scenario, we assume that the population mean and standard deviation are known. However, in many real-life situations, we don’t know the true population standard deviation, and we have to estimate it using the sample standard deviation (Equation 14.4). This estimation introduces some uncertainty into our calculations, which affects the p-values. When we include an estimate of the standard deviation, we switch from using the standard normal (z) distribution to the t-distribution for calculating p-values.\nWhat would happen if we used the normal distribution to calculate p-values when we use the sample standard deviation? Let’s find out!\n\nSimulate a bunch of samples of size n from the standard normal distribution\nCalculate the p-value distribution for those samples based on the normal.\nCalculate the p-value distribution for those samples based on the normal, but with the estimated standard deviation.\nCalculate the p-value distribution for those samples based on the t-distribution.\n\nCreate a function that draws a sample of size n from the standard normal distribution.\n\nzf = function(n) {\n    samp = rnorm(n)\n    z = sqrt(length(samp)) * mean(samp) / 1 #simplifying assumption (sigma=1, mu=0)\n    z\n}\n\nAnd give it a try:\n\nzf(5)\n\n[1] 0.7406094\n\n\nPerform 10000 replicates of our sampling and z-scoring. We are using the assumption that we know the population standard deviation; in this case, we do know since we are sampling from the standard normal distribution.\n\nz10k = replicate(10000,zf(5))\nhist(pnorm(z10k))\n\n\n\n\n\n\n\nAnd do the same, but now creating a t-score function. We are using the assumption that we don’t know the population standard deviation; in this case, we must estimate it from the data. Note the difference in the calculation of the t-score (ts) as compared to the z-score (z).\n\ntf = function(n) {\n    samp = rnorm(n)\n    # now, using the sample standard deviation since we \n    # \"don't know\" the population standard deviation\n    ts = sqrt(length(samp)) * mean(samp) / sd(samp)\n    ts\n}\n\nIf we use those t-scores and calculate the p-values based on the normal distribution, the histogram of those p-values looks like:\n\nt10k = replicate(10000,tf(5))\nhist(pnorm(t10k))\n\n\n\n\n\n\n\nSince we are using the normal distribution to calculate the p-values, we are, in effect, assuming that we know the population standard deviation. This assumption is incorrect, and we can see that the p-values are not uniformly distributed between 0 and 1.\nIf we use those t-scores and calculate the p-values based on the t-distribution, the histogram of those p-values looks like:\n\nhist(pt(t10k,5))\n\n\n\n\n\n\n\nNow, the p-values are uniformly distributed between 0 and 1, as expected.\nWhat is a qqplot and how do we use it? A qqplot is a plot of the quantiles of two distributions against each other. If the two distributions are identical, the points will fall on a straight line. If the two distributions are different, the points will deviate from the straight line. We can use a qqplot to compare the t-distribution to the normal distribution. If the t-distribution is identical to the normal distribution, the points will fall on a straight line. If the t-distribution is different from the normal distribution, the points will deviate from the straight line. In this case, we can see that the t-distribution is different from the normal distribution, as the points deviate from the straight line. What would happen if we increased the sample size? The t-distribution would approach the normal distribution, and the points would fall closer and closer to the straight line.\n\nqqplot(z10k,t10k)\nabline(0,1)",
    "crumbs": [
      "statististics",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>The t-statistic and t-distribution</span>"
    ]
  },
  {
    "objectID": "t-stats-and-tests.html#summary-of-t-distribution-vs-normal-distribution",
    "href": "t-stats-and-tests.html#summary-of-t-distribution-vs-normal-distribution",
    "title": "\n14  The t-statistic and t-distribution\n",
    "section": "\n14.4 Summary of t-distribution vs normal distribution",
    "text": "14.4 Summary of t-distribution vs normal distribution\nThe t-distribution is a family of probability distributions that depends on a parameter called degrees of freedom, which is related to the sample size. The t-distribution approaches the standard normal distribution as the sample size increases but has heavier tails for smaller sample sizes. This means that the t-distribution is more conservative in calculating p-values for small samples, making it harder to reject the null hypothesis. Including an estimate of the standard deviation changes the way we calculate p-values by switching from the standard normal distribution to the t-distribution, which accounts for the uncertainty introduced by estimating the population standard deviation from the sample. This adjustment is particularly important for small sample sizes, as it provides a more accurate assessment of the statistical significance of our results.",
    "crumbs": [
      "statististics",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>The t-statistic and t-distribution</span>"
    ]
  },
  {
    "objectID": "t-stats-and-tests.html#t.test",
    "href": "t-stats-and-tests.html#t.test",
    "title": "\n14  The t-statistic and t-distribution\n",
    "section": "\n14.5 t.test",
    "text": "14.5 t.test\n\n14.5.1 One-sample\nWe are going to use the t.test function to perform a one-sample t-test. The t.test function takes a vector of values as input that represents the sample values. In this case, we’ll simulate our sample using the rnorm function and presume that our “effect-size” is 1.\n\nx = rnorm(20,1)\n# small sample\n# Just use the first 5 values of the sample\nt.test(x[1:5])\n\n\n    One Sample t-test\n\ndata:  x[1:5]\nt = 0.97599, df = 4, p-value = 0.3843\nalternative hypothesis: true mean is not equal to 0\n95 percent confidence interval:\n -1.029600  2.145843\nsample estimates:\nmean of x \n0.5581214 \n\n\nIn this case, we set up the experiment so that the null hypothesis is true (the true mean is not zero, but actually 1). However, we only have a small sample size that leads to a modest p-value.\nIncreasing the sample size allows us to see the effect more clearly.\n\nt.test(x[1:20])\n\n\n    One Sample t-test\n\ndata:  x[1:20]\nt = 3.8245, df = 19, p-value = 0.001144\nalternative hypothesis: true mean is not equal to 0\n95 percent confidence interval:\n 0.3541055 1.2101894\nsample estimates:\nmean of x \n0.7821474 \n\n\n\n14.5.2 two-sample\n\nx = rnorm(10,0.5)\ny = rnorm(10,-0.5)\nt.test(x,y)\n\n\n    Welch Two Sample t-test\n\ndata:  x and y\nt = 3.4296, df = 17.926, p-value = 0.003003\nalternative hypothesis: true difference in means is not equal to 0\n95 percent confidence interval:\n 0.5811367 2.4204048\nsample estimates:\n mean of x  mean of y \n 0.7039205 -0.7968502 \n\n\n\n14.5.3 from a data.frame\nIn some situations, you may have data and groups as columns in a data.frame. See the following data.frame, for example\n\ndf = data.frame(value=c(x,y),group=as.factor(rep(c('g1','g2'),each=10)))\ndf\n\n         value group\n1   1.12896674    g1\n2  -1.26838101    g1\n3   1.04577597    g1\n4   1.69075585    g1\n5   0.18672204    g1\n6   1.99715092    g1\n7   1.15424947    g1\n8   0.37671442    g1\n9  -0.09565723    g1\n10  0.82290783    g1\n11 -1.48530261    g2\n12 -1.29200440    g2\n13 -0.18778362    g2\n14  0.59205742    g2\n15 -2.10065248    g2\n16 -0.29961560    g2\n17 -0.38985115    g2\n18 -2.47126235    g2\n19 -0.63654380    g2\n20  0.30245611    g2\n\n\nR allows us to perform a t-test using the formula notation.\n\nt.test(value ~ group, data=df)\n\n\n    Welch Two Sample t-test\n\ndata:  value by group\nt = 3.4296, df = 17.926, p-value = 0.003003\nalternative hypothesis: true difference in means between group g1 and group g2 is not equal to 0\n95 percent confidence interval:\n 0.5811367 2.4204048\nsample estimates:\nmean in group g1 mean in group g2 \n       0.7039205       -0.7968502 \n\n\nYou read that as value is a function of group. In practice, this will do a t-test between the values in g1 vs g2.\n\n14.5.4 Equivalence to linear model\n\nt.test(value ~ group, data=df, var.equal=TRUE)\n\n\n    Two Sample t-test\n\ndata:  value by group\nt = 3.4296, df = 18, p-value = 0.002989\nalternative hypothesis: true difference in means between group g1 and group g2 is not equal to 0\n95 percent confidence interval:\n 0.5814078 2.4201337\nsample estimates:\nmean in group g1 mean in group g2 \n       0.7039205       -0.7968502 \n\n\nThis is equivalent to:\n\nres = lm(value ~ group, data=df)\nsummary(res)\n\n\nCall:\nlm(formula = value ~ group, data = df)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-1.9723 -0.5600  0.2511  0.5252  1.3889 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)   \n(Intercept)   0.7039     0.3094   2.275  0.03538 * \ngroupg2      -1.5008     0.4376  -3.430  0.00299 **\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.9785 on 18 degrees of freedom\nMultiple R-squared:  0.3952,    Adjusted R-squared:  0.3616 \nF-statistic: 11.76 on 1 and 18 DF,  p-value: 0.002989",
    "crumbs": [
      "statististics",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>The t-statistic and t-distribution</span>"
    ]
  },
  {
    "objectID": "t-stats-and-tests.html#power-calculations",
    "href": "t-stats-and-tests.html#power-calculations",
    "title": "\n14  The t-statistic and t-distribution\n",
    "section": "\n14.6 Power calculations",
    "text": "14.6 Power calculations\nThe power of a statistical test is the probability that the test will reject the null hypothesis when the alternative hypothesis is true. In other words, the power of a statistical test is the probability of not making a Type II error. The power of a statistical test depends on the significance level (alpha), the sample size, and the effect size.\nThe power.t.test function can be used to calculate the power of a one-sample t-test.\nLooking at help(\"power.t.test\"), we see that the function takes the following arguments:\n\n\nn - sample size\n\ndelta - effect size\n\nsd - standard deviation of the sample\n\nsig.level - significance level\n\npower - power\n\nWe need to supply four of these arguments to calculate the fifth. For example, if we want to calculate the power of a one-sample t-test with a sample size of 5, a standard deviation of 1, and an effect size of 1, we can use the following command:\n\npower.t.test(n = 5, delta = 1, sd = 1, sig.level = 0.05)\n\n\n     Two-sample t test power calculation \n\n              n = 5\n          delta = 1\n             sd = 1\n      sig.level = 0.05\n          power = 0.2859276\n    alternative = two.sided\n\nNOTE: n is number in *each* group\n\n\nThis gives a nice summary of the power calculation. We can also extract the power value from the result:\n\npower.t.test(n = 5, delta = 1, sd = 1, \n             sig.level = 0.05, type='one.sample')$power\n\n[1] 0.4013203\n\n\n\n\n\n\n\n\nTip\n\n\n\nWhen getting results from a function that don’t look “computable” such as those from power.t.test, you can use the $ operator to extract the value you want. In this case, we want the power value from the result of power.t.test.\nHow would you know what to extract? You can use the names function or the str function to see the structure of the result. For example:\n\nnames(power.t.test(n = 5, delta = 1, sd = 1, \n             sig.level = 0.05, type='one.sample'))\n\n[1] \"n\"           \"delta\"       \"sd\"          \"sig.level\"   \"power\"      \n[6] \"alternative\" \"note\"        \"method\"     \n\n# or \nstr(power.t.test(n = 5, delta = 1, sd = 1, \n             sig.level = 0.05, type='one.sample'))\n\nList of 8\n $ n          : num 5\n $ delta      : num 1\n $ sd         : num 1\n $ sig.level  : num 0.05\n $ power      : num 0.401\n $ alternative: chr \"two.sided\"\n $ note       : NULL\n $ method     : chr \"One-sample t test power calculation\"\n - attr(*, \"class\")= chr \"power.htest\"\n\n\n\n\nAlternatively, we may know a lot about our experimental system and want to calculate the sample size needed to achieve a certain power. For example, if we want to achieve a power of 0.8 with a standard deviation of 1 and an effect size of 1, we can use the following command:\n\npower.t.test(delta = 1, sd = 1, sig.level = 0.05, power = 0.8, type = \"one.sample\")\n\n\n     One-sample t test power calculation \n\n              n = 9.937864\n          delta = 1\n             sd = 1\n      sig.level = 0.05\n          power = 0.8\n    alternative = two.sided\n\n\nThe power.t.test function is convenient and quite fast. As we’ve seen before, though, sometimes the distribution of the test statistics is now easily calculated. In those cases, we can use simulation to calculate the power of a statistical test. For example, if we want to calculate the power of a one-sample t-test with a sample size of 5, a standard deviation of 1, and an effect size of 1, we can use the following command:\n\nsim_t_test_pval &lt;- function(n = 5, delta = 1, sd = 1, sig.level = 0.05) {\n    x = rnorm(n, delta, sd)\n    t.test(x)$p.value &lt;= sig.level\n}\npow = mean(replicate(1000, sim_t_test_pval()))\npow\n\n[1] 0.405\n\n\nLet’s break this down. First, we define a function called sim_t_test_pval that takes the same arguments as the power.t.test function. Inside the function, we simulate a sample of size n from a normal distribution with mean delta and standard deviation sd. Then, we perform a one-sample t-test on the sample and return a logical value indicating whether the p-value is less than the significance level. Next, we use the replicate function to repeat the simulation 1000 times. Finally, we calculate the proportion of simulations in which the p-value was less than the significance level. This proportion is an estimate of the power of the one-sample t-test.\nLet’s compare the results of the power.t.test function and our simulation-based approach:\n\npower.t.test(n = 5, delta = 1, sd = 1, sig.level = 0.05, type='one.sample')$power\n\n[1] 0.4013203\n\nmean(replicate(1000, sim_t_test_pval(n = 5, delta = 1, sd = 1, sig.level = 0.05)))\n\n[1] 0.414",
    "crumbs": [
      "statististics",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>The t-statistic and t-distribution</span>"
    ]
  },
  {
    "objectID": "t-stats-and-tests.html#resources",
    "href": "t-stats-and-tests.html#resources",
    "title": "\n14  The t-statistic and t-distribution\n",
    "section": "\n14.7 Resources",
    "text": "14.7 Resources\nSee the pwr package for more information on power calculations.\n\n\n\n\nStudent. 1908. “The Probable Error of a Mean.” Biometrika 6 (1): 1–25. https://doi.org/10.2307/2331554.",
    "crumbs": [
      "statististics",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>The t-statistic and t-distribution</span>"
    ]
  },
  {
    "objectID": "kmeans.html",
    "href": "kmeans.html",
    "title": "\n15  K-means clustering\n",
    "section": "",
    "text": "15.1 History of the k-means algorithm\nThe k-means clustering algorithm was first proposed by Stuart Lloyd in 1957 as a technique for pulse-code modulation. However, it was not published until 1982. In 1965, Edward W. Forgy published an essentially identical method, which became widely known as the k-means algorithm. Since then, k-means clustering has become one of the most popular unsupervised learning techniques in data analysis and machine learning.\nK-means clustering is a method for finding patterns or groups in a dataset. It is an unsupervised learning technique, meaning that it doesn’t rely on previously labeled data for training. Instead, it identifies structures or patterns directly from the data based on the similarity between data points (see Figure 15.1).\nIn simple terms, k-means clustering aims to divide a dataset into k distinct groups or clusters, where each data point belongs to the cluster with the nearest mean (average). The goal is to minimize the variability within each cluster while maximizing the differences between clusters. This helps to reveal hidden patterns or relationships in the data that might not be apparent otherwise.",
    "crumbs": [
      "statististics",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>K-means clustering</span>"
    ]
  },
  {
    "objectID": "kmeans.html#history-of-the-k-means-algorithm",
    "href": "kmeans.html#history-of-the-k-means-algorithm",
    "title": "\n15  K-means clustering\n",
    "section": "",
    "text": "Figure 15.1: K-means clustering takes a dataset and divides it into k clusters.",
    "crumbs": [
      "statististics",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>K-means clustering</span>"
    ]
  },
  {
    "objectID": "kmeans.html#the-k-means-algorithm",
    "href": "kmeans.html#the-k-means-algorithm",
    "title": "\n15  K-means clustering\n",
    "section": "\n15.2 The k-means algorithm",
    "text": "15.2 The k-means algorithm\nThe k-means algorithm follows these general steps:\n\nChoose the number of clusters k.\nInitialize the cluster centroids randomly by selecting k data points from the dataset.\nAssign each data point to the nearest centroid.\nUpdate the centroids by computing the mean of all the data points assigned to each centroid.\nRepeat steps 3 and 4 until the centroids no longer change or a certain stopping criterion is met (e.g., a maximum number of iterations).\n\nThe algorithm converges when the centroids stabilize or no longer change significantly. The final clusters represent the underlying patterns or structures in the data. Advantages and disadvantages of k-means clustering",
    "crumbs": [
      "statististics",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>K-means clustering</span>"
    ]
  },
  {
    "objectID": "kmeans.html#pros-and-cons-of-k-means-clustering",
    "href": "kmeans.html#pros-and-cons-of-k-means-clustering",
    "title": "\n15  K-means clustering\n",
    "section": "\n15.3 Pros and cons of k-means clustering",
    "text": "15.3 Pros and cons of k-means clustering\nCompared to other clustering algorithms, k-means has several advantages:\n\n\nSimplicity and ease of implementation\n\nThe k-means algorithm is relatively straightforward and can be easily implemented, even for large datasets.\n\n\n\nScalability\n\nThe algorithm can be adapted for large datasets using various optimization techniques or parallel processing.\n\n\n\nSpeed\n\nK-means is generally faster than other clustering algorithms, especially when the number of clusters k is small.\n\n\n\nInterpretability\n\nThe results of k-means clustering are easy to understand, as the algorithm assigns each data point to a specific cluster based on its similarity to the cluster’s centroid.\n\n\n\nHowever, k-means clustering has several disadvantages as well:\n\n\nChoice of k\n\nSelecting the appropriate number of clusters can be challenging and often requires domain knowledge or experimentation. A poor choice of k may yield poor results.\n\n\n\nSensitivity to initial conditions\n\nThe algorithm’s results can vary depending on the initial placement of centroids. To overcome this issue, the algorithm can be run multiple times with different initializations and the best solution can be chosen based on a criterion (e.g., minimizing within-cluster variation).\n\n\n\nAssumes spherical clusters\n\nK-means assumes that clusters are spherical and evenly sized, which may not always be the case in real-world datasets. This can lead to poor performance if the underlying clusters have different shapes or densities.\n\n\n\nSensitivity to outliers\n\nThe algorithm is sensitive to outliers, which can heavily influence the position of centroids and the final clustering result. Preprocessing the data to remove or mitigate the impact of outliers can help improve the performance of k-means clustering.\n\n\n\nDespite limitations, k-means clustering remains a popular and widely used method for exploring and analyzing data, particularly in biological data analysis, where identifying patterns and relationships can provide valuable insights into complex systems and processes.",
    "crumbs": [
      "statististics",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>K-means clustering</span>"
    ]
  },
  {
    "objectID": "kmeans.html#an-example-of-k-means-clustering",
    "href": "kmeans.html#an-example-of-k-means-clustering",
    "title": "\n15  K-means clustering\n",
    "section": "\n15.4 An example of k-means clustering",
    "text": "15.4 An example of k-means clustering\n\n15.4.1 The data and experimental background\nThe data we are going to use are from DeRisi, Iyer, and Brown (1997). From their abstract:\n\nDNA microarrays containing virtually every gene of Saccharomyces cerevisiae were used to carry out a comprehensive investigation of the temporal program of gene expression accompanying the metabolic shift from fermentation to respiration. The expression profiles observed for genes with known metabolic functions pointed to features of the metabolic reprogramming that occur during the diauxic shift, and the expression patterns of many previously uncharacterized genes provided clues to their possible functions.\n\nThese data are available from NCBI GEO as GSE28.\nIn the case of the baker’s or brewer’s yeast Saccharomyces cerevisiae growing on glucose with plenty of aeration, the diauxic growth pattern is commonly observed in batch culture. During the first growth phase, when there is plenty of glucose and oxygen available, the yeast cells prefer glucose fermentation to aerobic respiration even though aerobic respiration is the more efficient pathway to grow on glucose. This experiment profiles gene expression for 6400 genes over a time course during which the cells are undergoing a diauxic shift.\nThe data in deRisi et al. have no replicates and are time course data. Sometimes, seeing how groups of genes behave can give biological insight into the experimental system or the function of individual genes. We can use clustering to group genes that have a similar expression pattern over time and then potentially look at the genes that do so.\nOur goal, then, is to use kmeans clustering to divide highly variable (informative) genes into groups and then to visualize those groups.",
    "crumbs": [
      "statististics",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>K-means clustering</span>"
    ]
  },
  {
    "objectID": "kmeans.html#getting-data",
    "href": "kmeans.html#getting-data",
    "title": "\n15  K-means clustering\n",
    "section": "\n15.5 Getting data",
    "text": "15.5 Getting data\nThese data were deposited at NCBI GEO back in 2002. GEOquery can pull them out easily.\n\nlibrary(GEOquery)\ngse = getGEO(\"GSE28\")[[1]]\nclass(gse)\n\n[1] \"ExpressionSet\"\nattr(,\"package\")\n[1] \"Biobase\"\n\n\nGEOquery is a little dated and was written before the SummarizedExperiment existed. However, Bioconductor makes a conversion from the old ExpressionSet that GEOquery uses to the SummarizedExperiment that we see so commonly used now.\n\nlibrary(SummarizedExperiment)\ngse = as(gse, \"SummarizedExperiment\")\ngse\n\nclass: SummarizedExperiment \ndim: 6400 7 \nmetadata(3): experimentData annotation protocolData\nassays(1): exprs\nrownames(6400): 1 2 ... 6399 6400\nrowData names(20): ID ORF ... FAILED IS_CONTAMINATED\ncolnames(7): GSM887 GSM888 ... GSM892 GSM893\ncolData names(33): title geo_accession ... supplementary_file\n  data_row_count\n\n\nTaking a quick look at the colData(), it might be that we want to reorder the columns a bit.\n\ncolData(gse)$title\n\n[1] \"diauxic shift timecourse: 15.5 hr\" \"diauxic shift timecourse: 0 hr\"   \n[3] \"diauxic shift timecourse: 18.5 hr\" \"diauxic shift timecourse: 9.5 hr\" \n[5] \"diauxic shift timecourse: 11.5 hr\" \"diauxic shift timecourse: 13.5 hr\"\n[7] \"diauxic shift timecourse: 20.5 hr\"\n\n\nSo, we can reorder by hand to get the time course correct:\n\ngse = gse[, c(2,4,5,6,1,3,7)]",
    "crumbs": [
      "statististics",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>K-means clustering</span>"
    ]
  },
  {
    "objectID": "kmeans.html#preprocessing",
    "href": "kmeans.html#preprocessing",
    "title": "\n15  K-means clustering\n",
    "section": "\n15.6 Preprocessing",
    "text": "15.6 Preprocessing\nIn gene expression data analysis, the primary objective is often to identify genes that exhibit significant differences in expression levels across various conditions, such as diseased vs. healthy samples or different time points in a time-course experiment. However, gene expression datasets are typically large, noisy, and contain numerous genes that do not exhibit substantial changes in expression levels. Analyzing all genes in the dataset can be computationally intensive and may introduce noise or false positives in the results.\nOne common approach to reduce the complexity of the dataset and focus on the most informative genes is to subset the genes based on their standard deviation in expression levels across the samples. The standard deviation is a measure of dispersion or variability in the data, and genes with high standard deviations have more variation in their expression levels across the samples.\nBy selecting genes with high standard deviations, we focus on genes that show relatively large changes in expression levels across different conditions. These genes are more likely to be biologically relevant and involved in the underlying processes or pathways of interest. In contrast, genes with low standard deviations exhibit little or no change in expression levels and are less likely to be informative for the analysis. It turns out that applying filtering based on criteria such as standard deviation can also increase power and reduce false positives in the analysis (Bourgon, Gentleman, and Huber 2010).\nTo subset the genes for analysis based on their standard deviation, the following steps can be followed: Calculate the standard deviation of each gene’s expression levels across all samples. Set a threshold for the standard deviation, which can be determined based on domain knowledge, data distribution, or a specific percentile of the standard deviation values (e.g., selecting the top 10% or 25% of genes with the highest standard deviations). Retain only the genes with a standard deviation above the chosen threshold for further analysis.\nBy subsetting the genes based on their standard deviation, we can reduce the complexity of the dataset, speed up the subsequent analysis, and increase the likelihood of detecting biologically meaningful patterns and relationships in the gene expression data. The threshold for the standard deviation cutoff is rather arbitrary, so it may be beneficial to try a few to check for sensitivity of findings.\n\nsds = apply(assays(gse)[[1]], 1, sd)\nhist(sds)\n\n\n\n\n\n\nFigure 15.2: Histogram of standard deviations for all genes in the deRisi dataset.\n\n\n\n\nExamining the plot, we can see that the most highly variable genes have an sd &gt; 0.8 or so (arbitrary). We can, for convenience, create a new SummarizedExperiment that contains only our most highly variable genes.\n\nidx = sds&gt;0.8 & !is.na(sds)\ngse_sub = gse[idx,]",
    "crumbs": [
      "statististics",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>K-means clustering</span>"
    ]
  },
  {
    "objectID": "kmeans.html#clustering",
    "href": "kmeans.html#clustering",
    "title": "\n15  K-means clustering\n",
    "section": "\n15.7 Clustering",
    "text": "15.7 Clustering\nNow, gse_sub contains a subset of our data.\nThe kmeans function takes a matrix and the number of clusters as arguments.\n\nk = 4\nkm = kmeans(assays(gse_sub)[[1]], 4)\n\nThe km kmeans result contains a vector, km$cluster, which gives the cluster associated with each gene. We can plot the genes for each cluster to see how these different genes behave.\n\nexpression_values = assays(gse_sub)[[1]]\npar(mfrow=c(2,2), mar=c(3,4,1,2)) # this allows multiple plots per page\nfor(i in 1:k) {\n    matplot(t(expression_values[km$cluster==i, ]), type='l', ylim=c(-3,3),\n            ylab = paste(\"cluster\", i))\n}\n\n\n\n\n\n\nFigure 15.3: Gene expression profiles for the four clusters identified by k-means clustering. Each line represents a gene in the cluster, and each column represents a time point in the experiment. Each cluster shows a distinct trend where the genes in the cluster are potentially co-regulated.\n\n\n\n\nTry this with different size k. Perhaps go back to choose more genes (using a smaller cutoff for sd).",
    "crumbs": [
      "statististics",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>K-means clustering</span>"
    ]
  },
  {
    "objectID": "kmeans.html#summary",
    "href": "kmeans.html#summary",
    "title": "\n15  K-means clustering\n",
    "section": "\n15.8 Summary",
    "text": "15.8 Summary\nIn this lesson, we have learned how to use k-means clustering to identify groups of genes that behave similarly over time. We have also learned how to subset our data to focus on the most informative genes.\n\n\n\n\nBourgon, Richard, Robert Gentleman, and Wolfgang Huber. 2010. “Independent Filtering Increases Detection Power for High-Throughput Experiments.” Proceedings of the National Academy of Sciences 107 (21): 9546–51. https://doi.org/10.1073/pnas.0914005107.\n\n\nDeRisi, J. L., V. R. Iyer, and P. O. Brown. 1997. “Exploring the Metabolic and Genetic Control of Gene Expression on a Genomic Scale.” Science (New York, N.Y.) 278 (5338): 680–86. https://doi.org/10.1126/science.278.5338.680.",
    "crumbs": [
      "statististics",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>K-means clustering</span>"
    ]
  },
  {
    "objectID": "geoquery.html",
    "href": "geoquery.html",
    "title": "\n16  Accessing and working with public omics data\n",
    "section": "",
    "text": "16.1 Background\nThe data we are going to access are from this paper.\nIn this little exercise, we will:",
    "crumbs": [
      "Bioconductor",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Accessing and working with public omics data</span>"
    ]
  },
  {
    "objectID": "geoquery.html#background",
    "href": "geoquery.html#background",
    "title": "\n16  Accessing and working with public omics data\n",
    "section": "",
    "text": "Background: The tumor microenvironment is an important factor in cancer immunotherapy response. To further understand how a tumor affects the local immune system, we analyzed immune gene expression differences between matching normal and tumor tissue.Methods: We analyzed public and new gene expression data from solid cancers and isolated immune cell populations. We also determined the correlation between CD8, FoxP3 IHC, and our gene signatures.Results: We observed that regulatory T cells (Tregs) were one of the main drivers of immune gene expression differences between normal and tumor tissue. A tumor-specific CD8 signature was slightly lower in tumor tissue compared with normal of most (12 of 16) cancers, whereas a Treg signature was higher in tumor tissue of all cancers except liver. Clustering by Treg signature found two groups in colorectal cancer datasets. The high Treg cluster had more samples that were consensus molecular subtype 1/4, right-sided, and microsatellite-instable, compared with the low Treg cluster. Finally, we found that the correlation between signature and IHC was low in our small dataset, but samples in the high Treg cluster had significantly more CD8+ and FoxP3+ cells compared with the low Treg cluster.Conclusions: Treg gene expression is highly indicative of the overall tumor immune environment.Impact: In comparison with the consensus molecular subtype and microsatellite status, the Treg signature identifies more colorectal tumors with high immune activation that may benefit from cancer immunotherapy.\n\n\n\nAccess public omics data using the GEOquery package\nGet an opportunity to work with another SummarizedExperiment object.\nPerform a simple unsupervised analysis to visualize these public data.",
    "crumbs": [
      "Bioconductor",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Accessing and working with public omics data</span>"
    ]
  },
  {
    "objectID": "geoquery.html#geoquery-to-pca",
    "href": "geoquery.html#geoquery-to-pca",
    "title": "\n16  Accessing and working with public omics data\n",
    "section": "\n16.2 GEOquery to PCA",
    "text": "16.2 GEOquery to PCA\nThe first step is to install the R package GEOquery. This package allows us to access data from the Gene Expression Omnibus (GEO) database. GEO is a public repository of omics data.\n\nBiocManager::install(\"GEOquery\")\n\nGEOquery has only one commonly used function, getGEO() which takes a GEO accession number as an argument. The GEO accession number is a unique identifier for a dataset.\nUse the GEOquery package to fetch data about GSE103512.\n\nlibrary(GEOquery)\ngse = getGEO(\"GSE103512\")[[1]]\n\nYou might ask why we are using [[1]] at the end of the getGEO() function. The reason is that getGEO() returns a list of GSE objects. We are only interested in the first one (and in this case, the only one). We return a list of GSE objects because in the early days, it was not unusual to have a single GEO accession number represent multiple datasets. While uncommon now, we’ve kept the convention since lots of “older” data is still quite useful.\nAgain, a historically-derived detail, is to convert from the older Bioconductor data structure (GEOquery was written in 2007), the ExpressionSet, to the newer SummarizedExperiment.\n\nlibrary(SummarizedExperiment)\nse = as(gse, \"SummarizedExperiment\")\n\nUse some code to determine the answers to the following:\n\nWhat is the class of se?\nWhat are the dimensions of se?\nWhat are the dimensions of the assay slot of se?\nWhat are the dimensions of the colData slot of se?\nWhat variables are in the colData slot of se?\n\nExamine two variables of interest, cancer type and tumor/normal status. The with function is a convenience to allow us to access variables in a data frame by name (rather than having to do dataframe$variable_name. Recalling that the table function is a convenient way to summarize the counts of unique values in a vector, we can use with to access the variables of interest and table to summarize the counts of unique values.\n\nwith(colData(se),table(`cancer.type.ch1`,`normal.ch1`))\n\n               normal.ch1\ncancer.type.ch1 no yes\n          BC    65  10\n          CRC   57  12\n          NSCLC 60   9\n          PCA   60   7\n\n\n\nHow many samples are there of each cancer type?\nHow many samples are there of each tumor/normal status?\n\nWhen performing unsupervised analysis, it is common to filter genes by variance to find the most informative genes. It is common practice to filter genes by standard deviation or some other measure of variability and keep the top X percent of them when performing dimensionality reduction. There is not a single right answer to what percentage to use, so try a few to see what happens. In the example code, I chose to use the top 500 genes by standard deviation, but you can play with the threshold to see what happens.\nRecall that the assay function is used to access the data matrix of the SummarizedExperiment object.\nThink through the code below and then run it.\n\nsds = apply(assay(se, 'exprs'),1,sd)\ndat = assay(se, 'exprs')[order(sds,decreasing = TRUE)[1:500],]\n\nIf you don’t recognize the function apply, it is a function that applies a function to each row or column of a matrix. In this case, we are applying the sd function to each row of the data matrix. The order function is used to sort the standard deviations in decreasing order (when decreasing=TRUE). And the [1:500] is used to subset the data matrix to the top 500 genes by standard deviation.\nPerform PCA and prepare for plotting. We will be using ggplot2, so we need to make a data.frame before plotting.\n\npca_results &lt;- prcomp(t(dat))\npca_df = as.data.frame(pca_results$x)\npca_df$Type=factor(colData(se)[,'cancer.type.ch1'])\npca_df$Normal = factor(colData(se)[,'normal.ch1'])\n\nNow, we are going to plot the results of the PCA, coloring the points by cancer type and using different shapes for normal and tumor samples.\n\nlibrary(ggplot2)\nggplot(pca_df, aes(x=PC1,y=PC2,shape=Normal,color=Type)) + \n    geom_point( alpha=0.6) + theme(text=element_text(size = 18))\n\n\n\n\n\n\n\nIn this case, the x-axis is the first principal component and the y-axis is the second principal component.\n\nWhat do you see?\nWhat about additional principal components?\nBonus: Try using the GGally package to plot principal components (using the ggpairs function).\nBonus: Calculate the variance explained by each principal component and plot the results.",
    "crumbs": [
      "Bioconductor",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Accessing and working with public omics data</span>"
    ]
  },
  {
    "objectID": "bioc-summarizedexperiment.html",
    "href": "bioc-summarizedexperiment.html",
    "title": "\n17  Introduction to SummarizedExperiment\n",
    "section": "",
    "text": "17.1 Anatomy of a SummarizedExperiment\nThe SummarizedExperiment package contains two classes: SummarizedExperiment and RangedSummarizedExperiment.\nSummarizedExperiment is a matrix-like container where rows represent features of interest (e.g. genes, transcripts, exons, etc.) and columns represent samples. The objects contain one or more assays, each represented by a matrix-like object of numeric or other mode. The rows of a SummarizedExperiment object represent features of interest. Information about these features is stored in a DataFrame object, accessible using the function rowData(). Each row of the DataFrame provides information on the feature in the corresponding row of the SummarizedExperiment object. Columns of the DataFrame represent different attributes of the features of interest, e.g., gene or transcript IDs, etc.\nRangedSummarizedExperiment is the “child”” of the SummarizedExperiment class which means that all the methods on SummarizedExperiment also work on a RangedSummarizedExperiment.\nThe fundamental difference between the two classes is that the rows of a RangedSummarizedExperiment object represent genomic ranges of interest instead of a DataFrame of features. The RangedSummarizedExperiment ranges are described by a GRanges or a GRangesList object, accessible using the rowRanges() function.\nFigure 17.1 displays the class geometry and highlights the vertical (column) and horizontal (row) relationships.",
    "crumbs": [
      "Bioconductor",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Introduction to `SummarizedExperiment`</span>"
    ]
  },
  {
    "objectID": "bioc-summarizedexperiment.html#anatomy-of-a-summarizedexperiment",
    "href": "bioc-summarizedexperiment.html#anatomy-of-a-summarizedexperiment",
    "title": "\n17  Introduction to SummarizedExperiment\n",
    "section": "",
    "text": "Figure 17.1: Summarized Experiment. There are three main components, the colData(), the rowData() and the assays(). The accessors for the various parts of a complete SummarizedExperiment object match the names.\n\n\n\n17.1.1 Assays\nThe airway package contains an example dataset from an RNA-Seq experiment of read counts per gene for airway smooth muscles. These data are stored in a RangedSummarizedExperiment object which contains 8 different experimental and assays 64,102 gene transcripts.\n\n\nLoading required package: airway\n\n\nWarning in library(package, lib.loc = lib.loc, character.only = TRUE,\nlogical.return = TRUE, : there is no package called 'airway'\n\n\nBioconductor version 3.19 (BiocManager 1.30.23), R 4.4.0 (2024-04-24)\n\n\nInstalling package(s) 'airway'\n\n\ninstalling the source package 'airway'\n\n\nOld packages: 'KernSmooth', 'survival'\n\n\n\nlibrary(SummarizedExperiment)\ndata(airway, package=\"airway\")\nse &lt;- airway\nse\n\nclass: RangedSummarizedExperiment \ndim: 63677 8 \nmetadata(1): ''\nassays(1): counts\nrownames(63677): ENSG00000000003 ENSG00000000005 ... ENSG00000273492\n  ENSG00000273493\nrowData names(10): gene_id gene_name ... seq_coord_system symbol\ncolnames(8): SRR1039508 SRR1039509 ... SRR1039520 SRR1039521\ncolData names(9): SampleName cell ... Sample BioSample\n\n\nTo retrieve the experiment data from a SummarizedExperiment object one can use the assays() accessor. An object can have multiple assay datasets each of which can be accessed using the $ operator. The airway dataset contains only one assay (counts). Here each row represents a gene transcript and each column one of the samples.\n\nassays(se)$counts\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSRR1039508\nSRR1039509\nSRR1039512\nSRR1039513\nSRR1039516\nSRR1039517\nSRR1039520\nSRR1039521\n\n\n\nENSG00000000003\n679\n448\n873\n408\n1138\n1047\n770\n572\n\n\nENSG00000000005\n0\n0\n0\n0\n0\n0\n0\n0\n\n\nENSG00000000419\n467\n515\n621\n365\n587\n799\n417\n508\n\n\nENSG00000000457\n260\n211\n263\n164\n245\n331\n233\n229\n\n\nENSG00000000460\n60\n55\n40\n35\n78\n63\n76\n60\n\n\nENSG00000000938\n0\n0\n2\n0\n1\n0\n0\n0\n\n\nENSG00000000971\n3251\n3679\n6177\n4252\n6721\n11027\n5176\n7995\n\n\nENSG00000001036\n1433\n1062\n1733\n881\n1424\n1439\n1359\n1109\n\n\nENSG00000001084\n519\n380\n595\n493\n820\n714\n696\n704\n\n\nENSG00000001167\n394\n236\n464\n175\n658\n584\n360\n269\n\n\n\n\n\n\n17.1.2 ‘Row’ (regions-of-interest) data\nThe rowRanges() accessor is used to view the range information for a RangedSummarizedExperiment. (Note if this were the parent SummarizedExperiment class we’d use rowData()). The data are stored in a GRangesList object, where each list element corresponds to one gene transcript and the ranges in each GRanges correspond to the exons in the transcript.\n\nrowRanges(se)\n\nGRangesList object of length 63677:\n$ENSG00000000003\nGRanges object with 17 ranges and 2 metadata columns:\n       seqnames            ranges strand |   exon_id       exon_name\n          &lt;Rle&gt;         &lt;IRanges&gt;  &lt;Rle&gt; | &lt;integer&gt;     &lt;character&gt;\n   [1]        X 99883667-99884983      - |    667145 ENSE00001459322\n   [2]        X 99885756-99885863      - |    667146 ENSE00000868868\n   [3]        X 99887482-99887565      - |    667147 ENSE00000401072\n   [4]        X 99887538-99887565      - |    667148 ENSE00001849132\n   [5]        X 99888402-99888536      - |    667149 ENSE00003554016\n   ...      ...               ...    ... .       ...             ...\n  [13]        X 99890555-99890743      - |    667156 ENSE00003512331\n  [14]        X 99891188-99891686      - |    667158 ENSE00001886883\n  [15]        X 99891605-99891803      - |    667159 ENSE00001855382\n  [16]        X 99891790-99892101      - |    667160 ENSE00001863395\n  [17]        X 99894942-99894988      - |    667161 ENSE00001828996\n  -------\n  seqinfo: 722 sequences (1 circular) from an unspecified genome\n\n...\n&lt;63676 more elements&gt;\n\n\n\n17.1.3 ‘Column’ (sample) data\nSample meta-data describing the samples can be accessed using colData(), and is a DataFrame that can store any number of descriptive columns for each sample row.\n\ncolData(se)\n\nDataFrame with 8 rows and 9 columns\n           SampleName     cell      dex    albut        Run avgLength\n             &lt;factor&gt; &lt;factor&gt; &lt;factor&gt; &lt;factor&gt;   &lt;factor&gt; &lt;integer&gt;\nSRR1039508 GSM1275862  N61311     untrt    untrt SRR1039508       126\nSRR1039509 GSM1275863  N61311     trt      untrt SRR1039509       126\nSRR1039512 GSM1275866  N052611    untrt    untrt SRR1039512       126\nSRR1039513 GSM1275867  N052611    trt      untrt SRR1039513        87\nSRR1039516 GSM1275870  N080611    untrt    untrt SRR1039516       120\nSRR1039517 GSM1275871  N080611    trt      untrt SRR1039517       126\nSRR1039520 GSM1275874  N061011    untrt    untrt SRR1039520       101\nSRR1039521 GSM1275875  N061011    trt      untrt SRR1039521        98\n           Experiment    Sample    BioSample\n             &lt;factor&gt;  &lt;factor&gt;     &lt;factor&gt;\nSRR1039508  SRX384345 SRS508568 SAMN02422669\nSRR1039509  SRX384346 SRS508567 SAMN02422675\nSRR1039512  SRX384349 SRS508571 SAMN02422678\nSRR1039513  SRX384350 SRS508572 SAMN02422670\nSRR1039516  SRX384353 SRS508575 SAMN02422682\nSRR1039517  SRX384354 SRS508576 SAMN02422673\nSRR1039520  SRX384357 SRS508579 SAMN02422683\nSRR1039521  SRX384358 SRS508580 SAMN02422677\n\n\nThis sample metadata can be accessed using the $ accessor which makes it easy to subset the entire object by a given phenotype.\n\n# subset for only those samples treated with dexamethasone\nse[, se$dex == \"trt\"]\n\nclass: RangedSummarizedExperiment \ndim: 63677 4 \nmetadata(1): ''\nassays(1): counts\nrownames(63677): ENSG00000000003 ENSG00000000005 ... ENSG00000273492\n  ENSG00000273493\nrowData names(10): gene_id gene_name ... seq_coord_system symbol\ncolnames(4): SRR1039509 SRR1039513 SRR1039517 SRR1039521\ncolData names(9): SampleName cell ... Sample BioSample\n\n\n\n17.1.4 Experiment-wide metadata\nMeta-data describing the experimental methods and publication references can be accessed using metadata().\n\nmetadata(se)\n\n[[1]]\nExperiment data\n  Experimenter name: Himes BE \n  Laboratory: NA \n  Contact information:  \n  Title: RNA-Seq transcriptome profiling identifies CRISPLD2 as a glucocorticoid responsive gene that modulates cytokine function in airway smooth muscle cells. \n  URL: http://www.ncbi.nlm.nih.gov/pubmed/24926665 \n  PMIDs: 24926665 \n\n  Abstract: A 226 word abstract is available. Use 'abstract' method.\n\n\nNote that metadata() is just a simple list, so it is appropriate for any experiment wide metadata the user wishes to save, such as storing model formulas.\n\nmetadata(se)$formula &lt;- counts ~ dex + albut\n\nmetadata(se)\n\n[[1]]\nExperiment data\n  Experimenter name: Himes BE \n  Laboratory: NA \n  Contact information:  \n  Title: RNA-Seq transcriptome profiling identifies CRISPLD2 as a glucocorticoid responsive gene that modulates cytokine function in airway smooth muscle cells. \n  URL: http://www.ncbi.nlm.nih.gov/pubmed/24926665 \n  PMIDs: 24926665 \n\n  Abstract: A 226 word abstract is available. Use 'abstract' method.\n\n$formula\ncounts ~ dex + albut",
    "crumbs": [
      "Bioconductor",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Introduction to `SummarizedExperiment`</span>"
    ]
  },
  {
    "objectID": "bioc-summarizedexperiment.html#common-operations-on-summarizedexperiment",
    "href": "bioc-summarizedexperiment.html#common-operations-on-summarizedexperiment",
    "title": "\n17  Introduction to SummarizedExperiment\n",
    "section": "\n17.2 Common operations on SummarizedExperiment\n",
    "text": "17.2 Common operations on SummarizedExperiment\n\n\n17.2.1 Subsetting\n\n\n[ Performs two dimensional subsetting, just like subsetting a matrix or data frame.\n\n\n# subset the first five transcripts and first three samples\nse[1:5, 1:3]\n\nclass: RangedSummarizedExperiment \ndim: 5 3 \nmetadata(2): '' formula\nassays(1): counts\nrownames(5): ENSG00000000003 ENSG00000000005 ENSG00000000419\n  ENSG00000000457 ENSG00000000460\nrowData names(10): gene_id gene_name ... seq_coord_system symbol\ncolnames(3): SRR1039508 SRR1039509 SRR1039512\ncolData names(9): SampleName cell ... Sample BioSample\n\n\n\n\n$ operates on colData() columns, for easy sample extraction.\n\n\nse[, se$cell == \"N61311\"]\n\nclass: RangedSummarizedExperiment \ndim: 63677 2 \nmetadata(2): '' formula\nassays(1): counts\nrownames(63677): ENSG00000000003 ENSG00000000005 ... ENSG00000273492\n  ENSG00000273493\nrowData names(10): gene_id gene_name ... seq_coord_system symbol\ncolnames(2): SRR1039508 SRR1039509\ncolData names(9): SampleName cell ... Sample BioSample\n\n\n\n17.2.2 Getters and setters\n\n\nrowRanges() / (rowData()), colData(), metadata()\n\n\n\ncounts &lt;- matrix(1:15, 5, 3, dimnames=list(LETTERS[1:5], LETTERS[1:3]))\n\ndates &lt;- SummarizedExperiment(assays=list(counts=counts),\n                              rowData=DataFrame(month=month.name[1:5], day=1:5))\n\n# Subset all January assays\ndates[rowData(dates)$month == \"January\", ]\n\nclass: SummarizedExperiment \ndim: 1 3 \nmetadata(0):\nassays(1): counts\nrownames(1): A\nrowData names(2): month day\ncolnames(3): A B C\ncolData names(0):\n\n\n\n\nassay() versus assays() There are two accessor functions for extracting the assay data from a SummarizedExperiment object. assays() operates on the entire list of assay data as a whole, while assay() operates on only one assay at a time. assay(x, i) is simply a convenience function which is equivalent to assays(x)[[i]].\n\n\nassays(se)\n\nList of length 1\nnames(1): counts\n\nassays(se)[[1]][1:5, 1:5]\n\n                SRR1039508 SRR1039509 SRR1039512 SRR1039513 SRR1039516\nENSG00000000003        679        448        873        408       1138\nENSG00000000005          0          0          0          0          0\nENSG00000000419        467        515        621        365        587\nENSG00000000457        260        211        263        164        245\nENSG00000000460         60         55         40         35         78\n\n# assay defaults to the first assay if no i is given\nassay(se)[1:5, 1:5]\n\n                SRR1039508 SRR1039509 SRR1039512 SRR1039513 SRR1039516\nENSG00000000003        679        448        873        408       1138\nENSG00000000005          0          0          0          0          0\nENSG00000000419        467        515        621        365        587\nENSG00000000457        260        211        263        164        245\nENSG00000000460         60         55         40         35         78\n\nassay(se, 1)[1:5, 1:5]\n\n                SRR1039508 SRR1039509 SRR1039512 SRR1039513 SRR1039516\nENSG00000000003        679        448        873        408       1138\nENSG00000000005          0          0          0          0          0\nENSG00000000419        467        515        621        365        587\nENSG00000000457        260        211        263        164        245\nENSG00000000460         60         55         40         35         78\n\n\n\n17.2.3 Range-based operations\n\n\nsubsetByOverlaps() SummarizedExperiment objects support all of the findOverlaps() methods and associated functions. This includes subsetByOverlaps(), which makes it easy to subset a SummarizedExperiment object by an interval.\n\nIn tne next code block, we define a region of interest (or many regions of interest) and then subset our SummarizedExperiment by overlaps with this region.\n\n# Subset for only rows which are in the interval 100,000 to 110,000 of\n# chromosome 1\nroi &lt;- GRanges(seqnames=\"1\", ranges=100000:1100000)\nsub_se = subsetByOverlaps(se, roi)\nsub_se\n\nclass: RangedSummarizedExperiment \ndim: 74 8 \nmetadata(2): '' formula\nassays(1): counts\nrownames(74): ENSG00000131591 ENSG00000177757 ... ENSG00000272512\n  ENSG00000273443\nrowData names(10): gene_id gene_name ... seq_coord_system symbol\ncolnames(8): SRR1039508 SRR1039509 ... SRR1039520 SRR1039521\ncolData names(9): SampleName cell ... Sample BioSample\n\ndim(sub_se)\n\n[1] 74  8",
    "crumbs": [
      "Bioconductor",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Introduction to `SummarizedExperiment`</span>"
    ]
  },
  {
    "objectID": "bioc-summarizedexperiment.html#constructing-a-summarizedexperiment",
    "href": "bioc-summarizedexperiment.html#constructing-a-summarizedexperiment",
    "title": "\n17  Introduction to SummarizedExperiment\n",
    "section": "\n17.3 Constructing a SummarizedExperiment\n",
    "text": "17.3 Constructing a SummarizedExperiment\n\nOften, SummarizedExperiment or RangedSummarizedExperiment objects are returned by functions written by other packages. However it is possible to create them by hand with a call to the SummarizedExperiment() constructor. The code below is simply to illustrate the mechanics of creating an object from scratch. In practice, you will probably have the pieces of the object from other sources such as Excel files or csv files.\nConstructing a RangedSummarizedExperiment with a GRanges as the rowRanges argument:\n\nnrows &lt;- 200\nncols &lt;- 6\ncounts &lt;- matrix(runif(nrows * ncols, 1, 1e4), nrows)\nrowRanges &lt;- GRanges(rep(c(\"chr1\", \"chr2\"), c(50, 150)),\n                     IRanges(floor(runif(200, 1e5, 1e6)), width=100),\n                     strand=sample(c(\"+\", \"-\"), 200, TRUE),\n                     feature_id=sprintf(\"ID%03d\", 1:200))\ncolData &lt;- DataFrame(Treatment=rep(c(\"ChIP\", \"Input\"), 3),\n                     row.names=LETTERS[1:6])\n\nSummarizedExperiment(assays=list(counts=counts),\n                     rowRanges=rowRanges, colData=colData)\n\nclass: RangedSummarizedExperiment \ndim: 200 6 \nmetadata(0):\nassays(1): counts\nrownames: NULL\nrowData names(1): feature_id\ncolnames(6): A B ... E F\ncolData names(1): Treatment\n\n\nA SummarizedExperiment can be constructed with or without supplying a DataFrame for the rowData argument:\n\nSummarizedExperiment(assays=list(counts=counts), colData=colData)\n\nclass: SummarizedExperiment \ndim: 200 6 \nmetadata(0):\nassays(1): counts\nrownames: NULL\nrowData names(0):\ncolnames(6): A B ... E F\ncolData names(1): Treatment",
    "crumbs": [
      "Bioconductor",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Introduction to `SummarizedExperiment`</span>"
    ]
  },
  {
    "objectID": "eda_with_pca.html",
    "href": "eda_with_pca.html",
    "title": "\n18  EDA with PCA\n",
    "section": "",
    "text": "18.1 Introduction\nIn this tutorial, we will use the GEOquery package to download a dataset from the Gene Expression Omnibus (GEO) and perform some exploratory data analysis (EDA) using principal components analysis (PCA).",
    "crumbs": [
      "Bioconductor",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>EDA with PCA</span>"
    ]
  },
  {
    "objectID": "eda_with_pca.html#downloading-data-from-geo",
    "href": "eda_with_pca.html#downloading-data-from-geo",
    "title": "\n18  EDA with PCA\n",
    "section": "\n18.2 Downloading data from GEO",
    "text": "18.2 Downloading data from GEO\nThe GEOquery package can be used to download data from GEO. The getGEO function takes a GEO accession number as an argument and returns a list of ExpressionSet objects. The [[1]] at the end of the getGEO call is used to extract the first (and only) ExpressionSet object from the list.\nHistorically, it was not uncommon for GEO datasets to contain multiple separate experiments. In those cases, the [[1]] would need to be replaced with the index of the experiment of interest. However, it is now uncommon for GEO datasets to contain multiple experiments, but the [[1]] is still needed to extract the ExpressionSet object from the list.\n\nlibrary(GEOquery)\nlibrary(SummarizedExperiment)\n\nExpressionSet objects are a type of Bioconductor object that is used to store gene expression data. The as function can be used to convert the ExpressionSet object to a SummarizedExperiment object, which is a newer Bioconductor object that is used to store gene expression data. The SummarizedExperiment object is preferred over the ExpressionSet object so we immediately convert the ExpressionSet object to a SummarizedExperiment.\n\ngse &lt;- getGEO(\"GSE30219\")[[1]]\n\nFound 1 file(s)\n\n\nGSE30219_series_matrix.txt.gz\n\nse &lt;- as(gse, \"SummarizedExperiment\")",
    "crumbs": [
      "Bioconductor",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>EDA with PCA</span>"
    ]
  },
  {
    "objectID": "eda_with_pca.html#filtering-genes",
    "href": "eda_with_pca.html#filtering-genes",
    "title": "\n18  EDA with PCA\n",
    "section": "\n18.3 Filtering genes",
    "text": "18.3 Filtering genes\nWhen performing PCA, it is common to filter to the most variable genes before performing the PCA. Limiting genes to the most variable genes can help to reduce the computational burden of the PCA.\nWe can calculate the standard deviation of each gene using the apply function. The apply function takes a matrix as the first argument and a 1 or 2 to indicate whether the function should be applied to the rows or columns of the matrix. The sd function calculates the standard deviation of a vector and is performed on each row of the matrix.\nA histogram of the standard deviations is not that useful, but it is easy to make.\n\nsds = apply(assay(se, 'exprs'), 1, sd)\nhist(sds)\n\n\n\n\n\n\n\nHere, we produce a subset of the SummarizedExperiment object that contains only the 500 most variable genes. We’ll use this subset for the rest of the tutorial. Feel free to revisit the number of genes you choose to keep and see how it affects the PCA.\n\nsub_se = se[order(sds,decreasing = TRUE)[1:500],]",
    "crumbs": [
      "Bioconductor",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>EDA with PCA</span>"
    ]
  },
  {
    "objectID": "eda_with_pca.html#pca",
    "href": "eda_with_pca.html#pca",
    "title": "\n18  EDA with PCA\n",
    "section": "\n18.4 PCA",
    "text": "18.4 PCA\nPCA is a method for dimensionality reduction. It is a linear transformation that finds the directions of maximum variance in a dataset and projects it onto a new subspace with equal or fewer dimensions than the original one. The orthogonal axes (principal components) of the new subspace can be interpreted as the directions of maximum variance given the constraint that the new feature axes are orthogonal to each other.\n\n\nThe matrix decomposition of the first PC and how we can use it to construct the dimensionally-reduced dataset.\n\n\n# read the help for prcomp here to see what the arguments are\n# ?prcomp\npca = prcomp(t(assay(sub_se,'exprs')))\n\nThe PCA algorithm results in a rotation matrix that can be used to transform the original data into the new subspace. The rotation matrix is stored in the rotation slot of the prcomp object and represents the loadings of each gene for each principle component. The prcomp function also stores the coordinates of the samples in the new subspace in the x slot, which represents the locations of the samples in principle component space.\n\nstr(pca)\n\nList of 5\n $ sdev    : num [1:307] 27.01 22.78 13.46 10.43 9.35 ...\n $ rotation: num [1:500, 1:307] -0.1091 0.0598 -0.0474 -0.0513 -0.0903 ...\n  ..- attr(*, \"dimnames\")=List of 2\n  .. ..$ : chr [1:500] \"209125_at\" \"209988_s_at\" \"223678_s_at\" \"218835_at\" ...\n  .. ..$ : chr [1:307] \"PC1\" \"PC2\" \"PC3\" \"PC4\" ...\n $ center  : Named num [1:500] 6.86 5.56 7.99 10.39 7.82 ...\n  ..- attr(*, \"names\")= chr [1:500] \"209125_at\" \"209988_s_at\" \"223678_s_at\" \"218835_at\" ...\n $ scale   : logi FALSE\n $ x       : num [1:307, 1:307] 0.571 -3.528 5.289 -31.486 1.273 ...\n  ..- attr(*, \"dimnames\")=List of 2\n  .. ..$ : chr [1:307] \"GSM748053\" \"GSM748054\" \"GSM748055\" \"GSM748056\" ...\n  .. ..$ : chr [1:307] \"PC1\" \"PC2\" \"PC3\" \"PC4\" ...\n - attr(*, \"class\")= chr \"prcomp\"\n\n\nThe prcomp function also centers the data by default. The centering values are stored in the center slot. The x slot contains the coordinates of the samples in the new subspace. The\nWe can plot the samples using the first two PCs as the x and y axes.\n\nplot(pca$x[,1], pca$x[,2], pch=20)\n\n\n\n\n\n\nFigure 18.1: PCA plot of samples in the first two PCs.\n\n\n\n\nIf we use ALL the PCs, we can perform a matrix multiplication to get the original data back.\n\norig_data = pca$rotation %*% t(pca$x) + pca$center\norig_data[1:5,1:5]\n\n            GSM748053 GSM748054 GSM748055 GSM748056 GSM748057\n209125_at    4.400830  4.349534  3.661922 10.289194  3.354648\n209988_s_at  3.078285  3.079797  3.467936  3.447669  3.141168\n223678_s_at 11.389715 10.637554  5.956832  9.311594 10.467811\n218835_at   13.541261 12.944545  9.725079 12.744177 12.896846\n201820_at    5.056486  5.030666  4.986433 11.284134  5.132626\n\n\nCompare to the original data:\n\nassay(sub_se,'exprs')[1:5,1:5]\n\n            GSM748053 GSM748054 GSM748055 GSM748056 GSM748057\n209125_at    4.400830  4.349534  3.661923 10.289194  3.354648\n209988_s_at  3.078285  3.079797  3.467936  3.447669  3.141168\n223678_s_at 11.389715 10.637554  5.956831  9.311594 10.467811\n218835_at   13.541261 12.944545  9.725079 12.744177 12.896846\n201820_at    5.056486  5.030666  4.986433 11.284134  5.132626\n\n\nAnd the same thing, but using only the first 3 PCs:\n\norig_data_3pcs = pca$rotation[,1:3] %*% t(pca$x[,1:3]) + pca$center\norig_data_3pcs[1:5,1:5]\n\n            GSM748053 GSM748054 GSM748055 GSM748056 GSM748057\n209125_at    4.302207  5.319141  4.660544  9.765880  4.368261\n209988_s_at  3.509123  4.372072  4.644271  4.552902  4.251617\n223678_s_at 12.637489 11.381274 10.702921  9.672075 11.985243\n218835_at   14.587942 13.535895 12.807482 12.279706 14.037817\n201820_at    5.300632  6.248694  5.741837 10.170421  5.391310",
    "crumbs": [
      "Bioconductor",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>EDA with PCA</span>"
    ]
  },
  {
    "objectID": "eda_with_pca.html#variance-explained",
    "href": "eda_with_pca.html#variance-explained",
    "title": "\n18  EDA with PCA\n",
    "section": "\n18.5 Variance explained",
    "text": "18.5 Variance explained\nOften, we want to know how much of the variance in the data is explained by each PC. The pca object has a slot called sdev that represents the standard deviation of the principle component. Variance is the square of sdev, so we can calculate the variance by squaring sdev.\n\nvar_explained = pca$sdev ^ 2\n\nThe total variance is just the sum of all the variances:\n\ntot_variance = sum(var_explained)\n\nAnd the proportion of the variance explained by each PC is then\n\nprop_var_explained = var_explained/tot_variance\nhead(prop_var_explained)\n\n[1] 0.27748451 0.19747527 0.06892190 0.04138543 0.03325101 0.02956515\n\n\nIf we plot the prop_var_explained, it is called a scree plot and can help us to choose an appropriate number of PCs to “keep” in order to reduce the dimensionality.\n\nplot(prop_var_explained[1:15], type='b')\n\n\n\n\n\n\n\nExamine the plot. How many PCs would you keep?",
    "crumbs": [
      "Bioconductor",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>EDA with PCA</span>"
    ]
  },
  {
    "objectID": "eda_with_pca.html#add-pcs-to-our-summarizedexperiment-object",
    "href": "eda_with_pca.html#add-pcs-to-our-summarizedexperiment-object",
    "title": "\n18  EDA with PCA\n",
    "section": "\n18.6 Add PCs to our SummarizedExperiment object",
    "text": "18.6 Add PCs to our SummarizedExperiment object\nRecall that the x matrix stored in the pca object represent the coordinates of the samples in the new subspace. We can look at the first five rows and columns of the x matrix to see what it looks like.\n\npca$x[1:5,1:5]\n\n                  PC1       PC2       PC3        PC4        PC5\nGSM748053   0.5712872 34.105929 15.208118  -4.738482  3.4101384\nGSM748054  -3.5283223 24.664382  7.632319 -11.746590  0.1405872\nGSM748055   5.2892621 21.841834  9.092392  -3.022823 11.7810845\nGSM748056 -31.4864198  3.762922 -5.332805   9.695495 -8.8944295\nGSM748057   1.2726085 30.640997 10.562888   6.294335  7.1601434\n\n\nSo, PC components for each sample are in columns and samples are in rows. For colData, the samples are also in rows. So, we can join the PC values to the SummarizedExperiment, sub_se, for later use and for comparison to other sample metadata.\n\n# We can use cbind to join the PC values to the colData\n# note that the names of the rows are the same for both\ncolData(sub_se) = cbind(colData(sub_se), pca$x[,1:5])\n\nWe now have the PCs stored conveniently with our SummarizedExperiment.\n\ncolnames(colData(sub_se))\n\n [1] \"title\"                               \"geo_accession\"                      \n [3] \"status\"                              \"submission_date\"                    \n [5] \"last_update_date\"                    \"type\"                               \n [7] \"channel_count\"                       \"source_name_ch1\"                    \n [9] \"organism_ch1\"                        \"characteristics_ch1\"                \n[11] \"characteristics_ch1.1\"               \"characteristics_ch1.2\"              \n[13] \"characteristics_ch1.3\"               \"characteristics_ch1.4\"              \n[15] \"characteristics_ch1.5\"               \"characteristics_ch1.6\"              \n[17] \"characteristics_ch1.7\"               \"characteristics_ch1.8\"              \n[19] \"characteristics_ch1.9\"               \"characteristics_ch1.10\"             \n[21] \"molecule_ch1\"                        \"extract_protocol_ch1\"               \n[23] \"label_ch1\"                           \"label_protocol_ch1\"                 \n[25] \"taxid_ch1\"                           \"hyb_protocol\"                       \n[27] \"scan_protocol\"                       \"description\"                        \n[29] \"data_processing\"                     \"platform_id\"                        \n[31] \"contact_name\"                        \"contact_laboratory\"                 \n[33] \"contact_department\"                  \"contact_institute\"                  \n[35] \"contact_address\"                     \"contact_city\"                       \n[37] \"contact_zip.postal_code\"             \"contact_country\"                    \n[39] \"supplementary_file\"                  \"data_row_count\"                     \n[41] \"age.at.surgery.ch1\"                  \"disease.free.survival.in.months.ch1\"\n[43] \"follow.up.time..months..ch1\"         \"gender.ch1\"                         \n[45] \"histology.ch1\"                       \"pm.stage.ch1\"                       \n[47] \"pn.stage.ch1\"                        \"pt.stage.ch1\"                       \n[49] \"relapse..event.1..no.event.0..ch1\"   \"status.ch1\"                         \n[51] \"tissue.ch1\"                          \"PC1\"                                \n[53] \"PC2\"                                 \"PC3\"                                \n[55] \"PC4\"                                 \"PC5\"",
    "crumbs": [
      "Bioconductor",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>EDA with PCA</span>"
    ]
  },
  {
    "objectID": "eda_with_pca.html#variable-relationships",
    "href": "eda_with_pca.html#variable-relationships",
    "title": "\n18  EDA with PCA\n",
    "section": "\n18.7 Variable relationships",
    "text": "18.7 Variable relationships\nLooking at relationships between variables can be a really useful way of generating hypotheses, performing quality control, and suggesting areas to focus in analysis. One common approach to looking at a few variables and their relationships is the “pairs” plot.\nThe GGally package has a function called ggpairs that can be used to generate a pairs plot for a few variables.\n\nlibrary(GGally)\n\nTake a look at this website and examine some variable relationships in the colData(sub_se). When working with ggplot (and ggpairs), you’ll likely want to convert the colData() to a data.frame first. See Figure 18.2 for an example.\n\nggpairs(as.data.frame(colData(sub_se)),columns=(c(\"PC1\",\"PC2\",\"PC3\",\"histology.ch1\")))\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\n\nFigure 18.2: A pairs plot of a few variables.\n\n\n\n\nThe ggpairs function is very flexible and plays well with ggplot. Therefore, you can add aes() to the ggpairs function to add colors, etc. to the plot (see Figure 18.3). Look at other variables that you might want to include and style the plot to your liking.\n\nggpairs(as.data.frame(colData(sub_se)),columns=(c(\"PC1\",\"PC2\",\"PC3\",\"histology.ch1\")), \n        aes(color=histology.ch1, alpha=0.5))\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\n\nFigure 18.3: A pairs plot colored by a variable of interest.",
    "crumbs": [
      "Bioconductor",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>EDA with PCA</span>"
    ]
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Bourgon, Richard, Robert Gentleman, and Wolfgang Huber. 2010.\n“Independent Filtering Increases Detection Power for\nHigh-Throughput Experiments.” Proceedings of the National\nAcademy of Sciences 107 (21): 9546–51. https://doi.org/10.1073/pnas.0914005107.\n\n\nCenter, Pew Research. 2016. “Lifelong Learning and\nTechnology.” Pew Research Center: Internet,\nScience & Tech. https://www.pewresearch.org/internet/2016/03/22/lifelong-learning-and-technology/.\n\n\nDeRisi, J. L., V. R. Iyer, and P. O. Brown. 1997. “Exploring the\nMetabolic and Genetic Control of Gene Expression on a Genomic\nScale.” Science (New York, N.Y.) 278 (5338): 680–86. https://doi.org/10.1126/science.278.5338.680.\n\n\nKnowles, Malcolm S., Elwood F. Holton, and Richard A. Swanson. 2005.\nThe Adult Learner: The Definitive Classic in Adult Education and\nHuman Resource Development. 6th ed. Amsterdam ; Boston: Elsevier.\n\n\nStudent. 1908. “The Probable Error of a\nMean.” Biometrika 6 (1): 1–25. https://doi.org/10.2307/2331554.",
    "crumbs": [
      "References"
    ]
  },
  {
    "objectID": "appendix.html",
    "href": "appendix.html",
    "title": "Appendix A — Appendix",
    "section": "",
    "text": "A.1 Data Sets",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>Appendix</span>"
    ]
  },
  {
    "objectID": "appendix.html#data-sets",
    "href": "appendix.html#data-sets",
    "title": "Appendix A — Appendix",
    "section": "",
    "text": "BRFSS subset\nALL clinical data\nALL expression data",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>Appendix</span>"
    ]
  },
  {
    "objectID": "appendix.html#swirl",
    "href": "appendix.html#swirl",
    "title": "Appendix A — Appendix",
    "section": "\nA.2 Swirl",
    "text": "A.2 Swirl\nThe following is from the swirl website.\n\nThe swirl R package makes it fun and easy to learn R programming and data science. If you are new to R, have no fear.\n\nTo get started, we need to install a new package into R.\n\ninstall.packages('swirl')\n\nOnce installed, we want to load it into the R workspace so we can use it.\n\nlibrary('swirl')\n\nFinally, to get going, start swirl and follow the instructions.\n\nswirl()",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>Appendix</span>"
    ]
  },
  {
    "objectID": "additional_resources.html",
    "href": "additional_resources.html",
    "title": "Appendix B — Additional resources",
    "section": "",
    "text": "Base R Cheat Sheet",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>B</span>  <span class='chapter-title'>Additional resources</span>"
    ]
  },
  {
    "objectID": "license.html",
    "href": "license.html",
    "title": "",
    "section": "",
    "text": "Code\n\n\n\n\n\n    To the extent possible under law,  Sean Davis has waived all copyright and related or neighboring rights to Statistical analysis of functional genomics dataa. This work is published from:  United States."
  }
]