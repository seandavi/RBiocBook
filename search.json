[
  {
    "objectID": "index.html#r",
    "href": "index.html#r",
    "title": "mybook",
    "section": "0.1 R",
    "text": "0.1 R\n\nGetting started: R Mechanics [Rmd]\nVectors [Rmd] [Exercises]\nMatrices [Rmd]\nDataframes [Rmd]\nIntroduction to dplyr [Rmd] [Exercises]\nControl structures, looping, and functions [Rmd]\nIntro to univariate statistics [Rmd]\nIntroduction to plotting [Rmd]\nData input and manipulation exercises\nSwirl\nt-statistic [Rmd]"
  },
  {
    "objectID": "index.html#bioconductor",
    "href": "index.html#bioconductor",
    "title": "mybook",
    "section": "0.2 Bioconductor",
    "text": "0.2 Bioconductor\n\nGetting Started with Bioconductor [Rmd]\nSummarizedExperiment [Rmd] [Exercises]\nAnnotationHub [Rmd] [Exercises]\nGEOquery and Multidimensional Scaling Exercise [Rmd]\nRanges [Rmd] Ranges Exercises [Rmd] [slides]\nTxDb [Rmd]\nKmeans example [Rmd]\nDimensionality Reduction [Rmd]\nGene Expression Prediction [Rmd] [Slides]\nGene Expression and DNAse setup [Rmd]"
  },
  {
    "objectID": "preface.html#why-this-book",
    "href": "preface.html#why-this-book",
    "title": "Preface",
    "section": "Why this book?",
    "text": "Why this book?"
  },
  {
    "objectID": "preface.html#who-is-this-book-for",
    "href": "preface.html#who-is-this-book-for",
    "title": "Preface",
    "section": "Who is this book for?",
    "text": "Who is this book for?\n\nPeople who want to learn data science\nPeople who want to teach data science\nPeople who want to learn how to teach data science\nPeople who want to learn how to learn data science"
  },
  {
    "objectID": "preface.html#approach-to-learning-data-science",
    "href": "preface.html#approach-to-learning-data-science",
    "title": "Preface",
    "section": "Approach to learning data science",
    "text": "Approach to learning data science\n\n\nHow to succeed at data science (or anything)\n\nRead, Do, Read, Do, Read, Do, Read, Do\n\n\n\nHow to stay stuck in data science (or anything)\n\nRead, Read, Read, Read, Do, Do, Do, Do.\n\n\n\nHow to REALLY stay stuck in data science (or anything)\n\nRead, Copy and Paste code, Read, Copy and Paste Code"
  },
  {
    "objectID": "intro.html#questions",
    "href": "intro.html#questions",
    "title": "2  Introducing R and RStudio",
    "section": "Questions",
    "text": "Questions\n\nWhat is R?\nWhy use R?\nWhy not use R?\nWhy use RStudio and how does it differ from R?"
  },
  {
    "objectID": "intro.html#learning-objectives",
    "href": "intro.html#learning-objectives",
    "title": "2  Introducing R and RStudio",
    "section": "Learning Objectives",
    "text": "Learning Objectives\n\nKnow advantages of analyzing data in R\nKnow advantages of using RStudio\nBe able to start RStudio on your computer\nIdentify the panels of the RStudio interface\nBe able to customize the RStudio layout"
  },
  {
    "objectID": "intro.html#introduction",
    "href": "intro.html#introduction",
    "title": "2  Introducing R and RStudio",
    "section": "2.1 Introduction",
    "text": "2.1 Introduction\nIn this chapter, we will discuss the basics of R and RStudio, two essential tools in genomics data analysis. We will cover the advantages of using R and RStudio, how to set up RStudio, and the different panels of the RStudio interface."
  },
  {
    "objectID": "intro.html#what-is-r",
    "href": "intro.html#what-is-r",
    "title": "2  Introducing R and RStudio",
    "section": "2.2 What is R?",
    "text": "2.2 What is R?\n[R](https://en.wikipedia.org/wiki/R_(programming_language) is a programming language and software environment designed for statistical computing and graphics. It is widely used by statisticians, data scientists, and researchers for data analysis and visualization. R is an open-source language, which means it is free to use, modify, and distribute. Over the years, R has become particularly popular in the fields of genomics and bioinformatics, owing to its extensive libraries and powerful data manipulation capabilities.\nThe R language is a dialect of the S language, which was developed in the 1970s at Bell Laboratories. The first version of R was written by Robert Gentleman and Ross Ihaka and released in 1995 (see this slide deck for Ross Ihaka’s take on R’s history). Since then, R has been continuously developed by the R Core Team, a group of statisticians and computer scientists. The R Core Team releases a new version of R every year.\n\n\n\n\n\nFigure 2.1: Google trends showing the popularity of R over time based on Google searches"
  },
  {
    "objectID": "intro.html#why-use-r",
    "href": "intro.html#why-use-r",
    "title": "2  Introducing R and RStudio",
    "section": "2.3 Why use R?",
    "text": "2.3 Why use R?\nThere are several reasons why R is a popular choice for data analysis, particularly in genomics and bioinformatics. These include:\n\nOpen-source: R is free to use and has a large community of developers who contribute to its growth and development. What is “open-source”?\nExtensive libraries: There are thousands of R packages available for a wide range of tasks, including specialized packages for genomics and bioinformatics. These libraries have been extensively tested and ara available for free.\nData manipulation: R has powerful data manipulation capabilities, making it easy (or at least possible) to clean, process, and analyze large datasets.\nGraphics and visualization: R has excellent tools for creating high-quality graphics and visualizations that can be customized to meet the specific needs of your analysis. In most cases, graphics produced by R are publication-quality.\nReproducible research: R enables you to create reproducible research by recording your analysis in a script, which can be easily shared and executed by others. In addition, R does not have a meaningful graphical user interface (GUI), which renders analysis in R much more reproducible than tools that rely on GUI interactions.\nCross-platform: R runs on Windows, Mac, and Linux (as well as more obscure systems).\nInteroperability with other languages: R can interfact with FORTRAN, C, and many other languages.\nScalability: R is useful for small and large projects.\n\nI can develop code for analysis on my Mac laptop. I can then install the same code on our 20k core cluster and run it in parallel on 100 samples, monitor the process, and then update a database (for example) with R when complete."
  },
  {
    "objectID": "intro.html#why-not-use-r",
    "href": "intro.html#why-not-use-r",
    "title": "2  Introducing R and RStudio",
    "section": "2.4 Why not use R?",
    "text": "2.4 Why not use R?\n\nR cannot do everything.\nR is not always the “best” tool for the job.\nR will not hold your hand. Often, it will slap your hand instead.\nThe documentation can be opaque (but there is documentation).\nR can drive you crazy (on a good day) or age you prematurely (on a bad one).\nFinding the right package to do the job you want to do can be challenging; worse, some contributed packages are unreliable.]{}\nR does not have a meaningfully useful graphical user interface (GUI)."
  },
  {
    "objectID": "intro.html#r-license-and-the-open-source-ideal",
    "href": "intro.html#r-license-and-the-open-source-ideal",
    "title": "2  Introducing R and RStudio",
    "section": "2.5 R License and the Open Source Ideal",
    "text": "2.5 R License and the Open Source Ideal\nR is free (yes, totally free!) and distributed under GNU license. In particular, this license allows one to:\n\nDownload the source code\nModify the source code to your heart’s content\nDistribute the modified source code and even charge money for it, but you must distribute the modified source code under the original GNU license]{}\n\nThis license means that R will always be available, will always be open source, and can grow organically without constraint."
  },
  {
    "objectID": "intro.html#rstudio",
    "href": "intro.html#rstudio",
    "title": "2  Introducing R and RStudio",
    "section": "2.6 RStudio",
    "text": "2.6 RStudio\nRStudio is an integrated development environment (IDE) for R. It provides a graphical user interface (GUI) for R, making it easier to write and execute R code. RStudio also provides several other useful features, including a built-in console, syntax-highlighting editor, and tools for plotting, history, debugging, workspace management, and workspace viewing. RStudio is available in both free and commercial editions; the commercial edition provides some additional features, including support for multiple sessions and enhanced debugging\n\n2.6.1 Getting started with RStudio\nTo get started with RStudio, you first need to install both R and RStudio on your computer. Follow these steps:\n\nDownload and install R from the official R website.\nDownload and install RStudio from the official RStudio website.\nLaunch RStudio. You should see the RStudio interface with four panels.\n\n\n\n2.6.2 The RStudio Interface\nRStudio’s interface consists of four panels (see Figure 2.2):\n\n\nSource\n\nThis panel is where you write and edit your R scripts. You can create new scripts, open existing ones, and run your code from this panel.\n\n\n\nConsole\n\nThis panel displays the R console, where you can enter and execute R commands directly. The console also shows the output of your code, error messages, and other information.\n\n\n\nEnvironment\n\nThis panel displays your current workspace, including all variables, data objects, and functions that you have created or loaded in your R session.\n\n\n\nPlots, Packages, Help, and Viewer\n\nThese panels display plots, installed packages, help files, and web content, respectively.\n\n\n\n\n\n\nFigure 2.2: The RStudio interface. In this layout, the source pane is in the upper left, the console is in the lower left, the environment panel is in the top right and the viewer/help/files panel is in the bottom right.\n\n\n\n\n\n\n\n\nCustomizing the RStudio Interface\n\n\n\nYou can customize the layout of RStudio to suit your preferences. To do so, go to Tools &gt; Global Options &gt; Appearance. Here, you can change the theme, font size, and panel layout.\n\n\nIn summary, R and RStudio are powerful tools for genomics data analysis. By understanding the advantages of using R and RStudio and familiarizing yourself with the RStudio interface, you can efficiently analyze and visualize your data. In the following chapters, we will delve deeper into the functionality of R, Bioconductor, and various statistical methods to help you gain a comprehensive understanding of genomics data analysis."
  },
  {
    "objectID": "r_intro_mechanics.html#learning-objectives",
    "href": "r_intro_mechanics.html#learning-objectives",
    "title": "3  R mechanics",
    "section": "3.1 Learning objectives",
    "text": "3.1 Learning objectives\n\nBe able to start R and RStudio\nLearn to interact with the R console\nKnow the difference between expressions and assignment\nRecognize valid and invalid R names\nKnow how to access the R help system\nKnow how to assign values to variables, find what is in R memory, and remove values from R memory"
  },
  {
    "objectID": "r_intro_mechanics.html#starting-r",
    "href": "r_intro_mechanics.html#starting-r",
    "title": "3  R mechanics",
    "section": "3.2 Starting R",
    "text": "3.2 Starting R\nHow to start R depends a bit on the operating system (Mac, Windows, Linux) and interface. In this course, we will largely be using an Integrated Development Environment (IDE) called RStudio, but there is nothing to prohibit using R at the command line or in some other interface (and there are a few)."
  },
  {
    "objectID": "r_intro_mechanics.html#rstudio-a-quick-tour",
    "href": "r_intro_mechanics.html#rstudio-a-quick-tour",
    "title": "3  R mechanics",
    "section": "3.3 RStudio: A Quick Tour",
    "text": "3.3 RStudio: A Quick Tour\nThe RStudio interface has multiple panes. All of these panes are simply for convenience except the “Console” panel, typically in the lower left corner (by default). The console pane contains the running R interface. If you choose to run R outside RStudio, the interaction will be identical to working in the console pane. This is useful to keep in mind as some environments, such as a computer cluster, encourage using R without RStudio.\n\nPanes\nOptions\nHelp\nEnvironment, History, and Files"
  },
  {
    "objectID": "r_intro_mechanics.html#interacting-with-r",
    "href": "r_intro_mechanics.html#interacting-with-r",
    "title": "3  R mechanics",
    "section": "3.4 Interacting with R",
    "text": "3.4 Interacting with R\nThe only meaningful way of interacting with R is by typing into the R console. At the most basic level, anything that we type at the command line will fall into one of two categories:\n\nAssignments\n\nx = 1\ny &lt;- 2\n\nExpressions\n\n1 + pi + sin(42)\n\n[1] 3.225071\n\n\n\nThe assignment type is obvious because either the The &lt;- or = are used. Note that when we type expressions, R will return a result. In this case, the result of R evaluating 1 + pi + sin(42) is 3.2250711.\nThe standard R prompt is a “&gt;” sign. When present, R is waiting for the next expression or assignment. If a line is not a complete R command, R will continue the next line with a “+”. For example, typing the fillowing with a “Return” after the second “+” will result in R giving back a “+” on the next line, a prompt to keep typing.\n\n1 + pi +\nsin(3.7)\n\n[1] 3.611757\n\n\nR can be used as a glorified calculator by using R expressions. Mathematical operations include:\n\nAddition: +\nSubtraction: -\nMultiplication: *\nDivision: /\nExponentiation: ^\nModulo: %%\n\nThe ^ operator raises the number to its left to the power of the number to its right: for example 3^2 is 9. The modulo returns the remainder of the division of the number to the left by the number on its right, for example 5 modulo 3 or 5 %% 3 is 2.\n\n3.4.1 Expressions\n\n5 + 2\n28 %% 3\n3^2\n5 + 4 * 4 + 4 ^ 4 / 10\n\nNote that R follows order-of-operations and groupings based on parentheses.\n\n5 + 4 / 9\n(5 + 4) / 9\n\n\n\n3.4.2 Assignment\nWhile using R as a calculator is interesting, to do useful and interesting things, we need to assign values to objects. To create objects, we need to give it a name followed by the assignment operator &lt;- (or, entirely equivalently, =) and the value we want to give it:\n\nweight_kg &lt;- 55 \n\n&lt;- is the assignment operator. Assigns values on the right to objects on the left, it is like an arrow that points from the value to the object. Using an = is equivalent (in nearly all cases). Learn to use &lt;- as it is good programming practice.\nObjects can be given any name such as x, current_temperature, or subject_id (see below). You want your object names to be explicit and not too long. They cannot start with a number (2x is not valid but x2 is). R is case sensitive (e.g., weight_kg is different from Weight_kg). There are some names that cannot be used because they represent the names of fundamental functions in R (e.g., if, else, for, see here for a complete list). In general, even if it’s allowed, it’s best to not use other function names, which we’ll get into shortly (e.g., c, T, mean, data, df, weights). When in doubt, check the help to see if the name is already in use. It’s also best to avoid dots (.) within a variable name as in my.dataset. It is also recommended to use nouns for variable names, and verbs for function names.\nWhen assigning a value to an object, R does not print anything. You can force to print the value by typing the name:\n\nweight_kg\n\n[1] 55\n\n\nNow that R has weight_kg in memory, which R refers to as the “global environment”, we can do arithmetic with it. For instance, we may want to convert this weight in pounds (weight in pounds is 2.2 times the weight in kg).\n\n2.2 * weight_kg\n\n[1] 121\n\n\nWe can also change a variable’s value by assigning it a new one:\n\nweight_kg &lt;- 57.5\n2.2 * weight_kg\n\n[1] 126.5\n\n\nThis means that assigning a value to one variable does not change the values of other variables. For example, let’s store the animal’s weight in pounds in a variable.\n\nweight_lb &lt;- 2.2 * weight_kg\n\nand then change weight_kg to 100.\n\nweight_kg &lt;- 100\n\nWhat do you think is the current content of the object weight_lb, 126.5 or 220?\nYou can see what objects (variables) are stored by viewing the Environment tab in Rstudio. You can also use the ls() function. You can remove objects (variables) with the rm() function. You can do this one at a time or remove several objects at once. You can also use the little broom button in your environment pane to remove everything from your environment.\n\nls()\nrm(weight_lb, weight_kg)\nls()\n\nWhat happens when you type the following, now?\n\nweight_lb # oops! you should get an error because weight_lb no longer exists!"
  },
  {
    "objectID": "r_intro_mechanics.html#rules-for-names-in-r",
    "href": "r_intro_mechanics.html#rules-for-names-in-r",
    "title": "3  R mechanics",
    "section": "3.5 Rules for Names in R",
    "text": "3.5 Rules for Names in R\nR allows users to assign names to objects such as variables, functions, and even dimensions of data. However, these names must follow a few rules.\n\nNames may contain any combination of letters, numbers, underscore, and “.”\nNames may not start with numbers, underscore.\nR names are case-sensitive.\n\nExamples of valid R names include:\npi\nx\ncamelCaps\nmy_stuff\nMY_Stuff\nthis.is.the.name.of.the.man\nABC123\nabc1234asdf\n.hi"
  },
  {
    "objectID": "r_intro_mechanics.html#resources-for-getting-help",
    "href": "r_intro_mechanics.html#resources-for-getting-help",
    "title": "3  R mechanics",
    "section": "3.6 Resources for Getting Help",
    "text": "3.6 Resources for Getting Help\nThere is extensive built-in help and documentation within R. A separate page contains a collection of additional resources.\nIf the name of the function or object on which help is sought is known, the following approaches with the name of the function or object will be helpful. For a concrete example, examine the help for the print method.\n\nhelp(print)\nhelp('print')\n?print\n\nIf the name of the function or object on which help is sought is not known, the following from within R will be helpful.\n\nhelp.search('microarray')\nRSiteSearch('microarray')\napropos('histogram')\n\nThere are also tons of online resources that Google will include in searches if online searching feels more appropriate.\nI strongly recommend using help(\"newfunction\"\") for all functions that are new or unfamiliar to you."
  },
  {
    "objectID": "r_intro_mechanics.html#further-practice",
    "href": "r_intro_mechanics.html#further-practice",
    "title": "3  R mechanics",
    "section": "3.7 Further practice",
    "text": "3.7 Further practice\nIf you are entirely new to R, you may want to complete an R tutorial to gain further experience with the basics of programming and R syntax.\nOne R-based system, swirl, teaches you R programming and data science interactively, at your own pace and in the R console. Once you have R installed, you can install swirl and run it the following way:\n\ninstall.packages(\"swirl\")\nlibrary(swirl)\nswirl()\n\nAlternatively you can take the try R interactive class from Code School.\nThere are also many open and free resources and reference guides for R. Two examples are:\n\nQuick-R: a quick online reference for data input, basic statistics and plots\nR reference card PDF by Tom Short\nRstudio cheatsheets"
  },
  {
    "objectID": "r_intro_mechanics.html#exercises",
    "href": "r_intro_mechanics.html#exercises",
    "title": "3  R mechanics",
    "section": "3.8 Exercises",
    "text": "3.8 Exercises\n\nWithout using R, what are the values of the following?\n\nmass &lt;- 50              # mass?\nage  &lt;- 30              # age?\nmass &lt;- mass * 2        # mass?\nage  &lt;- age - 10        # age?\nmass_index &lt;- mass/age  # massIndex?\n\nUse the R help() function to find information about the “hist” function. Follow up with running the example using example(\"hist\").\nWhich of these is a valid R name for a variable?\n\nx2\n2x\n.abc\nabc.123\n.123\n_my_value\nmy_value\nmy.value"
  },
  {
    "objectID": "r_basics.html#the-r-user-interface",
    "href": "r_basics.html#the-r-user-interface",
    "title": "4  The Very Basics",
    "section": "4.1 The R User Interface",
    "text": "4.1 The R User Interface\nRStudio gives us a way to talk to the computer. R gives us a language to speak in. To get started, open RStudio just as you would open any other application on your computer. When you do, a window should appear in your screen like the one shown in (fig?).\n\n\n\nFigure 4.1: Your computer does your bidding when you type R commands at the prompt in the bottom line of the console pane. Don’t forget to hit the Enter key. When you first open RStudio, the console appears in the pane on your left, but you can change this with File &gt; Tools &gt; Global Options in the menu bar.\n\n\nThe RStudio interface is simple. You type R code into the bottom line of the RStudio console pane and then click Enter to run it. The code you type is called a command, because it will command your computer to do something for you. The line you type it into is called the command line.\nWhen you type a command at the prompt and hit Enter, your computer executes the command and shows you the results. Then RStudio displays a fresh prompt for your next command. For example, if you type 1 + 1 and hit Enter, RStudio will display:\n&gt; 1 + 1\n[1] 2\n&gt;\nYou’ll notice that a [1] appears next to your result. R is just letting you know that this line begins with the first value in your result. Some commands return more than one value, and their results may fill up multiple lines. For example, the command 100:130 returns 31 values; it creates a sequence of integers from 100 to 130. Notice that new bracketed numbers appear at the start of the second and third lines of output. These numbers just mean that the second line begins with the 14th value in the result, and the third line begins with the 25th value. You can mostly ignore the numbers that appear in brackets:\n&gt; 100:130\n [1] 100 101 102 103 104 105 106 107 108 109 110 111 112\n[14] 113 114 115 116 117 118 119 120 121 122 123 124 125\n[25] 126 127 128 129 130\n\n\n\n\n\n\nTip\n\n\n\nThe colon operator (:) returns every integer between two integers. It is an easy way to create a sequence of numbers.\n\n\n\n\n\n\n\n\nNote\n\n\n\nIsn’t R a language?\nYou may hear me speak of R in the third person. For example, I might say, “Tell R to do this” or “Tell R to do that”, but of course R can’t do anything; it is just a language. This way of speaking is shorthand for saying, “Tell your computer to do this by writing a command in the R language at the command line of your RStudio console.” Your computer, and not R, does the actual work.\nIs this shorthand confusing and slightly lazy to use? Yes. Do a lot of people use it? Everyone I know–probably because it is so convenient.\n\n\n\n\n\n\n\n\nNote\n\n\n\nWhen do we compile?\nIn some languages, like C, Java, and FORTRAN, you have to compile your human-readable code into machine-readable code (often 1s and 0s) before you can run it. If you’ve programmed in such a language before, you may wonder whether you have to compile your R code before you can use it. The answer is no. R is a dynamic programming language, which means R automatically interprets your code as you run it.\n\n\nIf you type an incomplete command and press Enter, R will display a + prompt, which means R is waiting for you to type the rest of your command. Either finish the command or hit Escape to start over:\n&gt; 5 -\n+\n+ 1\n[1] 4\nIf you type a command that R doesn’t recognize, R will return an error message. If you ever see an error message, don’t panic. R is just telling you that your computer couldn’t understand or do what you asked it to do. You can then try a different command at the next prompt:\n&gt; 3 % 5\nError: unexpected input in \"3 % 5\"\n&gt;\nOnce you get the hang of the command line, you can easily do anything in R that you would do with a calculator. For example, you could do some basic arithmetic:\n2 * 3   \n## 6\n\n4 - 1   \n## 3\n\n6 / (4 - 1)   \n## 2\nDid you notice something different about this code? I’ve left out the &gt;’s and [1]’s. This will make the code easier to copy and paste if you want to put it in your own console.\nR treats the hashtag character, #, in a special way; R will not run anything that follows a hashtag on a line. This makes hashtags very useful for adding comments and annotations to your code. Humans will be able to read the comments, but your computer will pass over them. The hashtag is known as the commenting symbol in R.\nFor the remainder of the book, I’ll use hashtags to display the output of R code. I’ll use a single hashtag to add my own comments and a double hashtag, ##, to display the results of code. I’ll avoid showing &gt;s and [1]s unless I want you to look at them.\n\n\n\n\n\n\nImportant\n\n\n\nCancelling commands\nSome R commands may take a long time to run. You can cancel a command once it has begun by pressing ctrl + c. Note that it may also take R a long time to cancel the command.\n\n\n\nThat's the basic interface for executing R code in RStudio. Think you have it? If so, try doing these simple tasks. If you execute everything correctly, you should end up with the same number that you started with: \n\n1. Choose any number and add 2 to it.\n2. Multiply the result by 3.\n3. Subtract 6 from the answer.\n4. Divide what you get by 3.\n\nThroughout the book, I’ll put exercises in chunks, like the one above. I’ll follow each exercise with a model answer, like the one below.\n\nYou could start with the number 10, and then do the following steps:\n\n10 + 2\n## 12\n\n12 * 3\n## 36\n\n36 - 6\n## 30\n\n30 / 3\n## 10"
  },
  {
    "objectID": "r_basics.html#objects",
    "href": "r_basics.html#objects",
    "title": "4  The Very Basics",
    "section": "4.2 Objects",
    "text": "4.2 Objects\nNow that you know how to use R, let’s use it to make a virtual die. The : operator from a couple of pages ago gives you a nice way to create a group of numbers from one to six. The : operator returns its results as a vector, a one-dimensional set of numbers:\n1:6\n## 1 2 3 4 5 6\nThat’s all there is to how a virtual die looks! But you are not done yet. Running 1:6 generated a vector of numbers for you to see, but it didn’t save that vector anywhere in your computer’s memory. What you are looking at is basically the footprints of six numbers that existed briefly and then melted back into your computer’s RAM. If you want to use those numbers again, you’ll have to ask your computer to save them somewhere. You can do that by creating an R object.\nR lets you save data by storing it inside an R object. What is an object? Just a name that you can use to call up stored data. For example, you can save data into an object like a or b. Wherever R encounters the object, it will replace it with the data saved inside, like so:\na &lt;- 1\na\n## 1\n\na + 2\n## 3\n\nWhat just happened?\n\nTo create an R object, choose a name and then use the less-than symbol, &lt;, followed by a minus sign, -, to save data into it. This combination looks like an arrow, &lt;-. R will make an object, give it your name, and store in it whatever follows the arrow. So a &lt;- 1 stores 1 in an object named a.\nWhen you ask R what’s in a, R tells you on the next line.\nYou can use your object in new R commands, too. Since a previously stored the value of 1, you’re now adding 1 to 2.\n\n\nSo, for another example, the following code would create an object named die that contains the numbers one through six. To see what is stored in an object, just type the object’s name by itself:\ndie &lt;- 1:6\n\ndie\n## 1 2 3 4 5 6\nWhen you create an object, the object will appear in the environment pane of RStudio, as shown in Figure @ref(fig:environment). This pane will show you all of the objects you’ve created since opening RStudio.\n\n\n\nYou can name an object in R almost anything you want, but there are a few rules. First, a name cannot start with a number. Second, a name cannot use some special symbols, like ^, !, $, @, +, -, /, or *:\n\n\n\nGood names\nNames that cause errors\n\n\n\n\na\n1trial\n\n\nb\n$\n\n\nFOO\n^mean\n\n\nmy_var\n2nd\n\n\n.day\n!bad\n\n\n\n\n\n\n\n\n\nWarning\n\n\n\nCapitalization\nR is case-sensitive, so name and Name will refer to different objects:\nName &lt;- 1\nname &lt;- 0\nName + 1\n## 2\n\n\nFinally, R will overwrite any previous information stored in an object without asking you for permission. So, it is a good idea to not use names that are already taken:\nmy_number &lt;- 1\nmy_number \n## 1\n\nmy_number &lt;- 999\nmy_number\n## 999\nYou can see which object names you have already used with the function ls:\nls()\n## \"a\"         \"die\"       \"my_number\" \"name\"     \"Name\"     \nYou can also see which names you have used by examining RStudio’s environment pane.\nYou now have a virtual die that is stored in your computer’s memory. You can access it whenever you like by typing the word die. So what can you do with this die? Quite a lot. R will replace an object with its contents whenever the object’s name appears in a command. So, for example, you can do all sorts of math with the die. Math isn’t so helpful for rolling dice, but manipulating sets of numbers will be your stock and trade as a data scientist. So let’s take a look at how to do that:\ndie - 1\n## 0 1 2 3 4 5\n\ndie / 2\n## 0.5 1.0 1.5 2.0 2.5 3.0\n\ndie * die\n## 1  4  9 16 25 36\nIf you are a big fan of linear algebra (and who isn’t?), you may notice that R does not always follow the rules of matrix multiplication. Instead, R uses element-wise execution. When you manipulate a set of numbers, R will apply the same operation to each element in the set. So for example, when you run die - 1, R subtracts one from each element of die.\nWhen you use two or more vectors in an operation, R will line up the vectors and perform a sequence of individual operations. For example, when you run die * die, R lines up the two die vectors and then multiplies the first element of vector 1 by the first element of vector 2. R then multiplies the second element of vector 1 by the second element of vector 2, and so on, until every element has been multiplied. The result will be a new vector the same length as the first two, as shown in Figure @ref(fig:elementwise).\n\n\n\nIf you give R two vectors of unequal lengths, R will repeat the shorter vector until it is as long as the longer vector, and then do the math, as shown in Figure @ref(fig:recycle). This isn’t a permanent change–the shorter vector will be its original size after R does the math. If the length of the short vector does not divide evenly into the length of the long vector, R will return a warning message. This behavior is known as vector recycling, and it helps R do element-wise operations:\n1:2\n## 1 2\n\n1:4\n## 1 2 3 4\n\ndie\n## 1 2 3 4 5 6\n\ndie + 1:2\n## 2 4 4 6 6 8\n\ndie + 1:4\n## 2 4 6 8 6 8\nWarning message:\nIn die + 1:4 :\n  longer object length is not a multiple of shorter object length\n\n\n\nElement-wise operations are a very useful feature in R because they manipulate groups of values in an orderly way. When you start working with data sets, element-wise operations will ensure that values from one observation or case are only paired with values from the same observation or case. Element-wise operations also make it easier to write your own programs and functions in R.\nBut don’t think that R has given up on traditional matrix multiplication. You just have to ask for it when you want it. You can do inner multiplication with the %*% operator and outer multiplication with the %o% operator:\ndie %*% die\n## 91\n\ndie %o% die\n##      [,1] [,2] [,3] [,4] [,5] [,6]\n## [1,]    1    2    3    4    5    6\n## [2,]    2    4    6    8   10   12\n## [3,]    3    6    9   12   15   18\n## [4,]    4    8   12   16   20   24\n## [5,]    5   10   15   20   25   30\n## [6,]    6   12   18   24   30   36\nYou can also do things like transpose a matrix with t and take its determinant with det.\nDon’t worry if you’re not familiar with these operations. They are easy to look up, and you won’t need them for this book.\nNow that you can do math with your die object, let’s look at how you could “roll” it. Rolling your die will require something more sophisticated than basic arithmetic; you’ll need to randomly select one of the die’s values. And for that, you will need a function."
  },
  {
    "objectID": "r_basics.html#functions",
    "href": "r_basics.html#functions",
    "title": "4  The Very Basics",
    "section": "4.3 Functions",
    "text": "4.3 Functions\nR comes with many functions that you can use to do sophisticated tasks like random sampling. For example, you can round a number with the round function, or calculate its factorial with the factorial function. Using a function is pretty simple. Just write the name of the function and then the data you want the function to operate on in parentheses:\nround(3.1415)\n## 3\n\nfactorial(3)\n## 6\nThe data that you pass into the function is called the function’s argument. The argument can be raw data, an R object, or even the results of another R function. In this last case, R will work from the innermost function to the outermost, as in Figure @ref(fig:pemdas).\nmean(1:6)\n## 3.5\n\nmean(die)\n## 3.5\n\nround(mean(die))\n## 4\n\n\n\nLucky for us, there is an R function that can help “roll” the die. You can simulate a roll of the die with R’s sample function. sample takes two arguments: a vector named x and a number named size. sample will return size elements from the vector:\nsample(x = 1:4, size = 2)\n## 3 2\nTo roll your die and get a number back, set x to die and sample one element from it. You’ll get a new (maybe different) number each time you roll it:\nsample(x = die, size = 1)\n## 2\n\nsample(x = die, size = 1)\n## 1\n\nsample(x = die, size = 1)\n## 6\nMany R functions take multiple arguments that help them do their job. You can give a function as many arguments as you like as long as you separate each argument with a comma.\nYou may have noticed that I set die and 1 equal to the names of the arguments in sample, x and size. Every argument in every R function has a name. You can specify which data should be assigned to which argument by setting a name equal to data, as in the preceding code. This becomes important as you begin to pass multiple arguments to the same function; names help you avoid passing the wrong data to the wrong argument. However, using names is optional. You will notice that R users do not often use the name of the first argument in a function. So you might see the previous code written as:\nsample(die, size = 1)\n## 2\nOften, the name of the first argument is not very descriptive, and it is usually obvious what the first piece of data refers to anyways.\nBut how do you know which argument names to use? If you try to use a name that a function does not expect, you will likely get an error:\nround(3.1415, corners = 2)\n## Error in round(3.1415, corners = 2) : unused argument(s) (corners = 2)\nIf you’re not sure which names to use with a function, you can look up the function’s arguments with args. To do this, place the name of the function in the parentheses behind args. For example, you can see that the round function takes two arguments, one named x and one named digits:\nargs(round)\n## function (x, digits = 0) \n## NULL\nDid you notice that args shows that the digits argument of round is already set to 0? Frequently, an R function will take optional arguments like digits. These arguments are considered optional because they come with a default value. You can pass a new value to an optional argument if you want, and R will use the default value if you do not. For example, round will round your number to 0 digits past the decimal point by default. To override the default, supply your own value for digits:\nround(3.1415)\n## 3\n\nround(3.1415, digits = 2)\n## 3.14\nYou should write out the names of each argument after the first one or two when you call a function with multiple arguments. Why? First, this will help you and others understand your code. It is usually obvious which argument your first input refers to (and sometimes the second input as well). However, you’d need a large memory to remember the third and fourth arguments of every R function. Second, and more importantly, writing out argument names prevents errors.\nIf you do not write out the names of your arguments, R will match your values to the arguments in your function by order. For example, in the following code, the first value, die, will be matched to the first argument of sample, which is named x. The next value, 1, will be matched to the next argument, size:\nsample(die, 1)\n## 2\nAs you provide more arguments, it becomes more likely that your order and R’s order may not align. As a result, values may get passed to the wrong argument. Argument names prevent this. R will always match a value to its argument name, no matter where it appears in the order of arguments:\nsample(size = 1, x = die)\n## 2\n\n4.3.1 Sample with Replacement\nIf you set size = 2, you can almost simulate a pair of dice. Before we run that code, think for a minute why that might be the case. sample will return two numbers, one for each die:\nsample(die, size = 2)\n## 3 4\nI said this “almost” works because this method does something funny. If you use it many times, you’ll notice that the second die never has the same value as the first die, which means you’ll never roll something like a pair of threes or snake eyes. What is going on?\nBy default, sample builds a sample without replacement. To see what this means, imagine that sample places all of the values of die in a jar or urn. Then imagine that sample reaches into the jar and pulls out values one by one to build its sample. Once a value has been drawn from the jar, sample sets it aside. The value doesn’t go back into the jar, so it cannot be drawn again. So if sample selects a six on its first draw, it will not be able to select a six on the second draw; six is no longer in the jar to be selected. Although sample creates its sample electronically, it follows this seemingly physical behavior.\nOne side effect of this behavior is that each draw depends on the draws that come before it. In the real world, however, when you roll a pair of dice, each die is independent of the other. If the first die comes up six, it does not prevent the second die from coming up six. In fact, it doesn’t influence the second die in any way whatsoever. You can recreate this behavior in sample by adding the argument replace = TRUE:\nsample(die, size = 2, replace = TRUE)\n## 5 5\nThe argument replace = TRUE causes sample to sample with replacement. Our jar example provides a good way to understand the difference between sampling with replacement and without. When sample uses replacement, it draws a value from the jar and records the value. Then it puts the value back into the jar. In other words, sample replaces each value after each draw. As a result, sample may select the same value on the second draw. Each value has a chance of being selected each time. It is as if every draw were the first draw.\nSampling with replacement is an easy way to create independent random samples. Each value in your sample will be a sample of size one that is independent of the other values. This is the correct way to simulate a pair of dice:\nsample(die, size = 2, replace = TRUE)\n## 2 4\nCongratulate yourself; you’ve just run your first simulation in R! You now have a method for simulating the result of rolling a pair of dice. If you want to add up the dice, you can feed your result straight into the sum function:\ndice &lt;- sample(die, size = 2, replace = TRUE)\ndice\n## 2 4\n\nsum(dice)\n## 6\nWhat would happen if you call dice multiple times? Would R generate a new pair of dice values each time? Let’s give it a try:\ndice\n## 2 4\n\ndice\n## 2 4\n\ndice\n## 2 4\nNope. Each time you call dice, R will show you the result of that one time you called sample and saved the output to dice. R won’t rerun sample(die, 2, replace = TRUE) to create a new roll of the dice. This is a relief in a way. Once you save a set of results to an R object, those results do not change. Programming would be quite hard if the values of your objects changed each time you called them.\nHowever, it would be convenient to have an object that can re-roll the dice whenever you call it. You can make such an object by writing your own R function."
  },
  {
    "objectID": "r_basics.html#write-functions",
    "href": "r_basics.html#write-functions",
    "title": "4  The Very Basics",
    "section": "4.4 Writing Your Own Functions",
    "text": "4.4 Writing Your Own Functions\nTo recap, you already have working R code that simulates rolling a pair of dice:\ndie &lt;- 1:6\ndice &lt;- sample(die, size = 2, replace = TRUE)\nsum(dice)\nYou can retype this code into the console anytime you want to re-roll your dice. However, this is an awkward way to work with the code. It would be easier to use your code if you wrapped it into its own function, which is exactly what we’ll do now. We’re going to write a function named roll that you can use to roll your virtual dice. When you’re finished, the function will work like this: each time you call roll(), R will return the sum of rolling two dice:\nroll()\n## 8 \n\nroll()\n## 3\n\nroll()\n## 7\nFunctions may seem mysterious or fancy, but they are just another type of R object. Instead of containing data, they contain code. This code is stored in a special format that makes it easy to reuse the code in new situations. You can write your own functions by recreating this format.\n\n4.4.1 The Function Constructor\nEvery function in R has three basic parts: a name, a body of code, and a set of arguments. To make your own function, you need to replicate these parts and store them in an R object, which you can do with the function function. To do this, call function() and follow it with a pair of braces, {}:\nmy_function &lt;- function() {}\nfunction will build a function out of whatever R code you place between the braces. For example, you can turn your dice code into a function by calling:\nroll &lt;- function() {\n  die &lt;- 1:6\n  dice &lt;- sample(die, size = 2, replace = TRUE)\n  sum(dice)\n}\n\n\n\n\n\n\nNote\n\n\n\nNotice that I indented each line of code between the braces. This makes the code easier for you and me to read but has no impact on how the code runs. R ignores spaces and line breaks and executes one complete expression at a time.\n\n\nJust hit the Enter key between each line after the first brace, {. R will wait for you to type the last brace, }, before it responds.\nDon’t forget to save the output of function to an R object. This object will become your new function. To use it, write the object’s name followed by an open and closed parenthesis:\nroll()\n## 9\nYou can think of the parentheses as the “trigger” that causes R to run the function. If you type in a function’s name without the parentheses, R will show you the code that is stored inside the function. If you type in the name with the parentheses, R will run that code:\nroll\n## function() {\n##   die &lt;- 1:6\n##   dice &lt;- sample(die, size = 2, replace = TRUE)\n##   sum(dice)\n## }\n\nroll()\n## 6\nThe code that you place inside your function is known as the body of the function. When you run a function in R, R will execute all of the code in the body and then return the result of the last line of code. If the last line of code doesn’t return a value, neither will your function, so you want to ensure that your final line of code returns a value. One way to check this is to think about what would happen if you ran the body of code line by line in the command line. Would R display a result after the last line, or would it not?\nHere’s some code that would display a result:\ndice\n1 + 1\nsqrt(2)\nAnd here’s some code that would not:\ndice &lt;- sample(die, size = 2, replace = TRUE)\ntwo &lt;- 1 + 1\na &lt;- sqrt(2)\nDo you notice the pattern? These lines of code do not return a value to the command line; they save a value to an object."
  },
  {
    "objectID": "r_basics.html#arguments",
    "href": "r_basics.html#arguments",
    "title": "4  The Very Basics",
    "section": "4.5 Arguments",
    "text": "4.5 Arguments\nWhat if we removed one line of code from our function and changed the name die to bones, like this?\nroll2 &lt;- function() {\n  dice &lt;- sample(bones, size = 2, replace = TRUE)\n  sum(dice)\n}\nNow I’ll get an error when I run the function. The function needs the object bones to do its job, but there is no object named bones to be found:\nroll2()\n## Error in sample(bones, size = 2, replace = TRUE) : \n##   object 'bones' not found\nYou can supply bones when you call roll2 if you make bones an argument of the function. To do this, put the name bones in the parentheses that follow function when you define roll2:\nroll2 &lt;- function(bones) {\n  dice &lt;- sample(bones, size = 2, replace = TRUE)\n  sum(dice)\n}\nNow roll2 will work as long as you supply bones when you call the function. You can take advantage of this to roll different types of dice each time you call roll2. Dungeons and Dragons, here we come!\nRemember, we’re rolling pairs of dice:\nroll2(bones = 1:4)\n##  3\n\nroll2(bones = 1:6)\n## 10\n\nroll2(1:20)\n## 31\nNotice that roll2 will still give an error if you do not supply a value for the bones argument when you call roll2:\nroll2()\n## Error in sample(bones, size = 2, replace = TRUE) : \n##   argument \"bones\" is missing, with no default\nYou can prevent this error by giving the bones argument a default value. To do this, set bones equal to a value when you define roll2:\nroll2 &lt;- function(bones = 1:6) {\n  dice &lt;- sample(bones, size = 2, replace = TRUE)\n  sum(dice)\n}\nNow you can supply a new value for bones if you like, and roll2 will use the default if you do not:\nroll2()\n## 9\nYou can give your functions as many arguments as you like. Just list their names, separated by commas, in the parentheses that follow function. When the function is run, R will replace each argument name in the function body with the value that the user supplies for the argument. If the user does not supply a value, R will replace the argument name with the argument’s default value (if you defined one).\nTo summarize, function helps you construct your own R functions. You create a body of code for your function to run by writing code between the braces that follow function. You create arguments for your function to use by supplying their names in the parentheses that follow function. Finally, you give your function a name by saving its output to an R object, as shown in Figure @ref(fig:functions).\nOnce you’ve created your function, R will treat it like every other function in R. Think about how useful this is. Have you ever tried to create a new Excel option and add it to Microsoft’s menu bar? Or a new slide animation and add it to Powerpoint’s options? When you work with a programming language, you can do these types of things. As you learn to program in R, you will be able to create new, customized, reproducible tools for yourself whenever you like. Project 3: Slot Machine will teach you much more about writing functions in R."
  },
  {
    "objectID": "r_basics.html#scripts",
    "href": "r_basics.html#scripts",
    "title": "4  The Very Basics",
    "section": "4.6 Scripts",
    "text": "4.6 Scripts\nWhat if you want to edit roll2 again? You could go back and retype each line of code in roll2, but it would be so much easier if you had a draft of the code to start from. You can create a draft of your code as you go by using an R script. An R script is just a plain text file that you save R code in. You can open an R script in RStudio by going to File &gt; New File &gt; R script in the menu bar. RStudio will then open a fresh script above your console pane, as shown in Figure @ref(fig:script).\nI strongly encourage you to write and edit all of your R code in a script before you run it in the console. Why? This habit creates a reproducible record of your work. When you’re finished for the day, you can save your script and then use it to rerun your entire analysis the next day. Scripts are also very handy for editing and proofreading your code, and they make a nice copy of your work to share with others. To save a script, click the scripts pane, and then go to File &gt; Save As in the menu bar.\n\n\n\nRStudio comes with many built-in features that make it easy to work with scripts. First, you can automatically execute a line of code in a script by clicking the Run button, as shown in Figure @ref(fig:run).\nR will run whichever line of code your cursor is on. If you have a whole section highlighted, R will run the highlighted code. Alternatively, you can run the entire script by clicking the Source button. Don’t like clicking buttons? You can use Control + Return as a shortcut for the Run button. On Macs, that would be Command + Return.\n\n\n\nIf you’re not convinced about scripts, you soon will be. It becomes a pain to write multi-line code in the console’s single-line command line. Let’s avoid that headache and open your first script now before we move to the next chapter.\n\n\n\n\n\n\nTip\n\n\n\nExtract function\nRStudio comes with a tool that can help you build functions. To use it, highlight the lines of code in your R script that you want to turn into a function. Then click Code &gt; Extract Function in the menu bar. RStudio will ask you for a function name to use and then wrap your code in a function call. It will scan the code for undefined variables and use these as arguments.\nYou may want to double-check RStudio’s work. It assumes that your code is correct, so if it does something surprising, you may have a problem in your code."
  },
  {
    "objectID": "r_basics.html#summary",
    "href": "r_basics.html#summary",
    "title": "4  The Very Basics",
    "section": "4.7 Summary",
    "text": "4.7 Summary\nYou’ve covered a lot of ground already. You now have a virtual die stored in your computer’s memory, as well as your own R function that rolls a pair of dice. You’ve also begun speaking the R language.\nAs you’ve seen, R is a language that you can use to talk to your computer. You write commands in R and run them at the command line for your computer to read. Your computer will sometimes talk back–for example, when you commit an error–but it usually just does what you ask and then displays the result.\nThe two most important components of the R language are objects, which store data, and functions, which manipulate data. R also uses a host of operators like +, -, *, /, and &lt;- to do basic tasks. As a data scientist, you will use R objects to store data in your computer’s memory, and you will use functions to automate tasks and do complicated calculations. We will examine objects in more depth later in [Project 2: Playing Cards] and dig further into functions in [Project 3: Slot Machine]. The vocabulary you have developed here will make each of those projects easier to understand. However, we’re not done with your dice yet.\nIn Packages and Help Pages, you’ll run some simulations on your dice and build your first graphs in R. You’ll also look at two of the most useful components of the R language: R packages, which are collections of functions writted by R’s talented community of developers, and R documentation, which is a collection of help pages built into R that explains every function and data set in the language."
  },
  {
    "objectID": "data_structures_overview.html#chapter-overview",
    "href": "data_structures_overview.html#chapter-overview",
    "title": "Overview of R Data Structures",
    "section": "Chapter overview",
    "text": "Chapter overview\n\nVectors : In this chapter, we will introduce you to the simplest data structure in R, the vector. We will cover how to create, access, and manipulate vectors, as well as discuss their unique properties and limitations.\n\nMatrices\n\nNext, we will explore matrices, which are two-dimensional data structures that extend vectors. You will learn how to create, access, and manipulate matrices, and understand their usefulness in mathematical operations and data organization.\n\n\n\nLists\n\nThe third chapter will focus on lists, a versatile data structure that can store elements of different types and sizes. We will discuss how to create, access, and modify lists, and demonstrate their flexibility in handling complex data structures.\n\n\n\nData.frames\n\nFinally, we will examine data.frames, a widely-used data structure for organizing and manipulating tabular data. You will learn how to create, access, and manipulate data.frames, and understand their advantages over other data structures for data analysis tasks.\n\n\n\nArrays\n\nWhile we will not focus directly on the array data type, which are multidimensional data structures that extend matrices, they are very similar to matrices, but with a third dimension.\n\n\n\nAs you progress through these chapters, we encourage you to practice the examples and exercises provided, engage in discussion, and collaborate with your peers to deepen your understanding of R data structures. This solid foundation will serve as the basis for more advanced data manipulation, analysis, and visualization techniques in R."
  },
  {
    "objectID": "vectors.html#what-is-a-vector",
    "href": "vectors.html#what-is-a-vector",
    "title": "5  Vectors",
    "section": "5.1 What is a Vector?",
    "text": "5.1 What is a Vector?\nA vector is the simplest and most basic data structure in R. It is a one-dimensional, ordered collection of elements, where all the elements are of the same data type. Vectors can store various types of data, such as numeric, character, or logical values.\nIn this chapter, we will provide a comprehensive overview of vectors, including how to create, access, and manipulate them. We will also discuss some unique properties and rules associated with vectors, and explore their applications in data analysis tasks."
  },
  {
    "objectID": "vectors.html#creating-vectors",
    "href": "vectors.html#creating-vectors",
    "title": "5  Vectors",
    "section": "5.2 Creating Vectors",
    "text": "5.2 Creating Vectors\nIn this section, we will cover different ways of creating vectors. The most common method for creating vectors is by using the c() function, which stands for “concatenate.” The c() function combines its arguments into a single vector.\nHere’s an example of creating a numeric vector:\n\nnumeric_vector &lt;- c(3, 5, 7, 9)\nprint(numeric_vector)\n\n[1] 3 5 7 9\n\n\nSimilarly, you can create a character vector and a logical vector:\n\ncharacter_vector &lt;- c(\"apple\", \"banana\", \"cherry\")\nprint(character_vector)\n\n[1] \"apple\"  \"banana\" \"cherry\"\n\n\nA logical vector is a one-dimensional array of logical values in R. Logical values are either TRUE or FALSE, which represent the outcomes of logical expressions or conditions. Logical vectors are used for a variety of purposes, such as filtering data, controlling the flow of a program, or performing element-wise comparisons between vectors.\nIn R, you can create a logical vector using the c() function by specifying logical values as its arguments:\n\nlogical_vector &lt;- c(TRUE, FALSE, TRUE, FALSE)\n\n\n\n\n\n\n\nNote\n\n\n\nThe values TRUE and FALSE are reserved words in R, which means that they cannot be used as variable names. If you try to assign a value to TRUE or FALSE, R will throw an error. Also, notice that there are no quotation marks around TRUE and FALSE. The value “TRUE” with quotes is a character vector of length 1.\n\n\nLogical vectors can also be generated by applying logical or comparison operators on other vectors or values:\n\nnumeric_vector &lt;- c(1, 2, 3, 4, 5)\nlogical_vector &lt;- numeric_vector &gt; 3\n\nIn this example, logical_vector will contain the logical outcomes of the comparison numeric_vector &gt; 3, resulting in a logical vector with the values (FALSE, FALSE, FALSE, TRUE, TRUE).\n\nprint(logical_vector)\n\n[1] FALSE FALSE FALSE  TRUE  TRUE\n\n\nYou can also generate sequences using the seq() function or the : operator:\n\nsequence1 &lt;- seq(from = 1, to = 10, by = 2)\nprint(sequence1)\n\n[1] 1 3 5 7 9\n\nsequence2 &lt;- 1:5\nprint(sequence2)\n\n[1] 1 2 3 4 5\n\n\nThe rep() function allows you to create a vector by repeating elements:\n\nrepeated_vector &lt;- rep(1:3, times = 2)\nprint(repeated_vector)\n\n[1] 1 2 3 1 2 3"
  },
  {
    "objectID": "vectors.html#accessing-vector-elements",
    "href": "vectors.html#accessing-vector-elements",
    "title": "5  Vectors",
    "section": "5.3 Accessing Vector Elements",
    "text": "5.3 Accessing Vector Elements\nYou can access and extract elements from vectors using indexing. In R, indexing starts at 1. To access a single element, use square brackets [] and provide the index of the desired element:\n\nnumeric_vector &lt;- c(3, 5, 7, 9)\nthird_element &lt;- numeric_vector[3]\nprint(third_element)\n\n[1] 7\n\n\nTo access multiple elements, provide a vector of indices:\n\nelements &lt;- numeric_vector[c(1, 3)]\nprint(elements)\n\n[1] 3 7\n\n\nYou can also modify and update vector elements using indexing:\n\nnumeric_vector[2] &lt;- 10\nprint(numeric_vector)\n\n[1]  3 10  7  9\n\n\n\n\\begin{tikzpicture}\n\\foreach \\x in {0,1,2,3,4,5,6,7,8,9}\n\\draw (1,0) rectangle (\\x,1);\n\\end{tikzpicture}"
  },
  {
    "objectID": "vectors.html#vector-operations",
    "href": "vectors.html#vector-operations",
    "title": "5  Vectors",
    "section": "5.4 Vector Operations",
    "text": "5.4 Vector Operations\nYou can perform various vector operations, such as arithmetic, logical, and comparison operations. The operations are performed element-wise, which means that the operation is applied to each corresponding pair of elements in the input vectors.\nHere are some examples of arithmetic operations:\n\nvector1 &lt;- c(1, 2, 3)\nvector2 &lt;- c(4, 5, 6)\n\nsum_vectors &lt;- vector1 + vector2\nprint(sum_vectors)\n\n[1] 5 7 9\n\nproduct_vectors &lt;- vector1 * vector2\nprint(product_vectors)\n\n[1]  4 10 18\n\n\nLogical and comparison operations also work element-wise:\n\nvector1 &lt;- c(1, 2, 3)\nvector2 &lt;- c(3, 2, 1)\n\nequal_elements &lt;- vector1 == vector2\nprint(equal_elements)\n\n[1] FALSE  TRUE FALSE\n\ngreater_elements &lt;- vector1 &gt; vector2\nprint(greater_elements)\n\n[1] FALSE FALSE  TRUE\n\n\nOperations on a single vector are typically done element-by-element. For example, we can add 2 to a vector, 2 is added to each element of the vector and a new vector of the same length is returned.\n\nx = 1:10\nx + 2\n\n [1]  3  4  5  6  7  8  9 10 11 12\n\n\nIf the operation involves two vectors, the following rules apply. If the vectors are the same length: R simply applies the operation to each pair of elements.\n\nx + x\n\n [1]  2  4  6  8 10 12 14 16 18 20\n\n\nIf the vectors are different lengths, but one length a multiple of the other, R reuses the shorter vector as needed.\n\nx = 1:10\ny = c(1,2)\nx * y\n\n [1]  1  4  3  8  5 12  7 16  9 20\n\n\nIf the vectors are different lengths, but one length not a multiple of the other, R reuses the shorter vector as needed and delivers a warning.\n\nx = 1:10\ny = c(2,3,4)\nx * y\n\nWarning in x * y: longer object length is not a multiple of shorter object\nlength\n\n\n [1]  2  6 12  8 15 24 14 24 36 20\n\n\nTypical operations include multiplication (“*”), addition, subtraction, division, exponentiation (“^”), but many operations in R operate on vectors and are then called “vectorized”."
  },
  {
    "objectID": "vectors.html#named-vectors",
    "href": "vectors.html#named-vectors",
    "title": "5  Vectors",
    "section": "5.5 Named Vectors",
    "text": "5.5 Named Vectors\nNamed vectors are vectors with labels or names assigned to their elements. These names can be used to access and manipulate the elements in a more meaningful way.\nTo create a named vector, use the names() function:\n\nfruit_prices &lt;- c(0.5, 0.75, 1.25)\nnames(fruit_prices) &lt;- c(\"apple\", \"banana\", \"cherry\")\nprint(fruit_prices)\n\n apple banana cherry \n  0.50   0.75   1.25 \n\n\nYou can also access and modify elements using their names:\n\nbanana_price &lt;- fruit_prices[\"banana\"]\nprint(banana_price)\n\nbanana \n  0.75 \n\nfruit_prices[\"apple\"] &lt;- 0.6\nprint(fruit_prices)\n\n apple banana cherry \n  0.60   0.75   1.25"
  },
  {
    "objectID": "vectors.html#vector-recycling-and-recycling-rule",
    "href": "vectors.html#vector-recycling-and-recycling-rule",
    "title": "5  Vectors",
    "section": "5.6 Vector Recycling and Recycling Rule",
    "text": "5.6 Vector Recycling and Recycling Rule\nVector recycling is a unique feature of R that allows for element-wise operations on vectors of different lengths. When performing operations on vectors of unequal length, R will recycle the shorter vector by repeating its elements until it matches the length of the longer vector.\nHere’s an example of vector recycling:\n\nvector1 &lt;- c(1, 2, 3)\nvector2 &lt;- c(4, 5)\n\nsum_vectors &lt;- vector1 + vector2\n\nWarning in vector1 + vector2: longer object length is not a multiple of shorter\nobject length\n\nprint(sum_vectors)\n\n[1] 5 7 7\n\n\nIn this example, vector2 is shorter than vector1, so its elements are recycled to match the length of vector1. The result is equivalent to adding vector1 to c(4, 5, 4).\nBe aware of the recycling rule when working with vectors of different lengths, as it may lead to unexpected results if you’re not careful."
  },
  {
    "objectID": "vectors.html#active-learning-exercises",
    "href": "vectors.html#active-learning-exercises",
    "title": "5  Vectors",
    "section": "5.7 Active Learning Exercises",
    "text": "5.7 Active Learning Exercises\nNow that we have covered the basics of vectors, let’s put your knowledge into practice with some active learning exercises.\nIn R, even a single value is a vector with length=1.\n\nz = 1\nz\n\n[1] 1\n\nlength(z)\n\n[1] 1\n\n\nIn the code above, we “assigned” the value 1 to the variable named z. Typing z by itself is an “expression” that returns a result which is, in this case, the value that we just assigned. The length method takes an R object and returns the R length. There are numerous ways of asking R about what an object represents, and length is one of them.\nVectors can contain numbers, strings (character data), or logical values (TRUE and FALSE) or other “atomic” data types (tab-simpletypes?). Vectors cannot contain a mix of types! We will introduce another data structure, the R list for situations when we need to store a mix of base R data types.\n\n\n\n\n\nData type\nStores\n\n\n\n\nnumeric\nfloating point numbers\n\n\ninteger\nintegers\n\n\ncomplex\ncomplex numbers\n\n\nfactor\ncategorical data\n\n\ncharacter\nstrings\n\n\nlogical\nTRUE or FALSE\n\n\nNA\nmissing\n\n\nNULL\nempty\n\n\nfunction\nfunction type\n\n\n\nTable 5.1: Atomic (simplest) data types in R."
  },
  {
    "objectID": "vectors.html#creating-vectors-1",
    "href": "vectors.html#creating-vectors-1",
    "title": "5  Vectors",
    "section": "5.8 Creating vectors",
    "text": "5.8 Creating vectors\nCharacter vectors (also sometimes called “string” vectors) are entered with each value surrounded by single or double quotes; either is acceptable, but they must match. They are always displayed by R with double quotes. Here are some examples of creating vectors:\n\n# examples of vectors\nc('hello','world')\n\n[1] \"hello\" \"world\"\n\nc(1,3,4,5,1,2)\n\n[1] 1 3 4 5 1 2\n\nc(1.12341e7,78234.126)\n\n[1] 11234100.00    78234.13\n\nc(TRUE,FALSE,TRUE,TRUE)\n\n[1]  TRUE FALSE  TRUE  TRUE\n\n# note how in the next case the TRUE is converted to \"TRUE\"\n# with quotes around it.\nc(TRUE,'hello')\n\n[1] \"TRUE\"  \"hello\"\n\n\nWe can also create vectors as “regular sequences” of numbers. For example:\n\n# create a vector of integers from 1 to 10\nx = 1:10\n# and backwards\nx = 10:1\n\nThe seq function can create more flexible regular sequences.\n\n# create a vector of numbers from 1 to 4 skipping by 0.3\ny = seq(1,4,0.3)\n\nAnd creating a new vector by concatenating existing vectors is possible, as well.\n\n# create a sequence by concatenating two other sequences\nz = c(y,x)\nz\n\n [1]  1.0  1.3  1.6  1.9  2.2  2.5  2.8  3.1  3.4  3.7  4.0 10.0  9.0  8.0  7.0\n[16]  6.0  5.0  4.0  3.0  2.0  1.0"
  },
  {
    "objectID": "vectors.html#vector-operations-1",
    "href": "vectors.html#vector-operations-1",
    "title": "5  Vectors",
    "section": "5.9 Vector Operations",
    "text": "5.9 Vector Operations\nOperations on a single vector are typically done element-by-element. For example, we can add 2 to a vector, 2 is added to each element of the vector and a new vector of the same length is returned.\n\nx = 1:10\nx + 2\n\n [1]  3  4  5  6  7  8  9 10 11 12\n\n\nIf the operation involves two vectors, the following rules apply. If the vectors are the same length: R simply applies the operation to each pair of elements.\n\nx + x\n\n [1]  2  4  6  8 10 12 14 16 18 20\n\n\nIf the vectors are different lengths, but one length a multiple of the other, R reuses the shorter vector as needed.\n\nx = 1:10\ny = c(1,2)\nx * y\n\n [1]  1  4  3  8  5 12  7 16  9 20\n\n\nIf the vectors are different lengths, but one length not a multiple of the other, R reuses the shorter vector as needed and delivers a warning.\n\nx = 1:10\ny = c(2,3,4)\nx * y\n\nWarning in x * y: longer object length is not a multiple of shorter object\nlength\n\n\n [1]  2  6 12  8 15 24 14 24 36 20\n\n\nTypical operations include multiplication (“*”), addition, subtraction, division, exponentiation (“^”), but many operations in R operate on vectors and are then called “vectorized”."
  },
  {
    "objectID": "vectors.html#logical-vectors",
    "href": "vectors.html#logical-vectors",
    "title": "5  Vectors",
    "section": "5.10 Logical Vectors",
    "text": "5.10 Logical Vectors\nLogical vectors are vectors composed on only the values TRUE and FALSE. Note the all-upper-case and no quotation marks.\n\na = c(TRUE,FALSE,TRUE)\n\n# we can also create a logical vector from a numeric vector\n# 0 = false, everything else is 1\nb = c(1,0,217)\nd = as.logical(b)\nd\n\n[1]  TRUE FALSE  TRUE\n\n# test if a and d are the same at every element\nall.equal(a,d)\n\n[1] TRUE\n\n# We can also convert from logical to numeric\nas.numeric(a)\n\n[1] 1 0 1\n\n\n\n5.10.1 Logical Operators\nSome operators like &lt;, &gt;, ==, &gt;=, &lt;=, != can be used to create logical vectors.\n\n# create a numeric vector\nx = 1:10\n# testing whether x &gt; 5 creates a logical vector\nx &gt; 5\n\n [1] FALSE FALSE FALSE FALSE FALSE  TRUE  TRUE  TRUE  TRUE  TRUE\n\nx &lt;= 5\n\n [1]  TRUE  TRUE  TRUE  TRUE  TRUE FALSE FALSE FALSE FALSE FALSE\n\nx != 5\n\n [1]  TRUE  TRUE  TRUE  TRUE FALSE  TRUE  TRUE  TRUE  TRUE  TRUE\n\nx == 5\n\n [1] FALSE FALSE FALSE FALSE  TRUE FALSE FALSE FALSE FALSE FALSE\n\n\nWe can also assign the results to a variable:\n\ny = (x == 5)\ny\n\n [1] FALSE FALSE FALSE FALSE  TRUE FALSE FALSE FALSE FALSE FALSE"
  },
  {
    "objectID": "vectors.html#indexing-vectors",
    "href": "vectors.html#indexing-vectors",
    "title": "5  Vectors",
    "section": "5.11 Indexing Vectors",
    "text": "5.11 Indexing Vectors\nIn R, an index is used to refer to a specific element or set of elements in an vector (or other data structure). [R uses [ and ] to perform indexing, although other approaches to getting subsets of larger data structures are common in R.\n\nx = seq(0,1,0.1)\n# create a new vector from the 4th element of x\nx[4]\n\n[1] 0.3\n\n\nWe can even use other vectors to perform the “indexing”.\n\nx[c(3,5,6)]\n\n[1] 0.2 0.4 0.5\n\ny = 3:6\nx[y]\n\n[1] 0.2 0.3 0.4 0.5\n\n\nCombining the concept of indexing with the concept of logical vectors results in a very power combination.\n\n# use help('rnorm') to figure out what is happening next\nmyvec = rnorm(10)\n\n# create logical vector that is TRUE where myvec is &gt;0.25\ngt1 = (myvec &gt; 0.25)\nsum(gt1)\n\n[1] 1\n\n# and use our logical vector to create a vector of myvec values that are &gt;0.25\nmyvec[gt1]\n\n[1] 1.46055\n\n# or &lt;=0.25 using the logical \"not\" operator, \"!\"\nmyvec[!gt1]\n\n[1] -1.5739296644 -0.4509350203 -0.5446088728  0.0003685241 -0.0648357774\n[6] -0.1310728434  0.0559693924  0.2343975005 -0.7771408011\n\n# shorter, one line approach\nmyvec[myvec &gt; 0.25]\n\n[1] 1.46055"
  },
  {
    "objectID": "vectors.html#character-vectors-a.k.a.-strings",
    "href": "vectors.html#character-vectors-a.k.a.-strings",
    "title": "5  Vectors",
    "section": "5.12 Character Vectors, A.K.A. Strings",
    "text": "5.12 Character Vectors, A.K.A. Strings\nR uses the paste function to concatenate strings.\n\npaste(\"abc\",\"def\")\n\n[1] \"abc def\"\n\npaste(\"abc\",\"def\",sep=\"THISSEP\")\n\n[1] \"abcTHISSEPdef\"\n\npaste0(\"abc\",\"def\")\n\n[1] \"abcdef\"\n\n## [1] \"abcdef\"\npaste(c(\"X\",\"Y\"),1:10)\n\n [1] \"X 1\"  \"Y 2\"  \"X 3\"  \"Y 4\"  \"X 5\"  \"Y 6\"  \"X 7\"  \"Y 8\"  \"X 9\"  \"Y 10\"\n\npaste(c(\"X\",\"Y\"),1:10,sep=\"_\")\n\n [1] \"X_1\"  \"Y_2\"  \"X_3\"  \"Y_4\"  \"X_5\"  \"Y_6\"  \"X_7\"  \"Y_8\"  \"X_9\"  \"Y_10\"\n\n\nWe can count the number of characters in a string.\n\nnchar('abc')\n\n[1] 3\n\nnchar(c('abc','d',123456))\n\n[1] 3 1 6\n\n\nPulling out parts of strings is also sometimes useful.\n\nsubstr('This is a good sentence.',start=10,stop=15)\n\n[1] \" good \"\n\n\nAnother common operation is to replace something in a string with something (a find-and-replace).\n\nsub('This','That','This is a good sentence.')\n\n[1] \"That is a good sentence.\"\n\n\nWhen we want to find all strings that match some other string, we can use grep, or “grab regular expression”.\n\ngrep('bcd',c('abcdef','abcd','bcde','cdef','defg'))\n\n[1] 1 2 3\n\ngrep('bcd',c('abcdef','abcd','bcde','cdef','defg'),value=TRUE)\n\n[1] \"abcdef\" \"abcd\"   \"bcde\"  \n\n\nRead about the grepl function (?grepl). Use that function to return a logical vector (TRUE/FALSE) for each entry above with an a in it."
  },
  {
    "objectID": "vectors.html#missing-values-aka-na",
    "href": "vectors.html#missing-values-aka-na",
    "title": "5  Vectors",
    "section": "5.13 Missing Values, AKA “NA”",
    "text": "5.13 Missing Values, AKA “NA”\nR has a special value, “NA”, that represents a “missing” value, or Not Available, in a vector or other data structure. Here, we just create a vector to experiment.\n\nx = 1:5\nx\n\n[1] 1 2 3 4 5\n\nlength(x)\n\n[1] 5\n\n\n\nis.na(x)\n\n[1] FALSE FALSE FALSE FALSE FALSE\n\nx[2] = NA\nx\n\n[1]  1 NA  3  4  5\n\n\nThe length of x is unchanged, but there is one value that is marked as “missing” by virtue of being NA.\n\nlength(x)\n\n[1] 5\n\nis.na(x)\n\n[1] FALSE  TRUE FALSE FALSE FALSE\n\n\nWe can remove NA values by using indexing. In the following, is.na(x) returns a logical vector the length of x. The ! is the logical NOT operator and converts TRUE to FALSE and vice-versa.\n\nx[!is.na(x)]\n\n[1] 1 3 4 5"
  },
  {
    "objectID": "vectors.html#exercises",
    "href": "vectors.html#exercises",
    "title": "5  Vectors",
    "section": "5.14 Exercises",
    "text": "5.14 Exercises\n\nCreate a numeric vector called temperatures containing the following values: 72, 75, 78, 81, 76, 73.\n\n\nShow answer\ntemperatures &lt;- c(72, 75, 78, 81, 76, 73, 93)\n\n\nCreate a character vector called days containing the following values: “Monday”, “Tuesday”, “Wednesday”, “Thursday”, “Friday”, “Saturday”, “Sunday”.\n\n\nShow answer\ndays &lt;- c(\"Monday\", \"Tuesday\", \"Wednesday\", \"Thursday\", \"Friday\", \"Saturday\", \"Sunday\")\n\n\nCalculate the average temperature for the week and store it in a variable called average_temperature.\n\n\nShow answer\naverage_temperature &lt;- mean(temperatures)\n\n\nCreate a named vector called weekly_temperatures, where the names are the days of the week and the values are the temperatures from the temperatures vector.\n\n\nShow answer\nweekly_temperatures &lt;- temperatures\nnames(weekly_temperatures) &lt;- days\n\n\nCreate a numeric vector called ages containing the following values: 25, 30, 35, 40, 45, 50, 55, 60.\n\n\nShow answer\nages &lt;- c(25, 30, 35, 40, 45, 50, 55, 60)\n\n\nCreate a logical vector called is_adult by checking if the elements in the ages vector are greater than or equal to 18.\n\n\nShow answer\nis_adult &lt;- ages &gt;= 18\n\n\nCalculate the sum and product of the ages vector.\n\n\nShow answer\nsum_ages &lt;- sum(ages)\nproduct_ages &lt;- prod(ages)\n\n\nExtract the ages greater than or equal to 40 from the ages vector and store them in a variable called older_ages.\n\n\nShow answer\nolder_ages &lt;- ages[ages &gt;= 40]"
  },
  {
    "objectID": "matrices.html#creating-a-matrix",
    "href": "matrices.html#creating-a-matrix",
    "title": "6  Matrices",
    "section": "6.1 Creating a matrix",
    "text": "6.1 Creating a matrix\nThere are many ways to create a matrix in R. One of the simplest is to use the matrix() function. In the code below, we’ll create a matrix from a vector from 1:16.\n\nmat1 &lt;- matrix(1:16,nrow=4)\nmat1\n\n     [,1] [,2] [,3] [,4]\n[1,]    1    5    9   13\n[2,]    2    6   10   14\n[3,]    3    7   11   15\n[4,]    4    8   12   16\n\n\nThe same is possible, but specifying that the matrix be “filled” by row.\n\nmat1 &lt;- matrix(1:16,nrow=4,byrow = TRUE)\nmat1\n\n     [,1] [,2] [,3] [,4]\n[1,]    1    2    3    4\n[2,]    5    6    7    8\n[3,]    9   10   11   12\n[4,]   13   14   15   16\n\n\nNotice the subtle difference in the order that the numbers go into the matrix.\nWe can also build a matrix from parts by “binding” vectors together:\n\nx &lt;- 1:10 \ny &lt;- rnorm(10)\n\nEach of the vectors above is of length 10 and both are “numeric”, so we can make them into a matrix. Using rbind binds rows (r) into a matrix.\n\nmat &lt;- rbind(x,y)\nmat\n\n       [,1]      [,2]    [,3]      [,4]      [,5]     [,6]       [,7]      [,8]\nx 1.0000000 2.0000000 3.00000  4.000000  5.000000 6.000000  7.0000000  8.000000\ny 0.4213429 0.4326436 1.28799 -2.153729 -0.955121 1.369986 -0.1474719 -1.594401\n      [,9]      [,10]\nx 9.000000 10.0000000\ny 1.166565  0.5907749\n\n\nThe alternative to rbind is cbind that binds columns (c) together.\n\nmat &lt;- cbind(x,y)\nmat\n\n       x          y\n [1,]  1  0.4213429\n [2,]  2  0.4326436\n [3,]  3  1.2879903\n [4,]  4 -2.1537293\n [5,]  5 -0.9551210\n [6,]  6  1.3699864\n [7,]  7 -0.1474719\n [8,]  8 -1.5944011\n [9,]  9  1.1665650\n[10,] 10  0.5907749\n\n\nInspecting the names associated with rows and columns is often useful, particularly if the names have human meaning.\n\nrownames(mat)\n\nNULL\n\ncolnames(mat)\n\n[1] \"x\" \"y\"\n\n\nWe can also change the names of the matrix by assigning valid names to the columns or rows.\n\ncolnames(mat) = c('apples','oranges')\ncolnames(mat)\n\n[1] \"apples\"  \"oranges\"\n\nmat\n\n      apples    oranges\n [1,]      1  0.4213429\n [2,]      2  0.4326436\n [3,]      3  1.2879903\n [4,]      4 -2.1537293\n [5,]      5 -0.9551210\n [6,]      6  1.3699864\n [7,]      7 -0.1474719\n [8,]      8 -1.5944011\n [9,]      9  1.1665650\n[10,]     10  0.5907749\n\n\nMatrices have dimensions.\n\ndim(mat)\n\n[1] 10  2\n\nnrow(mat)\n\n[1] 10\n\nncol(mat)\n\n[1] 2"
  },
  {
    "objectID": "matrices.html#accessing-elements-of-a-matrix",
    "href": "matrices.html#accessing-elements-of-a-matrix",
    "title": "6  Matrices",
    "section": "6.2 Accessing elements of a matrix",
    "text": "6.2 Accessing elements of a matrix\nIndexing for matrices works as for vectors except that we now need to include both the row and column (in that order). We can access elements of a matrix using the square bracket [ indexing method. Elements can be accessed as var[r, c]. Here, r and c are vectors describing the elements of the matrix to select.\n\n\n\n\n\n\nImportant\n\n\n\nThe indices in R start with one, meaning that the first element of a vector or the first row/column of a matrix is indexed as one.\nThis is different from some other programming languages, such as Python, which use zero-based indexing, meaning that the first element of a vector or the first row/column of a matrix is indexed as zero.\nIt is important to be aware of this difference when working with data in R, especially if you are coming from a programming background that uses zero-based indexing. Using the wrong index can lead to unexpected results or errors in your code.\n\n\n\n# The 2nd element of the 1st row of mat\nmat[1,2]\n\n  oranges \n0.4213429 \n\n# The first ROW of mat\nmat[1,]\n\n   apples   oranges \n1.0000000 0.4213429 \n\n# The first COLUMN of mat\nmat[,1]\n\n [1]  1  2  3  4  5  6  7  8  9 10\n\n# and all elements of mat that are &gt; 4; note no comma\nmat[mat&gt;4]\n\n[1]  5  6  7  8  9 10\n\n## [1]  5  6  7  8  9 10\n\n\n\n\n\n\n\nCaution\n\n\n\nNote that in the last case, there is no “,”, so R treats the matrix as a long vector (length=20). This is convenient, sometimes, but it can also be a source of error, as some code may “work” but be doing something unexpected.\n\n\nWe can also use indexing to exclude a row or column by prefixing the selection with a - sign.\n\nmat[,-1]       # remove first column\n\n [1]  0.4213429  0.4326436  1.2879903 -2.1537293 -0.9551210  1.3699864\n [7] -0.1474719 -1.5944011  1.1665650  0.5907749\n\nmat[-c(1:5),]  # remove first five rows\n\n     apples    oranges\n[1,]      6  1.3699864\n[2,]      7 -0.1474719\n[3,]      8 -1.5944011\n[4,]      9  1.1665650\n[5,]     10  0.5907749"
  },
  {
    "objectID": "matrices.html#changing-values-in-a-matrix",
    "href": "matrices.html#changing-values-in-a-matrix",
    "title": "6  Matrices",
    "section": "6.3 Changing values in a matrix",
    "text": "6.3 Changing values in a matrix\nWe can create a matrix filled with random values drawn from a normal distribution for our work below.\n\nm = matrix(rnorm(20),nrow=10)\nsummary(m)\n\n       V1                V2         \n Min.   :-1.6420   Min.   :-0.7456  \n 1st Qu.:-0.8393   1st Qu.:-0.5711  \n Median : 0.2474   Median :-0.2029  \n Mean   : 0.2034   Mean   : 0.1043  \n 3rd Qu.: 0.5573   3rd Qu.: 0.2300  \n Max.   : 2.9066   Max.   : 2.1126  \n\n\nMultiplication and division works similarly to vectors. When multiplying by a vector, for example, the values of the vector are reused. In the simplest case, let’s multiply the matrix by a constant (vector of length 1).\n\n# multiply all values in the matrix by 20\nm2 = m*20\nsummary(m2)\n\n       V1                V2         \n Min.   :-32.840   Min.   :-14.912  \n 1st Qu.:-16.786   1st Qu.:-11.422  \n Median :  4.947   Median : -4.059  \n Mean   :  4.068   Mean   :  2.086  \n 3rd Qu.: 11.147   3rd Qu.:  4.599  \n Max.   : 58.131   Max.   : 42.251  \n\n\nBy combining subsetting with assignment, we can make changes to just part of a matrix.\n\n# and add 100 to the first column of m\nm2[,1] = m2[,1] + 100\n# summarize m\nsummary(m2)\n\n       V1               V2         \n Min.   : 67.16   Min.   :-14.912  \n 1st Qu.: 83.21   1st Qu.:-11.422  \n Median :104.95   Median : -4.059  \n Mean   :104.07   Mean   :  2.086  \n 3rd Qu.:111.15   3rd Qu.:  4.599  \n Max.   :158.13   Max.   : 42.251  \n\n\nA somewhat common transformation for a matrix is to transpose which changes rows to columns. One might need to do this if an assay output from a lab machine puts samples in rows and genes in columns, for example, while in Bioconductor/R, we often want the samples in columns and the genes in rows.\n\nt(m2)\n\n          [,1]        [,2]      [,3]       [,4]     [,5]       [,6]      [,7]\n[1,] 107.96863 104.0960114 101.49678 158.131297 105.7981 133.993020 72.711332\n[2,] -14.33319  -0.4561012  29.42504  -7.661384 -12.3741  -8.565006  5.457562\n          [,8]     [,9]     [,10]\n[1,] 77.119850 67.15979 112.20585\n[2,]  2.023542 42.25148 -14.91248"
  },
  {
    "objectID": "matrices.html#calculations-on-matrix-rows-and-columns",
    "href": "matrices.html#calculations-on-matrix-rows-and-columns",
    "title": "6  Matrices",
    "section": "6.4 Calculations on matrix rows and columns",
    "text": "6.4 Calculations on matrix rows and columns\nAgain, we just need a matrix to play with. We’ll use rnorm again, but with a slight twist.\n\nm3 = matrix(rnorm(100,5,2),ncol=10) # what does the 5 mean here? And the 2?\n\nSince these data are from a normal distribution, we can look at a row (or column) to see what the mean and standard deviation are.\n\nmean(m3[,1])\n\n[1] 4.720436\n\nsd(m3[,1])\n\n[1] 2.204187\n\n# or a row\nmean(m3[1,])\n\n[1] 4.330586\n\nsd(m3[1,])\n\n[1] 1.328035\n\n\nThere are some useful convenience functions for computing means and sums of data in all of the columns and rows of matrices.\n\ncolMeans(m3)\n\n [1] 4.720436 4.901195 4.912108 5.326346 5.300867 4.462975 3.948073 4.793714\n [9] 5.383735 4.405058\n\nrowMeans(m3)\n\n [1] 4.330586 4.098086 5.515398 4.497942 5.170548 5.157355 4.960425 5.033872\n [9] 4.735140 4.655157\n\nrowSums(m3)\n\n [1] 43.30586 40.98086 55.15398 44.97942 51.70548 51.57355 49.60425 50.33872\n [9] 47.35140 46.55157\n\ncolSums(m3)\n\n [1] 47.20436 49.01195 49.12108 53.26346 53.00867 44.62975 39.48073 47.93714\n [9] 53.83735 44.05058\n\n\nWe can look at the distribution of column means:\n\n# save as a variable\ncmeans = colMeans(m3)\nsummary(cmeans)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  3.948   4.527   4.847   4.815   5.204   5.384 \n\n\nNote that this is centered pretty closely around the selected mean of 5 above.\nHow about the standard deviation? There is not a colSd function, but it turns out that we can easily apply functions that take vectors as input, like sd and “apply” them across either the rows (the first dimension) or columns (the second) dimension.\n\ncsds = apply(m3, 2, sd)\nsummary(csds)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  1.106   1.710   1.995   1.869   2.193   2.286 \n\n\nAgain, take a look at the distribution which is centered quite close to the selected standard deviation when we created our matrix."
  },
  {
    "objectID": "matrices.html#exercises",
    "href": "matrices.html#exercises",
    "title": "6  Matrices",
    "section": "6.5 Exercises",
    "text": "6.5 Exercises\n\n6.5.1 Data preparation\nFor this set of exercises, we are going to rely on a dataset that comes with R. It gives the number of sunspots per month from 1749-1983. The dataset comes as a ts or time series data type which I convert to a matrix using the following code.\nJust run the code as is and focus on the rest of the exercises.\n\ndata(sunspots)\nsunspot_mat &lt;- matrix(as.vector(sunspots),ncol=12,byrow = TRUE)\ncolnames(sunspot_mat) &lt;- as.character(1:12)\nrownames(sunspot_mat) &lt;- as.character(1749:1983)\n\n\n\n6.5.2 Questions\n\nAfter the conversion above, what does sunspot_mat look like? Use functions to find the number of rows, the number of columns, the class, and some basic summary statistics.\n\n\nShow answer\nncol(sunspot_mat)\nnrow(sunspot_mat)\ndim(sunspot_mat)\nsummary(sunspot_mat)\nhead(sunspot_mat)\ntail(sunspot_mat)\n\n\nPractice subsetting the matrix a bit by selecting:\n\nThe first 10 years (rows)\nThe month of July (7th column)\nThe value for July, 1979 using the rowname to do the selection.\n\n\n\nShow answer\nsunspot_mat[1:10,]\nsunspot_mat[,7]\nsunspot_mat['1979',7]\n\n\n\n\nThese next few exercises take advantage of the fact that calling a univariate statistical function (one that expects a vector) works for matrices by just making a vector of all the values in the matrix. What is the highest (max) number of sunspots recorded in these data?\n\n\nShow answer\nmax(sunspot_mat)\n\n\nAnd the minimum?\n\n\nShow answer\nmin(sunspot_mat)\n\n\nAnd the overall mean and median?\n\n\nShow answer\nmean(sunspot_mat)\nmedian(sunspot_mat)\n\n\nUse the hist() function to look at the distribution of all the monthly sunspot data.\n\n\nShow answer\nhist(sunspot_mat)\n\n\nRead about the breaks argument to hist() to try to increase the number of breaks in the histogram to increase the resolution slightly. Adjust your hist() and breaks to your liking.\n\n\nShow answer\nhist(sunspot_mat, breaks=40)\n\n\nNow, let’s move on to summarizing the data a bit to learn about the pattern of sunspots varies by month or by year. Examine the dataset again. What do the columns represent? And the rows?\n\n\nShow answer\n# just a quick glimpse of the data will give us a sense\nhead(sunspot_mat)\n\n\nWe’d like to look at the distribution of sunspots by month. How can we do that?\n\n\nShow answer\n# the mean of the columns is the mean number of sunspots per month.\ncolMeans(sunspot_mat)\n\n# Another way to write the same thing:\napply(sunspot_mat, 2, mean)\n\n\nAssign the month summary above to a variable and summarize it to get a sense of the spread over months.\n\n\nShow answer\nmonthmeans = colMeans(sunspot_mat)\nsummary(monthmeans)\n\n\nPlay the same game for years to get the per-year mean?\n\n\nShow answer\nymeans = rowMeans(sunspot_mat)\nsummary(ymeans)\n\n\nMake a plot of the yearly means. Do you see a pattern?\n\n\nShow answer\nplot(ymeans)\n# or make it clearer\nplot(ymeans, type='l')"
  },
  {
    "objectID": "dataframes_intro.html#learning-goals",
    "href": "dataframes_intro.html#learning-goals",
    "title": "7  Data Frames",
    "section": "7.1 Learning goals",
    "text": "7.1 Learning goals\n\nUnderstand how data.frames are different from matrices.\nKnow a few functions for examing the contents of a data.frame.\nList approaches for subsetting data.frames.\nBe able to load and save tabular data from and to disk.\nShow how to create a data.frames from scratch."
  },
  {
    "objectID": "dataframes_intro.html#learning-objectives",
    "href": "dataframes_intro.html#learning-objectives",
    "title": "7  Data Frames",
    "section": "7.2 Learning objectives",
    "text": "7.2 Learning objectives\n\nLoad the yeast growth dataset into R using read.csv.\nExamine the contents of the dataset.\nUse subsetting to find genes that may be involved with nutrient metabolism and transport.\nSummarize data measurements by categories."
  },
  {
    "objectID": "dataframes_intro.html#dataset",
    "href": "dataframes_intro.html#dataset",
    "title": "7  Data Frames",
    "section": "7.3 Dataset",
    "text": "7.3 Dataset\nThe data used here are borrowed directly from the fantastic Bioconnector tutorials and are a cleaned up version of the data from Brauer et al. Coordination of Growth Rate, Cell Cycle, Stress Response, and Metabolic Activity in Yeast (2008) Mol Biol Cell 19:352-367. These data are from a gene expression microarray, and in this paper the authors examine the relationship between growth rate and gene expression in yeast cultures limited by one of six different nutrients (glucose, leucine, ammonium, sulfate, phosphate, uracil). If you give yeast a rich media loaded with nutrients except restrict the supply of a single nutrient, you can control the growth rate to any rate you choose. By starving yeast of specific nutrients you can find genes that:\n\nRaise or lower their expression in response to growth rate. Growth-rate dependent expression patterns can tell us a lot about cell cycle control, and how the cell responds to stress. The authors found that expression of &gt;25% of all yeast genes is linearly correlated with growth rate, independent of the limiting nutrient. They also found that the subset of negatively growth-correlated genes is enriched for peroxisomal functions, and positively correlated genes mainly encode ribosomal functions.\nRespond differently when different nutrients are being limited. If you see particular genes that respond very differently when a nutrient is sharply restricted, these genes might be involved in the transport or metabolism of that specific nutrient.\n\nThe dataset can be downloaded directly from:\n\nbrauer2007_tidy.csv\n\nWe are going to read this dataset into R and then use it as a playground for learning about data.frames."
  },
  {
    "objectID": "dataframes_intro.html#reading-in-data",
    "href": "dataframes_intro.html#reading-in-data",
    "title": "7  Data Frames",
    "section": "7.4 Reading in data",
    "text": "7.4 Reading in data\nR has many capabilities for reading in data. Many of the functions have names that help us to understand what data format is to be expected. In this case, the filename that we want to read ends in .csv, meaning comma-separated-values. The read.csv() function reads in .csv files. As usual, it is worth reading help('read.csv') to get a better sense of the possible bells-and-whistles.\nThe read.csv() function can read directly from a URL, so we do not need to download the file directly. This dataset is relatively large (about 16MB), so this may take a bit depending on your network connection speed.\n\noptions(width=60)\n\n\nurl = paste0(\n    'https://raw.githubusercontent.com',\n    '/bioconnector/workshops/master/data/brauer2007_tidy.csv'\n)\nydat &lt;- read.csv(url)\n\nOur variable, ydat, now “contains” the downloaded and read data. We can check to see what data type read.csv gave us:\n\nclass(ydat)\n\n[1] \"data.frame\""
  },
  {
    "objectID": "dataframes_intro.html#inspecting-data.frames",
    "href": "dataframes_intro.html#inspecting-data.frames",
    "title": "7  Data Frames",
    "section": "7.5 Inspecting data.frames",
    "text": "7.5 Inspecting data.frames\nOur ydat variable is a data.frame. As I mentioned, the dataset is fairly large, so we will not be able to look at it all at once on the screen. However, R gives us many tools to inspect a data.frame.\n\nOverviews of content\n\nhead() to show first few rows\ntail() to show last few rows\n\nSize\n\ndim() for dimensions (rows, columns)\nnrow()\nncol()\nobject.size() for power users interested in the memory used to store an object\n\nData and attribute summaries\n\ncolnames() to get the names of the columns\nrownames() to get the “names” of the rows–may not be present\nsummary() to get per-column summaries of the data in the data.frame.\n\n\n\nhead(ydat)\n\n  symbol systematic_name nutrient rate expression\n1   SFB2         YNL049C  Glucose 0.05      -0.24\n2   &lt;NA&gt;         YNL095C  Glucose 0.05       0.28\n3   QRI7         YDL104C  Glucose 0.05      -0.02\n4   CFT2         YLR115W  Glucose 0.05      -0.33\n5   SSO2         YMR183C  Glucose 0.05       0.05\n6   PSP2         YML017W  Glucose 0.05      -0.69\n                            bp\n1        ER to Golgi transport\n2   biological process unknown\n3 proteolysis and peptidolysis\n4      mRNA polyadenylylation*\n5              vesicle fusion*\n6   biological process unknown\n                             mf\n1    molecular function unknown\n2    molecular function unknown\n3 metalloendopeptidase activity\n4                   RNA binding\n5              t-SNARE activity\n6    molecular function unknown\n\ntail(ydat)\n\n       symbol systematic_name nutrient rate expression\n198425   DOA1         YKL213C   Uracil  0.3       0.14\n198426   KRE1         YNL322C   Uracil  0.3       0.28\n198427   MTL1         YGR023W   Uracil  0.3       0.27\n198428   KRE9         YJL174W   Uracil  0.3       0.43\n198429   UTH1         YKR042W   Uracil  0.3       0.19\n198430   &lt;NA&gt;         YOL111C   Uracil  0.3       0.04\n                                               bp\n198425    ubiquitin-dependent protein catabolism*\n198426      cell wall organization and biogenesis\n198427      cell wall organization and biogenesis\n198428     cell wall organization and biogenesis*\n198429 mitochondrion organization and biogenesis*\n198430                 biological process unknown\n                                        mf\n198425          molecular function unknown\n198426 structural constituent of cell wall\n198427          molecular function unknown\n198428          molecular function unknown\n198429          molecular function unknown\n198430          molecular function unknown\n\ndim(ydat)\n\n[1] 198430      7\n\nnrow(ydat)\n\n[1] 198430\n\nncol(ydat)\n\n[1] 7\n\ncolnames(ydat)\n\n[1] \"symbol\"          \"systematic_name\" \"nutrient\"       \n[4] \"rate\"            \"expression\"      \"bp\"             \n[7] \"mf\"             \n\nsummary(ydat)\n\n    symbol          systematic_name      nutrient        \n Length:198430      Length:198430      Length:198430     \n Class :character   Class :character   Class :character  \n Mode  :character   Mode  :character   Mode  :character  \n                                                         \n                                                         \n                                                         \n      rate          expression             bp           \n Min.   :0.0500   Min.   :-6.500000   Length:198430     \n 1st Qu.:0.1000   1st Qu.:-0.290000   Class :character  \n Median :0.2000   Median : 0.000000   Mode  :character  \n Mean   :0.1752   Mean   : 0.003367                     \n 3rd Qu.:0.2500   3rd Qu.: 0.290000                     \n Max.   :0.3000   Max.   : 6.640000                     \n      mf           \n Length:198430     \n Class :character  \n Mode  :character  \n                   \n                   \n                   \n\n\nIn RStudio, there is an additional function, View() (note the capital “V”) that opens the first 1000 rows (default) in the RStudio window, akin to a spreadsheet view.\n\nView(ydat)"
  },
  {
    "objectID": "dataframes_intro.html#accessing-variables-columns-and-subsetting",
    "href": "dataframes_intro.html#accessing-variables-columns-and-subsetting",
    "title": "7  Data Frames",
    "section": "7.6 Accessing variables (columns) and subsetting",
    "text": "7.6 Accessing variables (columns) and subsetting\nIn R, data.frames can be subset similarly to other two-dimensional data structures. The [ in R is used to denote subsetting of any kind. When working with two-dimensional data, we need two values inside the [ ] to specify the details. The specification is [rows, columns]. For example, to get the first three rows of ydat, use:\n\nydat[1:3, ]\n\n  symbol systematic_name nutrient rate expression\n1   SFB2         YNL049C  Glucose 0.05      -0.24\n2   &lt;NA&gt;         YNL095C  Glucose 0.05       0.28\n3   QRI7         YDL104C  Glucose 0.05      -0.02\n                            bp\n1        ER to Golgi transport\n2   biological process unknown\n3 proteolysis and peptidolysis\n                             mf\n1    molecular function unknown\n2    molecular function unknown\n3 metalloendopeptidase activity\n\n\nNote how the second number, the columns, is blank. R takes that to mean “all the columns”. Similarly, we can combine rows and columns specification arbitrarily.\n\nydat[1:3, 1:3]\n\n  symbol systematic_name nutrient\n1   SFB2         YNL049C  Glucose\n2   &lt;NA&gt;         YNL095C  Glucose\n3   QRI7         YDL104C  Glucose\n\n\nBecause selecting a single variable, or column, is such a common operation, there are two shortcuts for doing so with data.frames. The first, the $ operator works like so:\n\n# Look at the column names, just to refresh memory\ncolnames(ydat)\n\n[1] \"symbol\"          \"systematic_name\" \"nutrient\"       \n[4] \"rate\"            \"expression\"      \"bp\"             \n[7] \"mf\"             \n\n# Note that I am using \"head\" here to limit the output\nhead(ydat$symbol)\n\n[1] \"SFB2\" NA     \"QRI7\" \"CFT2\" \"SSO2\" \"PSP2\"\n\n# What is the actual length of \"symbol\"?\nlength(ydat$symbol)\n\n[1] 198430\n\n\nThe second is related to the fact that, in R, data.frames are also lists. We subset a list by using [[]] notation. To get the second column of ydat, we can use:\n\nhead(ydat[[2]])\n\n[1] \"YNL049C\" \"YNL095C\" \"YDL104C\" \"YLR115W\" \"YMR183C\"\n[6] \"YML017W\"\n\n\nAlternatively, we can use the column name:\n\nhead(ydat[[\"systematic_name\"]])\n\n[1] \"YNL049C\" \"YNL095C\" \"YDL104C\" \"YLR115W\" \"YMR183C\"\n[6] \"YML017W\"\n\n\n\n7.6.1 Some data exploration\nThere are a couple of columns that include numeric values. Which columns are numeric?\n\nclass(ydat$symbol)\n\n[1] \"character\"\n\nclass(ydat$rate)\n\n[1] \"numeric\"\n\nclass(ydat$expression)\n\n[1] \"numeric\"\n\n\nMake histograms of: - the expression values - the rate values\nWhat does the table() function do? Could you use that to look a the rate column given that that column appears to have repeated values?\nWhat rate corresponds to the most nutrient-starved condition?\n\n\n7.6.2 More advanced indexing and subsetting\nWe can use, for example, logical values (TRUE/FALSE) to subset data.frames.\n\nhead(ydat[ydat$symbol == 'LEU1', ])\n\n     symbol systematic_name nutrient rate expression   bp\nNA     &lt;NA&gt;            &lt;NA&gt;     &lt;NA&gt;   NA         NA &lt;NA&gt;\nNA.1   &lt;NA&gt;            &lt;NA&gt;     &lt;NA&gt;   NA         NA &lt;NA&gt;\nNA.2   &lt;NA&gt;            &lt;NA&gt;     &lt;NA&gt;   NA         NA &lt;NA&gt;\nNA.3   &lt;NA&gt;            &lt;NA&gt;     &lt;NA&gt;   NA         NA &lt;NA&gt;\nNA.4   &lt;NA&gt;            &lt;NA&gt;     &lt;NA&gt;   NA         NA &lt;NA&gt;\nNA.5   &lt;NA&gt;            &lt;NA&gt;     &lt;NA&gt;   NA         NA &lt;NA&gt;\n       mf\nNA   &lt;NA&gt;\nNA.1 &lt;NA&gt;\nNA.2 &lt;NA&gt;\nNA.3 &lt;NA&gt;\nNA.4 &lt;NA&gt;\nNA.5 &lt;NA&gt;\n\ntail(ydat[ydat$symbol == 'LEU1', ])\n\n         symbol systematic_name nutrient rate expression\nNA.47244   &lt;NA&gt;            &lt;NA&gt;     &lt;NA&gt;   NA         NA\nNA.47245   &lt;NA&gt;            &lt;NA&gt;     &lt;NA&gt;   NA         NA\nNA.47246   &lt;NA&gt;            &lt;NA&gt;     &lt;NA&gt;   NA         NA\nNA.47247   &lt;NA&gt;            &lt;NA&gt;     &lt;NA&gt;   NA         NA\nNA.47248   &lt;NA&gt;            &lt;NA&gt;     &lt;NA&gt;   NA         NA\nNA.47249   &lt;NA&gt;            &lt;NA&gt;     &lt;NA&gt;   NA         NA\n           bp   mf\nNA.47244 &lt;NA&gt; &lt;NA&gt;\nNA.47245 &lt;NA&gt; &lt;NA&gt;\nNA.47246 &lt;NA&gt; &lt;NA&gt;\nNA.47247 &lt;NA&gt; &lt;NA&gt;\nNA.47248 &lt;NA&gt; &lt;NA&gt;\nNA.47249 &lt;NA&gt; &lt;NA&gt;\n\n\nWhat is the problem with this approach? It appears that there are a bunch of NA values. Taking a quick look at the symbol column, we see what the problem.\n\nsummary(ydat$symbol)\n\n   Length     Class      Mode \n   198430 character character \n\n\nUsing the is.na() function, we can make filter further to get down to values of interest.\n\nhead(ydat[ydat$symbol == 'LEU1' & !is.na(ydat$symbol), ])\n\n      symbol systematic_name nutrient rate expression\n1526    LEU1         YGL009C  Glucose 0.05      -1.12\n7043    LEU1         YGL009C  Glucose 0.10      -0.77\n12555   LEU1         YGL009C  Glucose 0.15      -0.67\n18071   LEU1         YGL009C  Glucose 0.20      -0.59\n23603   LEU1         YGL009C  Glucose 0.25      -0.20\n29136   LEU1         YGL009C  Glucose 0.30       0.03\n                        bp\n1526  leucine biosynthesis\n7043  leucine biosynthesis\n12555 leucine biosynthesis\n18071 leucine biosynthesis\n23603 leucine biosynthesis\n29136 leucine biosynthesis\n                                          mf\n1526  3-isopropylmalate dehydratase activity\n7043  3-isopropylmalate dehydratase activity\n12555 3-isopropylmalate dehydratase activity\n18071 3-isopropylmalate dehydratase activity\n23603 3-isopropylmalate dehydratase activity\n29136 3-isopropylmalate dehydratase activity\n\n\nSometimes, looking at the data themselves is not that important. Using dim() is one possibility to look at the number of rows and columns after subsetting.\n\ndim(ydat[ydat$expression &gt; 3, ])\n\n[1] 714   7\n\n\nFind the high expressed genes when leucine-starved. For this task we can also use subset which allows us to treat column names as R variables (no $ needed).\n\nsubset(ydat, nutrient == 'Leucine' & rate == 0.05 & expression &gt; 3)\n\n       symbol systematic_name nutrient rate expression\n133768   QDR2         YIL121W  Leucine 0.05       4.61\n133772   LEU1         YGL009C  Leucine 0.05       3.84\n133858   BAP3         YDR046C  Leucine 0.05       4.29\n135186   &lt;NA&gt;         YPL033C  Leucine 0.05       3.43\n135187   &lt;NA&gt;         YLR267W  Leucine 0.05       3.23\n135288   HXT3         YDR345C  Leucine 0.05       5.16\n135963   TPO2         YGR138C  Leucine 0.05       3.75\n135965   YRO2         YBR054W  Leucine 0.05       4.40\n136102   GPG1         YGL121C  Leucine 0.05       3.08\n136109  HSP42         YDR171W  Leucine 0.05       3.07\n136119   HXT5         YHR096C  Leucine 0.05       4.90\n136151   &lt;NA&gt;         YJL144W  Leucine 0.05       3.06\n136152   MOH1         YBL049W  Leucine 0.05       3.43\n136153   &lt;NA&gt;         YBL048W  Leucine 0.05       3.95\n136189  HSP26         YBR072W  Leucine 0.05       4.86\n136231   NCA3         YJL116C  Leucine 0.05       4.03\n136233   &lt;NA&gt;         YBR116C  Leucine 0.05       3.28\n136486   &lt;NA&gt;         YGR043C  Leucine 0.05       3.07\n137443   ADH2         YMR303C  Leucine 0.05       4.15\n137448   ICL1         YER065C  Leucine 0.05       3.54\n137451   SFC1         YJR095W  Leucine 0.05       3.72\n137569   MLS1         YNL117W  Leucine 0.05       3.76\n                                              bp\n133768                       multidrug transport\n133772                      leucine biosynthesis\n133858                      amino acid transport\n135186                                  meiosis*\n135187                biological process unknown\n135288                          hexose transport\n135963                       polyamine transport\n135965                biological process unknown\n136102                       signal transduction\n136109                       response to stress*\n136119                          hexose transport\n136151                   response to dessication\n136152                biological process unknown\n136153                                      &lt;NA&gt;\n136189                       response to stress*\n136231 mitochondrion organization and biogenesis\n136233                                      &lt;NA&gt;\n136486                biological process unknown\n137443                             fermentation*\n137448                          glyoxylate cycle\n137451                       fumarate transport*\n137569                          glyoxylate cycle\n                                           mf\n133768         multidrug efflux pump activity\n133772 3-isopropylmalate dehydratase activity\n133858        amino acid transporter activity\n135186             molecular function unknown\n135187             molecular function unknown\n135288          glucose transporter activity*\n135963          spermine transporter activity\n135965             molecular function unknown\n136102             signal transducer activity\n136109               unfolded protein binding\n136119          glucose transporter activity*\n136151             molecular function unknown\n136152             molecular function unknown\n136153                                   &lt;NA&gt;\n136189               unfolded protein binding\n136231             molecular function unknown\n136233                                   &lt;NA&gt;\n136486                 transaldolase activity\n137443         alcohol dehydrogenase activity\n137448              isocitrate lyase activity\n137451 succinate:fumarate antiporter activity\n137569               malate synthase activity"
  },
  {
    "objectID": "dataframes_intro.html#aggregating-data",
    "href": "dataframes_intro.html#aggregating-data",
    "title": "7  Data Frames",
    "section": "7.7 Aggregating data",
    "text": "7.7 Aggregating data\nAggregating data, or summarizing by category, is a common way to look for trends or differences in measurements between categories. Use aggregate to find the mean expression by gene symbol.\n\nhead(aggregate(ydat$expression, by=list( ydat$symbol), mean))\n\n  Group.1           x\n1    AAC1  0.52888889\n2    AAC3 -0.21628571\n3   AAD10  0.43833333\n4   AAD14 -0.07166667\n5   AAD16  0.24194444\n6    AAD4 -0.79166667\n\n# or \nhead(aggregate(expression ~ symbol, mean, data=ydat))\n\n  symbol  expression\n1   AAC1  0.52888889\n2   AAC3 -0.21628571\n3  AAD10  0.43833333\n4  AAD14 -0.07166667\n5  AAD16  0.24194444\n6   AAD4 -0.79166667"
  },
  {
    "objectID": "dataframes_intro.html#creating-a-data.frame-from-scratch",
    "href": "dataframes_intro.html#creating-a-data.frame-from-scratch",
    "title": "7  Data Frames",
    "section": "7.8 Creating a data.frame from scratch",
    "text": "7.8 Creating a data.frame from scratch\nSometimes it is useful to combine related data into one object. For example, let’s simulate some data.\n\nsmoker = factor(rep(c(\"smoker\", \"non-smoker\"), each=50))\nsmoker_numeric = as.numeric(smoker)\nx = rnorm(100)\nrisk = x + 2*smoker_numeric\n\nWe have two varibles, risk and smoker that are related. We can make a data.frame out of them:\n\nsmoker_risk = data.frame(smoker = smoker, risk = risk)\nhead(smoker_risk)\n\n  smoker     risk\n1 smoker 2.676388\n2 smoker 4.354609\n3 smoker 4.755054\n4 smoker 4.670494\n5 smoker 3.945609\n6 smoker 3.516762\n\n\nR also has plotting shortcuts that work with data.frames to simplify plotting\n\nplot( risk ~ smoker, data=smoker_risk)"
  },
  {
    "objectID": "dataframes_intro.html#saving-a-data.frame",
    "href": "dataframes_intro.html#saving-a-data.frame",
    "title": "7  Data Frames",
    "section": "7.9 Saving a data.frame",
    "text": "7.9 Saving a data.frame\nOnce we have a data.frame of interest, we may want to save it. The most portable way to save a data.frame is to use one of the write functions. In this case, let’s save the data as a .csv file.\n\nwrite.csv(smoker_risk, \"smoker_risk.csv\")"
  },
  {
    "objectID": "factors.html#factors",
    "href": "factors.html#factors",
    "title": "8  Factors",
    "section": "8.1 Factors",
    "text": "8.1 Factors\nA factor is a special type of vector, normally used to hold a categorical variable–such as smoker/nonsmoker, state of residency, zipcode–in many statistical functions. Such vectors have class “factor”. Factors are primarily used in Analysis of Variance (ANOVA) or other situations when “categories” are needed. When a factor is used as a predictor variable, the corresponding indicator variables are created (more later).\nNote of caution that factors in R often appear to be character vectors when printed, but you will notice that they do not have double quotes around them. They are stored in R as numbers with a key name, so sometimes you will note that the factor behaves like a numeric vector.\n\n# create the character vector\ncitizen&lt;-c(\"uk\",\"us\",\"no\",\"au\",\"uk\",\"us\",\"us\",\"no\",\"au\") \n\n# convert to factor\ncitizenf&lt;-factor(citizen)                                \ncitizen             \n\n[1] \"uk\" \"us\" \"no\" \"au\" \"uk\" \"us\" \"us\" \"no\" \"au\"\n\ncitizenf\n\n[1] uk us no au uk us us no au\nLevels: au no uk us\n\n# convert factor back to character vector\nas.character(citizenf)\n\n[1] \"uk\" \"us\" \"no\" \"au\" \"uk\" \"us\" \"us\" \"no\" \"au\"\n\n# convert to numeric vector\nas.numeric(citizenf)\n\n[1] 3 4 2 1 3 4 4 2 1\n\n\nR stores many data structures as vectors with “attributes” and “class” (just so you have seen this).\n\nattributes(citizenf)\n\n$levels\n[1] \"au\" \"no\" \"uk\" \"us\"\n\n$class\n[1] \"factor\"\n\nclass(citizenf)\n\n[1] \"factor\"\n\n# note that after unclassing, we can see the \n# underlying numeric structure again\nunclass(citizenf)\n\n[1] 3 4 2 1 3 4 4 2 1\nattr(,\"levels\")\n[1] \"au\" \"no\" \"uk\" \"us\"\n\n\nTabulating factors is a useful way to get a sense of the “sample” set available.\n\ntable(citizenf)\n\ncitizenf\nau no uk us \n 2  2  2  3"
  },
  {
    "objectID": "eda_overview.html",
    "href": "eda_overview.html",
    "title": "Exploratory data analysis",
    "section": "",
    "text": "Imagine you’re on an adventure, about to embark on a journey into the unknown. You’ve just been handed a treasure map, with the promise of valuable insights waiting to be discovered. This map is your data set, and the journey is exploratory data analysis (EDA).\nAs you begin your exploration, you start by getting a feel for the terrain. You take a broad, bird’s-eye view of the data, examining its structure and dimensions. Are you dealing with a vast landscape or a small, confined area? Are there any missing pieces in the map that you’ll need to account for? Understanding the overall context of your data set is crucial before venturing further.\nWith a sense of the landscape, you now zoom in to identify key landmarks in the data. You might look for unusual patterns, trends, or relationships between variables. As you spot these landmarks, you start asking questions: What’s causing that spike in values? Are these two factors related, or is it just a coincidence? By asking these questions, you’re actively engaging with the data and forming hypotheses that could guide future analysis or experiments.\nAs you continue your journey, you realize that the map alone isn’t enough to fully understand the terrain. You need more tools to bring the data to life. You start visualizing the data using charts, plots, and graphs. These visualizations act as your binoculars, allowing you to see patterns and relationships more clearly. Through them, you can uncover the hidden treasures buried within the data.\nEDA isn’t a linear path from start to finish. As you explore, you’ll find yourself circling back to previous points, refining your questions, and digging deeper. The process is iterative, with each new discovery informing the next. And as you go, you’ll gain a deeper understanding of the data’s underlying structure and potential.\nFinally, after your thorough exploration, you’ll have a solid foundation to build upon. You’ll be better equipped to make informed decisions, test hypotheses, and draw meaningful conclusions. The insights you’ve gained through EDA will serve as a compass, guiding you towards the true value hidden within your data. And with that, you’ve successfully completed your journey through exploratory data analysis."
  },
  {
    "objectID": "eda_and_univariate_brfss.html#a-case-study-on-the-behavioral-risk-factor-surveillance-system",
    "href": "eda_and_univariate_brfss.html#a-case-study-on-the-behavioral-risk-factor-surveillance-system",
    "title": "9  Case Study: Behavioral Risk Factor Surveillance System",
    "section": "9.1 A Case Study on the Behavioral Risk Factor Surveillance System",
    "text": "9.1 A Case Study on the Behavioral Risk Factor Surveillance System\nThe Behavioral Risk Factor Surveillance System (BRFSS) is a large-scale health survey conducted annually by the Centers for Disease Control and Prevention (CDC) in the United States. The BRFSS collects information on various health-related behaviors, chronic health conditions, and the use of preventive services among the adult population (18 years and older) through telephone interviews. The main goal of the BRFSS is to identify and monitor the prevalence of risk factors associated with chronic diseases, inform public health policies, and evaluate the effectiveness of health promotion and disease prevention programs. The data collected through BRFSS is crucial for understanding the health status and needs of the population, and it serves as a valuable resource for researchers, policy makers, and healthcare professionals in making informed decisions and designing targeted interventions.\nIn this chapter, we will walk through an exploratory data analysis (EDA) of the Behavioral Risk Factor Surveillance System dataset using R. EDA is an important step in the data analysis process, as it helps you to understand your data, identify trends, and detect any anomalies before performing more advanced analyses. We will use various R functions and packages to explore the dataset, with a focus on active learning and hands-on experience."
  },
  {
    "objectID": "eda_and_univariate_brfss.html#loading-the-dataset",
    "href": "eda_and_univariate_brfss.html#loading-the-dataset",
    "title": "9  Case Study: Behavioral Risk Factor Surveillance System",
    "section": "9.2 Loading the Dataset",
    "text": "9.2 Loading the Dataset\nFirst, let’s load the dataset into R. We will use the read.csv() function from the base R package to read the data and store it in a data frame called brfss. Make sure the CSV file is in your working directory, or provide the full path to the file.\nFirst, we need to get the data. Either download the data from THIS LINK or have R do it directly from the command-line (preferred):\n\ndownload.file('https://raw.githubusercontent.com/seandavi/ITR/master/BRFSS-subset.csv',\n              destfile = 'BRFSS-subset.csv')\n\n\n\npath &lt;- file.choose()    # look for BRFSS-subset.csv\n\n\nstopifnot(file.exists(path))\nbrfss &lt;- read.csv(path)"
  },
  {
    "objectID": "eda_and_univariate_brfss.html#inspecting-the-data",
    "href": "eda_and_univariate_brfss.html#inspecting-the-data",
    "title": "9  Case Study: Behavioral Risk Factor Surveillance System",
    "section": "9.3 Inspecting the Data",
    "text": "9.3 Inspecting the Data\nOnce the data is loaded, let’s take a look at the first few rows of the dataset using the head() function:\n\nhead(brfss)\n\n  Age   Weight    Sex Height Year\n1  31 48.98798 Female 157.48 1990\n2  57 81.64663 Female 157.48 1990\n3  43 80.28585   Male 177.80 1990\n4  72 70.30682   Male 170.18 1990\n5  31 49.89516 Female 154.94 1990\n6  58 54.43108 Female 154.94 1990\n\n\nThis will display the first six rows of the dataset, allowing you to get a feel for the data structure and variable types.\nNext, let’s check the dimensions of the dataset using the dim() function:\n\ndim(brfss)\n\n[1] 20000     5\n\n\nThis will return the number of rows and columns in the dataset, which is important to know for subsequent analyses."
  },
  {
    "objectID": "eda_and_univariate_brfss.html#summary-statistics",
    "href": "eda_and_univariate_brfss.html#summary-statistics",
    "title": "9  Case Study: Behavioral Risk Factor Surveillance System",
    "section": "9.4 Summary Statistics",
    "text": "9.4 Summary Statistics\nNow that we have a basic understanding of the data structure, let’s calculate some summary statistics. The summary() function in R provides a quick overview of the main statistics for each variable in the dataset:\n\nsummary(brfss)\n\n      Age            Weight           Sex                Height     \n Min.   :18.00   Min.   : 34.93   Length:20000       Min.   :105.0  \n 1st Qu.:36.00   1st Qu.: 61.69   Class :character   1st Qu.:162.6  \n Median :51.00   Median : 72.57   Mode  :character   Median :168.0  \n Mean   :50.99   Mean   : 75.42                      Mean   :169.2  \n 3rd Qu.:65.00   3rd Qu.: 86.18                      3rd Qu.:177.8  \n Max.   :99.00   Max.   :278.96                      Max.   :218.0  \n NA's   :139     NA's   :649                         NA's   :184    \n      Year     \n Min.   :1990  \n 1st Qu.:1990  \n Median :2000  \n Mean   :2000  \n 3rd Qu.:2010  \n Max.   :2010  \n               \n\n\nThis will display the minimum, first quartile, median, mean, third quartile, and maximum for each numeric variable, and the frequency counts for each factor level for categorical variables."
  },
  {
    "objectID": "eda_and_univariate_brfss.html#data-visualization",
    "href": "eda_and_univariate_brfss.html#data-visualization",
    "title": "9  Case Study: Behavioral Risk Factor Surveillance System",
    "section": "9.5 Data Visualization",
    "text": "9.5 Data Visualization\nVisualizing the data can help you identify patterns and trends in the dataset. Let’s start by creating a histogram of the Age variable using the hist() function.\nThis will create a histogram showing the frequency distribution of ages in the dataset. You can customize the appearance of the histogram by adjusting the parameters within the hist() function.\n\nhist(brfss$Age, main = \"Age Distribution\", \n     xlab = \"Age\", col = \"lightblue\")\n\n\n\n\n\n\n\n\n\n\nWhat are the options for a histogram?\n\n\n\nThe hist() function has many options. For example, you can change the number of bins, the color of the bars, the title, and the x-axis label. You can also add a vertical line at the mean or median, or add a normal curve to the histogram. For more information, type ?hist in the R console.\nMore generally, it is important to understand the options available for each function you use. You can do this by reading the documentation for the function, which can be accessed by typing ?function_name or help(\"function_name\")in the R console.\n\n\nNext, let’s create a boxplot to compare the distribution of Weight between males and females. We will use the boxplot() function for this. This will create a boxplot comparing the weight distribution between males and females. You can customize the appearance of the boxplot by adjusting the parameters within the boxplot() function.\n\nboxplot(brfss$Weight ~ brfss$Sex, main = \"Weight Distribution by Sex\", \n        xlab = \"Sex\", ylab = \"Weight\", col = c(\"pink\", \"lightblue\"))"
  },
  {
    "objectID": "eda_and_univariate_brfss.html#analyzing-relationships-between-variables",
    "href": "eda_and_univariate_brfss.html#analyzing-relationships-between-variables",
    "title": "9  Case Study: Behavioral Risk Factor Surveillance System",
    "section": "9.6 Analyzing Relationships Between Variables",
    "text": "9.6 Analyzing Relationships Between Variables\nTo further explore the data, let’s investigate the relationship between age and weight using a scatterplot. We will use the plot() function for this:\nThis will create a scatterplot of age and weight, allowing you to visually assess the relationship between these two variables.\n\nplot(brfss$Age, brfss$Weight, main = \"Scatterplot of Age and Weight\", \n     xlab = \"Age\", ylab = \"Weight\", col = \"darkblue\")  \n\n\n\n\nTo quantify the strength of the relationship between age and weight, we can calculate the correlation coefficient using the cor() function:\nThis will return the correlation coefficient between age and weight, which can help you determine whether there is a linear relationship between these variables.\n\ncor(brfss$Age, brfss$Weight)\n\n[1] NA\n\n\nWhy does cor() give a value of NA? What can we do about it? A quick glance at help(\"cor\") will give you the answer.\n\ncor(brfss$Age, brfss$Weight, use = \"complete.obs\")\n\n[1] 0.02699989"
  },
  {
    "objectID": "eda_and_univariate_brfss.html#exercises",
    "href": "eda_and_univariate_brfss.html#exercises",
    "title": "9  Case Study: Behavioral Risk Factor Surveillance System",
    "section": "9.7 Exercises",
    "text": "9.7 Exercises\n\nWhat is the mean weight in this dataset? How about the median? What is the difference between the two? What does this tell you about the distribution of weights in the dataset?\n\n\nShow answer\nmean(brfss$Weight, na.rm = TRUE)\n\n\n[1] 75.42455\n\n\nShow answer\nmedian(brfss$Weight, na.rm = TRUE)\n\n\n[1] 72.57478\n\n\nShow answer\nmean(brfss$Weight, na.rm=TRUE) - median(brfss$Weight, na.rm = TRUE)\n\n\n[1] 2.849774\n\n\nGiven the findings about the mean and median in the previous exercise, use the hist() function to create a histogram of the weight distribution in this dataset. How would you describe the shape of this distribution?\n\n\nShow answer\nhist(brfss$Weight, xlab=\"Weight (kg)\", breaks = 30)\n\n\n\n\n\nUse plot() to examine the relationship between height and weight in this dataset.\n\n\nShow answer\nplot(brfss$Height, brfss$Weight)\n\n\n\n\n\nWhat is the correlation between height and weight? What does this tell you about the relationship between these two variables?\n\n\nShow answer\ncor(brfss$Height, brfss$Weight, use = \"complete.obs\")\n\n\n[1] 0.5140928\n\n\nCreate a histogram of the height distribution in this dataset. How would you describe the shape of this distribution?\n\n\nShow answer\nhist(brfss$Height, xlab=\"Height (cm)\", breaks = 30)"
  },
  {
    "objectID": "eda_and_univariate_brfss.html#conclusion",
    "href": "eda_and_univariate_brfss.html#conclusion",
    "title": "9  Case Study: Behavioral Risk Factor Surveillance System",
    "section": "9.8 Conclusion",
    "text": "9.8 Conclusion\nIn this chapter, we have demonstrated how to perform an exploratory data analysis on the Behavioral Risk Factor Surveillance System dataset using R. We covered data loading, inspection, summary statistics, visualization, and the analysis of relationships between variables. By actively engaging with the R code and data, you have gained valuable experience in using R for EDA and are well-equipped to tackle more complex analyses in your future work.\nRemember that EDA is just the beginning of the data analysis process, and further statistical modeling and hypothesis testing will likely be necessary to draw meaningful conclusions from your data. However, EDA is a crucial step in understanding your data and informing your subsequent analyses."
  },
  {
    "objectID": "eda_and_univariate_brfss.html#learn-about-the-data",
    "href": "eda_and_univariate_brfss.html#learn-about-the-data",
    "title": "9  Case Study: Behavioral Risk Factor Surveillance System",
    "section": "9.9 Learn about the data",
    "text": "9.9 Learn about the data\nUsing the data exploration techniques you have seen to explore the brfss dataset.\n\nsummary()\ndim()\ncolnames()\nhead()\ntail()\nclass()\nView()\n\nYou may want to investigate individual columns visually using plotting like hist(). For categorical data, consider using something like table()."
  },
  {
    "objectID": "eda_and_univariate_brfss.html#clean-data",
    "href": "eda_and_univariate_brfss.html#clean-data",
    "title": "9  Case Study: Behavioral Risk Factor Surveillance System",
    "section": "9.10 Clean data",
    "text": "9.10 Clean data\nR read Year as an integer value, but it’s really a factor\n\nbrfss$Year &lt;- factor(brfss$Year)"
  },
  {
    "objectID": "eda_and_univariate_brfss.html#weight-in-1990-vs.-2010-females",
    "href": "eda_and_univariate_brfss.html#weight-in-1990-vs.-2010-females",
    "title": "9  Case Study: Behavioral Risk Factor Surveillance System",
    "section": "9.11 Weight in 1990 vs. 2010 Females",
    "text": "9.11 Weight in 1990 vs. 2010 Females\n\nCreate a subset of the data\n\n\nbrfssFemale &lt;- brfss[brfss$Sex == \"Female\",]\nsummary(brfssFemale)\n\n      Age            Weight           Sex                Height     \n Min.   :18.00   Min.   : 36.29   Length:12039       Min.   :105.0  \n 1st Qu.:37.00   1st Qu.: 57.61   Class :character   1st Qu.:157.5  \n Median :52.00   Median : 65.77   Mode  :character   Median :163.0  \n Mean   :51.92   Mean   : 69.05                      Mean   :163.3  \n 3rd Qu.:67.00   3rd Qu.: 77.11                      3rd Qu.:168.0  \n Max.   :99.00   Max.   :272.16                      Max.   :200.7  \n NA's   :103     NA's   :560                         NA's   :140    \n   Year     \n 1990:5718  \n 2010:6321  \n            \n            \n            \n            \n            \n\n\n\nVisualize\n\n\nplot(Weight ~ Year, brfssFemale)\n\n\n\n\n\nStatistical test\n\n\nt.test(Weight ~ Year, brfssFemale)\n\n\n    Welch Two Sample t-test\n\ndata:  Weight by Year\nt = -27.133, df = 11079, p-value &lt; 2.2e-16\nalternative hypothesis: true difference in means between group 1990 and group 2010 is not equal to 0\n95 percent confidence interval:\n -8.723607 -7.548102\nsample estimates:\nmean in group 1990 mean in group 2010 \n          64.81838           72.95424"
  },
  {
    "objectID": "eda_and_univariate_brfss.html#weight-and-height-in-2010-males",
    "href": "eda_and_univariate_brfss.html#weight-and-height-in-2010-males",
    "title": "9  Case Study: Behavioral Risk Factor Surveillance System",
    "section": "9.12 Weight and height in 2010 Males",
    "text": "9.12 Weight and height in 2010 Males\n\nCreate a subset of the data\n\n\nbrfss2010Male &lt;- subset(brfss,  Year == 2010 & Sex == \"Male\")\nsummary(brfss2010Male)\n\n      Age            Weight           Sex                Height      Year     \n Min.   :18.00   Min.   : 36.29   Length:3679        Min.   :135   1990:   0  \n 1st Qu.:45.00   1st Qu.: 77.11   Class :character   1st Qu.:173   2010:3679  \n Median :57.00   Median : 86.18   Mode  :character   Median :178              \n Mean   :56.25   Mean   : 88.85                      Mean   :178              \n 3rd Qu.:68.00   3rd Qu.: 99.79                      3rd Qu.:183              \n Max.   :99.00   Max.   :278.96                      Max.   :218              \n NA's   :30      NA's   :49                          NA's   :31               \n\n\n\nVisualize the relationship\n\n\nhist(brfss2010Male$Weight)\n\n\n\nhist(brfss2010Male$Height)\n\n\n\nplot(Weight ~ Height, brfss2010Male)\n\n\n\n\n\nFit a linear model (regression)\n\n\nfit &lt;- lm(Weight ~ Height, brfss2010Male)\nfit\n\n\nCall:\nlm(formula = Weight ~ Height, data = brfss2010Male)\n\nCoefficients:\n(Intercept)       Height  \n   -86.8747       0.9873  \n\n\nSummarize as ANOVA table\n\nanova(fit)\n\nAnalysis of Variance Table\n\nResponse: Weight\n            Df  Sum Sq Mean Sq F value    Pr(&gt;F)    \nHeight       1  197664  197664   693.8 &lt; 2.2e-16 ***\nResiduals 3617 1030484     285                      \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\nPlot points, superpose fitted regression line; where am I?\n\n\nplot(Weight ~ Height, brfss2010Male)\nabline(fit, col=\"blue\", lwd=2)\n# Substitute your own weight and height...\npoints(73 * 2.54, 178 / 2.2, col=\"red\", cex=4, pch=20)\n\n\n\n\n\nClass and available ‘methods’\n\n\nclass(fit)                 # 'noun'\nmethods(class=class(fit))  # 'verb'\n\n\nDiagnostics\n\n\nplot(fit)\n# Note that the \"plot\" above does not have a \".lm\"\n# However, R will use \"plot.lm\". Why?\n?plot.lm"
  },
  {
    "objectID": "t-stats-and-tests.html#background",
    "href": "t-stats-and-tests.html#background",
    "title": "10  The t-statistic and t-distribution",
    "section": "10.1 Background",
    "text": "10.1 Background\nThe t-test is a statistical hypothesis test that is commonly used when the data are normally distributed (follow a normal distribution) if the value of the population standard deviation were known. When the population standard deviation is not known and is replaced by an estimate based no the data, the test statistic follows a Student’s t distribution.\nT-tests are handy hypothesis tests in statistics when you want to compare means. You can compare a sample mean to a hypothesized or target value using a one-sample t-test. You can compare the means of two groups with a two-sample t-test. If you have two groups with paired observations (e.g., before and after measurements), use the paired t-test.\nA t-test looks at the t-statistic, the t-distribution values, and the degrees of freedom to determine the statistical significance. To conduct a test with three or more means, we would use an analysis of variance.\nThe distriubution that the t-statistic follows was described in a famous paper (Student 1908) by “Student”, a pseudonym for William Sealy Gosset."
  },
  {
    "objectID": "t-stats-and-tests.html#the-z-score-and-probability",
    "href": "t-stats-and-tests.html#the-z-score-and-probability",
    "title": "10  The t-statistic and t-distribution",
    "section": "10.2 The Z-score and probability",
    "text": "10.2 The Z-score and probability\nBefore talking about the t-distribution and t-scores, lets review the Z-score, its relation to the normal distribution, and probability.\nThe Z-score is defined as:\n\\[Z = \\frac{x - \\mu}{\\sigma} \\tag{10.1}\\]\nwhere \\(\\mu\\) is a the population mean from which \\(x\\) is drawn and \\(\\sigma\\) is the population standard deviation (taken as known, not estimated from the data).\nThe probability of observing a \\(Z\\) score of \\(z\\) or greater can be calculated by \\(pnorm(z,\\mu,\\sigma)\\).\nFor example, let’s assume that our “population” is known and it truly has a mean 0 and standard deviation 1. If we have observations drawn from that population, we can assign a probability of seeing that observation by random chance under the assumption that the null hypothesis is TRUE.\n\nzscore = seq(-5,5,1)\n\nFor each value of zscore, let’s calculate the p-value and put the results in a data.frame.\n\ndf = data.frame(\n    zscore = zscore,\n    pval   = pnorm(zscore, 0, 1)\n)\ndf\n\n   zscore         pval\n1      -5 2.866516e-07\n2      -4 3.167124e-05\n3      -3 1.349898e-03\n4      -2 2.275013e-02\n5      -1 1.586553e-01\n6       0 5.000000e-01\n7       1 8.413447e-01\n8       2 9.772499e-01\n9       3 9.986501e-01\n10      4 9.999683e-01\n11      5 9.999997e-01\n\n\nWhy is the p-value of something 5 population standard deviations away from the mean (zscore=5) nearly 1 in this calculation? What is the default for pnorm with respect to being one-sided or two-sided?\nLet’s plot the values of probability vs z-score:\n\nplot(df$zscore, df$pval, type='b')\n\n\n\n\nThis plot is the empirical cumulative density function (cdf) for our data. How can we use it? If we know the z-score, we can look up the probability of observing that value. Since we have constructed our experiment to follow the standard normal distribution, this cdf also represents the cdf of the standard normal distribution.\n\n10.2.1 Small diversion: two-sided pnorm function\nThe pnorm function returns the “one-sided” probability of having a value at least as extreme as the observed \\(x\\) and uses the “lower” tail by default. Let’s create a function that computes two-sided p-values.\n\nTake the absolute value of x\nCompute pnorm with lower.tail=FALSE so we get lower p-values with larger values of \\(x\\).\nSince we want to include both tails, we need to multiply the area (probability) returned by pnorm by 2.\n\n\ntwosidedpnorm = function(x,mu=0,sd=1) {\n    2*pnorm(x,mu,sd,lower.tail=FALSE)\n}\n\nAnd we can test this to see how likely it is to be 2 or 3 standard deviations from the mean:\n\ntwosidedpnorm(2)\n\n[1] 0.04550026\n\ntwosidedpnorm(3)\n\n[1] 0.002699796"
  },
  {
    "objectID": "t-stats-and-tests.html#the-t-distribution",
    "href": "t-stats-and-tests.html#the-t-distribution",
    "title": "10  The t-statistic and t-distribution",
    "section": "10.3 The t-distribution",
    "text": "10.3 The t-distribution\nWe spent time above working with z-scores and probability. An important aspect of working with the normal distribution is that we MUST assume that we know the standard deviation. Remember that the Z-score is defined as:\n\\[Z = \\frac{x - \\mu}{\\sigma}\\]\nThe formula for the population standard deviation is:\n\\[\\sigma = \\sqrt{\\frac{1}{N}\\sum_{i=1}^{N}({xi - \\mu)^2}} \\tag{10.2}\\]\nIn general, the population standard deviation is taken as “known” as we did above.\nIf we do not but only have a sample from the population, instead of using the Z-score, we use the t-score defined as:\n\\[t = \\frac{x - \\bar{x}}{s} \\tag{10.3}\\]\nThis looks quite similar to the formula for Z-score, but here we have to estimate the standard deviation, \\(s\\) from the data. The formula for \\(s\\) is:\n\\[s = \\sqrt{\\frac{1}{N-1}\\sum_{i=1}^{N}({x_{i} - \\bar{x})^2}} \\tag{10.4}\\]\nSince we are estimating the standard deviation from the data, this leads to extra variability that shows up as “fatter tails” for smaller sample sizes than for larger sample sizes. We can see this by comparing the t-distribution for various numbers of degrees of freedom (sample sizes).\nWe can look at the effect of sample size on the distributions graphically by looking at the densities for 3, 5, 10, 20 degrees of freedom and the normal distribution:\n\nlibrary(dplyr)\nlibrary(ggplot2)\nt_values = seq(-6,6,0.01)\ndf = data.frame(\n    value = t_values,\n    t_3   = dt(t_values,3),\n    t_6   = dt(t_values,6),\n    t_10  = dt(t_values,10),\n    t_20  = dt(t_values,20),\n    Normal= dnorm(t_values)\n) |&gt;\n    tidyr::gather(\"Distribution\", \"density\", -value)\nggplot(df, aes(x=value, y=density, color=Distribution)) + \n    geom_line()\n\n\n\n\nFigure 10.1: t-distributions for various degrees of freedom. Note that the tails are fatter for smaller degrees of freedom, which is a result of estimating the standard deviation from the data.\n\n\n\n\nThe dt and dnorm functions give the density of the distributions for each point.\n\ndf2 = df |&gt; \n    group_by(Distribution) |&gt;\n    arrange(value) |&gt; \n    mutate(cdf=cumsum(density))\nggplot(df2, aes(x=value, y=cdf, color=Distribution)) + \n    geom_line()\n\n\n\n\n\n10.3.1 p-values based on Z vs t\nWhen we have a “sample” of data and want to compute the statistical significance of the difference of the mean from the population mean, we calculate the standard deviation of the sample means (standard error).\n\\[z = \\frac{x - \\mu}{\\sigma/\\sqrt{n}}\\]\nLet’s look at the relationship between the p-values of Z (from the normal distribution) vs t for a sample of data.\n\nset.seed(5432)\nsamp = rnorm(5)\nz = sqrt(length(samp)) * mean(samp) #simplifying assumption (sigma=1, mu=0)\n\nAnd the p-value if we assume we know the standard deviation:\n\npnorm(z)\n\n[1] 0.8035432\n\n\n\nts = sqrt(length(samp)) * mean(samp) / sd(samp)\npnorm(ts)\n\n[1] 0.8215048\n\npt(ts,5)\n\n[1] 0.800373\n\n\n\n\n10.3.2 Experiment\nWhen sampling from a normal distribution, we often calculate p-values to test hypotheses or determine the statistical significance of our results. The p-value represents the probability of obtaining a test statistic as extreme or more extreme than the one observed, under the null hypothesis.\nIn a typical scenario, we assume that the population mean and standard deviation are known. However, in many real-life situations, we don’t know the true population standard deviation, and we have to estimate it using the sample standard deviation (Equation 10.4). This estimation introduces some uncertainty into our calculations, which affects the p-values. When we include an estimate of the standard deviation, we switch from using the standard normal (z) distribution to the t-distribution for calculating p-values.\nWhat would happen if we used the normal distribution to calculate p-values when we use the sample standard deviation? Let’s find out!\n\nSimulate a bunch of samples of size n from the standard normal distribution\nCalculate the p-value distribution for those samples based on the normal.\nCalculate the p-value distribution for those samples based on the normal, but with the estimated standard deviation.\nCalculate the p-value distribution for those samples based on the t-distribution.\n\nCreate a function that draws a sample of size n from the standard normal distribution.\n\nzf = function(n) {\n    samp = rnorm(n)\n    z = sqrt(length(samp)) * mean(samp) / 1 #simplifying assumption (sigma=1, mu=0)\n    z\n}\n\nAnd give it a try:\n\nzf(5)\n\n[1] 0.7406094\n\n\nPerform 10000 replicates of our sampling and z-scoring. We are using the assumption that we know the population standard deviation; in this case, we do know since we are sampling from the standard normal distribution.\n\nz10k = replicate(10000,zf(5))\nhist(pnorm(z10k))\n\n\n\n\nAnd do the same, but now creating a t-score function. We are using the assumption that we don’t know the population standard deviation; in this case, we must estimate it from the data. Note the difference in the calculation of the t-score (ts) as compared to the z-score (z).\n\ntf = function(n) {\n    samp = rnorm(n)\n    # now, using the sample standard deviation since we \n    # \"don't know\" the population standard deviation\n    ts = sqrt(length(samp)) * mean(samp) / sd(samp)\n    ts\n}\n\nIf we use those t-scores and calculate the p-values based on the normal distribution, the histogram of those p-values looks like:\n\nt10k = replicate(10000,tf(5))\nhist(pnorm(t10k))\n\n\n\n\nSince we are using the normal distribution to calculate the p-values, we are, in effect, assuming that we know the population standard deviation. This assumption is incorrect, and we can see that the p-values are not uniformly distributed between 0 and 1.\nIf we use those t-scores and calculate the p-values based on the t-distribution, the histogram of those p-values looks like:\n\nhist(pt(t10k,5))\n\n\n\n\nNow, the p-values are uniformly distributed between 0 and 1, as expected.\nWhat is a qqplot and how do we use it? A qqplot is a plot of the quantiles of two distributions against each other. If the two distributions are identical, the points will fall on a straight line. If the two distributions are different, the points will deviate from the straight line. We can use a qqplot to compare the t-distribution to the normal distribution. If the t-distribution is identical to the normal distribution, the points will fall on a straight line. If the t-distribution is different from the normal distribution, the points will deviate from the straight line. In this case, we can see that the t-distribution is different from the normal distribution, as the points deviate from the straight line. What would happen if we increased the sample size? The t-distribution would approach the normal distribution, and the points would fall closer and closer to the straight line.\n\nqqplot(z10k,t10k)\nabline(0,1)"
  },
  {
    "objectID": "t-stats-and-tests.html#summary-of-t-distribution-vs-normal-distribution",
    "href": "t-stats-and-tests.html#summary-of-t-distribution-vs-normal-distribution",
    "title": "10  The t-statistic and t-distribution",
    "section": "10.4 Summary of t-distribution vs normal distribution",
    "text": "10.4 Summary of t-distribution vs normal distribution\nThe t-distribution is a family of probability distributions that depends on a parameter called degrees of freedom, which is related to the sample size. The t-distribution approaches the standard normal distribution as the sample size increases but has heavier tails for smaller sample sizes. This means that the t-distribution is more conservative in calculating p-values for small samples, making it harder to reject the null hypothesis. Including an estimate of the standard deviation changes the way we calculate p-values by switching from the standard normal distribution to the t-distribution, which accounts for the uncertainty introduced by estimating the population standard deviation from the sample. This adjustment is particularly important for small sample sizes, as it provides a more accurate assessment of the statistical significance of our results."
  },
  {
    "objectID": "t-stats-and-tests.html#t.test",
    "href": "t-stats-and-tests.html#t.test",
    "title": "10  The t-statistic and t-distribution",
    "section": "10.5 t.test",
    "text": "10.5 t.test\n\n10.5.1 One-sample\n\nx = rnorm(20,1)\n# small sample\nt.test(x[1:5])\n\n\n    One Sample t-test\n\ndata:  x[1:5]\nt = 0.97599, df = 4, p-value = 0.3843\nalternative hypothesis: true mean is not equal to 0\n95 percent confidence interval:\n -1.029600  2.145843\nsample estimates:\nmean of x \n0.5581214 \n\n\nIncrease sample size:\n\nt.test(x[1:20])\n\n\n    One Sample t-test\n\ndata:  x[1:20]\nt = 3.8245, df = 19, p-value = 0.001144\nalternative hypothesis: true mean is not equal to 0\n95 percent confidence interval:\n 0.3541055 1.2101894\nsample estimates:\nmean of x \n0.7821474 \n\n\n\n\n10.5.2 two-sample\n\nx = rnorm(10,0.5)\ny = rnorm(10,-0.5)\nt.test(x,y)\n\n\n    Welch Two Sample t-test\n\ndata:  x and y\nt = 3.4296, df = 17.926, p-value = 0.003003\nalternative hypothesis: true difference in means is not equal to 0\n95 percent confidence interval:\n 0.5811367 2.4204048\nsample estimates:\n mean of x  mean of y \n 0.7039205 -0.7968502 \n\n\n\n\n10.5.3 from a data.frame\n\ndf = data.frame(value=c(x,y),group=as.factor(rep(c('g1','g2'),each=10)))\nt.test(value ~ group, data=df)\n\n\n    Welch Two Sample t-test\n\ndata:  value by group\nt = 3.4296, df = 17.926, p-value = 0.003003\nalternative hypothesis: true difference in means between group g1 and group g2 is not equal to 0\n95 percent confidence interval:\n 0.5811367 2.4204048\nsample estimates:\nmean in group g1 mean in group g2 \n       0.7039205       -0.7968502 \n\n\n\n\n10.5.4 Equivalence to linear model\n\nt.test(value ~ group, data=df, var.equal=TRUE)\n\n\n    Two Sample t-test\n\ndata:  value by group\nt = 3.4296, df = 18, p-value = 0.002989\nalternative hypothesis: true difference in means between group g1 and group g2 is not equal to 0\n95 percent confidence interval:\n 0.5814078 2.4201337\nsample estimates:\nmean in group g1 mean in group g2 \n       0.7039205       -0.7968502 \n\n\nThis is equivalent to:\n\nres = lm(value ~ group, data=df)\nsummary(res)\n\n\nCall:\nlm(formula = value ~ group, data = df)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-1.9723 -0.5600  0.2511  0.5252  1.3889 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)   \n(Intercept)   0.7039     0.3094   2.275  0.03538 * \ngroupg2      -1.5008     0.4376  -3.430  0.00299 **\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.9785 on 18 degrees of freedom\nMultiple R-squared:  0.3952,    Adjusted R-squared:  0.3616 \nF-statistic: 11.76 on 1 and 18 DF,  p-value: 0.002989\n\n\n\n\n\n\nStudent. 1908. “The Probable Error of a Mean.” Biometrika 6 (1): 1–25. https://doi.org/10.2307/2331554."
  },
  {
    "objectID": "kmeans.html#history-of-the-k-means-algorithm",
    "href": "kmeans.html#history-of-the-k-means-algorithm",
    "title": "11  K-means clustering",
    "section": "11.1 History of the k-means algorithm",
    "text": "11.1 History of the k-means algorithm\nThe k-means clustering algorithm was first proposed by Stuart Lloyd in 1957 as a technique for pulse-code modulation. However, it was not published until 1982. In 1965, Edward W. Forgy published an essentially identical method, which became widely known as the k-means algorithm. Since then, k-means clustering has become one of the most popular unsupervised learning techniques in data analysis and machine learning.\nK-means clustering is a method for finding patterns or groups in a dataset. It is an unsupervised learning technique, meaning that it doesn’t rely on previously labeled data for training. Instead, it identifies structures or patterns directly from the data based on the similarity between data points (see Figure 11.1).\n\n\n\nFigure 11.1: K-means clustering takes a dataset and divides it into k clusters.\n\n\nIn simple terms, k-means clustering aims to divide a dataset into k distinct groups or clusters, where each data point belongs to the cluster with the nearest mean (average). The goal is to minimize the variability within each cluster while maximizing the differences between clusters. This helps to reveal hidden patterns or relationships in the data that might not be apparent otherwise."
  },
  {
    "objectID": "kmeans.html#the-k-means-algorithm",
    "href": "kmeans.html#the-k-means-algorithm",
    "title": "11  K-means clustering",
    "section": "11.2 The k-means algorithm",
    "text": "11.2 The k-means algorithm\nThe k-means algorithm follows these general steps:\n\nChoose the number of clusters k.\nInitialize the cluster centroids randomly by selecting k data points from the dataset.\nAssign each data point to the nearest centroid.\nUpdate the centroids by computing the mean of all the data points assigned to each centroid.\nRepeat steps 3 and 4 until the centroids no longer change or a certain stopping criterion is met (e.g., a maximum number of iterations).\n\nThe algorithm converges when the centroids stabilize or no longer change significantly. The final clusters represent the underlying patterns or structures in the data. Advantages and disadvantages of k-means clustering"
  },
  {
    "objectID": "kmeans.html#pros-and-cons-of-k-means-clustering",
    "href": "kmeans.html#pros-and-cons-of-k-means-clustering",
    "title": "11  K-means clustering",
    "section": "11.3 Pros and cons of k-means clustering",
    "text": "11.3 Pros and cons of k-means clustering\nCompared to other clustering algorithms, k-means has several advantages:\n\n\nSimplicity and ease of implementation\n\nThe k-means algorithm is relatively straightforward and can be easily implemented, even for large datasets.\n\n\n\nScalability\n\nThe algorithm can be adapted for large datasets using various optimization techniques or parallel processing.\n\n\n\nSpeed\n\nK-means is generally faster than other clustering algorithms, especially when the number of clusters k is small.\n\n\n\nInterpretability\n\nThe results of k-means clustering are easy to understand, as the algorithm assigns each data point to a specific cluster based on its similarity to the cluster’s centroid.\n\n\n\nHowever, k-means clustering has several disadvantages as well:\n\n\nChoice of k\n\nSelecting the appropriate number of clusters can be challenging and often requires domain knowledge or experimentation. A poor choice of k may yield poor results.\n\n\n\nSensitivity to initial conditions\n\nThe algorithm’s results can vary depending on the initial placement of centroids. To overcome this issue, the algorithm can be run multiple times with different initializations and the best solution can be chosen based on a criterion (e.g., minimizing within-cluster variation).\n\n\n\nAssumes spherical clusters\n\nK-means assumes that clusters are spherical and evenly sized, which may not always be the case in real-world datasets. This can lead to poor performance if the underlying clusters have different shapes or densities.\n\n\n\nSensitivity to outliers\n\nThe algorithm is sensitive to outliers, which can heavily influence the position of centroids and the final clustering result. Preprocessing the data to remove or mitigate the impact of outliers can help improve the performance of k-means clustering.\n\n\n\nDespite limitations, k-means clustering remains a popular and widely used method for exploring and analyzing data, particularly in biological data analysis, where identifying patterns and relationships can provide valuable insights into complex systems and processes."
  },
  {
    "objectID": "kmeans.html#an-example-of-k-means-clustering",
    "href": "kmeans.html#an-example-of-k-means-clustering",
    "title": "11  K-means clustering",
    "section": "11.4 An example of k-means clustering",
    "text": "11.4 An example of k-means clustering\n\n11.4.1 The data and experimental background\nThe data we are going to use are from DeRisi, Iyer, and Brown (1997). From their abstract:\n\nDNA microarrays containing virtually every gene of Saccharomyces cerevisiae were used to carry out a comprehensive investigation of the temporal program of gene expression accompanying the metabolic shift from fermentation to respiration. The expression profiles observed for genes with known metabolic functions pointed to features of the metabolic reprogramming that occur during the diauxic shift, and the expression patterns of many previously uncharacterized genes provided clues to their possible functions.\n\nThese data are available from NCBI GEO as GSE28.\nIn the case of the baker’s or brewer’s yeast Saccharomyces cerevisiae growing on glucose with plenty of aeration, the diauxic growth pattern is commonly observed in batch culture. During the first growth phase, when there is plenty of glucose and oxygen available, the yeast cells prefer glucose fermentation to aerobic respiration even though aerobic respiration is the more efficient pathway to grow on glucose. This experiment profiles gene expression for 6400 genes over a time course during which the cells are undergoing a diauxic shift.\nThe data in deRisi et al. have no replicates and are time course data. Sometimes, seeing how groups of genes behave can give biological insight into the experimental system or the function of individual genes. We can use clustering to group genes that have a similar expression pattern over time and then potentially look at the genes that do so.\nOur goal, then, is to use kmeans clustering to divide highly variable (informative) genes into groups and then to visualize those groups."
  },
  {
    "objectID": "kmeans.html#getting-data",
    "href": "kmeans.html#getting-data",
    "title": "11  K-means clustering",
    "section": "11.5 Getting data",
    "text": "11.5 Getting data\nThese data were deposited at NCBI GEO back in 2002. GEOquery can pull them out easily.\n\nlibrary(GEOquery)\ngse = getGEO(\"GSE28\")[[1]]\nclass(gse)\n\n[1] \"ExpressionSet\"\nattr(,\"package\")\n[1] \"Biobase\"\n\n\nGEOquery is a little dated and was written before the SummarizedExperiment existed. However, Bioconductor makes a conversion from the old ExpressionSet that GEOquery uses to the SummarizedExperiment that we see so commonly used now.\n\nlibrary(SummarizedExperiment)\ngse = as(gse, \"SummarizedExperiment\")\ngse\n\nclass: SummarizedExperiment \ndim: 6400 7 \nmetadata(3): experimentData annotation protocolData\nassays(1): exprs\nrownames(6400): 1 2 ... 6399 6400\nrowData names(20): ID ORF ... FAILED IS_CONTAMINATED\ncolnames(7): GSM887 GSM888 ... GSM892 GSM893\ncolData names(33): title geo_accession ... supplementary_file\n  data_row_count\n\n\nTaking a quick look at the colData(), it might be that we want to reorder the columns a bit.\n\ncolData(gse)$title\n\n[1] \"diauxic shift timecourse: 15.5 hr\" \"diauxic shift timecourse: 0 hr\"   \n[3] \"diauxic shift timecourse: 18.5 hr\" \"diauxic shift timecourse: 9.5 hr\" \n[5] \"diauxic shift timecourse: 11.5 hr\" \"diauxic shift timecourse: 13.5 hr\"\n[7] \"diauxic shift timecourse: 20.5 hr\"\n\n\nSo, we can reorder by hand to get the time course correct:\n\ngse = gse[, c(2,4,5,6,1,3,7)]"
  },
  {
    "objectID": "kmeans.html#preprocessing",
    "href": "kmeans.html#preprocessing",
    "title": "11  K-means clustering",
    "section": "11.6 Preprocessing",
    "text": "11.6 Preprocessing\nIn gene expression data analysis, the primary objective is often to identify genes that exhibit significant differences in expression levels across various conditions, such as diseased vs. healthy samples or different time points in a time-course experiment. However, gene expression datasets are typically large, noisy, and contain numerous genes that do not exhibit substantial changes in expression levels. Analyzing all genes in the dataset can be computationally intensive and may introduce noise or false positives in the results.\nOne common approach to reduce the complexity of the dataset and focus on the most informative genes is to subset the genes based on their standard deviation in expression levels across the samples. The standard deviation is a measure of dispersion or variability in the data, and genes with high standard deviations have more variation in their expression levels across the samples.\nBy selecting genes with high standard deviations, we focus on genes that show relatively large changes in expression levels across different conditions. These genes are more likely to be biologically relevant and involved in the underlying processes or pathways of interest. In contrast, genes with low standard deviations exhibit little or no change in expression levels and are less likely to be informative for the analysis. It turns out that applying filtering based on criteria such as standard deviation can also increase power and reduce false positives in the analysis (Bourgon, Gentleman, and Huber 2010).\nTo subset the genes for analysis based on their standard deviation, the following steps can be followed: Calculate the standard deviation of each gene’s expression levels across all samples. Set a threshold for the standard deviation, which can be determined based on domain knowledge, data distribution, or a specific percentile of the standard deviation values (e.g., selecting the top 10% or 25% of genes with the highest standard deviations). Retain only the genes with a standard deviation above the chosen threshold for further analysis.\nBy subsetting the genes based on their standard deviation, we can reduce the complexity of the dataset, speed up the subsequent analysis, and increase the likelihood of detecting biologically meaningful patterns and relationships in the gene expression data. The threshold for the standard deviation cutoff is rather arbitrary, so it may be beneficial to try a few to check for sensitivity of findings.\n\nsds = apply(assays(gse)[[1]], 1, sd)\nhist(sds)\n\n\n\n\nFigure 11.2: Histogram of standard deviations for all genes in the deRisi dataset.\n\n\n\n\nExamining the plot, we can see that the most highly variable genes have an sd &gt; 0.8 or so (arbitrary). We can, for convenience, create a new SummarizedExperiment that contains only our most highly variable genes.\n\nidx = sds&gt;0.8 & !is.na(sds)\ngse_sub = gse[idx,]"
  },
  {
    "objectID": "kmeans.html#clustering",
    "href": "kmeans.html#clustering",
    "title": "11  K-means clustering",
    "section": "11.7 Clustering",
    "text": "11.7 Clustering\nNow, gse_sub contains a subset of our data.\nThe kmeans function takes a matrix and the number of clusters as arguments.\n\nk = 4\nkm = kmeans(assays(gse_sub)[[1]], 4)\n\nThe km kmeans result contains a vector, km$cluster, which gives the cluster associated with each gene. We can plot the genes for each cluster to see how these different genes behave.\n\nexpression_values = assays(gse_sub)[[1]]\npar(mfrow=c(2,2), mar=c(3,4,1,2)) # this allows multiple plots per page\nfor(i in 1:k) {\n    matplot(t(expression_values[km$cluster==i, ]), type='l', ylim=c(-3,3),\n            ylab = paste(\"cluster\", i))\n}\n\n\n\n\nFigure 11.3: Gene expression profiles for the four clusters identified by k-means clustering. Each line represents a gene in the cluster, and each column represents a time point in the experiment. Each cluster shows a distinct trend where the genes in the cluster are potentially co-regulated.\n\n\n\n\nTry this with different size k. Perhaps go back to choose more genes (using a smaller cutoff for sd)."
  },
  {
    "objectID": "kmeans.html#summary",
    "href": "kmeans.html#summary",
    "title": "11  K-means clustering",
    "section": "11.8 Summary",
    "text": "11.8 Summary\nIn this lesson, we have learned how to use k-means clustering to identify groups of genes that behave similarly over time. We have also learned how to subset our data to focus on the most informative genes.\n\n\n\n\nBourgon, Richard, Robert Gentleman, and Wolfgang Huber. 2010. “Independent Filtering Increases Detection Power for High-Throughput Experiments.” Proceedings of the National Academy of Sciences 107 (21): 9546–51. https://doi.org/10.1073/pnas.0914005107.\n\n\nDeRisi, J. L., V. R. Iyer, and P. O. Brown. 1997. “Exploring the Metabolic and Genetic Control of Gene Expression on a Genomic Scale.” Science (New York, N.Y.) 278 (5338): 680–86. https://doi.org/10.1126/science.278.5338.680."
  },
  {
    "objectID": "bioc-summarizedexperiment.html#anatomy-of-a-summarizedexperiment",
    "href": "bioc-summarizedexperiment.html#anatomy-of-a-summarizedexperiment",
    "title": "12  Introduction to SummarizedExperiment",
    "section": "12.1 Anatomy of a SummarizedExperiment",
    "text": "12.1 Anatomy of a SummarizedExperiment\nThe SummarizedExperiment package contains two classes: SummarizedExperiment and RangedSummarizedExperiment.\nSummarizedExperiment is a matrix-like container where rows represent features of interest (e.g. genes, transcripts, exons, etc.) and columns represent samples. The objects contain one or more assays, each represented by a matrix-like object of numeric or other mode. The rows of a SummarizedExperiment object represent features of interest. Information about these features is stored in a DataFrame object, accessible using the function rowData(). Each row of the DataFrame provides information on the feature in the corresponding row of the SummarizedExperiment object. Columns of the DataFrame represent different attributes of the features of interest, e.g., gene or transcript IDs, etc.\nRangedSummarizedExperiment is the “child”” of the SummarizedExperiment class which means that all the methods on SummarizedExperiment also work on a RangedSummarizedExperiment.\nThe fundamental difference between the two classes is that the rows of a RangedSummarizedExperiment object represent genomic ranges of interest instead of a DataFrame of features. The RangedSummarizedExperiment ranges are described by a GRanges or a GRangesList object, accessible using the rowRanges() function.\nFigure 12.1 displays the class geometry and highlights the vertical (column) and horizontal (row) relationships.\n\n\n\nFigure 12.1: Summarized Experiment. There are three main components, the colData(), the rowData() and the assays(). The accessors for the various parts of a complete SummarizedExperiment object match the names.\n\n\n\n12.1.1 Assays\nThe airway package contains an example dataset from an RNA-Seq experiment of read counts per gene for airway smooth muscles. These data are stored in a RangedSummarizedExperiment object which contains 8 different experimental and assays 64,102 gene transcripts.\n\n\nLoading required package: airway\n\n\n\nlibrary(SummarizedExperiment)\ndata(airway, package=\"airway\")\nse &lt;- airway\nse\n\nclass: RangedSummarizedExperiment \ndim: 63677 8 \nmetadata(1): ''\nassays(1): counts\nrownames(63677): ENSG00000000003 ENSG00000000005 ... ENSG00000273492\n  ENSG00000273493\nrowData names(10): gene_id gene_name ... seq_coord_system symbol\ncolnames(8): SRR1039508 SRR1039509 ... SRR1039520 SRR1039521\ncolData names(9): SampleName cell ... Sample BioSample\n\n\nTo retrieve the experiment data from a SummarizedExperiment object one can use the assays() accessor. An object can have multiple assay datasets each of which can be accessed using the $ operator. The airway dataset contains only one assay (counts). Here each row represents a gene transcript and each column one of the samples.\n\nassays(se)$counts\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSRR1039508\nSRR1039509\nSRR1039512\nSRR1039513\nSRR1039516\nSRR1039517\nSRR1039520\nSRR1039521\n\n\n\n\nENSG00000000003\n679\n448\n873\n408\n1138\n1047\n770\n572\n\n\nENSG00000000005\n0\n0\n0\n0\n0\n0\n0\n0\n\n\nENSG00000000419\n467\n515\n621\n365\n587\n799\n417\n508\n\n\nENSG00000000457\n260\n211\n263\n164\n245\n331\n233\n229\n\n\nENSG00000000460\n60\n55\n40\n35\n78\n63\n76\n60\n\n\nENSG00000000938\n0\n0\n2\n0\n1\n0\n0\n0\n\n\nENSG00000000971\n3251\n3679\n6177\n4252\n6721\n11027\n5176\n7995\n\n\nENSG00000001036\n1433\n1062\n1733\n881\n1424\n1439\n1359\n1109\n\n\nENSG00000001084\n519\n380\n595\n493\n820\n714\n696\n704\n\n\nENSG00000001167\n394\n236\n464\n175\n658\n584\n360\n269\n\n\n\n\n\n\n\n12.1.2 ‘Row’ (regions-of-interest) data\nThe rowRanges() accessor is used to view the range information for a RangedSummarizedExperiment. (Note if this were the parent SummarizedExperiment class we’d use rowData()). The data are stored in a GRangesList object, where each list element corresponds to one gene transcript and the ranges in each GRanges correspond to the exons in the transcript.\n\nrowRanges(se)\n\nGRangesList object of length 63677:\n$ENSG00000000003\nGRanges object with 17 ranges and 2 metadata columns:\n       seqnames            ranges strand |   exon_id       exon_name\n          &lt;Rle&gt;         &lt;IRanges&gt;  &lt;Rle&gt; | &lt;integer&gt;     &lt;character&gt;\n   [1]        X 99883667-99884983      - |    667145 ENSE00001459322\n   [2]        X 99885756-99885863      - |    667146 ENSE00000868868\n   [3]        X 99887482-99887565      - |    667147 ENSE00000401072\n   [4]        X 99887538-99887565      - |    667148 ENSE00001849132\n   [5]        X 99888402-99888536      - |    667149 ENSE00003554016\n   ...      ...               ...    ... .       ...             ...\n  [13]        X 99890555-99890743      - |    667156 ENSE00003512331\n  [14]        X 99891188-99891686      - |    667158 ENSE00001886883\n  [15]        X 99891605-99891803      - |    667159 ENSE00001855382\n  [16]        X 99891790-99892101      - |    667160 ENSE00001863395\n  [17]        X 99894942-99894988      - |    667161 ENSE00001828996\n  -------\n  seqinfo: 722 sequences (1 circular) from an unspecified genome\n\n...\n&lt;63676 more elements&gt;\n\n\n\n\n12.1.3 ‘Column’ (sample) data\nSample meta-data describing the samples can be accessed using colData(), and is a DataFrame that can store any number of descriptive columns for each sample row.\n\ncolData(se)\n\nDataFrame with 8 rows and 9 columns\n           SampleName     cell      dex    albut        Run avgLength\n             &lt;factor&gt; &lt;factor&gt; &lt;factor&gt; &lt;factor&gt;   &lt;factor&gt; &lt;integer&gt;\nSRR1039508 GSM1275862  N61311     untrt    untrt SRR1039508       126\nSRR1039509 GSM1275863  N61311     trt      untrt SRR1039509       126\nSRR1039512 GSM1275866  N052611    untrt    untrt SRR1039512       126\nSRR1039513 GSM1275867  N052611    trt      untrt SRR1039513        87\nSRR1039516 GSM1275870  N080611    untrt    untrt SRR1039516       120\nSRR1039517 GSM1275871  N080611    trt      untrt SRR1039517       126\nSRR1039520 GSM1275874  N061011    untrt    untrt SRR1039520       101\nSRR1039521 GSM1275875  N061011    trt      untrt SRR1039521        98\n           Experiment    Sample    BioSample\n             &lt;factor&gt;  &lt;factor&gt;     &lt;factor&gt;\nSRR1039508  SRX384345 SRS508568 SAMN02422669\nSRR1039509  SRX384346 SRS508567 SAMN02422675\nSRR1039512  SRX384349 SRS508571 SAMN02422678\nSRR1039513  SRX384350 SRS508572 SAMN02422670\nSRR1039516  SRX384353 SRS508575 SAMN02422682\nSRR1039517  SRX384354 SRS508576 SAMN02422673\nSRR1039520  SRX384357 SRS508579 SAMN02422683\nSRR1039521  SRX384358 SRS508580 SAMN02422677\n\n\nThis sample metadata can be accessed using the $ accessor which makes it easy to subset the entire object by a given phenotype.\n\n# subset for only those samples treated with dexamethasone\nse[, se$dex == \"trt\"]\n\nclass: RangedSummarizedExperiment \ndim: 63677 4 \nmetadata(1): ''\nassays(1): counts\nrownames(63677): ENSG00000000003 ENSG00000000005 ... ENSG00000273492\n  ENSG00000273493\nrowData names(10): gene_id gene_name ... seq_coord_system symbol\ncolnames(4): SRR1039509 SRR1039513 SRR1039517 SRR1039521\ncolData names(9): SampleName cell ... Sample BioSample\n\n\n\n\n12.1.4 Experiment-wide metadata\nMeta-data describing the experimental methods and publication references can be accessed using metadata().\n\nmetadata(se)\n\n[[1]]\nExperiment data\n  Experimenter name: Himes BE \n  Laboratory: NA \n  Contact information:  \n  Title: RNA-Seq transcriptome profiling identifies CRISPLD2 as a glucocorticoid responsive gene that modulates cytokine function in airway smooth muscle cells. \n  URL: http://www.ncbi.nlm.nih.gov/pubmed/24926665 \n  PMIDs: 24926665 \n\n  Abstract: A 226 word abstract is available. Use 'abstract' method.\n\n\nNote that metadata() is just a simple list, so it is appropriate for any experiment wide metadata the user wishes to save, such as storing model formulas.\n\nmetadata(se)$formula &lt;- counts ~ dex + albut\n\nmetadata(se)\n\n[[1]]\nExperiment data\n  Experimenter name: Himes BE \n  Laboratory: NA \n  Contact information:  \n  Title: RNA-Seq transcriptome profiling identifies CRISPLD2 as a glucocorticoid responsive gene that modulates cytokine function in airway smooth muscle cells. \n  URL: http://www.ncbi.nlm.nih.gov/pubmed/24926665 \n  PMIDs: 24926665 \n\n  Abstract: A 226 word abstract is available. Use 'abstract' method.\n\n$formula\ncounts ~ dex + albut"
  },
  {
    "objectID": "bioc-summarizedexperiment.html#common-operations-on-summarizedexperiment",
    "href": "bioc-summarizedexperiment.html#common-operations-on-summarizedexperiment",
    "title": "12  Introduction to SummarizedExperiment",
    "section": "12.2 Common operations on SummarizedExperiment",
    "text": "12.2 Common operations on SummarizedExperiment\n\n12.2.1 Subsetting\n\n[ Performs two dimensional subsetting, just like subsetting a matrix or data frame.\n\n\n# subset the first five transcripts and first three samples\nse[1:5, 1:3]\n\nclass: RangedSummarizedExperiment \ndim: 5 3 \nmetadata(2): '' formula\nassays(1): counts\nrownames(5): ENSG00000000003 ENSG00000000005 ENSG00000000419\n  ENSG00000000457 ENSG00000000460\nrowData names(10): gene_id gene_name ... seq_coord_system symbol\ncolnames(3): SRR1039508 SRR1039509 SRR1039512\ncolData names(9): SampleName cell ... Sample BioSample\n\n\n\n$ operates on colData() columns, for easy sample extraction.\n\n\nse[, se$cell == \"N61311\"]\n\nclass: RangedSummarizedExperiment \ndim: 63677 2 \nmetadata(2): '' formula\nassays(1): counts\nrownames(63677): ENSG00000000003 ENSG00000000005 ... ENSG00000273492\n  ENSG00000273493\nrowData names(10): gene_id gene_name ... seq_coord_system symbol\ncolnames(2): SRR1039508 SRR1039509\ncolData names(9): SampleName cell ... Sample BioSample\n\n\n\n\n12.2.2 Getters and setters\n\nrowRanges() / (rowData()), colData(), metadata()\n\n\ncounts &lt;- matrix(1:15, 5, 3, dimnames=list(LETTERS[1:5], LETTERS[1:3]))\n\ndates &lt;- SummarizedExperiment(assays=list(counts=counts),\n                              rowData=DataFrame(month=month.name[1:5], day=1:5))\n\n# Subset all January assays\ndates[rowData(dates)$month == \"January\", ]\n\nclass: SummarizedExperiment \ndim: 1 3 \nmetadata(0):\nassays(1): counts\nrownames(1): A\nrowData names(2): month day\ncolnames(3): A B C\ncolData names(0):\n\n\n\nassay() versus assays() There are two accessor functions for extracting the assay data from a SummarizedExperiment object. assays() operates on the entire list of assay data as a whole, while assay() operates on only one assay at a time. assay(x, i) is simply a convenience function which is equivalent to assays(x)[[i]].\n\n\nassays(se)\n\nList of length 1\nnames(1): counts\n\nassays(se)[[1]][1:5, 1:5]\n\n                SRR1039508 SRR1039509 SRR1039512 SRR1039513 SRR1039516\nENSG00000000003        679        448        873        408       1138\nENSG00000000005          0          0          0          0          0\nENSG00000000419        467        515        621        365        587\nENSG00000000457        260        211        263        164        245\nENSG00000000460         60         55         40         35         78\n\n# assay defaults to the first assay if no i is given\nassay(se)[1:5, 1:5]\n\n                SRR1039508 SRR1039509 SRR1039512 SRR1039513 SRR1039516\nENSG00000000003        679        448        873        408       1138\nENSG00000000005          0          0          0          0          0\nENSG00000000419        467        515        621        365        587\nENSG00000000457        260        211        263        164        245\nENSG00000000460         60         55         40         35         78\n\nassay(se, 1)[1:5, 1:5]\n\n                SRR1039508 SRR1039509 SRR1039512 SRR1039513 SRR1039516\nENSG00000000003        679        448        873        408       1138\nENSG00000000005          0          0          0          0          0\nENSG00000000419        467        515        621        365        587\nENSG00000000457        260        211        263        164        245\nENSG00000000460         60         55         40         35         78\n\n\n\n\n12.2.3 Range-based operations\n\nsubsetByOverlaps() SummarizedExperiment objects support all of the findOverlaps() methods and associated functions. This includes subsetByOverlaps(), which makes it easy to subset a SummarizedExperiment object by an interval.\n\nIn tne next code block, we define a region of interest (or many regions of interest) and then subset our SummarizedExperiment by overlaps with this region.\n\n# Subset for only rows which are in the interval 100,000 to 110,000 of\n# chromosome 1\nroi &lt;- GRanges(seqnames=\"1\", ranges=100000:1100000)\nsub_se = subsetByOverlaps(se, roi)\nsub_se\n\nclass: RangedSummarizedExperiment \ndim: 74 8 \nmetadata(2): '' formula\nassays(1): counts\nrownames(74): ENSG00000131591 ENSG00000177757 ... ENSG00000272512\n  ENSG00000273443\nrowData names(10): gene_id gene_name ... seq_coord_system symbol\ncolnames(8): SRR1039508 SRR1039509 ... SRR1039520 SRR1039521\ncolData names(9): SampleName cell ... Sample BioSample\n\ndim(sub_se)\n\n[1] 74  8"
  },
  {
    "objectID": "bioc-summarizedexperiment.html#constructing-a-summarizedexperiment",
    "href": "bioc-summarizedexperiment.html#constructing-a-summarizedexperiment",
    "title": "12  Introduction to SummarizedExperiment",
    "section": "12.3 Constructing a SummarizedExperiment",
    "text": "12.3 Constructing a SummarizedExperiment\nOften, SummarizedExperiment or RangedSummarizedExperiment objects are returned by functions written by other packages. However it is possible to create them by hand with a call to the SummarizedExperiment() constructor. The code below is simply to illustrate the mechanics of creating an object from scratch. In practice, you will probably have the pieces of the object from other sources such as Excel files or csv files.\nConstructing a RangedSummarizedExperiment with a GRanges as the rowRanges argument:\n\nnrows &lt;- 200\nncols &lt;- 6\ncounts &lt;- matrix(runif(nrows * ncols, 1, 1e4), nrows)\nrowRanges &lt;- GRanges(rep(c(\"chr1\", \"chr2\"), c(50, 150)),\n                     IRanges(floor(runif(200, 1e5, 1e6)), width=100),\n                     strand=sample(c(\"+\", \"-\"), 200, TRUE),\n                     feature_id=sprintf(\"ID%03d\", 1:200))\ncolData &lt;- DataFrame(Treatment=rep(c(\"ChIP\", \"Input\"), 3),\n                     row.names=LETTERS[1:6])\n\nSummarizedExperiment(assays=list(counts=counts),\n                     rowRanges=rowRanges, colData=colData)\n\nclass: RangedSummarizedExperiment \ndim: 200 6 \nmetadata(0):\nassays(1): counts\nrownames: NULL\nrowData names(1): feature_id\ncolnames(6): A B ... E F\ncolData names(1): Treatment\n\n\nA SummarizedExperiment can be constructed with or without supplying a DataFrame for the rowData argument:\n\nSummarizedExperiment(assays=list(counts=counts), colData=colData)\n\nclass: SummarizedExperiment \ndim: 200 6 \nmetadata(0):\nassays(1): counts\nrownames: NULL\nrowData names(0):\ncolnames(6): A B ... E F\ncolData names(1): Treatment"
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Bourgon, Richard, Robert Gentleman, and Wolfgang Huber. 2010.\n“Independent Filtering Increases Detection Power for\nHigh-Throughput Experiments.” Proceedings of the National\nAcademy of Sciences 107 (21): 9546–51. https://doi.org/10.1073/pnas.0914005107.\n\n\nDeRisi, J. L., V. R. Iyer, and P. O. Brown. 1997. “Exploring the\nMetabolic and Genetic Control of Gene Expression on a Genomic\nScale.” Science (New York, N.Y.) 278 (5338): 680–86. https://doi.org/10.1126/science.278.5338.680.\n\n\nStudent. 1908. “The Probable Error of a\nMean.” Biometrika 6 (1): 1–25. https://doi.org/10.2307/2331554."
  },
  {
    "objectID": "appendix.html#data-sets",
    "href": "appendix.html#data-sets",
    "title": "Appendix A — Appendix",
    "section": "A.1 Data Sets",
    "text": "A.1 Data Sets\n\nBRFSS subset\nALL clinical data\nALL expression data"
  },
  {
    "objectID": "appendix.html#swirl",
    "href": "appendix.html#swirl",
    "title": "Appendix A — Appendix",
    "section": "A.2 Swirl",
    "text": "A.2 Swirl\nThe following is from the swirl website.\n\nThe swirl R package makes it fun and easy to learn R programming and data science. If you are new to R, have no fear.\n\nTo get started, we need to install a new package into R.\n\ninstall.packages('swirl')\n\nOnce installed, we want to load it into the R workspace so we can use it.\n\nlibrary('swirl')\n\nFinally, to get going, start swirl and follow the instructions.\n\nswirl()"
  },
  {
    "objectID": "additional_resources.html",
    "href": "additional_resources.html",
    "title": "Appendix B — Additional resources",
    "section": "",
    "text": "Base R Cheat Sheet"
  },
  {
    "objectID": "license.html",
    "href": "license.html",
    "title": "",
    "section": "",
    "text": "Code\n\n\n\n\n    To the extent possible under law,  Sean Davis has waived all copyright and related or neighboring rights to Statistical analysis of functional genomics dataa. This work is published from:  United States."
  }
]