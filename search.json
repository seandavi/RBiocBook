[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "The RBioc Book",
    "section": "",
    "text": "Preface\nThe goal of the material is NOT to teach you R or Bioconductor, but rather to provide enough groundwork to enable you to learn R and Bioconductor on your own. The idea is that we learn faster and better by developing “schemata” that allow us to organize and understand new information. When a new concept is introduced, it is easier to understand if it can be related to something we already know (see Figure 1).\nThis book is a collection of resources meant to help build your data science, statistical, and computational schemata. It is meant to be largely self-directed, but for those looking to teach data science, it can also be used as a guide for structuring a course. Material is a bit variable in terms of difficulty, prerequisites, and format which is a reflection of the organic creation of the material. See below for additional thoughts on adult learning and how it relates to this material.",
    "crumbs": [
      "Home",
      "Preface"
    ]
  },
  {
    "objectID": "index.html#what-are-the-goals",
    "href": "index.html#what-are-the-goals",
    "title": "The RBioc Book",
    "section": "What are the goals?",
    "text": "What are the goals?\nWe can often get lost in the weeds of technical details of lessons, focusing only on the syntax and semantics of R and Bioconductor. Let’s take a step back and consider the big picture. The goals are to:\n\nHave a foundation for reading and writing R code to solve problems and understand data.\nBe able to find and use online resources including AI and tutorials to solve problems and learn new concepts.\nBe able to effectively communicate with others about R code and data science concepts.\nMost importantly, to develop the confidence to become a self-directed learner, experimenting with concepts and practice of data science to address real-world problems.",
    "crumbs": [
      "Home",
      "Preface"
    ]
  },
  {
    "objectID": "index.html#adult-learners",
    "href": "index.html#adult-learners",
    "title": "The RBioc Book",
    "section": "Adult learners",
    "text": "Adult learners\nAdult Learning Theory, also known as Andragogy, is the concept and practice of designing, developing, and delivering instructional experiences for adult learners. It is based on the belief that adults learn differently than children, and thus, require distinct approaches to engage, motivate, and retain information (Center 2016). The term was first introduced by Malcolm Knowles, an American educator who is known for his work in adult education (Knowles, Holton, and Swanson 2005).\nOne of the fundamental principles of Adult Learning Theory is that adults are self-directed learners. This means that we prefer to take control of our own learning process and set personal goals for themselves. We are motivated by our desire to solve problems or gain knowledge to improve our lives (see Figure 2). As a result, educational content for adults should be relevant and applicable to real-life situations. Furthermore, adult learners should be given opportunities to actively engage in the learning process by making choices, setting goals, and evaluating their progress.\n\n\n\n\n\nFigure 2: Why do adults choose to learn something?\n\n\nAnother key aspect of Adult Learning Theory is the role of experience. We bring a wealth of experience to the learning process, which serves as a resource for new learning. We often have well-established beliefs, values, and mental models that can influence our willingness to accept new ideas and concepts. Therefore, it is essential to acknowledge and respect our shared and unique past experiences and create an environment where we all feel comfortable sharing our perspectives.\nTo effectively learn as a group of adult learners, it is crucial to establish a collaborative learning environment that promotes open communication and fosters trust among participants. We all appreciate and strive for a respectful and supportive atmosphere where we can express our opinions without fear of judgment. Instructors should help facilitate discussions, encourage peer-to-peer interactions, and incorporate group activities and collaboration to capitalize on the collective knowledge of participants.\nAdditionally, adult learners often have multiple responsibilities outside of the learning environment, such as work and family commitments. As a result, we require flexible learning opportunities that accommodate busy schedules. Offering a variety of instructional formats, such as online modules, self-paced learning, or evening classes, can help ensure that adult learners have access to education despite any time constraints.\nAdult learners benefit from a learner-centered approach that focuses on the individual needs, preferences, and interests of each participant can greatly enhance the overall learning experience. In addition, we tend to be more intrinsically motivated to learn when we have a sense of autonomy and can practice and experiment (see Figure 3) with new concepts in a safe environment.\n\n\n\n\n\n\n\nFigure 3: How to stay stuck in data science (or anything). The “Read-Do” loop tends to deliver the best results. Too much reading between doing can be somewhat effective. Reading and simply copy-paste is probably the least effective. When working through material, experiment. Try to break things. Incorporate your own experience or applications whenever possible.\n\n\n\n\nUnderstanding Adult Learning Theory and its principles can significantly enhance the effectiveness of teaching and learning as adults. By respecting our autonomy, acknowledging our experiences, creating a supportive learning environment, offering flexible learning opportunities, and utilizing diverse teaching methods, we can better cater to the unique needs and preferences of adult learners.\nIn practice, that means that we will will not be prescriptive in our approach to teaching data science. We will not tell you what to do, but rather we will provide you with a variety of options and you can choose what works best for you. We will also provide you with a variety of resources and you can choose where to focus your time. Given that we cannot possibly cover everything, we will provide you with a framework for learning and you can fill in the gaps as you see fit. A key component of our success as adult learners is to gain the confidence to ask questions and problem-solve on our own.",
    "crumbs": [
      "Home",
      "Preface"
    ]
  },
  {
    "objectID": "index.html#ai-is-part-of-the-solution",
    "href": "index.html#ai-is-part-of-the-solution",
    "title": "The RBioc Book",
    "section": "AI is part of the solution",
    "text": "AI is part of the solution\nArtificial Intelligence (AI) is becoming an increasingly important tool in education, and it can be a powerful ally in our quest to learn data science. As it turns out, AI is excellent at helping us learn. It can provide tailored explanations, generate practice problems, and even simulate real-world scenarios for us to work through. AI can also help us find resources, summarize complex topics, and even provide feedback on our work. However, it is important to remember that AI is a tool, not a replacement for our own learning. We still need to engage with the material, ask questions, and seek out additional resources when needed. AI can help us learn more efficiently, but it cannot do the learning for us.\nI encourage you to use AI tools to help you learn and to even do work for you when appropriate; AI is an excellent partner in analysis and programming. However, I also encourage you to be critical of the information that AI provides. AI is not perfect and can make mistakes. Throughout this course and book, use AI to dive deeper into the material, to provide additional explanations, and to generate additional depth and breadth to the material.\n\n\n\n\nCenter, Pew Research. 2016. “Lifelong Learning and Technology.” Pew Research Center: Internet, Science & Tech. https://www.pewresearch.org/internet/2016/03/22/lifelong-learning-and-technology/.\n\n\nKnowles, Malcolm S., Elwood F. Holton, and Richard A. Swanson. 2005. The Adult Learner: The Definitive Classic in Adult Education and Human Resource Development. 6th ed. Amsterdam ; Boston: Elsevier.",
    "crumbs": [
      "Home",
      "Preface"
    ]
  },
  {
    "objectID": "intro.html",
    "href": "intro.html",
    "title": "\n1  About R\n",
    "section": "",
    "text": "1.1 What is R?\nIn this chapter, we will discuss the basics of R and RStudio, two essential tools in genomics data analysis. We will cover the advantages of using R and RStudio, how to set up RStudio, and the different panels of the RStudio interface.\nR is a programming language and software environment designed for statistical computing and graphics. It is widely used by statisticians, data scientists, and researchers for data analysis and visualization. R is an open-source language, which means it is free to use, modify, and distribute. Over the years, R has become particularly popular in the fields of genomics and bioinformatics, owing to its extensive libraries and powerful data manipulation capabilities.\nThe R language is a dialect of the S language, which was developed in the 1970s at Bell Laboratories. The first version of R was written by Robert Gentleman and Ross Ihaka and released in 1995 (see this slide deck for Ross Ihaka’s take on R’s history). Since then, R has been continuously developed by the R Core Team, a group of statisticians and computer scientists. The R Core Team releases a new version of R every year.\nFigure 1.1: Google trends showing the popularity of R over time based on Google Trends. Note that google does not capture the context here; bioinformatics applications are still quite heavily reliant on R.",
    "crumbs": [
      "Home",
      "Introduction",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>About R</span>"
    ]
  },
  {
    "objectID": "intro.html#why-use-r",
    "href": "intro.html#why-use-r",
    "title": "\n1  About R\n",
    "section": "\n1.2 Why use R?",
    "text": "1.2 Why use R?\nThere are several reasons why R is a popular choice for data analysis, particularly in genomics and bioinformatics. These include:\n\n\nOpen-source: R is free to use and has a large community of developers who contribute to its growth and development. What is “open-source”?\n\n\nExtensive libraries: There are thousands of R packages available for a wide range of tasks, including specialized packages for genomics and bioinformatics. These libraries have been extensively tested and ara available for free.\n\nData manipulation: R has powerful data manipulation capabilities, making it easy (or at least possible) to clean, process, and analyze large datasets.\n\nGraphics and visualization: R has excellent tools for creating high-quality graphics and visualizations that can be customized to meet the specific needs of your analysis. In most cases, graphics produced by R are publication-quality.\n\nReproducible research: R enables you to create reproducible research by recording your analysis in a script, which can be easily shared and executed by others. In addition, R does not have a meaningful graphical user interface (GUI), which renders analysis in R much more reproducible than tools that rely on GUI interactions.\n\nCross-platform: R runs on Windows, Mac, and Linux (as well as more obscure systems).\n\nInteroperability with other languages: R can interfact with FORTRAN, C, and many other languages.\n\nScalability: R is useful for small and large projects.\n\nI can develop code for analysis on my Mac laptop. I can then install the same code on our 20k core cluster and run it in parallel on 100 samples, monitor the process, and then update a database (for example) with R when complete. In other words, R is a powerful tool that can be used for a wide range of tasks, from small-scale data analysis to large-scale genomics and omics data science projects.",
    "crumbs": [
      "Home",
      "Introduction",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>About R</span>"
    ]
  },
  {
    "objectID": "intro.html#why-not-use-r",
    "href": "intro.html#why-not-use-r",
    "title": "\n1  About R\n",
    "section": "\n1.3 Why not use R?",
    "text": "1.3 Why not use R?\n\nR cannot do everything.\nR is not always the “best” tool for the job.\nR will not hold your hand. Often, it will slap your hand instead.\nThe documentation can be opaque (but there is documentation).\nR can drive you crazy (on a good day) or age you prematurely (on a bad one).\nFinding the right package to do the job you want to do can be challenging; worse, some contributed packages are unreliable.]{}\nR does not have a meaningfully useful graphical user interface (GUI).\nAdditional languages are becoming increasingly popular for bioinformatics and biological data science, such as Python, Julia, and Rust.",
    "crumbs": [
      "Home",
      "Introduction",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>About R</span>"
    ]
  },
  {
    "objectID": "intro.html#r-license-and-the-open-source-ideal",
    "href": "intro.html#r-license-and-the-open-source-ideal",
    "title": "\n1  About R\n",
    "section": "\n1.4 R License and the Open Source Ideal",
    "text": "1.4 R License and the Open Source Ideal\nR is free (yes, totally free!) and distributed under GNU license. In particular, this license allows one to:\n\nDownload the source code\nModify the source code to your heart’s content\nDistribute the modified source code and even charge money for it, but you must distribute the modified source code under the original GNU license.\n\nThis license means that R will always be available, will always be open source, and can grow organically without constraint.",
    "crumbs": [
      "Home",
      "Introduction",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>About R</span>"
    ]
  },
  {
    "objectID": "intro.html#working-with-r",
    "href": "intro.html#working-with-r",
    "title": "\n1  About R\n",
    "section": "\n1.5 Working with R",
    "text": "1.5 Working with R\nR is a programming language, and as such, it requires you to write code to perform tasks. This can be intimidating for beginners, but it is also what makes R so powerful. In R, you can write scripts to automate tasks, create functions to encapsulate complex operations, and use packages to extend the functionality of R.\nR can be used interactively or as a scripting language. In interactive mode, you can enter commands directly into the R console and see the results immediately. In scripting mode, you can write a series of commands in a script file and then execute the entire script at once. This allows you to save your work, reuse code, and share your analysis with others.\nIn the next section, we will discuss how to set up RStudio, an integrated development environment (IDE) for R that makes it easier to write and execute R code. However, you can use R without RStudio if you prefer to work in the R console or another IDE. RStudio is not required to use R, but it does provide a more user-friendly interface and several useful features that can enhance your R programming experience.",
    "crumbs": [
      "Home",
      "Introduction",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>About R</span>"
    ]
  },
  {
    "objectID": "intro_to_rstudio.html",
    "href": "intro_to_rstudio.html",
    "title": "2  RStudio",
    "section": "",
    "text": "2.1 Getting started with RStudio\nRStudio is an integrated development environment (IDE) for R. It provides a graphical user interface (GUI) for R, making it easier to write and execute R code. RStudio also provides several other useful features, including a built-in console, syntax-highlighting editor, and tools for plotting, history, debugging, workspace management, and workspace viewing. RStudio is available in both free and commercial editions; the commercial edition provides some additional features, including support for multiple sessions and enhanced debugging.\nTo get started with RStudio, you first need to install both R and RStudio on your computer. Follow these steps:",
    "crumbs": [
      "Home",
      "Introduction",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>RStudio</span>"
    ]
  },
  {
    "objectID": "intro_to_rstudio.html#getting-started-with-rstudio",
    "href": "intro_to_rstudio.html#getting-started-with-rstudio",
    "title": "2  RStudio",
    "section": "",
    "text": "Download and install R from the official R website.\nDownload and install RStudio from the official RStudio website.\nLaunch RStudio. You should see the RStudio interface with four panels.\n\n\n\n\n\n\n\nR versions\n\n\n\nRStudio works with all versions of R, but it is recommended to use the latest version of R to take advantage of the latest features and improvements. You can check your R version by running version (no parentheses)in the R console.\nYou can check the latest version of R on the R-project website.",
    "crumbs": [
      "Home",
      "Introduction",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>RStudio</span>"
    ]
  },
  {
    "objectID": "intro_to_rstudio.html#the-rstudio-interface",
    "href": "intro_to_rstudio.html#the-rstudio-interface",
    "title": "2  RStudio",
    "section": "2.2 The RStudio Interface",
    "text": "2.2 The RStudio Interface\nRStudio’s interface consists of four panels (see Figure 2.1):\n\n\nConsole\n\nThis panel displays the R console, where you can enter and execute R commands directly. The console also shows the output of your code, error messages, and other information.\n\n\n\nSource\n\nThis panel is where you write and edit your R scripts. You can create new scripts, open existing ones, and run your code from this panel.\n\n\n\nEnvironment\n\nThis panel displays your current workspace, including all variables, data objects, and functions that you have created or loaded in your R session.\n\n\n\nPlots, Packages, Help, and Viewer\n\nThese panels display plots, installed packages, help files, and web content, respectively.\n\n\n\n\n\n\n\n\n\nFigure 2.1: The RStudio interface. In this layout, the source pane is in the upper left, the console is in the lower left, the environment panel is in the top right and the viewer/help/files panel is in the bottom right.\n\n\n\n\n\n\n\n\n\nDo I need to use RStudio?\n\n\n\nNo. You can use R without RStudio. However, RStudio makes it easier to write and execute R code, and it provides several useful features that are not available in the basic R console. Note that the only part of RStudio that is actually interacting with R directly is the console. The other panels are simply providing a GUI that enhances the user experience.\n\n\n\n\n\n\n\n\nCustomizing the RStudio Interface\n\n\n\nYou can customize the layout of RStudio to suit your preferences. To do so, go to Tools &gt; Global Options &gt; Appearance. Here, you can change the theme, font size, and panel layout. You can also resize the panels as needed to gain screen real estate (see Figure 2.2).\n\n\n\n\n\n\n\n\nFigure 2.2: Dealing with limited screen real estate can be a challenge, particularly when you want to open another window to, for example, view a web page. You can resize the panes by sliding the center divider (red arrows) or by clicking on the minimize/maximize buttons (see blue arrow).\n\n\n\nIn summary, R and RStudio are powerful tools for genomics data analysis. By understanding the advantages of using R and RStudio and familiarizing yourself with the RStudio interface, you can efficiently analyze and visualize your data. In the following chapters, we will delve deeper into the functionality of R, Bioconductor, and various statistical methods to help you gain a comprehensive understanding of genomics data analysis.",
    "crumbs": [
      "Home",
      "Introduction",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>RStudio</span>"
    ]
  },
  {
    "objectID": "intro_to_rstudio.html#alternatives-to-rstudio",
    "href": "intro_to_rstudio.html#alternatives-to-rstudio",
    "title": "2  RStudio",
    "section": "2.3 Alternatives to RStudio",
    "text": "2.3 Alternatives to RStudio\nWhile RStudio is a popular choice for R development, there are several alternatives you can consider:\n\nJupyter Notebooks: Jupyter Notebooks provide an interactive environment for writing and executing R code, along with rich text support for documentation. You can use the IRKernel to run R code in Jupyter.\n\n\n\n\nJupyter Notebook interface. This is an interactive environment for writing and executing R code, along with rich text support for documentation.\n\n\n\nVisual Studio Code: With the R extension for Visual Studio Code, you can write and execute R code in a lightweight editor. This setup provides features like syntax highlighting, code completion, and integrated terminal support.\n\n\n\n\nVisual Studio Code (VSCode) with the R extension. This is a lightweight alternative to RStudio that provides syntax highlighting, code completion, and integrated terminal support.\n\n\n\nPositron Workbench: This is a commercial IDE that supports R and Python. It provides a similar interface to RStudio but with additional features for data science workflows, including support for multiple languages and cloud integration.\n\n\n\n\nPositron Workbench interface. This IDE supports R and Python, providing a similar interface to RStudio with additional features for data science workflows.\n\n\n\nCommand Line R: For those who prefer a minimalistic approach, you can use R directly from the command line. This method lacks the GUI features of RStudio but can be efficient for quick tasks, scripting, automation, or when working on remote servers.\n\nEach of these alternatives has its own strengths and weaknesses, so you may want to try a few to see which one best fits your workflow. All are available for free, and you can install them alongside RStudio if you wish to use multiple environments. Each can be installed in Windows, Mac, and Linux.",
    "crumbs": [
      "Home",
      "Introduction",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>RStudio</span>"
    ]
  },
  {
    "objectID": "r_intro_mechanics.html",
    "href": "r_intro_mechanics.html",
    "title": "\n3  R mechanics\n",
    "section": "",
    "text": "3.1 Starting R\nWe’ve installed R and RStudio. Now, let’s start R and get going. How to start R depends a bit on the operating system (Mac, Windows, Linux) and interface. In this course, we will largely be using an Integrated Development Environment (IDE) called RStudio, but there is nothing to prohibit using R at the command line or in some other interface (and there are a few).",
    "crumbs": [
      "Home",
      "Introduction",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>R mechanics</span>"
    ]
  },
  {
    "objectID": "r_intro_mechanics.html#rstudio-a-quick-tour",
    "href": "r_intro_mechanics.html#rstudio-a-quick-tour",
    "title": "\n3  R mechanics\n",
    "section": "\n3.2 RStudio: A Quick Tour",
    "text": "3.2 RStudio: A Quick Tour\nThe RStudio interface has multiple panes. All of these panes are simply for convenience except the “Console” panel, typically in the lower left corner (by default). The console pane contains the running R interface. If you choose to run R outside RStudio, the interaction will be identical to working in the console pane. This is useful to keep in mind as some environments, such as a computer cluster, encourage using R without RStudio.\n\nPanes\nOptions\nHelp\nEnvironment, History, and Files",
    "crumbs": [
      "Home",
      "Introduction",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>R mechanics</span>"
    ]
  },
  {
    "objectID": "r_intro_mechanics.html#interacting-with-r",
    "href": "r_intro_mechanics.html#interacting-with-r",
    "title": "\n3  R mechanics\n",
    "section": "\n3.3 Interacting with R",
    "text": "3.3 Interacting with R\nThe only meaningful way of interacting with R is by typing into the R console. At the most basic level, anything that we type at the command line will fall into one of two categories:\n\n\nAssignments\n\nx = 1\ny &lt;- 2\n\n\n\nExpressions\n\n1 + pi + sin(42)\n\n[1] 3.225071\n\n\n\n\nThe assignment type is obvious because either the The &lt;- or = are used. Note that when we type expressions, R will return a result. In this case, the result of R evaluating 1 + pi + sin(42) is 3.2250711.\nThe standard R prompt is a “&gt;” sign. When present, R is waiting for the next expression or assignment. If a line is not a complete R command, R will continue the next line with a “+”. For example, typing the fillowing with a “Return” after the second “+” will result in R giving back a “+” on the next line, a prompt to keep typing.\n\n1 + pi +\nsin(3.7)\n\n[1] 3.611757\n\n\nR can be used as a glorified calculator by using R expressions. Mathematical operations include:\n\nAddition: +\n\nSubtraction: -\n\nMultiplication: *\n\nDivision: /\n\nExponentiation: ^\n\nModulo: %%\n\n\nThe ^ operator raises the number to its left to the power of the number to its right: for example 3^2 is 9. The modulo returns the remainder of the division of the number to the left by the number on its right, for example 5 modulo 3 or 5 %% 3 is 2.\n\n3.3.1 Expressions\n\n5 + 2\n28 %% 3\n3^2\n5 + 4 * 4 + 4 ^ 4 / 10\n\nNote that R follows order-of-operations and groupings based on parentheses.\n\n5 + 4 / 9\n(5 + 4) / 9\n\n\n3.3.2 Assignment\nWhile using R as a calculator is interesting, to do useful and interesting things, we need to assign values to objects. To create objects, we need to give it a name followed by the assignment operator &lt;- (or, entirely equivalently, =) and the value we want to give it:\n\nweight_kg &lt;- 55 \n\n&lt;- is the assignment operator. Assigns values on the right to objects on the left, it is like an arrow that points from the value to the object. Using an = is equivalent (in nearly all cases). Learn to use &lt;- as it is good programming practice.\nObjects can be given any name such as x, current_temperature, or subject_id (see below). You want your object names to be explicit and not too long. They cannot start with a number (2x is not valid but x2 is). R is case sensitive (e.g., weight_kg is different from Weight_kg). There are some names that cannot be used because they represent the names of fundamental functions in R (e.g., if, else, for, see here for a complete list). In general, even if it’s allowed, it’s best to not use other function names, which we’ll get into shortly (e.g., c, T, mean, data, df, weights). When in doubt, check the help to see if the name is already in use. It’s also best to avoid dots (.) within a variable name as in my.dataset. It is also recommended to use nouns for variable names, and verbs for function names.\nWhen assigning a value to an object, R does not print anything. You can force to print the value by typing the name:\n\nweight_kg\n\n[1] 55\n\n\nNow that R has weight_kg in memory, which R refers to as the “global environment”, we can do arithmetic with it. For instance, we may want to convert this weight in pounds (weight in pounds is 2.2 times the weight in kg).\n\n2.2 * weight_kg\n\n[1] 121\n\n\nWe can also change a variable’s value by assigning it a new one:\n\nweight_kg &lt;- 57.5\n2.2 * weight_kg\n\n[1] 126.5\n\n\nThis means that assigning a value to one variable does not change the values of other variables. For example, let’s store the animal’s weight in pounds in a variable.\n\nweight_lb &lt;- 2.2 * weight_kg\n\nand then change weight_kg to 100.\n\nweight_kg &lt;- 100\n\nWhat do you think is the current content of the object weight_lb, 126.5 or 220?\nYou can see what objects (variables) are stored by viewing the Environment tab in Rstudio. You can also use the ls() function. You can remove objects (variables) with the rm() function. You can do this one at a time or remove several objects at once. You can also use the little broom button in your environment pane to remove everything from your environment.\n\nls()\nrm(weight_lb, weight_kg)\nls()\n\nWhat happens when you type the following, now?\n\nweight_lb # oops! you should get an error because weight_lb no longer exists!",
    "crumbs": [
      "Home",
      "Introduction",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>R mechanics</span>"
    ]
  },
  {
    "objectID": "r_intro_mechanics.html#rules-for-names-in-r",
    "href": "r_intro_mechanics.html#rules-for-names-in-r",
    "title": "\n3  R mechanics\n",
    "section": "\n3.4 Rules for Names in R",
    "text": "3.4 Rules for Names in R\nR allows users to assign names to objects such as variables, functions, and even dimensions of data. However, these names must follow a few rules.\n\nNames may contain any combination of letters, numbers, underscore, and “.”\nNames may not start with numbers, underscore.\nR names are case-sensitive.\n\nExamples of valid R names include:\npi\nx\ncamelCaps\nmy_stuff\nMY_Stuff\nthis.is.the.name.of.the.man\nABC123\nabc1234asdf\n.hi",
    "crumbs": [
      "Home",
      "Introduction",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>R mechanics</span>"
    ]
  },
  {
    "objectID": "r_intro_mechanics.html#resources-for-getting-help",
    "href": "r_intro_mechanics.html#resources-for-getting-help",
    "title": "\n3  R mechanics\n",
    "section": "\n3.5 Resources for Getting Help",
    "text": "3.5 Resources for Getting Help\nThere is extensive built-in help and documentation within R. A separate page contains a collection of additional resources.\nIf the name of the function or object on which help is sought is known, the following approaches with the name of the function or object will be helpful. For a concrete example, examine the help for the print method.\n\nhelp(print)\nhelp('print')\n?print\n\nIf the name of the function or object on which help is sought is not known, the following from within R will be helpful.\n\nhelp.search('microarray')\nRSiteSearch('microarray')\napropos('histogram')\n\nThere are also tons of online resources that Google will include in searches if online searching feels more appropriate.\nI strongly recommend using help(\"newfunction\"\") for all functions that are new or unfamiliar to you.\nThere are also many open and free resources and reference guides for R.\n\n\nQuick-R: a quick online reference for data input, basic statistics and plots\nR reference card PDF by Tom Short\nRstudio cheatsheets",
    "crumbs": [
      "Home",
      "Introduction",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>R mechanics</span>"
    ]
  },
  {
    "objectID": "r_basics.html",
    "href": "r_basics.html",
    "title": "\n4  Up and Running with R\n",
    "section": "",
    "text": "4.1 The R User Interface\nIn this chapter, we’re going to get an introduction to the R language, so we can dive right into programming. We’re going to create a pair of virtual dice that can generate random numbers. No need to worry if you’re new to programming. We’ll return to many of the concepts here in more detail later.\nTo simulate a pair of dice, we need to break down each die into its essential features. A die can only show one of six numbers: 1, 2, 3, 4, 5, and 6. We can capture the die’s essential characteristics by saving these numbers as a group of values in the computer. Let’s save these numbers first and then figure out a way to “roll” our virtual die.\nThe RStudio interface is simple. You type R code into the bottom line of the RStudio console pane and then click Enter to run it. The code you type is called a command, because it will command your computer to do something for you. The line you type it into is called the command line.\nWhen you type a command at the prompt and hit Enter, your computer executes the command and shows you the results. Then RStudio displays a fresh prompt for your next command. For example, if you type 1 + 1 and hit Enter, RStudio will display:\nYou’ll notice that a [1] appears next to your result. R is just letting you know that this line begins with the first value in your result. Some commands return more than one value, and their results may fill up multiple lines. For example, the command 100:130 returns 31 values; it creates a sequence of integers from 100 to 130. Notice that new bracketed numbers appear at the start of the second and third lines of output. These numbers just mean that the second line begins with the 14th value in the result, and the third line begins with the 25th value. You can mostly ignore the numbers that appear in brackets:\nIf you type an incomplete command and press Enter, R will display a + prompt, which means R is waiting for you to type the rest of your command. Either finish the command or hit Escape to start over:\nIf you type a command that R doesn’t recognize, R will return an error message. If you ever see an error message, don’t panic. R is just telling you that your computer couldn’t understand or do what you asked it to do. You can then try a different command at the next prompt:\nOnce you get the hang of the command line, you can easily do anything in R that you would do with a calculator. For example, you could do some basic arithmetic:\n2 * 3   \n\n[1] 6\n\n4 - 1   \n\n[1] 3\n\n# this obeys order-of-operations\n6 / (4 - 1)   \n\n[1] 2",
    "crumbs": [
      "Home",
      "Introduction",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Up and Running with R</span>"
    ]
  },
  {
    "objectID": "r_basics.html#the-r-user-interface",
    "href": "r_basics.html#the-r-user-interface",
    "title": "\n4  Up and Running with R\n",
    "section": "",
    "text": "Figure 4.1: Your computer does your bidding when you type R commands at the prompt in the bottom line of the console pane. Don’t forget to hit the Enter key. When you first open RStudio, the console appears in the pane on your left, but you can change this with File &gt; Tools &gt; Global Options in the menu bar.\n\n\n\n&gt; 1 + 1\n[1] 2\n&gt;\n\n&gt; 100:130\n [1] 100 101 102 103 104 105 106 107 108 109 110 111 112\n[14] 113 114 115 116 117 118 119 120 121 122 123 124 125\n[25] 126 127 128 129 130\n\n\n\n\n\n\nTip\n\n\n\nThe colon operator (:) returns every integer between two integers. It is an easy way to create a sequence of numbers.\n\n\n\n\n\n\n\n\nWhen do we compile?\n\n\n\nIn some languages, like C, Java, and FORTRAN, you have to compile your human-readable code into machine-readable code (often 1s and 0s) before you can run it. If you’ve programmed in such a language before, you may wonder whether you have to compile your R code before you can use it. The answer is no. R is a dynamic programming language, which means R automatically interprets your code as you run it.\n\n\n\n&gt; 5 -\n+\n+ 1\n[1] 4\n\n&gt; 3 % 5\nError: unexpected input in \"3 % 5\"\n&gt;\n\n\n\n\n\n\nTip\n\n\n\nWhenever you get an error message in R, consider googling the error message. You’ll often find that someone else has had the same problem and has posted a solution online. Simply cutting-and-pasting the error message into a search engine will often work\n\n\n\n\n\n\n\n\n\n\nTip\n\n\n\nR treats the hashtag character, #, in a special way; R will not run anything that follows a hashtag on a line. This makes hashtags very useful for adding comments and annotations to your code. Humans will be able to read the comments, but your computer will pass over them. The hashtag is known as the commenting symbol in R.\n\n\n\n\n\n\n\n\nCancelling commands\n\n\n\nSome R commands may take a long time to run. You can cancel a command once it has begun by pressing ctrl + c or by clicking the “stop sign” if it is available in Rstudio. Note that it may also take R a long time to cancel the command.\n\n\n\n4.1.1 An exercise\nThat’s the basic interface for executing R code in RStudio. Think you have it? If so, try doing these simple tasks. If you execute everything correctly, you should end up with the same number that you started with:\n\nChoose any number and add 2 to it.\nMultiply the result by 3.\nSubtract 6 from the answer.\nDivide what you get by 3.\n\n\n10 + 2\n\n[1] 12\n\n12 * 3\n\n[1] 36\n\n36 - 6\n\n[1] 30\n\n30 / 3\n\n[1] 10",
    "crumbs": [
      "Home",
      "Introduction",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Up and Running with R</span>"
    ]
  },
  {
    "objectID": "r_basics.html#objects",
    "href": "r_basics.html#objects",
    "title": "\n4  Up and Running with R\n",
    "section": "\n4.2 Objects",
    "text": "4.2 Objects\nNow that you know how to use R, let’s use it to make a virtual die. The : operator from a couple of pages ago gives you a nice way to create a group of numbers from one to six. The : operator returns its results as a vector (we are going to work with vectors in more detail), a one-dimensional set of numbers:\n1:6\n## 1 2 3 4 5 6\nThat’s all there is to how a virtual die looks! But you are not done yet. Running 1:6 generated a vector of numbers for you to see, but it didn’t save that vector anywhere for later use. If we want to use those numbers again, we’ll have to ask your computer to save them somewhere. You can do that by creating an R object.\nR lets you save data by storing it inside an R object. What is an object? Just a name that you can use to call up stored data. For example, you can save data into an object like a or b. Wherever R encounters the object, it will replace it with the data saved inside, like so:\n\na &lt;- 1\na\n\n[1] 1\n\n\n\na + 2\n\n[1] 3\n\n\n\n\n\n\n\n\nWhat just happened?\n\n\n\n\nTo create an R object, choose a name and then use the less-than symbol, &lt;, followed by a minus sign, -, to save data into it. This combination looks like an arrow, &lt;-. R will make an object, give it your name, and store in it whatever follows the arrow. So a &lt;- 1 stores 1 in an object named a.\nWhen you ask R what’s in a, R tells you on the next line.\nYou can use your object in new R commands, too. Since a previously stored the value of 1, you’re now adding 1 to 2.\n\n\n\n\n\n\n\n\n\nAssignment vs expressions\n\n\n\nEverything that you type into the R console can be assigned to one of two categories:\n\nAssignments\nExpressions\n\nAn expression is a command that tells R to do something. For example, 1 + 2 is an expression that tells R to add 1 and 2. When you type an expression into the R console, R will evaluate the expression and return the result. For example, if you type 1 + 2 into the R console, R will return 3. Expressions can have “side effects” but they don’t explicitly result in anything being added to R memory.\n\n5 + 2\n\n[1] 7\n\n28 %% 3\n\n[1] 1\n\n3^2\n\n[1] 9\n\n5 + 4 * 4 + 4 ^ 4 / 10\n\n[1] 46.6\n\n\nWhile using R as a calculator is interesting, to do useful and interesting things, we need to assign values to objects. To create objects, we need to give it a name followed by the assignment operator &lt;- (or, entirely equivalently, =) and the value we want to give it:\n\nweight_kg &lt;- 55\n\n\n\nSo, for another example, the following code would create an object named die that contains the numbers one through six. To see what is stored in an object, just type the object’s name by itself:\n\ndie &lt;- 1:6\ndie\n\n[1] 1 2 3 4 5 6\n\n\nWhen you create an object, the object will appear in the environment pane of RStudio, as shown in Figure 4.2. This pane will show you all of the objects you’ve created since opening RStudio.\n\n\n\n\n\nFigure 4.2: Assignment creates an object in the environment pane.\n\n\nYou can name an object in R almost anything you want, but there are a few rules. First, a name cannot start with a number. Second, a name cannot use some special symbols, like ^, !, $, @, +, -, /, or *:\n\n\nGood names\nNames that cause errors\n\n\n\na\n1trial\n\n\nb\n$\n\n\nFOO\n^mean\n\n\nmy_var\n2nd\n\n\n.day\n!bad\n\n\n\n\n\n\n\n\n\nCapitalization matters\n\n\n\nR is case-sensitive, so name and Name will refer to different objects:\n&gt; Name = 0\n&gt; Name + 1\n[1] 1\n&gt; name + 1\nError: object 'name' not found\nThe error above is a common one!\n\n\nFinally, R will overwrite any previous information stored in an object without asking you for permission. So, it is a good idea to not use names that are already taken:\n\nmy_number &lt;- 1\nmy_number \n\n[1] 1\n\n\n\nmy_number &lt;- 999\nmy_number\n\n[1] 999\n\n\nYou can see which object names you have already used with the function ls:\nls()\nYour environment will contain different names than mine, because you have probably created different objects.\nYou can also see which names you have used by examining RStudio’s environment pane.\nWe now have a virtual die that is stored in the computer’s memory and which has a name that we can use to refer to it. You can access it whenever you like by typing the word die.\nSo what can you do with this die? Quite a lot. R will replace an object with its contents whenever the object’s name appears in a command. So, for example, you can do all sorts of math with the die. Math isn’t so helpful for rolling dice, but manipulating sets of numbers will be your stock and trade as a data scientist. So let’s take a look at how to do that:\n\ndie - 1\n\n[1] 0 1 2 3 4 5\n\ndie / 2\n\n[1] 0.5 1.0 1.5 2.0 2.5 3.0\n\ndie * die\n\n[1]  1  4  9 16 25 36\n\n\nR uses element-wise execution when working with a vector like die. When you manipulate a set of numbers, R will apply the same operation to each element in the set. So for example, when you run die - 1, R subtracts one from each element of die.\nWhen you use two or more vectors in an operation, R will line up the vectors and perform a sequence of individual operations. For example, when you run die * die, R lines up the two die vectors and then multiplies the first element of vector 1 by the first element of vector 2. R then multiplies the second element of vector 1 by the second element of vector 2, and so on, until every element has been multiplied. The result will be a new vector the same length as the first two {Figure 4.3}.\n\n\n\n\n\nFigure 4.3: “When R performs element-wise execution, it matches up vectors and then manipulates each pair of elements independently.”\n\n\nIf you give R two vectors of unequal lengths, R will repeat the shorter vector until it is as long as the longer vector, and then do the math, as shown in Figure 4.4. This isn’t a permanent change–the shorter vector will be its original size after R does the math. If the length of the short vector does not divide evenly into the length of the long vector, R will return a warning message. This behavior is known as vector recycling, and it helps R do element-wise operations:\n\n1:2\n\n[1] 1 2\n\n1:4\n\n[1] 1 2 3 4\n\ndie\n\n[1] 1 2 3 4 5 6\n\ndie + 1:2\n\n[1] 2 4 4 6 6 8\n\ndie + 1:4\n\nWarning in die + 1:4: longer object length is not a multiple of shorter object\nlength\n\n\n[1] 2 4 6 8 6 8\n\n\n\n\n\n\n\nFigure 4.4: “R will repeat a short vector to do element-wise operations with two vectors of uneven lengths.”\n\n\nElement-wise operations are a very useful feature in R because they manipulate groups of values in an orderly way. When you start working with data sets, element-wise operations will ensure that values from one observation or case are only paired with values from the same observation or case. Element-wise operations also make it easier to write your own programs and functions in R.\n\n\n\n\n\n\nElement-wise operations are not matrix operations\n\n\n\nIt is important to know that operations with vectors are not the same that you might expect if you are expecting R to perform “matrix” operations. R can do inner multiplication with the %*% operator and outer multiplication with the %o% operator:\n# Inner product (1*1 + 2*2 + 3*3 + 4*4 + 5*5 + 6*6)\ndie %*% die\n# Outer product\ndie %o% die\n\n\nNow that you can do math with your die object, let’s look at how you could “roll” it. Rolling your die will require something more sophisticated than basic arithmetic; you’ll need to randomly select one of the die’s values. And for that, you will need a function.",
    "crumbs": [
      "Home",
      "Introduction",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Up and Running with R</span>"
    ]
  },
  {
    "objectID": "r_basics.html#functions",
    "href": "r_basics.html#functions",
    "title": "\n4  Up and Running with R\n",
    "section": "\n4.3 Functions",
    "text": "4.3 Functions\nR has many functions and puts them all at our disposal. We can use functions to do simple and sophisticated tasks. For example, we can round a number with the round function, or calculate its factorial with the factorial function. Using a function is pretty simple. Just write the name of the function and then the data you want the function to operate on in parentheses:\n\nround(3.1415)\n\n[1] 3\n\nfactorial(3)\n\n[1] 6\n\n\nThe data that you pass into the function is called the function’s argument. The argument can be raw data, an R object, or even the results of another R function. In this last case, R will work from the innermost function to the outermost Figure 4.5.\n\nmean(1:6)\n\n[1] 3.5\n\nmean(die)\n\n[1] 3.5\n\nround(mean(die))\n\n[1] 4\n\n\n\n\n\n\n\nFigure 4.5: “When you link functions together, R will resolve them from the innermost operation to the outermost. Here R first looks up die, then calculates the mean of one through six, then rounds the mean.”\n\n\nReturning to our die, we can use the sample function to randomly select one of the die’s values; in other words, the sample function can simulate rolling the die.\nThe sample function takes two arguments: a vector named x and a number named size. sample will return size elements from the vector:\n\nsample(x = 1:4, size = 2)\n\n[1] 3 4\n\n\nTo roll your die and get a number back, set x to die and sample one element from it. You’ll get a new (maybe different) number each time you roll it:\n\nsample(x = die, size = 1)\n\n[1] 2\n\nsample(x = die, size = 1)\n\n[1] 2\n\nsample(x = die, size = 1)\n\n[1] 5\n\n\nMany R functions take multiple arguments that help them do their job. You can give a function as many arguments as you like as long as you separate each argument with a comma.\nYou may have noticed that I set die and 1 equal to the names of the arguments in sample, x and size. Every argument in every R function has a name. You can specify which data should be assigned to which argument by setting a name equal to data, as in the preceding code. This becomes important as you begin to pass multiple arguments to the same function; names help you avoid passing the wrong data to the wrong argument. However, using names is optional. You will notice that R users do not often use the name of the first argument in a function. So you might see the previous code written as:\n\nsample(die, size = 1)\n\n[1] 5\n\n\nOften, the name of the first argument is not very descriptive, and it is usually obvious what the first piece of data refers to anyways.\nBut how do you know which argument names to use? If you try to use a name that a function does not expect, you will likely get an error:\nround(3.1415, corners = 2)\n## Error in round(3.1415, corners = 2) : unused argument(s) (corners = 2)\nIf you’re not sure which names to use with a function, you can look up the function’s arguments with args. To do this, place the name of the function in the parentheses behind args. For example, you can see that the round function takes two arguments, one named x and one named digits:\n\nargs(round)\n\nfunction (x, digits = 0, ...) \nNULL\n\n\nDid you notice that args shows that the digits argument of round is already set to 0? Frequently, an R function will take optional arguments like digits. These arguments are considered optional because they come with a default value. You can pass a new value to an optional argument if you want, and R will use the default value if you do not. For example, round will round your number to 0 digits past the decimal point by default. To override the default, supply your own value for digits:\n\nround(3.1415)\n\n[1] 3\n\nround(3.1415, digits = 2)\n\n[1] 3.14\n\n# pi happens to be a built-in value in R\npi\n\n[1] 3.141593\n\nround(pi)\n\n[1] 3\n\n\nYou should write out the names of each argument after the first one or two when you call a function with multiple arguments. Why? First, this will help you and others understand your code. It is usually obvious which argument your first input refers to (and sometimes the second input as well). However, you’d need a large memory to remember the third and fourth arguments of every R function. Second, and more importantly, writing out argument names prevents errors.\nIf you do not write out the names of your arguments, R will match your values to the arguments in your function by order. For example, in the following code, the first value, die, will be matched to the first argument of sample, which is named x. The next value, 1, will be matched to the next argument, size:\n\nsample(die, 1)\n\n[1] 6\n\n\nAs you provide more arguments, it becomes more likely that your order and R’s order may not align. As a result, values may get passed to the wrong argument. Argument names prevent this. R will always match a value to its argument name, no matter where it appears in the order of arguments:\n\nsample(size = 1, x = die)\n\n[1] 3\n\n\n\n4.3.1 Sample with Replacement\nIf you set size = 2, you can almost simulate a pair of dice. Before we run that code, think for a minute why that might be the case. sample will return two numbers, one for each die:\n\nsample(die, size = 2)\n\n[1] 1 6\n\n\nI said this “almost” works because this method does something funny. If you use it many times, you’ll notice that the second die never has the same value as the first die, which means you’ll never roll something like a pair of threes or snake eyes. What is going on?\nBy default, sample builds a sample without replacement. To see what this means, imagine that sample places all of the values of die in a jar or urn. Then imagine that sample reaches into the jar and pulls out values one by one to build its sample. Once a value has been drawn from the jar, sample sets it aside. The value doesn’t go back into the jar, so it cannot be drawn again. So if sample selects a six on its first draw, it will not be able to select a six on the second draw; six is no longer in the jar to be selected. Although sample creates its sample electronically, it follows this seemingly physical behavior.\nOne side effect of this behavior is that each draw depends on the draws that come before it. In the real world, however, when you roll a pair of dice, each die is independent of the other. If the first die comes up six, it does not prevent the second die from coming up six. In fact, it doesn’t influence the second die in any way whatsoever. You can recreate this behavior in sample by adding the argument replace = TRUE:\n\nsample(die, size = 2, replace = TRUE)\n\n[1] 2 4\n\n\nThe argument replace = TRUE causes sample to sample with replacement. Our jar example provides a good way to understand the difference between sampling with replacement and without. When sample uses replacement, it draws a value from the jar and records the value. Then it puts the value back into the jar. In other words, sample replaces each value after each draw. As a result, sample may select the same value on the second draw. Each value has a chance of being selected each time. It is as if every draw were the first draw.\nSampling with replacement is an easy way to create independent random samples. Each value in your sample will be a sample of size one that is independent of the other values. This is the correct way to simulate a pair of dice:\n\nsample(die, size = 2, replace = TRUE)\n\n[1] 1 5\n\n\nCongratulate yourself; you’ve just run your first simulation in R! You now have a method for simulating the result of rolling a pair of dice. If you want to add up the dice, you can feed your result straight into the sum function:\n\ndice &lt;- sample(die, size = 2, replace = TRUE)\ndice\n\n[1] 6 2\n\nsum(dice)\n\n[1] 8\n\n\nWhat would happen if you call dice multiple times? Would R generate a new pair of dice values each time? Let’s give it a try:\n\ndice\n\n[1] 6 2\n\ndice\n\n[1] 6 2\n\ndice\n\n[1] 6 2\n\n\nThe name dice refers to a vector of two numbers. Calling more than once does not change the favlue. Each time you call dice, R will show you the result of that one time you called sample and saved the output to dice. R won’t rerun sample(die, 2, replace = TRUE) to create a new roll of the dice. Once you save a set of results to an R object, those results do not change.\nHowever, it would be convenient to have an object that can re-roll the dice whenever you call it. You can make such an object by writing your own R function.",
    "crumbs": [
      "Home",
      "Introduction",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Up and Running with R</span>"
    ]
  },
  {
    "objectID": "r_basics.html#write-functions",
    "href": "r_basics.html#write-functions",
    "title": "\n4  Up and Running with R\n",
    "section": "\n4.4 Writing Your Own Functions",
    "text": "4.4 Writing Your Own Functions\nTo recap, you already have working R code that simulates rolling a pair of dice:\n\ndie &lt;- 1:6\ndice &lt;- sample(die, size = 2, replace = TRUE)\nsum(dice)\n\n[1] 11\n\n\nYou can retype this code into the console anytime you want to re-roll your dice. However, this is an awkward way to work with the code. It would be easier to use your code if you wrapped it into its own function, which is exactly what we’ll do now. We’re going to write a function named roll that you can use to roll your virtual dice. When you’re finished, the function will work like this: each time you call roll(), R will return the sum of rolling two dice:\nroll()\n## 8 \n\nroll()\n## 3\n\nroll()\n## 7\nFunctions may seem mysterious or fancy, but they are just another type of R object. Instead of containing data, they contain code. This code is stored in a special format that makes it easy to reuse the code in new situations. You can write your own functions by recreating this format.\n\n4.4.1 The Function Constructor\nEvery function in R has three basic parts: a name, a body of code, and a set of arguments. To make your own function, you need to replicate these parts and store them in an R object, which you can do with the function function. To do this, call function() and follow it with a pair of braces, {}:\n\nmy_function &lt;- function() {}\n\nThis function, as written, doesn’t do anything (yet). However, it is a valid function. You can call it by typing its name followed by an open and closed parenthesis:\n\nmy_function()\n\nNULL\n\n\nfunction will build a function out of whatever R code you place between the braces. For example, you can turn your dice code into a function by calling:\n\nroll &lt;- function() {\n  die &lt;- 1:6\n  dice &lt;- sample(die, size = 2, replace = TRUE)\n  sum(dice)\n}\n\n\n\n\n\n\n\nIndentation and readability\n\n\n\nNotice each line of code between the braces is indented. This makes the code easier to read but has no impact on how the code runs. R ignores spaces and line breaks and executes one complete expression at a time. Note that in other languages like python, spacing is extremely important and part of the language.\n\n\nJust hit the Enter key between each line after the first brace, {. R will wait for you to type the last brace, }, before it responds.\nDon’t forget to save the output of function to an R object. This object will become your new function. To use it, write the object’s name followed by an open and closed parenthesis:\n\nroll()\n\n[1] 7\n\n\nYou can think of the parentheses as the “trigger” that causes R to run the function. If you type in a function’s name without the parentheses, R will show you the code that is stored inside the function. If you type in the name with the parentheses, R will run that code:\n\nroll\n\nfunction () \n{\n    die &lt;- 1:6\n    dice &lt;- sample(die, size = 2, replace = TRUE)\n    sum(dice)\n}\n\nroll()\n\n[1] 3\n\n\nThe code that you place inside your function is known as the body of the function. When you run a function in R, R will execute all of the code in the body and then return the result of the last line of code. If the last line of code doesn’t return a value, neither will your function, so you want to ensure that your final line of code returns a value. One way to check this is to think about what would happen if you ran the body of code line by line in the command line. Would R display a result after the last line, or would it not?\nHere’s some code that would display a result:\ndice\n1 + 1\nsqrt(2)\nAnd here’s some code that would not:\ndice &lt;- sample(die, size = 2, replace = TRUE)\ntwo &lt;- 1 + 1\na &lt;- sqrt(2)\nAgain, this is just showing the distinction between expressions and assignments.",
    "crumbs": [
      "Home",
      "Introduction",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Up and Running with R</span>"
    ]
  },
  {
    "objectID": "r_basics.html#arguments",
    "href": "r_basics.html#arguments",
    "title": "\n4  Up and Running with R\n",
    "section": "\n4.5 Arguments",
    "text": "4.5 Arguments\nWhat if we removed one line of code from our function and changed the name die to bones (just a name–don’t think of it as important), like this?\n\nroll2 &lt;- function() {\n  dice &lt;- sample(bones, size = 2, replace = TRUE)\n  sum(dice)\n}\n\nNow I’ll get an error when I run the function. The function needs the object bones to do its job, but there is no object named bones to be found (you can check by typing ls() which will show you the names in the environment, or memory).\nroll2()\n## Error in sample(bones, size = 2, replace = TRUE) : \n##   object 'bones' not found\nYou can supply bones when you call roll2 if you make bones an argument of the function. To do this, put the name bones in the parentheses that follow function when you define roll2:\n\nroll2 &lt;- function(bones) {\n  dice &lt;- sample(bones, size = 2, replace = TRUE)\n  sum(dice)\n}\n\nNow roll2 will work as long as you supply bones when you call the function. You can take advantage of this to roll different types of dice each time you call roll2.\nRemember, we’re rolling pairs of dice:\n\nroll2(bones = 1:4)\n\n[1] 4\n\nroll2(bones = 1:6)\n\n[1] 6\n\nroll2(1:20)\n\n[1] 17\n\n\nNotice that roll2 will still give an error if you do not supply a value for the bones argument when you call roll2:\nroll2()\n## Error in sample(bones, size = 2, replace = TRUE) : \n##   argument \"bones\" is missing, with no default\nYou can prevent this error by giving the bones argument a default value. To do this, set bones equal to a value when you define roll2:\n\nroll2 &lt;- function(bones = 1:6) {\n  dice &lt;- sample(bones, size = 2, replace = TRUE)\n  sum(dice)\n}\n\nNow you can supply a new value for bones if you like, and roll2 will use the default if you do not:\n\nroll2()\n\n[1] 4\n\n\nYou can give your functions as many arguments as you like. Just list their names, separated by commas, in the parentheses that follow function. When the function is run, R will replace each argument name in the function body with the value that the user supplies for the argument. If the user does not supply a value, R will replace the argument name with the argument’s default value (if you defined one).\nTo summarize, function helps you construct your own R functions. You create a body of code for your function to run by writing code between the braces that follow function. You create arguments for your function to use by supplying their names in the parentheses that follow function. Finally, you give your function a name by saving its output to an R object, as shown in Figure 4.6.\nOnce you’ve created your function, R will treat it like every other function in R. Think about how useful this is. Have you ever tried to create a new Excel option and add it to Microsoft’s menu bar? Or a new slide animation and add it to Powerpoint’s options? When you work with a programming language, you can do these types of things. As you learn to program in R, you will be able to create new, customized, reproducible tools for yourself whenever you like.\n\n\n\n\n\nFigure 4.6: “Every function in R has the same parts, and you can use function to create these parts. Assign the result to a name, so you can call the function later.”",
    "crumbs": [
      "Home",
      "Introduction",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Up and Running with R</span>"
    ]
  },
  {
    "objectID": "r_basics.html#scripts",
    "href": "r_basics.html#scripts",
    "title": "\n4  Up and Running with R\n",
    "section": "\n4.6 Scripts",
    "text": "4.6 Scripts\nScripts are code that are saved for later reuse or editing. An R script is just a plain text file that you save R code in. You can open an R script in RStudio by going to File &gt; New File &gt; R script in the menu bar. RStudio will then open a fresh script above your console pane, as shown in Figure 4.7.\nI strongly encourage you to write and edit all of your R code in a script before you run it in the console. Why? This habit creates a reproducible record of your work. When you’re finished for the day, you can save your script and then use it to rerun your entire analysis the next day. Scripts are also very handy for editing and proofreading your code, and they make a nice copy of your work to share with others. To save a script, click the scripts pane, and then go to File &gt; Save As in the menu bar.\n\n\n\n\n\nFigure 4.7: “When you open an R Script (File &gt; New File &gt; R Script in the menu bar), RStudio creates a fourth pane (or puts a new tab in the existing pane) above the console where you can write and edit your code.”\n\n\nRStudio comes with many built-in features that make it easy to work with scripts. First, you can automatically execute a line of code in a script by clicking the Run button at the top of the editor panel.\nR will run whichever line of code your cursor is on. If you have a whole section highlighted, R will run the highlighted code. Alternatively, you can run the entire script by clicking the Source button. Don’t like clicking buttons? You can use Control + Return as a shortcut for the Run button. On Macs, that would be Command + Return.\nIf you’re not convinced about scripts, you soon will be. It becomes a pain to write multi-line code in the console’s single-line command line. Let’s avoid that headache and open your first script now before we move to the next chapter.\n\n\n\n\n\n\nTip\n\n\n\nExtract function\nRStudio comes with a tool that can help you build functions. To use it, highlight the lines of code in your R script that you want to turn into a function. Then click Code &gt; Extract Function in the menu bar. RStudio will ask you for a function name to use and then wrap your code in a function call. It will scan the code for undefined variables and use these as arguments.\nYou may want to double-check RStudio’s work. It assumes that your code is correct, so if it does something surprising, you may have a problem in your code.",
    "crumbs": [
      "Home",
      "Introduction",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Up and Running with R</span>"
    ]
  },
  {
    "objectID": "r_basics.html#summary",
    "href": "r_basics.html#summary",
    "title": "\n4  Up and Running with R\n",
    "section": "\n4.7 Summary",
    "text": "4.7 Summary\nWe’ve covered a lot of ground already. You now have a virtual die stored in your computer’s memory, as well as your own R function that rolls a pair of dice. You’ve also begun speaking the R language.\nThe two most important components of the R language are objects, which store data, and functions, which manipulate data. R also uses a host of operators like +, -, *, /, and &lt;- to do basic tasks. As a data scientist, you will use R objects to store data in your computer’s memory, and you will use functions to automate tasks and do complicated calculations.",
    "crumbs": [
      "Home",
      "Introduction",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Up and Running with R</span>"
    ]
  },
  {
    "objectID": "packages_and_dice.html",
    "href": "packages_and_dice.html",
    "title": "\n5  Packages and more dice\n",
    "section": "",
    "text": "5.1 Packages\nWe now have code that allows us to roll two dice and add the results together. To keep things interesting, let’s aim to weight the dice so that we can fool our friends into thinking we are lucky.\nFirst, though, we should prove to ourselves that our dice are fair. We can investigate the behavior of our dice using two powerful and general tools;\nFor the repetition part of things, we will use a built-in R function, replicate. For visualization, we are going to use a convenient plotting function, qplot. However, qplot does not come built into R. We must install a package to gain access to it.\nR is a powerful language for data science and programming, allowing beginners and experts alike to manipulate, analyze, and visualize data effectively. One of the most appealing features of R is its extensive library of packages, which are essential tools for expanding its capabilities and streamlining the coding process.\nAn R package is a collection of reusable functions, datasets, and compiled code created by other users and developers to extend the functionality of the base R language. These packages cover a wide range of applications, such as data manipulation, statistical analysis, machine learning, and data visualization. By utilizing existing R packages, you can leverage the expertise of others and save time by avoiding the need to create custom functions from scratch.\nUsing others’ R packages is incredibly beneficial as it allows you to take advantage of the collective knowledge of the R community. Developers often create packages to address specific challenges, optimize performance, or implement popular algorithms or methodologies. By incorporating these packages into your projects, you can enhance your productivity, reduce development time, and ensure that you are using well-tested and reliable code.",
    "crumbs": [
      "Home",
      "Introduction",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Packages and more dice</span>"
    ]
  },
  {
    "objectID": "packages_and_dice.html#packages",
    "href": "packages_and_dice.html#packages",
    "title": "\n5  Packages and more dice\n",
    "section": "",
    "text": "5.1.1 Installing R packages\nTo install an R package, you can use the install.packages() function in the R console or script. For example, to install the popular data manipulation package “dplyr,” simply type install.packages(“dplyr”). This command will download the package from the Comprehensive R Archive Network (CRAN) and install it on your local machine. Keep in mind that you only need to install a package once, unless you want to update it to a newer version.\nFor those who are going to be using R for bioinformatics or biological data science, you will also want to install packages from Bioconductor, which is a repository of R packages specifically designed for bioinformatics and computational biology. To install Bioconductor packages, you can use the BiocManager::install() function.\nTo use this recommended approach, you first need to install the BiocManager package, which is the package manager for Bioconductor.\n\ninstall.packages('BiocManager')\n\nThis is a one-time installation. After that, you can install any R, Bioconductor, rOpenSci, or even GitHub package using the BiocManager::install() function. For example, to install the ggplot2 package, which is widely used for data visualization, you would run:\n\nBiocManager::install(\"ggplot2\")\n\n\n5.1.2 Installing vs loading (library) R packages\nAfter installing an R package, you will need to load it into your R session before using its functions. To load a package, use the library() function followed by the package name, such as library(dplyr). Loading a package makes its functions and datasets available for use in your current R session. Note that you need to load a package every time you start a new R session.\n\nlibrary(ggplot2)\n\nNow, the functionality of the ggplot2 package is available in our R session.\n\n\n\n\n\n\nInstalling vs loading packages\n\n\n\nThe main thing to remember is that you only need to install a package once, but you need to load it with library each time you wish to use it in a new R session. R will unload all of its packages each time you close RStudio.\n\n\n\n\n\n\nAs in {Figure 5.1}, screw in the lightbulb (eg., BiocManager::install) only once and then to use it, you need to turn on the switch each time you want to use it (library).\n\n\n\nFigure 5.1: Installing vs loading R packages.\n\n5.1.3 Finding R packages\nFinding useful R packages can be done in several ways. First, browsing CRAN (https://cran.r-project.org/) and Bioconductor (more later, https://bioconductor.org) are an excellent starting points, as they host thousands of packages categorized by topic. Additionally, online forums like Stack Overflow and R-bloggers can provide valuable recommendations based on user experiences. Social media platforms such as Twitter, where developers and data scientists often share new packages and updates, can also be a helpful resource. Finally, don’t forget to ask your colleagues or fellow R users for their favorite packages, as they may have insights on which ones best suit your specific needs.",
    "crumbs": [
      "Home",
      "Introduction",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Packages and more dice</span>"
    ]
  },
  {
    "objectID": "packages_and_dice.html#are-our-dice-fair",
    "href": "packages_and_dice.html#are-our-dice-fair",
    "title": "\n5  Packages and more dice\n",
    "section": "\n5.2 Are our dice fair?",
    "text": "5.2 Are our dice fair?\nWell, let’s review our code.\n\nroll2 &lt;- function(bones = 1:6) {\n  dice = sample(bones, size = 2, replace = TRUE)\n  sum(dice)\n}\n\nIf our dice are fair, then each number should show up equally. What does the sum look like with our two dice?\n\n\n\n\n\nFigure 5.2: In an ideal world, a histogram of the results would look like this\n\n\nRead the help page for replicate (i.e., help(\"replicate\")). In short, it suggests that we can repeat our dice rolling as many times as we like and replicate will return a vector of the sums for each roll.\n\nrolls = replicate(n = 100, roll2())\n\nWhat does rolls look like?\n\nhead(rolls)\n\n[1]  5 12  7  8  3  9\n\nlength(rolls)\n\n[1] 100\n\nmean(rolls)\n\n[1] 6.78\n\nsummary(rolls)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n   2.00    5.00    6.50    6.78    9.00   12.00 \n\n\nThis looks like it roughly agrees with our sketched out ideal histogram in Figure 5.2. However, now that we’ve loaded the qplot function from the ggplot2 package, we can make a histogram of the data themselves.\n\nqplot(rolls, binwidth=1)\n\nWarning: `qplot()` was deprecated in ggplot2 3.4.0.\n\n\n\n\n\n\n\nFigure 5.3: Histogram of the sums from 100 rolls of our fair dice\n\n\n\n\nHow does your histogram look (and yours will be different from mine since we are sampling random values)? Is it what you expect?\nWhat happens to our histogram as we increase the number of replicates?\n\nrolls = replicate(n = 100000, roll2())\nqplot(rolls, binwidth=1)\n\n\n\n\n\n\nFigure 5.4: Histogram with 100000 rolls much more closely approximates the pyramidal shape we anticipated",
    "crumbs": [
      "Home",
      "Introduction",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Packages and more dice</span>"
    ]
  },
  {
    "objectID": "packages_and_dice.html#bonus-exercise",
    "href": "packages_and_dice.html#bonus-exercise",
    "title": "\n5  Packages and more dice\n",
    "section": "\n5.3 Bonus exercise",
    "text": "5.3 Bonus exercise\nHow would you change the roll2 function to weight the dice?\n\n\n\n\n\n\nHint\n\n\n\nRead the help page for sample (i.e., help(\"sample\")).",
    "crumbs": [
      "Home",
      "Introduction",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Packages and more dice</span>"
    ]
  },
  {
    "objectID": "reading_and_writing.html",
    "href": "reading_and_writing.html",
    "title": "\n6  Reading and writing data files\n",
    "section": "",
    "text": "6.1 Introduction\nIn this chapter, we will discuss how to read and write data files in R. Data files are essential for storing and sharing data across different platforms and applications. R provides a variety of functions and packages to read and write data files in different formats, such as text files, CSV files, Excel files. By mastering these functions, you can efficiently import and export data in R, enabling you to perform data analysis and visualization tasks effectively.",
    "crumbs": [
      "Home",
      "Introduction",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Reading and writing data files</span>"
    ]
  },
  {
    "objectID": "reading_and_writing.html#csv-files",
    "href": "reading_and_writing.html#csv-files",
    "title": "\n6  Reading and writing data files\n",
    "section": "\n6.2 CSV files",
    "text": "6.2 CSV files\nComma-Separated Values (CSV) files are a common file format for storing tabular data. They consist of rows and columns, with each row representing a record and each column representing a variable or attribute. CSV files are widely used for data storage and exchange due to their simplicity and compatibility with various software applications. In R, you can read and write CSV files using the read.csv() and write.csv() functions, respectively. A commonly used alternative is to use the readr package, which provides faster and more user-friendly functions for reading and writing CSV files.\n\n6.2.1 Writing a CSV file\nSince we are going to use the readr package, we need to install it first. You can install the readr package using the following command:\n\ninstall.packages(\"readr\")\n\nOnce the package is installed, you can load it into your R session using the library() function:\n\nlibrary(readr)\n\nSince we don’t have a CSV file sitting around, let’s create a simple data frame to write to a CSV file. Here’s an example data frame:\n\ndf &lt;- data.frame(\n  id = c(1, 2, 3, 4, 5),\n  name = c(\"Alice\", \"Bob\", \"Charlie\", \"David\", \"Eve\"),\n  age = c(25, 30, 35, 40, 45)\n)\n\nNow, you can write this data frame to a CSV file using the write_csv() function from the readr package. Here’s how you can do it:\n\nwrite_csv(df, \"data.csv\")\n\nYou can check the current working directory to see if the CSV file was created successfully. If you want to specify a different directory or file path, you can provide the full path in the write_csv() function.\n\n# see what the current working directory is\ngetwd()\n\n[1] \"/Users/davsean/Documents/git/RBiocBook\"\n\n# and check to see that the file was created\ndir(pattern = \"data.csv\")\n\n[1] \"data.csv\"\n\n\n\n6.2.2 Reading a CSV file\nNow that we have a CSV file, let’s read it back into R using the read_csv() function from the readr package. Here’s how you can do it:\n\ndf2 &lt;- read_csv(\"data.csv\")\n\nRows: 5 Columns: 3\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (1): name\ndbl (2): id, age\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nYou can check the structure of the data frame df2 to verify that the data was read correctly:\n\ndf2\n\n# A tibble: 5 × 3\n     id name      age\n  &lt;dbl&gt; &lt;chr&gt;   &lt;dbl&gt;\n1     1 Alice      25\n2     2 Bob        30\n3     3 Charlie    35\n4     4 David      40\n5     5 Eve        45\n\n\nThe readr package can read CSV files with various delimiters, headers, and data types, making it a versatile tool for handling tabular data in R. It can also read CSV files directly from web locations like so:\n\ndf3 &lt;- read_csv(\"https://data.cdc.gov/resource/pwn4-m3yp.csv\")\n\nRows: 1000 Columns: 10\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr  (1): state\ndbl  (6): tot_cases, new_cases, tot_deaths, new_deaths, new_historic_cases, ...\ndttm (3): date_updated, start_date, end_date\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nThe dataset that you just downloaded is described here: Covid-19 data from CDC",
    "crumbs": [
      "Home",
      "Introduction",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Reading and writing data files</span>"
    ]
  },
  {
    "objectID": "reading_and_writing.html#excel-files",
    "href": "reading_and_writing.html#excel-files",
    "title": "\n6  Reading and writing data files\n",
    "section": "\n6.3 Excel files",
    "text": "6.3 Excel files\nMicrosoft Excel files are another common file format for storing tabular data. Excel files can contain multiple sheets, formulas, and formatting options, making them a popular choice for data storage and analysis. In R, you can read and write Excel files using the readxl package. This package provides functions to import and export data from Excel files, enabling you to work with Excel data in R.\n\n6.3.1 Reading an Excel file\nTo read an Excel file in R, you need to install and load the readxl package. You can install the readxl package using the following command:\n\ninstall.packages(\"readxl\")\n\nOnce the package is installed, you can load it into your R session using the library() function:\n\nlibrary(readxl)\n\nNow, you can read an Excel file using the read_excel() function from the readxl package. We don’t have an excel file available, so let’s download one from the internet. Here’s an example:\n\ndownload.file('https://www.w3resource.com/python-exercises/pandas/excel/SaleData.xlsx', 'SaleData.xlsx')\n\nNow, you can read the Excel file into R using the read_excel() function:\n\ndf_excel &lt;- read_excel(\"SaleData.xlsx\")\n\nYou can check the structure of the data frame df_excel to verify that the data was read correctly:\n\ndf_excel\n\n# A tibble: 45 × 8\n   OrderDate           Region  Manager SalesMan  Item  Units Unit_price Sale_amt\n   &lt;dttm&gt;              &lt;chr&gt;   &lt;chr&gt;   &lt;chr&gt;     &lt;chr&gt; &lt;dbl&gt;      &lt;dbl&gt;    &lt;dbl&gt;\n 1 2018-01-06 00:00:00 East    Martha  Alexander Tele…    95       1198   113810\n 2 2018-01-23 00:00:00 Central Hermann Shelli    Home…    50        500    25000\n 3 2018-02-09 00:00:00 Central Hermann Luis      Tele…    36       1198    43128\n 4 2018-02-26 00:00:00 Central Timothy David     Cell…    27        225     6075\n 5 2018-03-15 00:00:00 West    Timothy Stephen   Tele…    56       1198    67088\n 6 2018-04-01 00:00:00 East    Martha  Alexander Home…    60        500    30000\n 7 2018-04-18 00:00:00 Central Martha  Steven    Tele…    75       1198    89850\n 8 2018-05-05 00:00:00 Central Hermann Luis      Tele…    90       1198   107820\n 9 2018-05-22 00:00:00 West    Douglas Michael   Tele…    32       1198    38336\n10 2018-06-08 00:00:00 East    Martha  Alexander Home…    60        500    30000\n# ℹ 35 more rows\n\n\nThe readxl package provides various options to read Excel files with multiple sheets, specific ranges, and data types, making it a versatile tool for handling Excel data in R.\n\n6.3.2 Writing an Excel file\nTo write an Excel file in R, you can use the write_xlsx() function from the writexl package. You can install the writexl package using the following command:\n\ninstall.packages(\"writexl\")\n\nOnce the package is installed, you can load it into your R session using the library() function:\n\nlibrary(writexl)\n\nThe write_xlsx() function allows you to write a data frame to an Excel file. Here’s an example:\n\nwrite_xlsx(df, \"data.xlsx\")\n\nYou can check the current working directory to see if the Excel file was created successfully. If you want to specify a different directory or file path, you can provide the full path in the write_xlsx() function.\n\n# see what the current working directory is\ngetwd()\n\n[1] \"/Users/davsean/Documents/git/RBiocBook\"\n\n# and check to see that the file was created\ndir(pattern = \"data.xlsx\")\n\n[1] \"data.xlsx\"",
    "crumbs": [
      "Home",
      "Introduction",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Reading and writing data files</span>"
    ]
  },
  {
    "objectID": "reading_and_writing.html#additional-options",
    "href": "reading_and_writing.html#additional-options",
    "title": "\n6  Reading and writing data files\n",
    "section": "\n6.4 Additional options",
    "text": "6.4 Additional options\n\nGoogle Sheets: You can read and write data from Google Sheets using the googlesheets4 package. This package provides functions to interact with Google Sheets, enabling you to import and export data from Google Sheets to R.\nJSON files: You can read and write JSON files using the jsonlite package. This package provides functions to convert R objects to JSON format and vice versa, enabling you to work with JSON data in R.\nDatabase files: You can read and write data from database files using the DBI and RSQLite packages. These packages provide functions to interact with various database systems, enabling you to import and export data from databases to R.",
    "crumbs": [
      "Home",
      "Introduction",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Reading and writing data files</span>"
    ]
  },
  {
    "objectID": "data_structures_overview.html",
    "href": "data_structures_overview.html",
    "title": "R Data Structures",
    "section": "",
    "text": "Chapter overview\nWelcome to the section on R data structures! As you begin your journey in learning R, it is essential to understand the fundamental building blocks of this powerful programming language. R offers a variety of data structures to store and manipulate data, each with its unique properties and capabilities. In this section, we will cover the core data structures in R, including:\nBy the end of this section, you will have a solid understanding of these data structures, and you will be able to choose and utilize the appropriate data structure for your specific data manipulation and analysis tasks.\nIn each chapter, we will delve into the properties and usage of each data structure, starting with their definitions and moving on to their practical applications. We will provide examples, exercises, and active learning approaches to help you better understand and apply these concepts in your work.\nAs you progress through these chapters, practice the examples and exercises provided, engage in discussion, and collaborate with your peers to deepen your understanding of R data structures. This solid foundation will serve as the basis for more advanced data manipulation, analysis, and visualization techniques in R.",
    "crumbs": [
      "Home",
      "R Data Structures"
    ]
  },
  {
    "objectID": "data_structures_overview.html#chapter-overview",
    "href": "data_structures_overview.html#chapter-overview",
    "title": "R Data Structures",
    "section": "",
    "text": "Vectors : In this chapter, we will introduce you to the simplest data structure in R, the vector. We will cover how to create, access, and manipulate vectors, as well as discuss their unique properties and limitations.\n\nMatrices\n\nNext, we will explore matrices, which are two-dimensional data structures that extend vectors. You will learn how to create, access, and manipulate matrices, and understand their usefulness in mathematical operations and data organization.\n\n\n\nLists\n\nThe third chapter will focus on lists, a versatile data structure that can store elements of different types and sizes. We will discuss how to create, access, and modify lists, and demonstrate their flexibility in handling complex data structures.\n\n\n\nData.frames\n\nFinally, we will examine data.frames, a widely-used data structure for organizing and manipulating tabular data. You will learn how to create, access, and manipulate data.frames, and understand their advantages over other data structures for data analysis tasks.\n\n\n\nArrays\n\nWhile we will not focus directly on the array data type, which are multidimensional data structures that extend matrices, they are very similar to matrices, but with a third dimension.",
    "crumbs": [
      "Home",
      "R Data Structures"
    ]
  },
  {
    "objectID": "vectors.html",
    "href": "vectors.html",
    "title": "\n7  Vectors\n",
    "section": "",
    "text": "7.1 What is a Vector?\nA vector is the simplest and most basic data structure in R. It is a one-dimensional, ordered collection of elements, where all the elements are of the same data type. Vectors can store various types of data, such as numeric, character, or logical values. Figure 7.1 shows a pictorial representation of three vector examples.\nIn this chapter, we will provide a comprehensive overview of vectors, including how to create, access, and manipulate them. We will also discuss some unique properties and rules associated with vectors, and explore their applications in data analysis tasks.\nIn R, even a single value is a vector with length=1.\nz = 1\nz\n\n[1] 1\n\nlength(z)\n\n[1] 1\nIn the code above, we “assigned” the value 1 to the variable named z. Typing z by itself is an “expression” that returns a result which is, in this case, the value that we just assigned. The length method takes an R object and returns the R length. There are numerous ways of asking R about what an object represents, and length is one of them.\nVectors can contain numbers, strings (character data), or logical values (TRUE and FALSE) or other “atomic” data types Table 7.1. Vectors cannot contain a mix of types! We will introduce another data structure, the R list for situations when we need to store a mix of base R data types.",
    "crumbs": [
      "Home",
      "R Data Structures",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Vectors</span>"
    ]
  },
  {
    "objectID": "vectors.html#what-is-a-vector",
    "href": "vectors.html#what-is-a-vector",
    "title": "\n7  Vectors\n",
    "section": "",
    "text": "Figure 7.1: “Pictorial representation of three vector examples. The first vector is a numeric vector. The second is a ‘logical’ vector. The third is a character vector. Vectors also have indices and, optionally, names.”\n\n\n\n\n\n\n\n\n\n\n\nData type\nStores\n\n\n\nnumeric\nfloating point numbers\n\n\ninteger\nintegers\n\n\ncomplex\ncomplex numbers\n\n\nfactor\ncategorical data\n\n\ncharacter\nstrings\n\n\nlogical\nTRUE or FALSE\n\n\nNA\nmissing\n\n\nNULL\nempty\n\n\nfunction\nfunction type\n\n\n\n\n\nTable 7.1: Atomic (simplest) data types in R.",
    "crumbs": [
      "Home",
      "R Data Structures",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Vectors</span>"
    ]
  },
  {
    "objectID": "vectors.html#creating-vectors",
    "href": "vectors.html#creating-vectors",
    "title": "\n7  Vectors\n",
    "section": "\n7.2 Creating vectors",
    "text": "7.2 Creating vectors\nCharacter vectors (also sometimes called “string” vectors) are entered with each value surrounded by single or double quotes; either is acceptable, but they must match. They are always displayed by R with double quotes. Here are some examples of creating vectors:\n\n# examples of vectors\nc('hello','world')\n\n[1] \"hello\" \"world\"\n\nc(1,3,4,5,1,2)\n\n[1] 1 3 4 5 1 2\n\nc(1.12341e7,78234.126)\n\n[1] 11234100.00    78234.13\n\nc(TRUE,FALSE,TRUE,TRUE)\n\n[1]  TRUE FALSE  TRUE  TRUE\n\n# note how in the next case the TRUE is converted to \"TRUE\"\n# with quotes around it.\nc(TRUE,'hello')\n\n[1] \"TRUE\"  \"hello\"\n\n\nWe can also create vectors as “regular sequences” of numbers. For example:\n\n# create a vector of integers from 1 to 10\nx = 1:10\n# and backwards\nx = 10:1\n\nThe seq function can create more flexible regular sequences.\n\n# create a vector of numbers from 1 to 4 skipping by 0.3\ny = seq(1,4,0.3)\n\nAnd creating a new vector by concatenating existing vectors is possible, as well.\n\n# create a sequence by concatenating two other sequences\nz = c(y,x)\nz\n\n [1]  1.0  1.3  1.6  1.9  2.2  2.5  2.8  3.1  3.4  3.7  4.0 10.0  9.0  8.0  7.0\n[16]  6.0  5.0  4.0  3.0  2.0  1.0",
    "crumbs": [
      "Home",
      "R Data Structures",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Vectors</span>"
    ]
  },
  {
    "objectID": "vectors.html#vector-operations",
    "href": "vectors.html#vector-operations",
    "title": "\n7  Vectors\n",
    "section": "\n7.3 Vector Operations",
    "text": "7.3 Vector Operations\nOperations on a single vector are typically done element-by-element. For example, we can add 2 to a vector, 2 is added to each element of the vector and a new vector of the same length is returned.\n\nx = 1:10\nx + 2\n\n [1]  3  4  5  6  7  8  9 10 11 12\n\n\nIf the operation involves two vectors, the following rules apply. If the vectors are the same length: R simply applies the operation to each pair of elements.\n\nx + x\n\n [1]  2  4  6  8 10 12 14 16 18 20\n\n\nIf the vectors are different lengths, but one length a multiple of the other, R reuses the shorter vector as needed.\n\nx = 1:10\ny = c(1,2)\nx * y\n\n [1]  1  4  3  8  5 12  7 16  9 20\n\n\nIf the vectors are different lengths, but one length not a multiple of the other, R reuses the shorter vector as needed and delivers a warning.\n\nx = 1:10\ny = c(2,3,4)\nx * y\n\nWarning in x * y: longer object length is not a multiple of shorter object\nlength\n\n\n [1]  2  6 12  8 15 24 14 24 36 20\n\n\nTypical operations include multiplication (“*”), addition, subtraction, division, exponentiation (“^”), but many operations in R operate on vectors and are then called “vectorized”.\nBe aware of the recycling rule when working with vectors of different lengths, as it may lead to unexpected results if you’re not careful.",
    "crumbs": [
      "Home",
      "R Data Structures",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Vectors</span>"
    ]
  },
  {
    "objectID": "vectors.html#logical-vectors",
    "href": "vectors.html#logical-vectors",
    "title": "\n7  Vectors\n",
    "section": "\n7.4 Logical Vectors",
    "text": "7.4 Logical Vectors\nLogical vectors are vectors composed on only the values TRUE and FALSE. Note the all-upper-case and no quotation marks.\n\na = c(TRUE,FALSE,TRUE)\n\n# we can also create a logical vector from a numeric vector\n# 0 = false, everything else is 1\nb = c(1,0,217)\nd = as.logical(b)\nd\n\n[1]  TRUE FALSE  TRUE\n\n# test if a and d are the same at every element\nall.equal(a,d)\n\n[1] TRUE\n\n# We can also convert from logical to numeric\nas.numeric(a)\n\n[1] 1 0 1\n\n\n\n7.4.1 Logical Operators\nSome operators like &lt;, &gt;, ==, &gt;=, &lt;=, != can be used to create logical vectors.\n\n# create a numeric vector\nx = 1:10\n# testing whether x &gt; 5 creates a logical vector\nx &gt; 5\n\n [1] FALSE FALSE FALSE FALSE FALSE  TRUE  TRUE  TRUE  TRUE  TRUE\n\nx &lt;= 5\n\n [1]  TRUE  TRUE  TRUE  TRUE  TRUE FALSE FALSE FALSE FALSE FALSE\n\nx != 5\n\n [1]  TRUE  TRUE  TRUE  TRUE FALSE  TRUE  TRUE  TRUE  TRUE  TRUE\n\nx == 5\n\n [1] FALSE FALSE FALSE FALSE  TRUE FALSE FALSE FALSE FALSE FALSE\n\n\nWe can also assign the results to a variable:\n\ny = (x == 5)\ny\n\n [1] FALSE FALSE FALSE FALSE  TRUE FALSE FALSE FALSE FALSE FALSE",
    "crumbs": [
      "Home",
      "R Data Structures",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Vectors</span>"
    ]
  },
  {
    "objectID": "vectors.html#indexing-vectors",
    "href": "vectors.html#indexing-vectors",
    "title": "\n7  Vectors\n",
    "section": "\n7.5 Indexing Vectors",
    "text": "7.5 Indexing Vectors\nIn R, an index is used to refer to a specific element or set of elements in an vector (or other data structure). [R uses [ and ] to perform indexing, although other approaches to getting subsets of larger data structures are common in R.\n\nx = seq(0,1,0.1)\n# create a new vector from the 4th element of x\nx[4]\n\n[1] 0.3\n\n\nWe can even use other vectors to perform the “indexing”.\n\nx[c(3,5,6)]\n\n[1] 0.2 0.4 0.5\n\ny = 3:6\nx[y]\n\n[1] 0.2 0.3 0.4 0.5\n\n\nCombining the concept of indexing with the concept of logical vectors results in a very power combination.\n\n# use help('rnorm') to figure out what is happening next\nmyvec = rnorm(10)\n\n# create logical vector that is TRUE where myvec is &gt;0.25\ngt1 = (myvec &gt; 0.25)\nsum(gt1)\n\n[1] 5\n\n# and use our logical vector to create a vector of myvec values that are &gt;0.25\nmyvec[gt1]\n\n[1] 1.5407162 0.7762003 0.3014347 0.6117903 0.3803251\n\n# or &lt;=0.25 using the logical \"not\" operator, \"!\"\nmyvec[!gt1]\n\n[1] -0.280651418  0.002451042 -0.484479797 -1.713216580 -0.381300050\n\n# shorter, one line approach\nmyvec[myvec &gt; 0.25]\n\n[1] 1.5407162 0.7762003 0.3014347 0.6117903 0.3803251",
    "crumbs": [
      "Home",
      "R Data Structures",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Vectors</span>"
    ]
  },
  {
    "objectID": "vectors.html#named-vectors",
    "href": "vectors.html#named-vectors",
    "title": "\n7  Vectors\n",
    "section": "\n7.6 Named Vectors",
    "text": "7.6 Named Vectors\nNamed vectors are vectors with labels or names assigned to their elements. These names can be used to access and manipulate the elements in a more meaningful way.\nTo create a named vector, use the names() function:\n\nfruit_prices &lt;- c(0.5, 0.75, 1.25)\nnames(fruit_prices) &lt;- c(\"apple\", \"banana\", \"cherry\")\nprint(fruit_prices)\n\n apple banana cherry \n  0.50   0.75   1.25 \n\n\nYou can also access and modify elements using their names:\n\nbanana_price &lt;- fruit_prices[\"banana\"]\nprint(banana_price)\n\nbanana \n  0.75 \n\nfruit_prices[\"apple\"] &lt;- 0.6\nprint(fruit_prices)\n\n apple banana cherry \n  0.60   0.75   1.25",
    "crumbs": [
      "Home",
      "R Data Structures",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Vectors</span>"
    ]
  },
  {
    "objectID": "vectors.html#character-vectors-a.k.a.-strings",
    "href": "vectors.html#character-vectors-a.k.a.-strings",
    "title": "\n7  Vectors\n",
    "section": "\n7.7 Character Vectors, A.K.A. Strings",
    "text": "7.7 Character Vectors, A.K.A. Strings\nR uses the paste function to concatenate strings.\n\npaste(\"abc\",\"def\")\n\n[1] \"abc def\"\n\npaste(\"abc\",\"def\",sep=\"THISSEP\")\n\n[1] \"abcTHISSEPdef\"\n\npaste0(\"abc\",\"def\")\n\n[1] \"abcdef\"\n\n## [1] \"abcdef\"\npaste(c(\"X\",\"Y\"),1:10)\n\n [1] \"X 1\"  \"Y 2\"  \"X 3\"  \"Y 4\"  \"X 5\"  \"Y 6\"  \"X 7\"  \"Y 8\"  \"X 9\"  \"Y 10\"\n\npaste(c(\"X\",\"Y\"),1:10,sep=\"_\")\n\n [1] \"X_1\"  \"Y_2\"  \"X_3\"  \"Y_4\"  \"X_5\"  \"Y_6\"  \"X_7\"  \"Y_8\"  \"X_9\"  \"Y_10\"\n\n\nWe can count the number of characters in a string.\n\nnchar('abc')\n\n[1] 3\n\nnchar(c('abc','d',123456))\n\n[1] 3 1 6\n\n\nPulling out parts of strings is also sometimes useful.\n\nsubstr('This is a good sentence.',start=10,stop=15)\n\n[1] \" good \"\n\n\nAnother common operation is to replace something in a string with something (a find-and-replace).\n\nsub('This','That','This is a good sentence.')\n\n[1] \"That is a good sentence.\"\n\n\nWhen we want to find all strings that match some other string, we can use grep, or “grab regular expression”.\n\ngrep('bcd',c('abcdef','abcd','bcde','cdef','defg'))\n\n[1] 1 2 3\n\ngrep('bcd',c('abcdef','abcd','bcde','cdef','defg'),value=TRUE)\n\n[1] \"abcdef\" \"abcd\"   \"bcde\"  \n\n\nRead about the grepl function (?grepl). Use that function to return a logical vector (TRUE/FALSE) for each entry above with an a in it.",
    "crumbs": [
      "Home",
      "R Data Structures",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Vectors</span>"
    ]
  },
  {
    "objectID": "vectors.html#missing-values-aka-na",
    "href": "vectors.html#missing-values-aka-na",
    "title": "\n7  Vectors\n",
    "section": "\n7.8 Missing Values, AKA “NA”",
    "text": "7.8 Missing Values, AKA “NA”\nR has a special value, “NA”, that represents a “missing” value, or Not Available, in a vector or other data structure. Here, we just create a vector to experiment.\n\nx = 1:5\nx\n\n[1] 1 2 3 4 5\n\nlength(x)\n\n[1] 5\n\n\n\nis.na(x)\n\n[1] FALSE FALSE FALSE FALSE FALSE\n\nx[2] = NA\nx\n\n[1]  1 NA  3  4  5\n\n\nThe length of x is unchanged, but there is one value that is marked as “missing” by virtue of being NA.\n\nlength(x)\n\n[1] 5\n\nis.na(x)\n\n[1] FALSE  TRUE FALSE FALSE FALSE\n\n\nWe can remove NA values by using indexing. In the following, is.na(x) returns a logical vector the length of x. The ! is the logical NOT operator and converts TRUE to FALSE and vice-versa.\n\nx[!is.na(x)]\n\n[1] 1 3 4 5",
    "crumbs": [
      "Home",
      "R Data Structures",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Vectors</span>"
    ]
  },
  {
    "objectID": "vectors.html#exercises",
    "href": "vectors.html#exercises",
    "title": "\n7  Vectors\n",
    "section": "\n7.9 Exercises",
    "text": "7.9 Exercises\n\n\nCreate a numeric vector called temperatures containing the following values: 72, 75, 78, 81, 76, 73.\n\nShow answertemperatures &lt;- c(72, 75, 78, 81, 76, 73, 93)\n\n\n\n\nCreate a character vector called days containing the following values: “Monday”, “Tuesday”, “Wednesday”, “Thursday”, “Friday”, “Saturday”, “Sunday”.\n\nShow answerdays &lt;- c(\"Monday\", \"Tuesday\", \"Wednesday\", \"Thursday\", \"Friday\", \"Saturday\", \"Sunday\")\n\n\n\n\nCalculate the average temperature for the week and store it in a variable called average_temperature.\n\nShow answeraverage_temperature &lt;- mean(temperatures)\n\n\n\n\nCreate a named vector called weekly_temperatures, where the names are the days of the week and the values are the temperatures from the temperatures vector.\n\nShow answerweekly_temperatures &lt;- temperatures\nnames(weekly_temperatures) &lt;- days\n\n\n\n\nCreate a numeric vector called ages containing the following values: 25, 30, 35, 40, 45, 50, 55, 60.\n\nShow answerages &lt;- c(25, 30, 35, 40, 45, 50, 55, 60)\n\n\n\n\nCreate a logical vector called is_adult by checking if the elements in the ages vector are greater than or equal to 18.\n\nShow answeris_adult &lt;- ages &gt;= 18\n\n\n\n\nCalculate the sum and product of the ages vector.\n\nShow answersum_ages &lt;- sum(ages)\nproduct_ages &lt;- prod(ages)\n\n\n\n\nExtract the ages greater than or equal to 40 from the ages vector and store them in a variable called older_ages.\n\nShow answerolder_ages &lt;- ages[ages &gt;= 40]",
    "crumbs": [
      "Home",
      "R Data Structures",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Vectors</span>"
    ]
  },
  {
    "objectID": "matrices.html",
    "href": "matrices.html",
    "title": "\n8  Matrices\n",
    "section": "",
    "text": "8.1 Creating a matrix\nA matrix is a rectangular collection of the same data type (see Figure 8.1). It can be viewed as a collection of column vectors all of the same length and the same type (i.e. numeric, character or logical) OR a collection of row vectors, again all of the same type and length. A data.frame is also a rectangular array. All of the columns must be the same length, but they may be of different types. The rows and columns of a matrix or data frame can be given names. However these are implemented differently in R; many operations will work for one but not both, often a source of confusion.\nThere are many ways to create a matrix in R. One of the simplest is to use the matrix() function. In the code below, we’ll create a matrix from a vector from 1:16.\nmat1 &lt;- matrix(1:16,nrow=4)\nmat1\n\n     [,1] [,2] [,3] [,4]\n[1,]    1    5    9   13\n[2,]    2    6   10   14\n[3,]    3    7   11   15\n[4,]    4    8   12   16\nThe same is possible, but specifying that the matrix be “filled” by row.\nmat1 &lt;- matrix(1:16,nrow=4,byrow = TRUE)\nmat1\n\n     [,1] [,2] [,3] [,4]\n[1,]    1    2    3    4\n[2,]    5    6    7    8\n[3,]    9   10   11   12\n[4,]   13   14   15   16\nNotice the subtle difference in the order that the numbers go into the matrix.\nWe can also build a matrix from parts by “binding” vectors together:\nx &lt;- 1:10 \ny &lt;- rnorm(10)\nEach of the vectors above is of length 10 and both are “numeric”, so we can make them into a matrix. Using rbind binds rows (r) into a matrix.\nmat &lt;- rbind(x,y)\nmat\n\n       [,1]        [,2]     [,3]       [,4]       [,5]      [,6]      [,7]\nx  1.000000  2.00000000 3.000000  4.0000000  5.0000000  6.000000  7.000000\ny -1.151193 -0.09401175 1.016198 -0.1788507 -0.9842443 -1.214426 -1.089794\n      [,8]       [,9]      [,10]\nx  8.00000  9.0000000 10.0000000\ny -2.23542 -0.4612952  0.4178649\nThe alternative to rbind is cbind that binds columns (c) together.\nmat &lt;- cbind(x,y)\nmat\n\n       x           y\n [1,]  1 -1.15119280\n [2,]  2 -0.09401175\n [3,]  3  1.01619820\n [4,]  4 -0.17885066\n [5,]  5 -0.98424426\n [6,]  6 -1.21442596\n [7,]  7 -1.08979410\n [8,]  8 -2.23542017\n [9,]  9 -0.46129523\n[10,] 10  0.41786492\nInspecting the names associated with rows and columns is often useful, particularly if the names have human meaning.\nrownames(mat)\n\nNULL\n\ncolnames(mat)\n\n[1] \"x\" \"y\"\nWe can also change the names of the matrix by assigning valid names to the columns or rows.\ncolnames(mat) = c('apples','oranges')\ncolnames(mat)\n\n[1] \"apples\"  \"oranges\"\n\nmat\n\n      apples     oranges\n [1,]      1 -1.15119280\n [2,]      2 -0.09401175\n [3,]      3  1.01619820\n [4,]      4 -0.17885066\n [5,]      5 -0.98424426\n [6,]      6 -1.21442596\n [7,]      7 -1.08979410\n [8,]      8 -2.23542017\n [9,]      9 -0.46129523\n[10,]     10  0.41786492\nMatrices have dimensions.\ndim(mat)\n\n[1] 10  2\n\nnrow(mat)\n\n[1] 10\n\nncol(mat)\n\n[1] 2",
    "crumbs": [
      "Home",
      "R Data Structures",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Matrices</span>"
    ]
  },
  {
    "objectID": "matrices.html#accessing-elements-of-a-matrix",
    "href": "matrices.html#accessing-elements-of-a-matrix",
    "title": "\n8  Matrices\n",
    "section": "\n8.2 Accessing elements of a matrix",
    "text": "8.2 Accessing elements of a matrix\nIndexing for matrices works as for vectors except that we now need to include both the row and column (in that order). We can access elements of a matrix using the square bracket [ indexing method. Elements can be accessed as var[r, c]. Here, r and c are vectors describing the elements of the matrix to select.\n\n\n\n\n\n\nImportant\n\n\n\nThe indices in R start with one, meaning that the first element of a vector or the first row/column of a matrix is indexed as one.\nThis is different from some other programming languages, such as Python, which use zero-based indexing, meaning that the first element of a vector or the first row/column of a matrix is indexed as zero.\nIt is important to be aware of this difference when working with data in R, especially if you are coming from a programming background that uses zero-based indexing. Using the wrong index can lead to unexpected results or errors in your code.\n\n\n\n# The 2nd element of the 1st row of mat\nmat[1,2]\n\n  oranges \n-1.151193 \n\n# The first ROW of mat\nmat[1,]\n\n   apples   oranges \n 1.000000 -1.151193 \n\n# The first COLUMN of mat\nmat[,1]\n\n [1]  1  2  3  4  5  6  7  8  9 10\n\n# and all elements of mat that are &gt; 4; note no comma\nmat[mat&gt;4]\n\n[1]  5  6  7  8  9 10\n\n## [1]  5  6  7  8  9 10\n\n\n\n\n\n\n\nCaution\n\n\n\nNote that in the last case, there is no “,”, so R treats the matrix as a long vector (length=20). This is convenient, sometimes, but it can also be a source of error, as some code may “work” but be doing something unexpected.\n\n\nWe can also use indexing to exclude a row or column by prefixing the selection with a - sign.\n\nmat[,-1]       # remove first column\n\n [1] -1.15119280 -0.09401175  1.01619820 -0.17885066 -0.98424426 -1.21442596\n [7] -1.08979410 -2.23542017 -0.46129523  0.41786492\n\nmat[-c(1:5),]  # remove first five rows\n\n     apples    oranges\n[1,]      6 -1.2144260\n[2,]      7 -1.0897941\n[3,]      8 -2.2354202\n[4,]      9 -0.4612952\n[5,]     10  0.4178649",
    "crumbs": [
      "Home",
      "R Data Structures",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Matrices</span>"
    ]
  },
  {
    "objectID": "matrices.html#changing-values-in-a-matrix",
    "href": "matrices.html#changing-values-in-a-matrix",
    "title": "\n8  Matrices\n",
    "section": "\n8.3 Changing values in a matrix",
    "text": "8.3 Changing values in a matrix\nWe can create a matrix filled with random values drawn from a normal distribution for our work below.\n\nm = matrix(rnorm(20),nrow=10)\nsummary(m)\n\n       V1                V2         \n Min.   :-1.8330   Min.   :-2.0497  \n 1st Qu.:-0.7606   1st Qu.:-0.4389  \n Median : 0.7864   Median : 0.6033  \n Mean   : 0.2856   Mean   : 0.6738  \n 3rd Qu.: 0.9019   3rd Qu.: 1.7985  \n Max.   : 2.3502   Max.   : 2.9927  \n\n\nMultiplication and division works similarly to vectors. When multiplying by a vector, for example, the values of the vector are reused. In the simplest case, let’s multiply the matrix by a constant (vector of length 1).\n\n# multiply all values in the matrix by 20\nm2 = m*20\nsummary(m2)\n\n       V1                V2         \n Min.   :-36.661   Min.   :-40.994  \n 1st Qu.:-15.211   1st Qu.: -8.779  \n Median : 15.729   Median : 12.065  \n Mean   :  5.711   Mean   : 13.475  \n 3rd Qu.: 18.038   3rd Qu.: 35.969  \n Max.   : 47.004   Max.   : 59.854  \n\n\nBy combining subsetting with assignment, we can make changes to just part of a matrix.\n\n# and add 100 to the first column of m\nm2[,1] = m2[,1] + 100\n# summarize m\nsummary(m2)\n\n       V1               V2         \n Min.   : 63.34   Min.   :-40.994  \n 1st Qu.: 84.79   1st Qu.: -8.779  \n Median :115.73   Median : 12.065  \n Mean   :105.71   Mean   : 13.475  \n 3rd Qu.:118.04   3rd Qu.: 35.969  \n Max.   :147.00   Max.   : 59.854  \n\n\nA somewhat common transformation for a matrix is to transpose which changes rows to columns. One might need to do this if an assay output from a lab machine puts samples in rows and genes in columns, for example, while in Bioconductor/R, we often want the samples in columns and the genes in rows.\n\nt(m2)\n\n          [,1]       [,2]      [,3]      [,4]      [,5]      [,6]       [,7]\n[1,] 116.89722 147.004355 116.19088  99.50672 121.81061  63.33906 118.417923\n[2,]  59.85408  -9.877637  12.99534 -40.99377  51.33027 -11.52863  -5.482144\n         [,8]      [,9]    [,10]\n[1,] 78.79549 115.26654 79.88263\n[2,] 11.13558  29.04172 38.27873",
    "crumbs": [
      "Home",
      "R Data Structures",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Matrices</span>"
    ]
  },
  {
    "objectID": "matrices.html#calculations-on-matrix-rows-and-columns",
    "href": "matrices.html#calculations-on-matrix-rows-and-columns",
    "title": "\n8  Matrices\n",
    "section": "\n8.4 Calculations on matrix rows and columns",
    "text": "8.4 Calculations on matrix rows and columns\nAgain, we just need a matrix to play with. We’ll use rnorm again, but with a slight twist.\n\nm3 = matrix(rnorm(100,5,2),ncol=10) # what does the 5 mean here? And the 2?\n\nSince these data are from a normal distribution, we can look at a row (or column) to see what the mean and standard deviation are.\n\nmean(m3[,1])\n\n[1] 6.709692\n\nsd(m3[,1])\n\n[1] 1.425251\n\n# or a row\nmean(m3[1,])\n\n[1] 4.86667\n\nsd(m3[1,])\n\n[1] 2.102434\n\n\nThere are some useful convenience functions for computing means and sums of data in all of the columns and rows of matrices.\n\ncolMeans(m3)\n\n [1] 6.709692 4.968957 5.961377 4.572742 5.293570 4.130627 4.195079 4.475901\n [9] 5.076318 5.360578\n\nrowMeans(m3)\n\n [1] 4.866670 4.100055 4.814678 5.800869 5.813489 5.485770 4.889247 5.444376\n [9] 4.603745 4.925941\n\nrowSums(m3)\n\n [1] 48.66670 41.00055 48.14678 58.00869 58.13489 54.85770 48.89247 54.44376\n [9] 46.03745 49.25941\n\ncolSums(m3)\n\n [1] 67.09692 49.68957 59.61377 45.72742 52.93570 41.30627 41.95079 44.75901\n [9] 50.76318 53.60578\n\n\nWe can look at the distribution of column means:\n\n# save as a variable\ncmeans = colMeans(m3)\nsummary(cmeans)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  4.131   4.500   5.023   5.074   5.344   6.710 \n\n\nNote that this is centered pretty closely around the selected mean of 5 above.\nHow about the standard deviation? There is not a colSd function, but it turns out that we can easily apply functions that take vectors as input, like sd and “apply” them across either the rows (the first dimension) or columns (the second) dimension.\n\ncsds = apply(m3, 2, sd)\nsummary(csds)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n 0.7965  1.8319  2.0329  1.8965  2.1867  2.3309 \n\n\nAgain, take a look at the distribution which is centered quite close to the selected standard deviation when we created our matrix.",
    "crumbs": [
      "Home",
      "R Data Structures",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Matrices</span>"
    ]
  },
  {
    "objectID": "matrices.html#exercises",
    "href": "matrices.html#exercises",
    "title": "\n8  Matrices\n",
    "section": "\n8.5 Exercises",
    "text": "8.5 Exercises\n\n8.5.1 Data preparation\nFor this set of exercises, we are going to rely on a dataset that comes with R. It gives the number of sunspots per month from 1749-1983. The dataset comes as a ts or time series data type which I convert to a matrix using the following code.\nJust run the code as is and focus on the rest of the exercises.\n\ndata(sunspots)\nsunspot_mat &lt;- matrix(as.vector(sunspots),ncol=12,byrow = TRUE)\ncolnames(sunspot_mat) &lt;- as.character(1:12)\nrownames(sunspot_mat) &lt;- as.character(1749:1983)\n\n\n8.5.2 Questions\n\n\nAfter the conversion above, what does sunspot_mat look like? Use functions to find the number of rows, the number of columns, the class, and some basic summary statistics.\n\nShow answerncol(sunspot_mat)\nnrow(sunspot_mat)\ndim(sunspot_mat)\nsummary(sunspot_mat)\nhead(sunspot_mat)\ntail(sunspot_mat)\n\n\n\n\nPractice subsetting the matrix a bit by selecting:\n\nThe first 10 years (rows)\nThe month of July (7th column)\nThe value for July, 1979 using the rowname to do the selection.\n\n\nShow answersunspot_mat[1:10,]\nsunspot_mat[,7]\nsunspot_mat['1979',7]\n\n\n\n\n\n\nThese next few exercises take advantage of the fact that calling a univariate statistical function (one that expects a vector) works for matrices by just making a vector of all the values in the matrix. What is the highest (max) number of sunspots recorded in these data?\n\nShow answermax(sunspot_mat)\n\n\n\n\nAnd the minimum?\n\nShow answermin(sunspot_mat)\n\n\n\n\nAnd the overall mean and median?\n\nShow answermean(sunspot_mat)\nmedian(sunspot_mat)\n\n\n\n\nUse the hist() function to look at the distribution of all the monthly sunspot data.\n\nShow answerhist(sunspot_mat)\n\n\n\n\nRead about the breaks argument to hist() to try to increase the number of breaks in the histogram to increase the resolution slightly. Adjust your hist() and breaks to your liking.\n\nShow answerhist(sunspot_mat, breaks=40)\n\n\n\n\nNow, let’s move on to summarizing the data a bit to learn about the pattern of sunspots varies by month or by year. Examine the dataset again. What do the columns represent? And the rows?\n\nShow answer# just a quick glimpse of the data will give us a sense\nhead(sunspot_mat)\n\n\n\n\nWe’d like to look at the distribution of sunspots by month. How can we do that?\n\nShow answer# the mean of the columns is the mean number of sunspots per month.\ncolMeans(sunspot_mat)\n\n# Another way to write the same thing:\napply(sunspot_mat, 2, mean)\n\n\n\n\nAssign the month summary above to a variable and summarize it to get a sense of the spread over months.\n\nShow answermonthmeans = colMeans(sunspot_mat)\nsummary(monthmeans)\n\n\n\n\nPlay the same game for years to get the per-year mean?\n\nShow answerymeans = rowMeans(sunspot_mat)\nsummary(ymeans)\n\n\n\n\nMake a plot of the yearly means. Do you see a pattern?\n\nShow answerplot(ymeans)\n# or make it clearer\nplot(ymeans, type='l')",
    "crumbs": [
      "Home",
      "R Data Structures",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Matrices</span>"
    ]
  },
  {
    "objectID": "lists.html",
    "href": "lists.html",
    "title": "\n9  Lists\n",
    "section": "",
    "text": "9.1 The Power of a “Catch-All” Container\nSo far in our journey through R’s data structures, we’ve dealt with vectors and matrices. These are fantastic tools, but they have one strict rule: all their elements must be of the same data type. You can have a vector of numbers or a matrix of characters, but you can’t mix and match.\nBut what about real-world biological data? A single experiment can generate a dizzying variety of information. Imagine you’re studying a particular gene. You might have:\nHow could you possibly store all of this related, yet different, information together? You could create many separate variables, but that would be clunky and hard to manage. This is exactly the problem that lists are designed to solve.\nA list in R is like a flexible, multi-compartment container. It’s a single object that can hold a collection of other R objects, and those objects can be of any type, length, or dimension. You can put vectors, matrices, logical values, and even other lists inside a single list. This makes them one of the most fundamental and powerful data structures for bioinformatics analysis.\nThe key features of lists are:",
    "crumbs": [
      "Home",
      "R Data Structures",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Lists</span>"
    ]
  },
  {
    "objectID": "lists.html#the-power-of-a-catch-all-container",
    "href": "lists.html#the-power-of-a-catch-all-container",
    "title": "\n9  Lists\n",
    "section": "",
    "text": "The gene’s name (text).\nIts expression level across several samples (a set of numbers).\nA record of whether it’s a known cancer-related gene (a simple TRUE/FALSE).\nThe raw fluorescence values from your qPCR machine (a matrix of numbers).\nSome personal notes about the experiment (a paragraph of text).\n\n\n\n\n\n\nFlexibility: They can contain a mix of any data type.\n\nOrganization: You can and should name the elements of a list, making your data self-describing.\n\nHierarchy: Because lists can contain other lists, you can create complex, nested data structures to represent sophisticated relationships in your data.",
    "crumbs": [
      "Home",
      "R Data Structures",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Lists</span>"
    ]
  },
  {
    "objectID": "lists.html#creating-a-list",
    "href": "lists.html#creating-a-list",
    "title": "\n9  Lists\n",
    "section": "\n9.2 Creating a List",
    "text": "9.2 Creating a List\nYou create a list with the list() function. The best practice is to name the elements as you create them. This makes your code infinitely more readable and your data easier to work with.\nLet’s create a list to store the information for our hypothetical gene study.\n\n# An experiment tracking list for the gene TP53\nexperiment_data &lt;- list(\n  experiment_id = \"EXP042\",\n  gene_name = \"TP53\",\n  read_counts = c(120, 155, 98, 210),\n  is_control = FALSE,\n  sample_matrix = matrix(1:4, nrow = 2, dimnames = list(c(\"Treated\", \"Untreated\"), c(\"Replicate1\", \"Replicate2\")))\n)\n\n# --- Function Explainer: print() ---\n# The print() function displays the contents of an R object in the console. \n# For a list, it shows each element and its contents. It's the default action \n# when you just type the variable's name and hit Enter.\nprint(experiment_data)\n\n$experiment_id\n[1] \"EXP042\"\n\n$gene_name\n[1] \"TP53\"\n\n$read_counts\n[1] 120 155  98 210\n\n$is_control\n[1] FALSE\n\n$sample_matrix\n          Replicate1 Replicate2\nTreated            1          3\nUntreated          2          4",
    "crumbs": [
      "Home",
      "R Data Structures",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Lists</span>"
    ]
  },
  {
    "objectID": "lists.html#inspecting-your-list-whats-inside",
    "href": "lists.html#inspecting-your-list-whats-inside",
    "title": "\n9  Lists\n",
    "section": "\n9.3 Inspecting Your List: What’s Inside?",
    "text": "9.3 Inspecting Your List: What’s Inside?\nWhen someone hands you a tube in the lab, the first thing you do is look at the label. When R gives you a complex object like a list, you need to do the same. R provides several “introspection” functions to help you understand the contents and structure of your lists.\n\n9.3.1 str(): The Structure Function\nThis is arguably the most useful function for inspecting any R object, especially lists.\n\n# --- Function Explainer: str() ---\n# The str() function provides a compact, human-readable summary of an \n# object's internal \"str\"ucture. It's your best friend for understanding \n# what's inside a list, including the type and a preview of each element.\nstr(experiment_data)\n\nList of 5\n $ experiment_id: chr \"EXP042\"\n $ gene_name    : chr \"TP53\"\n $ read_counts  : num [1:4] 120 155 98 210\n $ is_control   : logi FALSE\n $ sample_matrix: int [1:2, 1:2] 1 2 3 4\n  ..- attr(*, \"dimnames\")=List of 2\n  .. ..$ : chr [1:2] \"Treated\" \"Untreated\"\n  .. ..$ : chr [1:2] \"Replicate1\" \"Replicate2\"\n\n\nThe output of str() tells us everything we need to know: it’s a “List of 5”, and for each of the 5 elements, it shows the name (e.g., experiment_id), the data type (e.g., chr for character, num for numeric), and a preview of the content.\n\n9.3.2 length(), names(), and class()\n\nThese functions give you more specific information about the list itself.\n\n# --- Function Explainer: length() ---\n# For a list, length() tells you how many top-level elements it contains.\nlength(experiment_data)\n\n[1] 5\n\n# --- Function Explainer: names() ---\n# The names() function extracts the names of the elements in a list as a \n# character vector. It's a great way to see what you can access.\nnames(experiment_data)\n\n[1] \"experiment_id\" \"gene_name\"     \"read_counts\"   \"is_control\"   \n[5] \"sample_matrix\"\n\n# --- Function Explainer: class() ---\n# The class() function tells you the type of the object itself. \n# This is useful to confirm you are indeed working with a list.\nclass(experiment_data)\n\n[1] \"list\"",
    "crumbs": [
      "Home",
      "R Data Structures",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Lists</span>"
    ]
  },
  {
    "objectID": "lists.html#accessing-list-elements-getting-things-out",
    "href": "lists.html#accessing-list-elements-getting-things-out",
    "title": "\n9  Lists\n",
    "section": "\n9.4 Accessing List Elements: Getting Things Out",
    "text": "9.4 Accessing List Elements: Getting Things Out\nOkay, you’ve packed your experimental data into a list. Now, how do you get specific items out? This is a critical concept, and R has a few ways to do it, each with a distinct purpose.\n\n9.4.1 The Mighty [[...]] and $ for Single Items\nTo pull out a single element from a list in its original form, you use either double square brackets [[...]] or the dollar sign $ (for named lists). Think of this as carefully reaching into a specific compartment of your container and taking out the item itself.\nLet’s use our experiment_data list.\n\n# Get the gene name using [[...]]\ngene &lt;- experiment_data[[\"gene_name\"]]\nprint(gene)\n\n[1] \"TP53\"\n\nclass(gene) # It's a character vector, just as it was when we put it in.\n\n[1] \"character\"\n\n# Get the read counts using the $ shortcut. This is often easier to read.\nreads &lt;- experiment_data$read_counts\nprint(reads)\n\n[1] 120 155  98 210\n\nclass(reads) # It's a numeric vector.\n\n[1] \"numeric\"\n\n# The [[...]] has a neat trick: you can use a variable to specify the name.\nelement_to_get &lt;- \"read_counts\"\nexperiment_data[[element_to_get]]\n\n[1] 120 155  98 210\n\n\nThe key takeaway is that [[...]] and $ extract the element. The result is the object that was stored inside the list.\n\n9.4.2 The Subsetting [...] for New Lists\nThe single square bracket [...] behaves differently. It always returns a new, smaller list that is a subset of the original list. It’s like taking a whole compartment, label and all, out of your larger container.\n\n# Get the gene name using [...]\ngene_sublist &lt;- experiment_data[\"gene_name\"]\n\nprint(gene_sublist)\n\n$gene_name\n[1] \"TP53\"\n\n# --- Note the class! ---\n# The result is another list, which contains the gene_name element.\nclass(gene_sublist) \n\n[1] \"list\"\n\n\nThis distinction is vital. If you want to perform a calculation on an element (like finding the mean() of read_counts), you must extract it with [[...]] or $. If you tried mean(experiment_data[\"read_counts\"]), R would give you an error because you can’t calculate the mean of a list!",
    "crumbs": [
      "Home",
      "R Data Structures",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Lists</span>"
    ]
  },
  {
    "objectID": "lists.html#modifying-lists",
    "href": "lists.html#modifying-lists",
    "title": "\n9  Lists\n",
    "section": "\n9.5 Modifying Lists",
    "text": "9.5 Modifying Lists\nYour data is rarely static. You can easily add, remove, or update elements in a list after you’ve created it.\n\n9.5.1 Adding and Updating Elements\nYou can add a new element or change an existing one by using the $ or [[...]] assignment syntax.\n\n# Add the date of the experiment\nexperiment_data$date &lt;- \"2024-06-05\"\n\n# Add some notes using the [[...]] syntax\nexperiment_data[[\"notes\"]] &lt;- \"Initial pilot experiment. High variance in read counts.\"\n\n# Let's update the control status\nexperiment_data$is_control &lt;- TRUE\n\n# Let's look at the structure now\nstr(experiment_data)\n\nList of 7\n $ experiment_id: chr \"EXP042\"\n $ gene_name    : chr \"TP53\"\n $ read_counts  : num [1:4] 120 155 98 210\n $ is_control   : logi TRUE\n $ sample_matrix: int [1:2, 1:2] 1 2 3 4\n  ..- attr(*, \"dimnames\")=List of 2\n  .. ..$ : chr [1:2] \"Treated\" \"Untreated\"\n  .. ..$ : chr [1:2] \"Replicate1\" \"Replicate2\"\n $ date         : chr \"2024-06-05\"\n $ notes        : chr \"Initial pilot experiment. High variance in read counts.\"\n\n\n\n9.5.2 Removing Elements\nTo remove an element from a list, you simply assign NULL to it. NULL is R’s special object representing nothingness.\n\n# We've decided the matrix isn't needed for this summary object.\nexperiment_data$sample_matrix &lt;- NULL\n\n# See the final structure of our list\nstr(experiment_data)\n\nList of 6\n $ experiment_id: chr \"EXP042\"\n $ gene_name    : chr \"TP53\"\n $ read_counts  : num [1:4] 120 155 98 210\n $ is_control   : logi TRUE\n $ date         : chr \"2024-06-05\"\n $ notes        : chr \"Initial pilot experiment. High variance in read counts.\"",
    "crumbs": [
      "Home",
      "R Data Structures",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Lists</span>"
    ]
  },
  {
    "objectID": "lists.html#a-biological-example-a-self-contained-gene-record",
    "href": "lists.html#a-biological-example-a-self-contained-gene-record",
    "title": "\n9  Lists\n",
    "section": "\n9.6 A Biological Example: A Self-Contained Gene Record",
    "text": "9.6 A Biological Example: A Self-Contained Gene Record\nLet’s put this all together. Lists are perfect for creating self-contained records that you can easily pass to functions or combine into larger lists.\n\n# --- Function Explainer: log2() ---\n# The log2() function calculates the base-2 logarithm. It's very common in \n# gene expression analysis to transform skewed count data to make it more \n# symmetric and easier to model.\n\nbrca1_gene &lt;- list(\n  gene_symbol = \"BRCA1\",\n  full_name = \"BRCA1 DNA repair associated\",\n  chromosome = \"17\",\n  expression_log2 = log2(c(45, 50, 30, 88, 120)),\n  related_diseases = c(\"Breast Cancer\", \"Ovarian Cancer\")\n)\n\n# Now we can easily work with this structured information\n\n# --- Function Explainer: cat() ---\n# The cat() function concatenates and prints its arguments to the console.\n# Unlike print(), it allows you to seamlessly join text and variables, and \n# the \"\\n\" character is used to add a newline (a line break).\ncat(\"Analyzing gene:\", brca1_gene$gene_symbol, \"\\n\")\n\nAnalyzing gene: BRCA1 \n\ncat(\"Located on chromosome:\", brca1_gene$chromosome, \"\\n\")\n\nLocated on chromosome: 17 \n\n# Calculate the average log2 expression\n# --- Function Explainer: mean() ---\n# The mean() function calculates the arithmetic average of a numeric vector.\navg_expression &lt;- mean(brca1_gene$expression_log2)\ncat(\"Average log2 expression:\", avg_expression, \"\\n\")\n\nAverage log2 expression: 5.881784 \n\n\nThis simple brca1_gene list is now a complete, portable record. You could imagine creating a list of these gene records, creating a powerful, hierarchical database for your entire project.",
    "crumbs": [
      "Home",
      "R Data Structures",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Lists</span>"
    ]
  },
  {
    "objectID": "dataframes_intro.html",
    "href": "dataframes_intro.html",
    "title": "\n10  Data Frames\n",
    "section": "",
    "text": "10.1 Learning goals\nWhile R has many different data types, the one that is central to much of the power and popularity of R is the data.frame. A data.frame looks a bit like an R matrix in that it has two dimensions, rows and columns. However, data.frames are usually viewed as a set of columns representing variables and the rows representing the values of those variables. Importantly, a data.frame may contain different data types in each of its columns; matrices must contain only one data type. This distinction is important to remember, as there are specific approaches to working with R data.frames that may be different than those for working with matrices.",
    "crumbs": [
      "Home",
      "R Data Structures",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Data Frames</span>"
    ]
  },
  {
    "objectID": "dataframes_intro.html#learning-goals",
    "href": "dataframes_intro.html#learning-goals",
    "title": "\n10  Data Frames\n",
    "section": "",
    "text": "Understand how data.frames are different from matrices.\nKnow a few functions for examing the contents of a data.frame.\nList approaches for subsetting data.frames.\nBe able to load and save tabular data from and to disk.\nShow how to create a data.frames from scratch.",
    "crumbs": [
      "Home",
      "R Data Structures",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Data Frames</span>"
    ]
  },
  {
    "objectID": "dataframes_intro.html#learning-objectives",
    "href": "dataframes_intro.html#learning-objectives",
    "title": "\n10  Data Frames\n",
    "section": "\n10.2 Learning objectives",
    "text": "10.2 Learning objectives\n\nLoad the yeast growth dataset into R using read.csv.\nExamine the contents of the dataset.\nUse subsetting to find genes that may be involved with nutrient metabolism and transport.\nSummarize data measurements by categories.",
    "crumbs": [
      "Home",
      "R Data Structures",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Data Frames</span>"
    ]
  },
  {
    "objectID": "dataframes_intro.html#dataset",
    "href": "dataframes_intro.html#dataset",
    "title": "\n10  Data Frames\n",
    "section": "\n10.3 Dataset",
    "text": "10.3 Dataset\nThe data used here are borrowed directly from the fantastic Bioconnector tutorials and are a cleaned up version of the data from Brauer et al. Coordination of Growth Rate, Cell Cycle, Stress Response, and Metabolic Activity in Yeast (2008) Mol Biol Cell 19:352-367. These data are from a gene expression microarray, and in this paper the authors examine the relationship between growth rate and gene expression in yeast cultures limited by one of six different nutrients (glucose, leucine, ammonium, sulfate, phosphate, uracil). If you give yeast a rich media loaded with nutrients except restrict the supply of a single nutrient, you can control the growth rate to any rate you choose. By starving yeast of specific nutrients you can find genes that:\n\nRaise or lower their expression in response to growth rate. Growth-rate dependent expression patterns can tell us a lot about cell cycle control, and how the cell responds to stress. The authors found that expression of &gt;25% of all yeast genes is linearly correlated with growth rate, independent of the limiting nutrient. They also found that the subset of negatively growth-correlated genes is enriched for peroxisomal functions, and positively correlated genes mainly encode ribosomal functions.\nRespond differently when different nutrients are being limited. If you see particular genes that respond very differently when a nutrient is sharply restricted, these genes might be involved in the transport or metabolism of that specific nutrient.\n\nThe dataset can be downloaded directly from:\n\nbrauer2007_tidy.csv\n\nWe are going to read this dataset into R and then use it as a playground for learning about data.frames.",
    "crumbs": [
      "Home",
      "R Data Structures",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Data Frames</span>"
    ]
  },
  {
    "objectID": "dataframes_intro.html#reading-in-data",
    "href": "dataframes_intro.html#reading-in-data",
    "title": "\n10  Data Frames\n",
    "section": "\n10.4 Reading in data",
    "text": "10.4 Reading in data\nR has many capabilities for reading in data. Many of the functions have names that help us to understand what data format is to be expected. In this case, the filename that we want to read ends in .csv, meaning comma-separated-values. The read.csv() function reads in .csv files. As usual, it is worth reading help('read.csv') to get a better sense of the possible bells-and-whistles.\nThe read.csv() function can read directly from a URL, so we do not need to download the file directly. This dataset is relatively large (about 16MB), so this may take a bit depending on your network connection speed.\n\noptions(width=60)\n\n\nurl = paste0(\n    'https://raw.githubusercontent.com',\n    '/bioconnector/workshops/master/data/brauer2007_tidy.csv'\n)\nydat &lt;- read.csv(url)\n\nOur variable, ydat, now “contains” the downloaded and read data. We can check to see what data type read.csv gave us:\n\nclass(ydat)\n\n[1] \"data.frame\"",
    "crumbs": [
      "Home",
      "R Data Structures",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Data Frames</span>"
    ]
  },
  {
    "objectID": "dataframes_intro.html#inspecting-data.frames",
    "href": "dataframes_intro.html#inspecting-data.frames",
    "title": "\n10  Data Frames\n",
    "section": "\n10.5 Inspecting data.frames",
    "text": "10.5 Inspecting data.frames\nOur ydat variable is a data.frame. As I mentioned, the dataset is fairly large, so we will not be able to look at it all at once on the screen. However, R gives us many tools to inspect a data.frame.\n\nOverviews of content\n\n\nhead() to show first few rows\n\ntail() to show last few rows\n\n\nSize\n\n\ndim() for dimensions (rows, columns)\nnrow()\nncol()\n\nobject.size() for power users interested in the memory used to store an object\n\n\nData and attribute summaries\n\n\ncolnames() to get the names of the columns\n\nrownames() to get the “names” of the rows–may not be present\n\nsummary() to get per-column summaries of the data in the data.frame.\n\n\n\n\nhead(ydat)\n\n  symbol systematic_name nutrient rate expression\n1   SFB2         YNL049C  Glucose 0.05      -0.24\n2   &lt;NA&gt;         YNL095C  Glucose 0.05       0.28\n3   QRI7         YDL104C  Glucose 0.05      -0.02\n4   CFT2         YLR115W  Glucose 0.05      -0.33\n5   SSO2         YMR183C  Glucose 0.05       0.05\n6   PSP2         YML017W  Glucose 0.05      -0.69\n                            bp\n1        ER to Golgi transport\n2   biological process unknown\n3 proteolysis and peptidolysis\n4      mRNA polyadenylylation*\n5              vesicle fusion*\n6   biological process unknown\n                             mf\n1    molecular function unknown\n2    molecular function unknown\n3 metalloendopeptidase activity\n4                   RNA binding\n5              t-SNARE activity\n6    molecular function unknown\n\ntail(ydat)\n\n       symbol systematic_name nutrient rate expression\n198425   DOA1         YKL213C   Uracil  0.3       0.14\n198426   KRE1         YNL322C   Uracil  0.3       0.28\n198427   MTL1         YGR023W   Uracil  0.3       0.27\n198428   KRE9         YJL174W   Uracil  0.3       0.43\n198429   UTH1         YKR042W   Uracil  0.3       0.19\n198430   &lt;NA&gt;         YOL111C   Uracil  0.3       0.04\n                                               bp\n198425    ubiquitin-dependent protein catabolism*\n198426      cell wall organization and biogenesis\n198427      cell wall organization and biogenesis\n198428     cell wall organization and biogenesis*\n198429 mitochondrion organization and biogenesis*\n198430                 biological process unknown\n                                        mf\n198425          molecular function unknown\n198426 structural constituent of cell wall\n198427          molecular function unknown\n198428          molecular function unknown\n198429          molecular function unknown\n198430          molecular function unknown\n\ndim(ydat)\n\n[1] 198430      7\n\nnrow(ydat)\n\n[1] 198430\n\nncol(ydat)\n\n[1] 7\n\ncolnames(ydat)\n\n[1] \"symbol\"          \"systematic_name\" \"nutrient\"       \n[4] \"rate\"            \"expression\"      \"bp\"             \n[7] \"mf\"             \n\nsummary(ydat)\n\n    symbol          systematic_name      nutrient        \n Length:198430      Length:198430      Length:198430     \n Class :character   Class :character   Class :character  \n Mode  :character   Mode  :character   Mode  :character  \n                                                         \n                                                         \n                                                         \n      rate          expression             bp           \n Min.   :0.0500   Min.   :-6.500000   Length:198430     \n 1st Qu.:0.1000   1st Qu.:-0.290000   Class :character  \n Median :0.2000   Median : 0.000000   Mode  :character  \n Mean   :0.1752   Mean   : 0.003367                     \n 3rd Qu.:0.2500   3rd Qu.: 0.290000                     \n Max.   :0.3000   Max.   : 6.640000                     \n      mf           \n Length:198430     \n Class :character  \n Mode  :character  \n                   \n                   \n                   \n\n\nIn RStudio, there is an additional function, View() (note the capital “V”) that opens the first 1000 rows (default) in the RStudio window, akin to a spreadsheet view.\n\nView(ydat)",
    "crumbs": [
      "Home",
      "R Data Structures",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Data Frames</span>"
    ]
  },
  {
    "objectID": "dataframes_intro.html#accessing-variables-columns-and-subsetting",
    "href": "dataframes_intro.html#accessing-variables-columns-and-subsetting",
    "title": "\n10  Data Frames\n",
    "section": "\n10.6 Accessing variables (columns) and subsetting",
    "text": "10.6 Accessing variables (columns) and subsetting\nIn R, data.frames can be subset similarly to other two-dimensional data structures. The [ in R is used to denote subsetting of any kind. When working with two-dimensional data, we need two values inside the [ ] to specify the details. The specification is [rows, columns]. For example, to get the first three rows of ydat, use:\n\nydat[1:3, ]\n\n  symbol systematic_name nutrient rate expression\n1   SFB2         YNL049C  Glucose 0.05      -0.24\n2   &lt;NA&gt;         YNL095C  Glucose 0.05       0.28\n3   QRI7         YDL104C  Glucose 0.05      -0.02\n                            bp\n1        ER to Golgi transport\n2   biological process unknown\n3 proteolysis and peptidolysis\n                             mf\n1    molecular function unknown\n2    molecular function unknown\n3 metalloendopeptidase activity\n\n\nNote how the second number, the columns, is blank. R takes that to mean “all the columns”. Similarly, we can combine rows and columns specification arbitrarily.\n\nydat[1:3, 1:3]\n\n  symbol systematic_name nutrient\n1   SFB2         YNL049C  Glucose\n2   &lt;NA&gt;         YNL095C  Glucose\n3   QRI7         YDL104C  Glucose\n\n\nBecause selecting a single variable, or column, is such a common operation, there are two shortcuts for doing so with data.frames. The first, the $ operator works like so:\n\n# Look at the column names, just to refresh memory\ncolnames(ydat)\n\n[1] \"symbol\"          \"systematic_name\" \"nutrient\"       \n[4] \"rate\"            \"expression\"      \"bp\"             \n[7] \"mf\"             \n\n# Note that I am using \"head\" here to limit the output\nhead(ydat$symbol)\n\n[1] \"SFB2\" NA     \"QRI7\" \"CFT2\" \"SSO2\" \"PSP2\"\n\n# What is the actual length of \"symbol\"?\nlength(ydat$symbol)\n\n[1] 198430\n\n\nThe second is related to the fact that, in R, data.frames are also lists. We subset a list by using [[]] notation. To get the second column of ydat, we can use:\n\nhead(ydat[[2]])\n\n[1] \"YNL049C\" \"YNL095C\" \"YDL104C\" \"YLR115W\" \"YMR183C\"\n[6] \"YML017W\"\n\n\nAlternatively, we can use the column name:\n\nhead(ydat[[\"systematic_name\"]])\n\n[1] \"YNL049C\" \"YNL095C\" \"YDL104C\" \"YLR115W\" \"YMR183C\"\n[6] \"YML017W\"\n\n\n\n10.6.1 Some data exploration\nThere are a couple of columns that include numeric values. Which columns are numeric?\n\nclass(ydat$symbol)\n\n[1] \"character\"\n\nclass(ydat$rate)\n\n[1] \"numeric\"\n\nclass(ydat$expression)\n\n[1] \"numeric\"\n\n\nMake histograms of: - the expression values - the rate values\nWhat does the table() function do? Could you use that to look a the rate column given that that column appears to have repeated values?\nWhat rate corresponds to the most nutrient-starved condition?\n\n10.6.2 More advanced indexing and subsetting\nWe can use, for example, logical values (TRUE/FALSE) to subset data.frames.\n\nhead(ydat[ydat$symbol == 'LEU1', ])\n\n     symbol systematic_name nutrient rate expression   bp\nNA     &lt;NA&gt;            &lt;NA&gt;     &lt;NA&gt;   NA         NA &lt;NA&gt;\nNA.1   &lt;NA&gt;            &lt;NA&gt;     &lt;NA&gt;   NA         NA &lt;NA&gt;\nNA.2   &lt;NA&gt;            &lt;NA&gt;     &lt;NA&gt;   NA         NA &lt;NA&gt;\nNA.3   &lt;NA&gt;            &lt;NA&gt;     &lt;NA&gt;   NA         NA &lt;NA&gt;\nNA.4   &lt;NA&gt;            &lt;NA&gt;     &lt;NA&gt;   NA         NA &lt;NA&gt;\nNA.5   &lt;NA&gt;            &lt;NA&gt;     &lt;NA&gt;   NA         NA &lt;NA&gt;\n       mf\nNA   &lt;NA&gt;\nNA.1 &lt;NA&gt;\nNA.2 &lt;NA&gt;\nNA.3 &lt;NA&gt;\nNA.4 &lt;NA&gt;\nNA.5 &lt;NA&gt;\n\ntail(ydat[ydat$symbol == 'LEU1', ])\n\n         symbol systematic_name nutrient rate expression\nNA.47244   &lt;NA&gt;            &lt;NA&gt;     &lt;NA&gt;   NA         NA\nNA.47245   &lt;NA&gt;            &lt;NA&gt;     &lt;NA&gt;   NA         NA\nNA.47246   &lt;NA&gt;            &lt;NA&gt;     &lt;NA&gt;   NA         NA\nNA.47247   &lt;NA&gt;            &lt;NA&gt;     &lt;NA&gt;   NA         NA\nNA.47248   &lt;NA&gt;            &lt;NA&gt;     &lt;NA&gt;   NA         NA\nNA.47249   &lt;NA&gt;            &lt;NA&gt;     &lt;NA&gt;   NA         NA\n           bp   mf\nNA.47244 &lt;NA&gt; &lt;NA&gt;\nNA.47245 &lt;NA&gt; &lt;NA&gt;\nNA.47246 &lt;NA&gt; &lt;NA&gt;\nNA.47247 &lt;NA&gt; &lt;NA&gt;\nNA.47248 &lt;NA&gt; &lt;NA&gt;\nNA.47249 &lt;NA&gt; &lt;NA&gt;\n\n\nWhat is the problem with this approach? It appears that there are a bunch of NA values. Taking a quick look at the symbol column, we see what the problem.\n\nsummary(ydat$symbol)\n\n   Length     Class      Mode \n   198430 character character \n\n\nUsing the is.na() function, we can make filter further to get down to values of interest.\n\nhead(ydat[ydat$symbol == 'LEU1' & !is.na(ydat$symbol), ])\n\n      symbol systematic_name nutrient rate expression\n1526    LEU1         YGL009C  Glucose 0.05      -1.12\n7043    LEU1         YGL009C  Glucose 0.10      -0.77\n12555   LEU1         YGL009C  Glucose 0.15      -0.67\n18071   LEU1         YGL009C  Glucose 0.20      -0.59\n23603   LEU1         YGL009C  Glucose 0.25      -0.20\n29136   LEU1         YGL009C  Glucose 0.30       0.03\n                        bp\n1526  leucine biosynthesis\n7043  leucine biosynthesis\n12555 leucine biosynthesis\n18071 leucine biosynthesis\n23603 leucine biosynthesis\n29136 leucine biosynthesis\n                                          mf\n1526  3-isopropylmalate dehydratase activity\n7043  3-isopropylmalate dehydratase activity\n12555 3-isopropylmalate dehydratase activity\n18071 3-isopropylmalate dehydratase activity\n23603 3-isopropylmalate dehydratase activity\n29136 3-isopropylmalate dehydratase activity\n\n\nSometimes, looking at the data themselves is not that important. Using dim() is one possibility to look at the number of rows and columns after subsetting.\n\ndim(ydat[ydat$expression &gt; 3, ])\n\n[1] 714   7\n\n\nFind the high expressed genes when leucine-starved. For this task we can also use subset which allows us to treat column names as R variables (no $ needed).\n\nsubset(ydat, nutrient == 'Leucine' & rate == 0.05 & expression &gt; 3)\n\n       symbol systematic_name nutrient rate expression\n133768   QDR2         YIL121W  Leucine 0.05       4.61\n133772   LEU1         YGL009C  Leucine 0.05       3.84\n133858   BAP3         YDR046C  Leucine 0.05       4.29\n135186   &lt;NA&gt;         YPL033C  Leucine 0.05       3.43\n135187   &lt;NA&gt;         YLR267W  Leucine 0.05       3.23\n135288   HXT3         YDR345C  Leucine 0.05       5.16\n135963   TPO2         YGR138C  Leucine 0.05       3.75\n135965   YRO2         YBR054W  Leucine 0.05       4.40\n136102   GPG1         YGL121C  Leucine 0.05       3.08\n136109  HSP42         YDR171W  Leucine 0.05       3.07\n136119   HXT5         YHR096C  Leucine 0.05       4.90\n136151   &lt;NA&gt;         YJL144W  Leucine 0.05       3.06\n136152   MOH1         YBL049W  Leucine 0.05       3.43\n136153   &lt;NA&gt;         YBL048W  Leucine 0.05       3.95\n136189  HSP26         YBR072W  Leucine 0.05       4.86\n136231   NCA3         YJL116C  Leucine 0.05       4.03\n136233   &lt;NA&gt;         YBR116C  Leucine 0.05       3.28\n136486   &lt;NA&gt;         YGR043C  Leucine 0.05       3.07\n137443   ADH2         YMR303C  Leucine 0.05       4.15\n137448   ICL1         YER065C  Leucine 0.05       3.54\n137451   SFC1         YJR095W  Leucine 0.05       3.72\n137569   MLS1         YNL117W  Leucine 0.05       3.76\n                                              bp\n133768                       multidrug transport\n133772                      leucine biosynthesis\n133858                      amino acid transport\n135186                                  meiosis*\n135187                biological process unknown\n135288                          hexose transport\n135963                       polyamine transport\n135965                biological process unknown\n136102                       signal transduction\n136109                       response to stress*\n136119                          hexose transport\n136151                   response to dessication\n136152                biological process unknown\n136153                                      &lt;NA&gt;\n136189                       response to stress*\n136231 mitochondrion organization and biogenesis\n136233                                      &lt;NA&gt;\n136486                biological process unknown\n137443                             fermentation*\n137448                          glyoxylate cycle\n137451                       fumarate transport*\n137569                          glyoxylate cycle\n                                           mf\n133768         multidrug efflux pump activity\n133772 3-isopropylmalate dehydratase activity\n133858        amino acid transporter activity\n135186             molecular function unknown\n135187             molecular function unknown\n135288          glucose transporter activity*\n135963          spermine transporter activity\n135965             molecular function unknown\n136102             signal transducer activity\n136109               unfolded protein binding\n136119          glucose transporter activity*\n136151             molecular function unknown\n136152             molecular function unknown\n136153                                   &lt;NA&gt;\n136189               unfolded protein binding\n136231             molecular function unknown\n136233                                   &lt;NA&gt;\n136486                 transaldolase activity\n137443         alcohol dehydrogenase activity\n137448              isocitrate lyase activity\n137451 succinate:fumarate antiporter activity\n137569               malate synthase activity",
    "crumbs": [
      "Home",
      "R Data Structures",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Data Frames</span>"
    ]
  },
  {
    "objectID": "dataframes_intro.html#aggregating-data",
    "href": "dataframes_intro.html#aggregating-data",
    "title": "\n10  Data Frames\n",
    "section": "\n10.7 Aggregating data",
    "text": "10.7 Aggregating data\nAggregating data, or summarizing by category, is a common way to look for trends or differences in measurements between categories. Use aggregate to find the mean expression by gene symbol.\n\nhead(aggregate(ydat$expression, by=list( ydat$symbol), mean))\n\n  Group.1           x\n1    AAC1  0.52888889\n2    AAC3 -0.21628571\n3   AAD10  0.43833333\n4   AAD14 -0.07166667\n5   AAD16  0.24194444\n6    AAD4 -0.79166667\n\n# or \nhead(aggregate(expression ~ symbol, mean, data=ydat))\n\n  symbol  expression\n1   AAC1  0.52888889\n2   AAC3 -0.21628571\n3  AAD10  0.43833333\n4  AAD14 -0.07166667\n5  AAD16  0.24194444\n6   AAD4 -0.79166667",
    "crumbs": [
      "Home",
      "R Data Structures",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Data Frames</span>"
    ]
  },
  {
    "objectID": "dataframes_intro.html#creating-a-data.frame-from-scratch",
    "href": "dataframes_intro.html#creating-a-data.frame-from-scratch",
    "title": "\n10  Data Frames\n",
    "section": "\n10.8 Creating a data.frame from scratch",
    "text": "10.8 Creating a data.frame from scratch\nSometimes it is useful to combine related data into one object. For example, let’s simulate some data.\n\nsmoker = factor(rep(c(\"smoker\", \"non-smoker\"), each=50))\nsmoker_numeric = as.numeric(smoker)\nx = rnorm(100)\nrisk = x + 2*smoker_numeric\n\nWe have two varibles, risk and smoker that are related. We can make a data.frame out of them:\n\nsmoker_risk = data.frame(smoker = smoker, risk = risk)\nhead(smoker_risk)\n\n  smoker     risk\n1 smoker 3.335436\n2 smoker 4.267366\n3 smoker 3.849471\n4 smoker 2.813967\n5 smoker 3.110960\n6 smoker 3.397461\n\n\nR also has plotting shortcuts that work with data.frames to simplify plotting\n\nplot( risk ~ smoker, data=smoker_risk)",
    "crumbs": [
      "Home",
      "R Data Structures",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Data Frames</span>"
    ]
  },
  {
    "objectID": "dataframes_intro.html#saving-a-data.frame",
    "href": "dataframes_intro.html#saving-a-data.frame",
    "title": "\n10  Data Frames\n",
    "section": "\n10.9 Saving a data.frame",
    "text": "10.9 Saving a data.frame\nOnce we have a data.frame of interest, we may want to save it. The most portable way to save a data.frame is to use one of the write functions. In this case, let’s save the data as a .csv file.\n\nwrite.csv(smoker_risk, \"smoker_risk.csv\")",
    "crumbs": [
      "Home",
      "R Data Structures",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Data Frames</span>"
    ]
  },
  {
    "objectID": "factors.html",
    "href": "factors.html",
    "title": "\n11  Factors\n",
    "section": "",
    "text": "11.1 Factors\nA factor is a special type of vector, normally used to hold a categorical variable–such as smoker/nonsmoker, state of residency, zipcode–in many statistical functions. Such vectors have class “factor”. Factors are primarily used in Analysis of Variance (ANOVA) or other situations when “categories” are needed. When a factor is used as a predictor variable, the corresponding indicator variables are created (more later).\nNote of caution that factors in R often appear to be character vectors when printed, but you will notice that they do not have double quotes around them. They are stored in R as numbers with a key name, so sometimes you will note that the factor behaves like a numeric vector.\n# create the character vector\ncitizen&lt;-c(\"uk\",\"us\",\"no\",\"au\",\"uk\",\"us\",\"us\",\"no\",\"au\") \n\n# convert to factor\ncitizenf&lt;-factor(citizen)                                \ncitizen             \n\n[1] \"uk\" \"us\" \"no\" \"au\" \"uk\" \"us\" \"us\" \"no\" \"au\"\n\ncitizenf\n\n[1] uk us no au uk us us no au\nLevels: au no uk us\n\n# convert factor back to character vector\nas.character(citizenf)\n\n[1] \"uk\" \"us\" \"no\" \"au\" \"uk\" \"us\" \"us\" \"no\" \"au\"\n\n# convert to numeric vector\nas.numeric(citizenf)\n\n[1] 3 4 2 1 3 4 4 2 1\nR stores many data structures as vectors with “attributes” and “class” (just so you have seen this).\nattributes(citizenf)\n\n$levels\n[1] \"au\" \"no\" \"uk\" \"us\"\n\n$class\n[1] \"factor\"\n\nclass(citizenf)\n\n[1] \"factor\"\n\n# note that after unclassing, we can see the \n# underlying numeric structure again\nunclass(citizenf)\n\n[1] 3 4 2 1 3 4 4 2 1\nattr(,\"levels\")\n[1] \"au\" \"no\" \"uk\" \"us\"\nTabulating factors is a useful way to get a sense of the “sample” set available.\ntable(citizenf)\n\ncitizenf\nau no uk us \n 2  2  2  3",
    "crumbs": [
      "Home",
      "R Data Structures",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Factors</span>"
    ]
  },
  {
    "objectID": "eda_overview.html",
    "href": "eda_overview.html",
    "title": "Exploratory data analysis",
    "section": "",
    "text": "Imagine you’re on an adventure, about to embark on a journey into the unknown. You’ve just been handed a treasure map, with the promise of valuable insights waiting to be discovered. This map is your data set, and the journey is exploratory data analysis (EDA).\nAs you begin your exploration, you start by getting a feel for the terrain. You take a broad, bird’s-eye view of the data, examining its structure and dimensions. Are you dealing with a vast landscape or a small, confined area? Are there any missing pieces in the map that you’ll need to account for? Understanding the overall context of your data set is crucial before venturing further.\nWith a sense of the landscape, you now zoom in to identify key landmarks in the data. You might look for unusual patterns, trends, or relationships between variables. As you spot these landmarks, you start asking questions: What’s causing that spike in values? Are these two factors related, or is it just a coincidence? By asking these questions, you’re actively engaging with the data and forming hypotheses that could guide future analysis or experiments.\nAs you continue your journey, you realize that the map alone isn’t enough to fully understand the terrain. You need more tools to bring the data to life. You start visualizing the data using charts, plots, and graphs. These visualizations act as your binoculars, allowing you to see patterns and relationships more clearly. Through them, you can uncover the hidden treasures buried within the data.\nEDA isn’t a linear path from start to finish. As you explore, you’ll find yourself circling back to previous points, refining your questions, and digging deeper. The process is iterative, with each new discovery informing the next. And as you go, you’ll gain a deeper understanding of the data’s underlying structure and potential.\nFinally, after your thorough exploration, you’ll have a solid foundation to build upon. You’ll be better equipped to make informed decisions, test hypotheses, and draw meaningful conclusions. The insights you’ve gained through EDA will serve as a compass, guiding you towards the true value hidden within your data. And with that, you’ve successfully completed your journey through exploratory data analysis.",
    "crumbs": [
      "Home",
      "Exploratory data analysis"
    ]
  },
  {
    "objectID": "dplyr_intro_msleep.html",
    "href": "dplyr_intro_msleep.html",
    "title": "\n12  Introduction to dplyr: mammal sleep dataset\n",
    "section": "",
    "text": "12.1 Learning goals\nThe dataset we will be using to introduce the dplyr package is an updated and expanded version of the mammals sleep dataset. Updated sleep times and weights were taken from V. M. Savage and G. B. West. A quantitative, theoretical framework for understanding mammalian sleep1.",
    "crumbs": [
      "Home",
      "Exploratory data analysis",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Introduction to dplyr: mammal sleep dataset</span>"
    ]
  },
  {
    "objectID": "dplyr_intro_msleep.html#learning-goals",
    "href": "dplyr_intro_msleep.html#learning-goals",
    "title": "\n12  Introduction to dplyr: mammal sleep dataset\n",
    "section": "",
    "text": "Know that dplyr is just a different approach to manipulating data in data.frames.\nList the commonly used dplyr verbs and how they can be used to manipulate data.frames.\nShow how to aggregate and summarized data using dplyr\n\nKnow what the piping operator, |&gt;, is and how it can be used.",
    "crumbs": [
      "Home",
      "Exploratory data analysis",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Introduction to dplyr: mammal sleep dataset</span>"
    ]
  },
  {
    "objectID": "dplyr_intro_msleep.html#learning-objectives",
    "href": "dplyr_intro_msleep.html#learning-objectives",
    "title": "\n12  Introduction to dplyr: mammal sleep dataset\n",
    "section": "\n12.2 Learning objectives",
    "text": "12.2 Learning objectives\n\nSelect subsets of the mammal sleep dataset.\nReorder the dataset.\nAdd columns to the dataset based on existing columns.\nSummarize the amount of sleep by categorical variables using group_by and summarize.",
    "crumbs": [
      "Home",
      "Exploratory data analysis",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Introduction to dplyr: mammal sleep dataset</span>"
    ]
  },
  {
    "objectID": "dplyr_intro_msleep.html#what-is-dplyr",
    "href": "dplyr_intro_msleep.html#what-is-dplyr",
    "title": "\n12  Introduction to dplyr: mammal sleep dataset\n",
    "section": "\n12.3 What is dplyr?",
    "text": "12.3 What is dplyr?\nThe dplyr package is a specialized package for working with data.frames (and the related tibble) to transform and summarize tabular data with rows and columns. For another explanation of dplyr see the dplyr package vignette: Introduction to dplyr",
    "crumbs": [
      "Home",
      "Exploratory data analysis",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Introduction to dplyr: mammal sleep dataset</span>"
    ]
  },
  {
    "objectID": "dplyr_intro_msleep.html#why-is-dplyr-userful",
    "href": "dplyr_intro_msleep.html#why-is-dplyr-userful",
    "title": "\n12  Introduction to dplyr: mammal sleep dataset\n",
    "section": "\n12.4 Why Is dplyr userful?",
    "text": "12.4 Why Is dplyr userful?\ndplyr contains a set of functions–commonly called the dplyr “verbs”–that perform common data manipulations such as filtering for rows, selecting specific columns, re-ordering rows, adding new columns and summarizing data. In addition, dplyr contains a useful function to perform another common task which is the “split-apply-combine” concept.\nCompared to base functions in R, the functions in dplyr are often easier to work with, are more consistent in the syntax and are targeted for data analysis around data frames, instead of just vectors.",
    "crumbs": [
      "Home",
      "Exploratory data analysis",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Introduction to dplyr: mammal sleep dataset</span>"
    ]
  },
  {
    "objectID": "dplyr_intro_msleep.html#data-mammals-sleep",
    "href": "dplyr_intro_msleep.html#data-mammals-sleep",
    "title": "\n12  Introduction to dplyr: mammal sleep dataset\n",
    "section": "\n12.5 Data: Mammals Sleep",
    "text": "12.5 Data: Mammals Sleep\nThe msleep (mammals sleep) data set contains the sleep times and weights for a set of mammals and is available in the dagdata repository on github. This data set contains 83 rows and 11 variables. The data happen to be available as a dataset in the ggplot2 package. To get access to the msleep dataset, we need to first install the ggplot2 package.\n\ninstall.packages('ggplot2')\n\nThen, we can load the library.\n\nlibrary(ggplot2)\ndata(msleep)\n\nAs with many datasets in R, “help” is available to describe the dataset itself.\n\n?msleep\n\nThe columns are described in the help page, but are included here, also.\n\n\ncolumn name\nDescription\n\n\n\nname\ncommon name\n\n\ngenus\ntaxonomic rank\n\n\nvore\ncarnivore, omnivore or herbivore?\n\n\norder\ntaxonomic rank\n\n\nconservation\nthe conservation status of the mammal\n\n\nsleep_total\ntotal amount of sleep, in hours\n\n\nsleep_rem\nrem sleep, in hours\n\n\nsleep_cycle\nlength of sleep cycle, in hours\n\n\nawake\namount of time spent awake, in hours\n\n\nbrainwt\nbrain weight in kilograms\n\n\nbodywt\nbody weight in kilograms",
    "crumbs": [
      "Home",
      "Exploratory data analysis",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Introduction to dplyr: mammal sleep dataset</span>"
    ]
  },
  {
    "objectID": "dplyr_intro_msleep.html#dplyr-verbs",
    "href": "dplyr_intro_msleep.html#dplyr-verbs",
    "title": "\n12  Introduction to dplyr: mammal sleep dataset\n",
    "section": "\n12.6 dplyr verbs",
    "text": "12.6 dplyr verbs\nThe dplyr verbs are listed here. There are many other functions available in dplyr, but we will focus on just these.\n\n\n\n\n\n\ndplyr verbs\nDescription\n\n\n\nselect()\nselect columns\n\n\nfilter()\nfilter rows\n\n\narrange()\nre-order or arrange rows\n\n\nmutate()\ncreate new columns\n\n\nsummarise()\nsummarise values\n\n\ngroup_by()\nallows for group operations in the “split-apply-combine” concept",
    "crumbs": [
      "Home",
      "Exploratory data analysis",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Introduction to dplyr: mammal sleep dataset</span>"
    ]
  },
  {
    "objectID": "dplyr_intro_msleep.html#using-the-dplyr-verbs",
    "href": "dplyr_intro_msleep.html#using-the-dplyr-verbs",
    "title": "\n12  Introduction to dplyr: mammal sleep dataset\n",
    "section": "\n12.7 Using the dplyr verbs",
    "text": "12.7 Using the dplyr verbs\nThe two most basic functions are select() and filter(), which selects columns and filters rows respectively. What are the equivalent ways to select columns without dplyr? And filtering to include only specific rows?\nBefore proceeding, we need to install the dplyr package:\n\ninstall.packages('dplyr')\n\nAnd then load the library:\n\nlibrary(dplyr)\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\n\n\n12.7.1 Selecting columns: select()\n\nSelect a set of columns such as the name and the sleep_total columns.\n\nsleepData &lt;- select(msleep, name, sleep_total)\nhead(sleepData)\n\n# A tibble: 6 × 2\n  name                       sleep_total\n  &lt;chr&gt;                            &lt;dbl&gt;\n1 Cheetah                           12.1\n2 Owl monkey                        17  \n3 Mountain beaver                   14.4\n4 Greater short-tailed shrew        14.9\n5 Cow                                4  \n6 Three-toed sloth                  14.4\n\n\nTo select all the columns except a specific column, use the “-” (subtraction) operator (also known as negative indexing). For example, to select all columns except name:\n\nhead(select(msleep, -name))\n\n# A tibble: 6 × 10\n  genus      vore  order    conservation sleep_total sleep_rem sleep_cycle awake\n  &lt;chr&gt;      &lt;chr&gt; &lt;chr&gt;    &lt;chr&gt;              &lt;dbl&gt;     &lt;dbl&gt;       &lt;dbl&gt; &lt;dbl&gt;\n1 Acinonyx   carni Carnivo… lc                  12.1      NA        NA      11.9\n2 Aotus      omni  Primates &lt;NA&gt;                17         1.8      NA       7  \n3 Aplodontia herbi Rodentia nt                  14.4       2.4      NA       9.6\n4 Blarina    omni  Soricom… lc                  14.9       2.3       0.133   9.1\n5 Bos        herbi Artioda… domesticated         4         0.7       0.667  20  \n6 Bradypus   herbi Pilosa   &lt;NA&gt;                14.4       2.2       0.767   9.6\n# ℹ 2 more variables: brainwt &lt;dbl&gt;, bodywt &lt;dbl&gt;\n\n\nTo select a range of columns by name, use the “:” operator. Note that dplyr allows us to use the column names without quotes and as “indices” of the columns.\n\nhead(select(msleep, name:order))\n\n# A tibble: 6 × 4\n  name                       genus      vore  order       \n  &lt;chr&gt;                      &lt;chr&gt;      &lt;chr&gt; &lt;chr&gt;       \n1 Cheetah                    Acinonyx   carni Carnivora   \n2 Owl monkey                 Aotus      omni  Primates    \n3 Mountain beaver            Aplodontia herbi Rodentia    \n4 Greater short-tailed shrew Blarina    omni  Soricomorpha\n5 Cow                        Bos        herbi Artiodactyla\n6 Three-toed sloth           Bradypus   herbi Pilosa      \n\n\nTo select all columns that start with the character string “sl”, use the function starts_with().\n\nhead(select(msleep, starts_with(\"sl\")))\n\n# A tibble: 6 × 3\n  sleep_total sleep_rem sleep_cycle\n        &lt;dbl&gt;     &lt;dbl&gt;       &lt;dbl&gt;\n1        12.1      NA        NA    \n2        17         1.8      NA    \n3        14.4       2.4      NA    \n4        14.9       2.3       0.133\n5         4         0.7       0.667\n6        14.4       2.2       0.767\n\n\nSome additional options to select columns based on a specific criteria include:\n\n\nends_with() = Select columns that end with a character string\n\ncontains() = Select columns that contain a character string\n\nmatches() = Select columns that match a regular expression\n\none_of() = Select column names that are from a group of names\n\n12.7.2 Selecting rows: filter()\n\nThe filter() function allows us to filter rows to include only those rows that match the filter. For example, we can filter the rows for mammals that sleep a total of more than 16 hours.\n\nfilter(msleep, sleep_total &gt;= 16)\n\n# A tibble: 8 × 11\n  name    genus vore  order conservation sleep_total sleep_rem sleep_cycle awake\n  &lt;chr&gt;   &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt;              &lt;dbl&gt;     &lt;dbl&gt;       &lt;dbl&gt; &lt;dbl&gt;\n1 Owl mo… Aotus omni  Prim… &lt;NA&gt;                17         1.8      NA       7  \n2 Long-n… Dasy… carni Cing… lc                  17.4       3.1       0.383   6.6\n3 North … Dide… omni  Dide… lc                  18         4.9       0.333   6  \n4 Big br… Epte… inse… Chir… lc                  19.7       3.9       0.117   4.3\n5 Thick-… Lutr… carni Dide… lc                  19.4       6.6      NA       4.6\n6 Little… Myot… inse… Chir… &lt;NA&gt;                19.9       2         0.2     4.1\n7 Giant … Prio… inse… Cing… en                  18.1       6.1      NA       5.9\n8 Arctic… Sper… herbi Rode… lc                  16.6      NA        NA       7.4\n# ℹ 2 more variables: brainwt &lt;dbl&gt;, bodywt &lt;dbl&gt;\n\n\nFilter the rows for mammals that sleep a total of more than 16 hours and have a body weight of greater than 1 kilogram.\n\nfilter(msleep, sleep_total &gt;= 16, bodywt &gt;= 1)\n\n# A tibble: 3 × 11\n  name    genus vore  order conservation sleep_total sleep_rem sleep_cycle awake\n  &lt;chr&gt;   &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt;              &lt;dbl&gt;     &lt;dbl&gt;       &lt;dbl&gt; &lt;dbl&gt;\n1 Long-n… Dasy… carni Cing… lc                  17.4       3.1       0.383   6.6\n2 North … Dide… omni  Dide… lc                  18         4.9       0.333   6  \n3 Giant … Prio… inse… Cing… en                  18.1       6.1      NA       5.9\n# ℹ 2 more variables: brainwt &lt;dbl&gt;, bodywt &lt;dbl&gt;\n\n\nFilter the rows for mammals in the Perissodactyla and Primates taxonomic order. The %in% operator is a logical operator that returns TRUE for values of a vector that are present in a second vector.\n\nfilter(msleep, order %in% c(\"Perissodactyla\", \"Primates\"))\n\n# A tibble: 15 × 11\n   name   genus vore  order conservation sleep_total sleep_rem sleep_cycle awake\n   &lt;chr&gt;  &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt;              &lt;dbl&gt;     &lt;dbl&gt;       &lt;dbl&gt; &lt;dbl&gt;\n 1 Owl m… Aotus omni  Prim… &lt;NA&gt;                17         1.8      NA       7  \n 2 Grivet Cerc… omni  Prim… lc                  10         0.7      NA      14  \n 3 Horse  Equus herbi Peri… domesticated         2.9       0.6       1      21.1\n 4 Donkey Equus herbi Peri… domesticated         3.1       0.4      NA      20.9\n 5 Patas… Eryt… omni  Prim… lc                  10.9       1.1      NA      13.1\n 6 Galago Gala… omni  Prim… &lt;NA&gt;                 9.8       1.1       0.55   14.2\n 7 Human  Homo  omni  Prim… &lt;NA&gt;                 8         1.9       1.5    16  \n 8 Mongo… Lemur herbi Prim… vu                   9.5       0.9      NA      14.5\n 9 Macaq… Maca… omni  Prim… &lt;NA&gt;                10.1       1.2       0.75   13.9\n10 Slow … Nyct… carni Prim… &lt;NA&gt;                11        NA        NA      13  \n11 Chimp… Pan   omni  Prim… &lt;NA&gt;                 9.7       1.4       1.42   14.3\n12 Baboon Papio omni  Prim… &lt;NA&gt;                 9.4       1         0.667  14.6\n13 Potto  Pero… omni  Prim… lc                  11        NA        NA      13  \n14 Squir… Saim… omni  Prim… &lt;NA&gt;                 9.6       1.4      NA      14.4\n15 Brazi… Tapi… herbi Peri… vu                   4.4       1         0.9    19.6\n# ℹ 2 more variables: brainwt &lt;dbl&gt;, bodywt &lt;dbl&gt;\n\n\nYou can use the boolean operators (e.g. &gt;, &lt;, &gt;=, &lt;=, !=, %in%) to create the logical tests.",
    "crumbs": [
      "Home",
      "Exploratory data analysis",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Introduction to dplyr: mammal sleep dataset</span>"
    ]
  },
  {
    "objectID": "dplyr_intro_msleep.html#piping-with",
    "href": "dplyr_intro_msleep.html#piping-with",
    "title": "\n12  Introduction to dplyr: mammal sleep dataset\n",
    "section": "\n12.8 “Piping”” with |>\n",
    "text": "12.8 “Piping”” with |&gt;\n\nIt is not unusual to want to perform a set of operations using dplyr. The pipe operator |&gt; allows us to “pipe” the output from one function into the input of the next. While there is nothing special about how R treats operations that are written in a pipe, the idea of piping is to allow us to read multiple functions operating one after another from left-to-right. Without piping, one would either 1) save each step in set of functions as a temporary variable and then pass that variable along the chain or 2) have to “nest” functions, which can be hard to read.\nHere’s an example we have already used:\n\nhead(select(msleep, name, sleep_total))\n\n# A tibble: 6 × 2\n  name                       sleep_total\n  &lt;chr&gt;                            &lt;dbl&gt;\n1 Cheetah                           12.1\n2 Owl monkey                        17  \n3 Mountain beaver                   14.4\n4 Greater short-tailed shrew        14.9\n5 Cow                                4  \n6 Three-toed sloth                  14.4\n\n\nNow in this case, we will pipe the msleep data frame to the function that will select two columns (name and sleep\\_total) and then pipe the new data frame to the function head(), which will return the head of the new data frame.\n\nmsleep |&gt; \n    select(name, sleep_total) |&gt; \n    head()\n\n# A tibble: 6 × 2\n  name                       sleep_total\n  &lt;chr&gt;                            &lt;dbl&gt;\n1 Cheetah                           12.1\n2 Owl monkey                        17  \n3 Mountain beaver                   14.4\n4 Greater short-tailed shrew        14.9\n5 Cow                                4  \n6 Three-toed sloth                  14.4\n\n\nYou will soon see how useful the pipe operator is when we start to combine many functions.\nNow that you know about the pipe operator (|&gt;), we will use it throughout the rest of this tutorial.\n\n12.8.1 Arrange Or Re-order Rows Using arrange()\n\nTo arrange (or re-order) rows by a particular column, such as the taxonomic order, list the name of the column you want to arrange the rows by:\n\nmsleep |&gt; arrange(order) |&gt; head()\n\n# A tibble: 6 × 11\n  name    genus vore  order conservation sleep_total sleep_rem sleep_cycle awake\n  &lt;chr&gt;   &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt;              &lt;dbl&gt;     &lt;dbl&gt;       &lt;dbl&gt; &lt;dbl&gt;\n1 Tenrec  Tenr… omni  Afro… &lt;NA&gt;                15.6       2.3      NA       8.4\n2 Cow     Bos   herbi Arti… domesticated         4         0.7       0.667  20  \n3 Roe de… Capr… herbi Arti… lc                   3        NA        NA      21  \n4 Goat    Capri herbi Arti… lc                   5.3       0.6      NA      18.7\n5 Giraffe Gira… herbi Arti… cd                   1.9       0.4      NA      22.1\n6 Sheep   Ovis  herbi Arti… domesticated         3.8       0.6      NA      20.2\n# ℹ 2 more variables: brainwt &lt;dbl&gt;, bodywt &lt;dbl&gt;\n\n\nNow we will select three columns from msleep, arrange the rows by the taxonomic order and then arrange the rows by sleep_total. Finally, show the head of the final data frame:\n\nmsleep |&gt; \n    select(name, order, sleep_total) |&gt;\n    arrange(order, sleep_total) |&gt; \n    head()\n\n# A tibble: 6 × 3\n  name     order        sleep_total\n  &lt;chr&gt;    &lt;chr&gt;              &lt;dbl&gt;\n1 Tenrec   Afrosoricida        15.6\n2 Giraffe  Artiodactyla         1.9\n3 Roe deer Artiodactyla         3  \n4 Sheep    Artiodactyla         3.8\n5 Cow      Artiodactyla         4  \n6 Goat     Artiodactyla         5.3\n\n\nSame as above, except here we filter the rows for mammals that sleep for 16 or more hours, instead of showing the head of the final data frame:\n\nmsleep |&gt; \n    select(name, order, sleep_total) |&gt;\n    arrange(order, sleep_total) |&gt; \n    filter(sleep_total &gt;= 16)\n\n# A tibble: 8 × 3\n  name                   order           sleep_total\n  &lt;chr&gt;                  &lt;chr&gt;                 &lt;dbl&gt;\n1 Big brown bat          Chiroptera             19.7\n2 Little brown bat       Chiroptera             19.9\n3 Long-nosed armadillo   Cingulata              17.4\n4 Giant armadillo        Cingulata              18.1\n5 North American Opossum Didelphimorphia        18  \n6 Thick-tailed opposum   Didelphimorphia        19.4\n7 Owl monkey             Primates               17  \n8 Arctic ground squirrel Rodentia               16.6\n\n\nFor something slightly more complicated do the same as above, except arrange the rows in the sleep_total column in a descending order. For this, use the function desc()\n\nmsleep |&gt; \n    select(name, order, sleep_total) |&gt;\n    arrange(order, desc(sleep_total)) |&gt; \n    filter(sleep_total &gt;= 16)\n\n# A tibble: 8 × 3\n  name                   order           sleep_total\n  &lt;chr&gt;                  &lt;chr&gt;                 &lt;dbl&gt;\n1 Little brown bat       Chiroptera             19.9\n2 Big brown bat          Chiroptera             19.7\n3 Giant armadillo        Cingulata              18.1\n4 Long-nosed armadillo   Cingulata              17.4\n5 Thick-tailed opposum   Didelphimorphia        19.4\n6 North American Opossum Didelphimorphia        18  \n7 Owl monkey             Primates               17  \n8 Arctic ground squirrel Rodentia               16.6",
    "crumbs": [
      "Home",
      "Exploratory data analysis",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Introduction to dplyr: mammal sleep dataset</span>"
    ]
  },
  {
    "objectID": "dplyr_intro_msleep.html#create-new-columns-using-mutate",
    "href": "dplyr_intro_msleep.html#create-new-columns-using-mutate",
    "title": "\n12  Introduction to dplyr: mammal sleep dataset\n",
    "section": "\n12.9 Create New Columns Using mutate()\n",
    "text": "12.9 Create New Columns Using mutate()\n\nThe mutate() function will add new columns to the data frame. Create a new column called rem_proportion, which is the ratio of rem sleep to total amount of sleep.\n\nmsleep |&gt; \n    mutate(rem_proportion = sleep_rem / sleep_total) |&gt;\n    head()\n\n# A tibble: 6 × 12\n  name    genus vore  order conservation sleep_total sleep_rem sleep_cycle awake\n  &lt;chr&gt;   &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt;              &lt;dbl&gt;     &lt;dbl&gt;       &lt;dbl&gt; &lt;dbl&gt;\n1 Cheetah Acin… carni Carn… lc                  12.1      NA        NA      11.9\n2 Owl mo… Aotus omni  Prim… &lt;NA&gt;                17         1.8      NA       7  \n3 Mounta… Aplo… herbi Rode… nt                  14.4       2.4      NA       9.6\n4 Greate… Blar… omni  Sori… lc                  14.9       2.3       0.133   9.1\n5 Cow     Bos   herbi Arti… domesticated         4         0.7       0.667  20  \n6 Three-… Brad… herbi Pilo… &lt;NA&gt;                14.4       2.2       0.767   9.6\n# ℹ 3 more variables: brainwt &lt;dbl&gt;, bodywt &lt;dbl&gt;, rem_proportion &lt;dbl&gt;\n\n\nYou can add many new columns using mutate (separated by commas). Here we add a second column called bodywt_grams which is the bodywt column in grams.\n\nmsleep |&gt; \n    mutate(rem_proportion = sleep_rem / sleep_total, \n           bodywt_grams = bodywt * 1000) |&gt;\n    head()\n\n# A tibble: 6 × 13\n  name    genus vore  order conservation sleep_total sleep_rem sleep_cycle awake\n  &lt;chr&gt;   &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt;              &lt;dbl&gt;     &lt;dbl&gt;       &lt;dbl&gt; &lt;dbl&gt;\n1 Cheetah Acin… carni Carn… lc                  12.1      NA        NA      11.9\n2 Owl mo… Aotus omni  Prim… &lt;NA&gt;                17         1.8      NA       7  \n3 Mounta… Aplo… herbi Rode… nt                  14.4       2.4      NA       9.6\n4 Greate… Blar… omni  Sori… lc                  14.9       2.3       0.133   9.1\n5 Cow     Bos   herbi Arti… domesticated         4         0.7       0.667  20  \n6 Three-… Brad… herbi Pilo… &lt;NA&gt;                14.4       2.2       0.767   9.6\n# ℹ 4 more variables: brainwt &lt;dbl&gt;, bodywt &lt;dbl&gt;, rem_proportion &lt;dbl&gt;,\n#   bodywt_grams &lt;dbl&gt;\n\n\nIs there a relationship between rem_proportion and bodywt? How about sleep_total?\n\n12.9.1 Create summaries: summarise()\n\nThe summarise() function will create summary statistics for a given column in the data frame such as finding the mean. For example, to compute the average number of hours of sleep, apply the mean() function to the column sleep_total and call the summary value avg_sleep.\n\nmsleep |&gt; \n    summarise(avg_sleep = mean(sleep_total))\n\n# A tibble: 1 × 1\n  avg_sleep\n      &lt;dbl&gt;\n1      10.4\n\n\nThere are many other summary statistics you could consider such sd(), min(), max(), median(), sum(), n() (returns the length of vector), first() (returns first value in vector), last() (returns last value in vector) and n_distinct() (number of distinct values in vector).\n\nmsleep |&gt; \n    summarise(avg_sleep = mean(sleep_total), \n              min_sleep = min(sleep_total),\n              max_sleep = max(sleep_total),\n              total = n())\n\n# A tibble: 1 × 4\n  avg_sleep min_sleep max_sleep total\n      &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt; &lt;int&gt;\n1      10.4       1.9      19.9    83",
    "crumbs": [
      "Home",
      "Exploratory data analysis",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Introduction to dplyr: mammal sleep dataset</span>"
    ]
  },
  {
    "objectID": "dplyr_intro_msleep.html#grouping-data-group_by",
    "href": "dplyr_intro_msleep.html#grouping-data-group_by",
    "title": "\n12  Introduction to dplyr: mammal sleep dataset\n",
    "section": "\n12.10 Grouping data: group_by()\n",
    "text": "12.10 Grouping data: group_by()\n\nThe group_by() verb is an important function in dplyr. The group_by allows us to use the concept of “split-apply-combine”. We literally want to split the data frame by some variable (e.g. taxonomic order), apply a function to the individual data frames and then combine the output. This approach is similar to the aggregate function from R, but group_by integrates with dplyr.\nLet’s do that: split the msleep data frame by the taxonomic order, then ask for the same summary statistics as above. We expect a set of summary statistics for each taxonomic order.\n\nmsleep |&gt; \n    group_by(order) |&gt;\n    summarise(avg_sleep = mean(sleep_total), \n              min_sleep = min(sleep_total), \n              max_sleep = max(sleep_total),\n              total = n())\n\n# A tibble: 19 × 5\n   order           avg_sleep min_sleep max_sleep total\n   &lt;chr&gt;               &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt; &lt;int&gt;\n 1 Afrosoricida        15.6       15.6      15.6     1\n 2 Artiodactyla         4.52       1.9       9.1     6\n 3 Carnivora           10.1        3.5      15.8    12\n 4 Cetacea              4.5        2.7       5.6     3\n 5 Chiroptera          19.8       19.7      19.9     2\n 6 Cingulata           17.8       17.4      18.1     2\n 7 Didelphimorphia     18.7       18        19.4     2\n 8 Diprotodontia       12.4       11.1      13.7     2\n 9 Erinaceomorpha      10.2       10.1      10.3     2\n10 Hyracoidea           5.67       5.3       6.3     3\n11 Lagomorpha           8.4        8.4       8.4     1\n12 Monotremata          8.6        8.6       8.6     1\n13 Perissodactyla       3.47       2.9       4.4     3\n14 Pilosa              14.4       14.4      14.4     1\n15 Primates            10.5        8        17      12\n16 Proboscidea          3.6        3.3       3.9     2\n17 Rodentia            12.5        7        16.6    22\n18 Scandentia           8.9        8.9       8.9     1\n19 Soricomorpha        11.1        8.4      14.9     5",
    "crumbs": [
      "Home",
      "Exploratory data analysis",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Introduction to dplyr: mammal sleep dataset</span>"
    ]
  },
  {
    "objectID": "eda_and_univariate_brfss.html",
    "href": "eda_and_univariate_brfss.html",
    "title": "\n13  Case Study: Behavioral Risk Factor Surveillance System\n",
    "section": "",
    "text": "13.1 A Case Study on the Behavioral Risk Factor Surveillance System\nThe Behavioral Risk Factor Surveillance System (BRFSS) is a large-scale health survey conducted annually by the Centers for Disease Control and Prevention (CDC) in the United States. The BRFSS collects information on various health-related behaviors, chronic health conditions, and the use of preventive services among the adult population (18 years and older) through telephone interviews. The main goal of the BRFSS is to identify and monitor the prevalence of risk factors associated with chronic diseases, inform public health policies, and evaluate the effectiveness of health promotion and disease prevention programs. The data collected through BRFSS is crucial for understanding the health status and needs of the population, and it serves as a valuable resource for researchers, policy makers, and healthcare professionals in making informed decisions and designing targeted interventions.\nIn this chapter, we will walk through an exploratory data analysis (EDA) of the Behavioral Risk Factor Surveillance System dataset using R. EDA is an important step in the data analysis process, as it helps you to understand your data, identify trends, and detect any anomalies before performing more advanced analyses. We will use various R functions and packages to explore the dataset, with a focus on active learning and hands-on experience.",
    "crumbs": [
      "Home",
      "Exploratory data analysis",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Case Study: Behavioral Risk Factor Surveillance System</span>"
    ]
  },
  {
    "objectID": "eda_and_univariate_brfss.html#loading-the-dataset",
    "href": "eda_and_univariate_brfss.html#loading-the-dataset",
    "title": "\n13  Case Study: Behavioral Risk Factor Surveillance System\n",
    "section": "\n13.2 Loading the Dataset",
    "text": "13.2 Loading the Dataset\nFirst, let’s load the dataset into R. We will use the read.csv() function from the base R package to read the data and store it in a data frame called brfss. Make sure the CSV file is in your working directory, or provide the full path to the file.\nFirst, we need to get the data. Either download the data from THIS LINK or have R do it directly from the command-line (preferred):\n\ndownload.file('https://raw.githubusercontent.com/seandavi/ITR/master/BRFSS-subset.csv',\n              destfile = 'BRFSS-subset.csv')\n\n\n\npath &lt;- file.choose()    # look for BRFSS-subset.csv\n\n\nstopifnot(file.exists(path))\nbrfss &lt;- read.csv(path)",
    "crumbs": [
      "Home",
      "Exploratory data analysis",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Case Study: Behavioral Risk Factor Surveillance System</span>"
    ]
  },
  {
    "objectID": "eda_and_univariate_brfss.html#inspecting-the-data",
    "href": "eda_and_univariate_brfss.html#inspecting-the-data",
    "title": "\n13  Case Study: Behavioral Risk Factor Surveillance System\n",
    "section": "\n13.3 Inspecting the Data",
    "text": "13.3 Inspecting the Data\nOnce the data is loaded, let’s take a look at the first few rows of the dataset using the head() function:\n\nhead(brfss)\n\n  Age   Weight    Sex Height Year\n1  31 48.98798 Female 157.48 1990\n2  57 81.64663 Female 157.48 1990\n3  43 80.28585   Male 177.80 1990\n4  72 70.30682   Male 170.18 1990\n5  31 49.89516 Female 154.94 1990\n6  58 54.43108 Female 154.94 1990\n\n\nThis will display the first six rows of the dataset, allowing you to get a feel for the data structure and variable types.\nNext, let’s check the dimensions of the dataset using the dim() function:\n\ndim(brfss)\n\n[1] 20000     5\n\n\nThis will return the number of rows and columns in the dataset, which is important to know for subsequent analyses.",
    "crumbs": [
      "Home",
      "Exploratory data analysis",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Case Study: Behavioral Risk Factor Surveillance System</span>"
    ]
  },
  {
    "objectID": "eda_and_univariate_brfss.html#summary-statistics",
    "href": "eda_and_univariate_brfss.html#summary-statistics",
    "title": "\n13  Case Study: Behavioral Risk Factor Surveillance System\n",
    "section": "\n13.4 Summary Statistics",
    "text": "13.4 Summary Statistics\nNow that we have a basic understanding of the data structure, let’s calculate some summary statistics. The summary() function in R provides a quick overview of the main statistics for each variable in the dataset:\n\nsummary(brfss)\n\n      Age            Weight           Sex                Height     \n Min.   :18.00   Min.   : 34.93   Length:20000       Min.   :105.0  \n 1st Qu.:36.00   1st Qu.: 61.69   Class :character   1st Qu.:162.6  \n Median :51.00   Median : 72.57   Mode  :character   Median :168.0  \n Mean   :50.99   Mean   : 75.42                      Mean   :169.2  \n 3rd Qu.:65.00   3rd Qu.: 86.18                      3rd Qu.:177.8  \n Max.   :99.00   Max.   :278.96                      Max.   :218.0  \n NA's   :139     NA's   :649                         NA's   :184    \n      Year     \n Min.   :1990  \n 1st Qu.:1990  \n Median :2000  \n Mean   :2000  \n 3rd Qu.:2010  \n Max.   :2010  \n               \n\n\nThis will display the minimum, first quartile, median, mean, third quartile, and maximum for each numeric variable, and the frequency counts for each factor level for categorical variables.",
    "crumbs": [
      "Home",
      "Exploratory data analysis",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Case Study: Behavioral Risk Factor Surveillance System</span>"
    ]
  },
  {
    "objectID": "eda_and_univariate_brfss.html#data-visualization",
    "href": "eda_and_univariate_brfss.html#data-visualization",
    "title": "\n13  Case Study: Behavioral Risk Factor Surveillance System\n",
    "section": "\n13.5 Data Visualization",
    "text": "13.5 Data Visualization\nVisualizing the data can help you identify patterns and trends in the dataset. Let’s start by creating a histogram of the Age variable using the hist() function.\nThis will create a histogram showing the frequency distribution of ages in the dataset. You can customize the appearance of the histogram by adjusting the parameters within the hist() function.\n\nhist(brfss$Age, main = \"Age Distribution\", \n     xlab = \"Age\", col = \"lightblue\")\n\n\n\n\n\n\n\n\n\n\n\n\n\nWhat are the options for a histogram?\n\n\n\nThe hist() function has many options. For example, you can change the number of bins, the color of the bars, the title, and the x-axis label. You can also add a vertical line at the mean or median, or add a normal curve to the histogram. For more information, type ?hist in the R console.\nMore generally, it is important to understand the options available for each function you use. You can do this by reading the documentation for the function, which can be accessed by typing ?function_name or help(\"function_name\")in the R console.\n\n\nNext, let’s create a boxplot to compare the distribution of Weight between males and females. We will use the boxplot() function for this. This will create a boxplot comparing the weight distribution between males and females. You can customize the appearance of the boxplot by adjusting the parameters within the boxplot() function.\n\nboxplot(brfss$Weight ~ brfss$Sex, main = \"Weight Distribution by Sex\", \n        xlab = \"Sex\", ylab = \"Weight\", col = c(\"pink\", \"lightblue\"))",
    "crumbs": [
      "Home",
      "Exploratory data analysis",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Case Study: Behavioral Risk Factor Surveillance System</span>"
    ]
  },
  {
    "objectID": "eda_and_univariate_brfss.html#analyzing-relationships-between-variables",
    "href": "eda_and_univariate_brfss.html#analyzing-relationships-between-variables",
    "title": "\n13  Case Study: Behavioral Risk Factor Surveillance System\n",
    "section": "\n13.6 Analyzing Relationships Between Variables",
    "text": "13.6 Analyzing Relationships Between Variables\nTo further explore the data, let’s investigate the relationship between age and weight using a scatterplot. We will use the plot() function for this:\nThis will create a scatterplot of age and weight, allowing you to visually assess the relationship between these two variables.\n\nplot(brfss$Age, brfss$Weight, main = \"Scatterplot of Age and Weight\", \n     xlab = \"Age\", ylab = \"Weight\", col = \"darkblue\")  \n\n\n\n\n\n\n\nTo quantify the strength of the relationship between age and weight, we can calculate the correlation coefficient using the cor() function:\nThis will return the correlation coefficient between age and weight, which can help you determine whether there is a linear relationship between these variables.\n\ncor(brfss$Age, brfss$Weight)\n\n[1] NA\n\n\nWhy does cor() give a value of NA? What can we do about it? A quick glance at help(\"cor\") will give you the answer.\n\ncor(brfss$Age, brfss$Weight, use = \"complete.obs\")\n\n[1] 0.02699989",
    "crumbs": [
      "Home",
      "Exploratory data analysis",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Case Study: Behavioral Risk Factor Surveillance System</span>"
    ]
  },
  {
    "objectID": "eda_and_univariate_brfss.html#exercises",
    "href": "eda_and_univariate_brfss.html#exercises",
    "title": "\n13  Case Study: Behavioral Risk Factor Surveillance System\n",
    "section": "\n13.7 Exercises",
    "text": "13.7 Exercises\n\n\nWhat is the mean weight in this dataset? How about the median? What is the difference between the two? What does this tell you about the distribution of weights in the dataset?\n\nShow answermean(brfss$Weight, na.rm = TRUE)\n\n[1] 75.42455\n\nShow answermedian(brfss$Weight, na.rm = TRUE)\n\n[1] 72.57478\n\nShow answermean(brfss$Weight, na.rm=TRUE) - median(brfss$Weight, na.rm = TRUE)\n\n[1] 2.849774\n\n\n\n\nGiven the findings about the mean and median in the previous exercise, use the hist() function to create a histogram of the weight distribution in this dataset. How would you describe the shape of this distribution?\n\nShow answerhist(brfss$Weight, xlab=\"Weight (kg)\", breaks = 30)\n\n\n\n\n\n\n\n\n\nUse plot() to examine the relationship between height and weight in this dataset.\n\nShow answerplot(brfss$Height, brfss$Weight)\n\n\n\n\n\n\n\n\n\nWhat is the correlation between height and weight? What does this tell you about the relationship between these two variables?\n\nShow answercor(brfss$Height, brfss$Weight, use = \"complete.obs\")\n\n[1] 0.5140928\n\n\n\n\nCreate a histogram of the height distribution in this dataset. How would you describe the shape of this distribution?\n\nShow answerhist(brfss$Height, xlab=\"Height (cm)\", breaks = 30)",
    "crumbs": [
      "Home",
      "Exploratory data analysis",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Case Study: Behavioral Risk Factor Surveillance System</span>"
    ]
  },
  {
    "objectID": "eda_and_univariate_brfss.html#conclusion",
    "href": "eda_and_univariate_brfss.html#conclusion",
    "title": "\n13  Case Study: Behavioral Risk Factor Surveillance System\n",
    "section": "\n13.8 Conclusion",
    "text": "13.8 Conclusion\nIn this chapter, we have demonstrated how to perform an exploratory data analysis on the Behavioral Risk Factor Surveillance System dataset using R. We covered data loading, inspection, summary statistics, visualization, and the analysis of relationships between variables. By actively engaging with the R code and data, you have gained valuable experience in using R for EDA and are well-equipped to tackle more complex analyses in your future work.\nRemember that EDA is just the beginning of the data analysis process, and further statistical modeling and hypothesis testing will likely be necessary to draw meaningful conclusions from your data. However, EDA is a crucial step in understanding your data and informing your subsequent analyses.",
    "crumbs": [
      "Home",
      "Exploratory data analysis",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Case Study: Behavioral Risk Factor Surveillance System</span>"
    ]
  },
  {
    "objectID": "eda_and_univariate_brfss.html#learn-about-the-data",
    "href": "eda_and_univariate_brfss.html#learn-about-the-data",
    "title": "\n13  Case Study: Behavioral Risk Factor Surveillance System\n",
    "section": "\n13.9 Learn about the data",
    "text": "13.9 Learn about the data\nUsing the data exploration techniques you have seen to explore the brfss dataset.\n\nsummary()\ndim()\ncolnames()\nhead()\ntail()\nclass()\nView()\n\nYou may want to investigate individual columns visually using plotting like hist(). For categorical data, consider using something like table().",
    "crumbs": [
      "Home",
      "Exploratory data analysis",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Case Study: Behavioral Risk Factor Surveillance System</span>"
    ]
  },
  {
    "objectID": "eda_and_univariate_brfss.html#clean-data",
    "href": "eda_and_univariate_brfss.html#clean-data",
    "title": "\n13  Case Study: Behavioral Risk Factor Surveillance System\n",
    "section": "\n13.10 Clean data",
    "text": "13.10 Clean data\nR read Year as an integer value, but it’s really a factor\n\nbrfss$Year &lt;- factor(brfss$Year)",
    "crumbs": [
      "Home",
      "Exploratory data analysis",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Case Study: Behavioral Risk Factor Surveillance System</span>"
    ]
  },
  {
    "objectID": "eda_and_univariate_brfss.html#weight-in-1990-vs.-2010-females",
    "href": "eda_and_univariate_brfss.html#weight-in-1990-vs.-2010-females",
    "title": "\n13  Case Study: Behavioral Risk Factor Surveillance System\n",
    "section": "\n13.11 Weight in 1990 vs. 2010 Females",
    "text": "13.11 Weight in 1990 vs. 2010 Females\n\nCreate a subset of the data\n\n\nbrfssFemale &lt;- brfss[brfss$Sex == \"Female\",]\nsummary(brfssFemale)\n\n      Age            Weight           Sex                Height     \n Min.   :18.00   Min.   : 36.29   Length:12039       Min.   :105.0  \n 1st Qu.:37.00   1st Qu.: 57.61   Class :character   1st Qu.:157.5  \n Median :52.00   Median : 65.77   Mode  :character   Median :163.0  \n Mean   :51.92   Mean   : 69.05                      Mean   :163.3  \n 3rd Qu.:67.00   3rd Qu.: 77.11                      3rd Qu.:168.0  \n Max.   :99.00   Max.   :272.16                      Max.   :200.7  \n NA's   :103     NA's   :560                         NA's   :140    \n   Year     \n 1990:5718  \n 2010:6321  \n            \n            \n            \n            \n            \n\n\n\nVisualize\n\n\nplot(Weight ~ Year, brfssFemale)\n\n\n\n\n\n\n\n\nStatistical test\n\n\nt.test(Weight ~ Year, brfssFemale)\n\n\n    Welch Two Sample t-test\n\ndata:  Weight by Year\nt = -27.133, df = 11079, p-value &lt; 2.2e-16\nalternative hypothesis: true difference in means between group 1990 and group 2010 is not equal to 0\n95 percent confidence interval:\n -8.723607 -7.548102\nsample estimates:\nmean in group 1990 mean in group 2010 \n          64.81838           72.95424",
    "crumbs": [
      "Home",
      "Exploratory data analysis",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Case Study: Behavioral Risk Factor Surveillance System</span>"
    ]
  },
  {
    "objectID": "eda_and_univariate_brfss.html#weight-and-height-in-2010-males",
    "href": "eda_and_univariate_brfss.html#weight-and-height-in-2010-males",
    "title": "\n13  Case Study: Behavioral Risk Factor Surveillance System\n",
    "section": "\n13.12 Weight and height in 2010 Males",
    "text": "13.12 Weight and height in 2010 Males\n\nCreate a subset of the data\n\n\nbrfss2010Male &lt;- subset(brfss,  Year == 2010 & Sex == \"Male\")\nsummary(brfss2010Male)\n\n      Age            Weight           Sex                Height      Year     \n Min.   :18.00   Min.   : 36.29   Length:3679        Min.   :135   1990:   0  \n 1st Qu.:45.00   1st Qu.: 77.11   Class :character   1st Qu.:173   2010:3679  \n Median :57.00   Median : 86.18   Mode  :character   Median :178              \n Mean   :56.25   Mean   : 88.85                      Mean   :178              \n 3rd Qu.:68.00   3rd Qu.: 99.79                      3rd Qu.:183              \n Max.   :99.00   Max.   :278.96                      Max.   :218              \n NA's   :30      NA's   :49                          NA's   :31               \n\n\n\nVisualize the relationship\n\n\nhist(brfss2010Male$Weight)\n\n\n\n\n\n\nhist(brfss2010Male$Height)\n\n\n\n\n\n\nplot(Weight ~ Height, brfss2010Male)\n\n\n\n\n\n\n\n\nFit a linear model (regression)\n\n\nfit &lt;- lm(Weight ~ Height, brfss2010Male)\nfit\n\n\nCall:\nlm(formula = Weight ~ Height, data = brfss2010Male)\n\nCoefficients:\n(Intercept)       Height  \n   -86.8747       0.9873  \n\n\nSummarize as ANOVA table\n\nanova(fit)\n\nAnalysis of Variance Table\n\nResponse: Weight\n            Df  Sum Sq Mean Sq F value    Pr(&gt;F)    \nHeight       1  197664  197664   693.8 &lt; 2.2e-16 ***\nResiduals 3617 1030484     285                      \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\nPlot points, superpose fitted regression line; where am I?\n\n\nplot(Weight ~ Height, brfss2010Male)\nabline(fit, col=\"blue\", lwd=2)\n# Substitute your own weight and height...\npoints(73 * 2.54, 178 / 2.2, col=\"red\", cex=4, pch=20)\n\n\n\n\n\n\n\n\nClass and available ‘methods’\n\n\nclass(fit)                 # 'noun'\nmethods(class=class(fit))  # 'verb'\n\n\nDiagnostics\n\n\nplot(fit)\n# Note that the \"plot\" above does not have a \".lm\"\n# However, R will use \"plot.lm\". Why?\n?plot.lm",
    "crumbs": [
      "Home",
      "Exploratory data analysis",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Case Study: Behavioral Risk Factor Surveillance System</span>"
    ]
  },
  {
    "objectID": "visualization_guide.html",
    "href": "visualization_guide.html",
    "title": "\n14  Self-Guided Data Visualization in R\n",
    "section": "",
    "text": "14.1 Getting Started with ggplot2\nData visualization is a critical skill in the data scientist’s toolkit. It’s the bridge between raw data and human understanding. Effective visualizations can reveal patterns, trends, and outliers that might be missed in a table of numbers. In the R programming language, the ggplot2 package stands as the gold standard for creating beautiful, flexible, and powerful graphics.\nThis document will guide you through the principles of effective data visualization and show you how to apply them using ggplot2. We’ll cover best practices, common plot types, and the “grammar of graphics” methodology that makes ggplot2 so intuitive.\nBefore diving in, though, there are a some truly amazing online resources that showcase what can be done with R graphics and also stimulate your imagination. Two of the best are:\nAfter walking through this document, go back to these resources and explore the examples. You’ll see how the principles we discuss here are applied in real-world scenarios, and you’ll gain inspiration for your own visualizations.\nTo get started, we need to load the ggplot2 package, which is part of the tidyverse.\n# Load the necessary R packages for data visualization\nlibrary(ggplot2)\nlibrary(dplyr)",
    "crumbs": [
      "Home",
      "Exploratory data analysis",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Self-Guided Data Visualization in R</span>"
    ]
  },
  {
    "objectID": "visualization_guide.html#core-principles-of-effective-data-visualization",
    "href": "visualization_guide.html#core-principles-of-effective-data-visualization",
    "title": "\n14  Self-Guided Data Visualization in R\n",
    "section": "\n14.2 Core Principles of Effective Data Visualization",
    "text": "14.2 Core Principles of Effective Data Visualization\nBefore we start plotting, it’s essential to understand what makes a visualization effective. Two key principles are maximizing the data-ink ratio and using clear labels.\n\n14.2.1 The “Least Ink” Principle\nCoined by the statistician Edward Tufte, the data-ink ratio is the proportion of a graphic’s ink devoted to the non-redundant display of data information. The goal is to maximize this ratio. In simpler terms, every single pixel should have a reason to be there.\n\n\n\n\n\nFigure 14.1: Edward Tufte’s landmark book, “The Visual Display of Quantitative Information,” emphasizes the importance of maximizing the data-ink ratio.\n\n\nAvoid chart junk like:\n\nRedundant grid lines\nUnnecessary backgrounds or colors\n3D effects on 2D plots\nShadows and other decorative elements\n\nLet’s look at an example. The first plot has a lot of “chart junk,” while the second one is cleaner and focuses on the data.\n\n# Using the built-in 'mpg' dataset\nggplot(mpg, aes(x = displ, y = hwy, color = class)) +\n  geom_point() +\n  theme_gray() + # A theme with a lot of non-data ink\n  labs(title = \"Fuel Efficiency vs. Engine Displacement\",\n       subtitle = \"This is a very busy plot\",\n       x = \"Engine Displacement (Liters)\",\n       y = \"Highway Miles per Gallon\",\n       color = \"Vehicle Class\")\n\n\n\n\n\n\nFigure 14.2: A cluttered plot with low data-ink ratio. The gray background, heavy gridlines, and legend title are all unnecessary.\n\n\n\n\nHere’s a cleaner version of the same plot that follows the “least ink” principle. Notice how it removes unnecessary elements while still conveying the same information.\n\nggplot(mpg, aes(x = displ, y = hwy, color = class)) +\n  geom_point() +\n  theme_minimal() + # A cleaner theme\n  labs(title = \"Fuel Efficiency vs. Engine Displacement\",\n       x = \"Engine Displacement (Liters)\",\n       y = \"Highway Miles per Gallon\",\n       color = \"Class\") # Simpler legend title\n\n\n\n\n\n\nFigure 14.3: A clean, minimalist plot that follows the ‘least ink’ principle. The focus is entirely on the relationship between the data points.\n\n\n\n\nWhile this is a subjective topic, the goal is to make your plots as clear and informative as possible. The “least ink” principle is a guideline, not a rule, but it can help you create more effective visualizations.\n\n14.2.2 The Importance of Clear Labeling\nA plot without labels is just a picture. To be a useful piece of analysis, it needs to communicate context. Always ensure your plots have:\n\nA clear and descriptive title.\nLabeled axes with units (\\(e.g.\\), “Temperature (°C)”).\nAn informative legend if you’re using color, shape, or size to encode data.\n\n14.2.3 Color and Contrast\nColor is a powerful tool in data visualization, but it can also be misused. R provides several built-in color palettes, and you can also use packages like RColorBrewer for more options. Think in terms of colorblind-friendly palettes, and avoid using too many colors in a single plot.\nColor palettes can be roughly categorized into:\n\nSequential palettes (first list of colors), which are suited to ordered data that progress from low to high (gradient).\nQualitative palettes (second list of colors), which are best suited to represent nominal or categorical data. They not imply magnitude differences between groups.\nDiverging palettes (third list of colors), which put equal emphasis on mid-range critical values and extremes at both ends of the data range.\n\n\nlibrary(RColorBrewer)\n\ndisplay.brewer.all()\n\n\n\n\n\n\nFigure 14.4: Examples of color palettes in R. The first row shows sequential palettes, the second row shows qualitative palettes, and the third row shows diverging palettes.\n\n\n\n\nIt is also important to consider colorblindness when choosing colors for your plots. The most common types of color blindness are red-green and blue-yellow. You can use tools like the colorspace package to check how your plots will look to people with different types of color vision deficiencies.\n\ndisplay.brewer.all(colorblindFriendly=TRUE)\n\n\n\n\n\n\nFigure 14.5: A colorblind-friendly palette from the ‘colorspace’ package. This palette is designed to be distinguishable for people with various types of color vision deficiencies.",
    "crumbs": [
      "Home",
      "Exploratory data analysis",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Self-Guided Data Visualization in R</span>"
    ]
  },
  {
    "objectID": "visualization_guide.html#introduction-to-ggplot2-the-grammar-of-graphics",
    "href": "visualization_guide.html#introduction-to-ggplot2-the-grammar-of-graphics",
    "title": "\n14  Self-Guided Data Visualization in R\n",
    "section": "\n14.3 Introduction to ggplot2: The Grammar of Graphics",
    "text": "14.3 Introduction to ggplot2: The Grammar of Graphics\nRather than reproducing excellent online resources, for this section, pick any or all of the following resources to learn about the grammar of graphics and how to use ggplot2:\n\nR for Data Science: Data Visualization\nggplot2 documentation\nModern Data Visualization with R",
    "crumbs": [
      "Home",
      "Exploratory data analysis",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Self-Guided Data Visualization in R</span>"
    ]
  },
  {
    "objectID": "visualization_guide.html#sets-and-intersections-upset-plots",
    "href": "visualization_guide.html#sets-and-intersections-upset-plots",
    "title": "\n14  Self-Guided Data Visualization in R\n",
    "section": "\n14.4 Sets and Intersections: UpSet Plots",
    "text": "14.4 Sets and Intersections: UpSet Plots\nWhen dealing with multiple sets, visualizing their intersections can be challenging. Traditional Venn diagrams become cluttered and hard to read with more than three sets. An UpSet plot is a powerful alternative for visualizing the intersections of multiple sets. It consists of two main parts: a matrix that shows which sets are part of an intersection, and a bar chart that shows the size of each intersection. This makes it far more scalable and easier to interpret than a complex Venn diagram.\nTo create an UpSet plot, we use the UpSetR package. It takes a specific input format where 0s and 1s indicate the absence or presence of an element in a set.\n\n# install.packages(\"UpSetR\")\nlibrary(UpSetR)\nmovies &lt;- read.csv(system.file(\"extdata\", \"movies.csv\", package = \"UpSetR\"), \n    header = T, sep = \";\")\n\n# Use the 'movies' dataset that comes with UpSetR\n# This dataset is already in the correct binary format\nupset(movies, \n      nsets = 5, # Show the 5 most frequent genres\n      order.by = \"freq\",\n      mainbar.y.label = \"Intersection Size\",\n      sets.x.label = \"Total Movies in Genre\")\n\n\n\n\n\n\nFigure 14.6: An UpSet plot visualizing movie genres. The main bar chart shows the size of intersections (e.g., how many movies are both ‘Comedy’ and ‘Romance’). The bottom-left matrix indicates which genres are part of each intersection. This is much clearer than a 5-set Venn diagram.\n\n\n\n\nThe UpSet plot clearly shows us, for instance, the number of movies that are exclusively “Drama” versus those that are a combination of “Drama,” “Comedy,” and “Romance.” This level of detail is difficult to achieve with a Venn diagram.",
    "crumbs": [
      "Home",
      "Exploratory data analysis",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Self-Guided Data Visualization in R</span>"
    ]
  },
  {
    "objectID": "visualization_guide.html#complex-heatmaps",
    "href": "visualization_guide.html#complex-heatmaps",
    "title": "\n14  Self-Guided Data Visualization in R\n",
    "section": "\n14.5 Complex Heatmaps",
    "text": "14.5 Complex Heatmaps\nHeatmaps are a powerful way to visualize complex data matrices, especially when dealing with large datasets. They allow you to see patterns and relationships in the data at a glance.\nWhat is a heatmap? It’s a graphical representation of data where individual values are represented as colors. The color intensity indicates the magnitude of the value, making it easy to spot trends and outliers. The underlying data is typically a matrix of numbers.\n\n\n\n\n\n\nNote\n\n\n\nA matrix is a two-dimensional array of numbers, where each element is identified by its row and column indices. Matrices can include only ONE data type.\n\n\nThere are many ways to create heatmaps in R including the base R heatmap() function, the ggplot2 package, and specialized packages like ComplexHeatmap.\nFeel free to explore the following resources for some of the most popular heatmap packages in R:\n\n\nggplot2 Heatmaps allows you to create basic heatmaps using the geom_tile() function. This is a good starting point for simple heatmaps.\n\nThe heatmap() function in base R is a simple way to create heatmaps. It automatically scales the data and provides options for clustering rows and columns.\n\nThe pheatmamp package is a popular package for creating heatmaps with more customization options. It allows you to add annotations, customize colors, and control clustering.\n\nComplex Heatmaps is a powerful R package for creating complex heatmaps. It allows you to visualize data matrices with multiple annotations, making it ideal for genomic data analysis.\n\nInteractive Complex Heatmaps is an extension of the Complex Heatmaps package that allows you to create interactive heatmaps. This can be useful for exploring large datasets and identifying patterns.",
    "crumbs": [
      "Home",
      "Exploratory data analysis",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Self-Guided Data Visualization in R</span>"
    ]
  },
  {
    "objectID": "visualization_guide.html#genome-and-genomic-data-visualization",
    "href": "visualization_guide.html#genome-and-genomic-data-visualization",
    "title": "\n14  Self-Guided Data Visualization in R\n",
    "section": "\n14.6 Genome and Genomic Data Visualization",
    "text": "14.6 Genome and Genomic Data Visualization\nThe Gviz package is a powerful tool for visualizing genomic data in R. It allows you to create publication-quality plots of genomic features, such as gene annotations, sequence alignments, and expression data.\nThe GenomicDistributions package is another useful package for visualizing genomic data. It provides functions for creating distribution plots of genomic features, such as coverage, chip-seq or atac-seq distributions relative to genomic features, etc.",
    "crumbs": [
      "Home",
      "Exploratory data analysis",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Self-Guided Data Visualization in R</span>"
    ]
  },
  {
    "objectID": "visualization_guide.html#conclusion",
    "href": "visualization_guide.html#conclusion",
    "title": "\n14  Self-Guided Data Visualization in R\n",
    "section": "\n14.7 Conclusion",
    "text": "14.7 Conclusion\nThis document has provided a comprehensive foundation for creating effective data visualizations in R with ggplot2. We’ve covered the core principles of good design, explored a wide range of common plot types including heatmaps, and seen how ggplot2’s layered grammar allows for the creation of complex, insightful graphics by mapping multiple data dimensions to aesthetics. We also discussed why certain plots like Venn diagrams can be problematic and introduced powerful alternatives like UpSet plots.\nThe key to mastering data visualization is practice. Experiment with different datasets, try new geoms, and always think critically about the story your plot is telling and the best way to tell it.",
    "crumbs": [
      "Home",
      "Exploratory data analysis",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Self-Guided Data Visualization in R</span>"
    ]
  },
  {
    "objectID": "norm.html",
    "href": "norm.html",
    "title": "\n15  Working with distribution functions\n",
    "section": "",
    "text": "15.1 pnorm\nWhich values do pnorm, dnorm, qnorm, and rnorm return? How do I remember the difference between these?\nI find it helpful to have visual representations of distributions as pictures. It is difficult for me to think of distributions, or differences between probability, density, and quantiles without visualizing the shape of the distribution. So I figured it would be helpful to have a visual guide to pnorm, dnorm, qnorm, and rnorm.\nWhat is the relationship between these functions? The pnorm function gives the area under the curve to the left of a given point, while dnorm gives the height of the curve at that point. The qnorm function is the inverse of pnorm, returning the x value for a given probability. Finally, rnorm generates random samples from the normal distribution.\nThis function gives the probability function for a normal distribution. If you do not specify the mean and standard deviation, R defaults to standard normal. Figure 15.1\npnorm(q, mean = 0, sd = 1, lower.tail = TRUE, log.p = FALSE)\nThe R help file for pnorm provides the template above. The value you input for q is a value on the x-axis, and the returned value is the area under the distribution curve to the left of that point.\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\nℹ Please use `linewidth` instead.\nThis function gives the probability function for a normal distribution. If you do not specify the mean and standard deviation, R defaults to standard normal.\npnorm(q, mean = 0, sd = 1, lower.tail = TRUE, log.p = FALSE) The R help file for pnorm provides the template above. The value you input for q is a value on the x-axis, and the returned value is the area under the distribution curve to the left of that point.\nThe option lower.tail = TRUE tells R to use the area to the left of the given point. This is the default, so will remain true even without entering it. In order to compute the area to the right of the given point, you can either switch to lower.tail = FALSE, or simply calculate 1-pnorm() instead. This is demonstrated below.\nFigure 15.1: The pnorm function takes a quantile (value on the x-axis) and returns the area under the curve to the left of that value.\n\n\n\n\n\n\n\n\n\nFigure 15.2: The pnorm function takes a quantile (value on the x-axis) and returns the area under the curve to the left of that value.\n\n\n\n\n\n\n\n\n\nFigure 15.3: The pnorm function takes a quantile (value on the x-axis) and returns the area under the curve to the left of that value.\n\n\n\n\n\n\n\n\n\nFigure 15.4: The pnorm function takes a quantile (value on the x-axis) and returns the area under the curve to the left of that value.\nThe option lower.tail = TRUE tells R to use the area to the left of the given point. This is the default, so will remain true even without entering it. In order to compute the area to the right of the given point, you can either switch to lower.tail = FALSE, or simply calculate 1-pnorm() instead.",
    "crumbs": [
      "Home",
      "statististics",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Working with distribution functions</span>"
    ]
  },
  {
    "objectID": "norm.html#dnorm",
    "href": "norm.html#dnorm",
    "title": "\n15  Working with distribution functions\n",
    "section": "\n15.2 dnorm",
    "text": "15.2 dnorm\nThis function calculates the probability density function (PDF) for the normal distribution. It gives the probability density (height of the curve) at a specified value (x).\n\n\n\n\n\n\n\nFigure 15.5: The dnorm function returns the height of the normal distribution at a given point.\n\n\n\n\n\n\n\n\n\nFigure 15.6: The dnorm function returns the height of the normal distribution at a given point.\n\n\n\n\n\n\n\n\n\nFigure 15.7: The dnorm function returns the height of the normal distribution at a given point.",
    "crumbs": [
      "Home",
      "statististics",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Working with distribution functions</span>"
    ]
  },
  {
    "objectID": "norm.html#qnorm",
    "href": "norm.html#qnorm",
    "title": "\n15  Working with distribution functions\n",
    "section": "\n15.3 qnorm",
    "text": "15.3 qnorm\nThis function calculates the quantiles of the normal distribution. It returns the value (x) corresponding to a specified probability (p). It is the inverse of thepnorm function.\n\n\n\n\n\n\n\nFigure 15.8: The qnorm function is the inverse of the pnorm function in that it takes a probability and gives the quantile.\n\n\n\n\n\n\n\n\n\nFigure 15.9: The qnorm function is the inverse of the pnorm function in that it takes a probability and gives the quantile.\n\n\n\n\n\n\n\n\n\nFigure 15.10: The qnorm function is the inverse of the pnorm function in that it takes a probability and gives the quantile.\n\n\n\n\n\n\n\n\n\nFigure 15.11: The qnorm function is the inverse of the pnorm function in that it takes a probability and gives the quantile.\n\n\n\n\n\n\n\n\n\nFigure 15.12: The qnorm function is the inverse of the pnorm function in that it takes a probability and gives the quantile.",
    "crumbs": [
      "Home",
      "statististics",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Working with distribution functions</span>"
    ]
  },
  {
    "objectID": "norm.html#rnorm",
    "href": "norm.html#rnorm",
    "title": "\n15  Working with distribution functions\n",
    "section": "\n15.4 rnorm",
    "text": "15.4 rnorm\n\nprint(r1)\n\n\n\n\n\n\nFigure 15.13: The rnorm function takes a number of samples and returns a vector of random numbers from the normal distribution (with mean=0, sd=1 as defaults)",
    "crumbs": [
      "Home",
      "statististics",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Working with distribution functions</span>"
    ]
  },
  {
    "objectID": "norm.html#iq-scores",
    "href": "norm.html#iq-scores",
    "title": "\n15  Working with distribution functions\n",
    "section": "\n15.5 IQ scores",
    "text": "15.5 IQ scores\nNormal Distribution and its Application with IQ\nThe normal distribution, also known as the Gaussian distribution, is a continuous probability distribution characterized by its bell-shaped curve. It is defined by two parameters: the mean (µ) and the standard deviation (σ). The mean represents the central tendency of the distribution, while the standard deviation represents the dispersion or spread of the data.\nThe IQ scores are an excellent example of the normal distribution, as they are designed to follow this distribution pattern. The mean IQ score is set at 100, and the standard deviation is set at 15. This means that the majority of the population (about 68%) have an IQ score between 85 and 115, while 95% of the population have an IQ score between 70 and 130.\n\n\nWhat is the probability of having an IQ score between 85 and 115?\n\nShow answerpnorm(115, mean = 100, sd = 15) - pnorm(85, mean = 100, sd = 15)\n\n\n\n\nWhat is the 90th percentile of the IQ scores?\n\nShow answerqnorm(0.9, mean = 100, sd = 15)\n\n\n\n\nWhat is the probability of having an IQ score above 130?\n\nShow answer1 - pnorm(130, mean = 100, sd = 15)\n\n\n\n\nWhat is the probability of having an IQ score below 70?\n\nShow answerpnorm(70, mean = 100, sd = 15)",
    "crumbs": [
      "Home",
      "statististics",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Working with distribution functions</span>"
    ]
  },
  {
    "objectID": "t-stats-and-tests.html",
    "href": "t-stats-and-tests.html",
    "title": "\n16  The t-statistic and t-distribution\n",
    "section": "",
    "text": "16.1 Background\nThe t-test is a statistical hypothesis test that is commonly used when the data are normally distributed (follow a normal distribution) if the value of the population standard deviation were known. When the population standard deviation is not known and is replaced by an estimate based no the data, the test statistic follows a Student’s t distribution.\nT-tests are handy hypothesis tests in statistics when you want to compare means. You can compare a sample mean to a hypothesized or target value using a one-sample t-test. You can compare the means of two groups with a two-sample t-test. If you have two groups with paired observations (e.g., before and after measurements), use the paired t-test.\nA t-test looks at the t-statistic, the t-distribution values, and the degrees of freedom to determine the statistical significance. To conduct a test with three or more means, we would use an analysis of variance.\nThe distriubution that the t-statistic follows was described in a famous paper (Student 1908) by “Student”, a pseudonym for William Sealy Gosset.",
    "crumbs": [
      "Home",
      "statististics",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>The t-statistic and t-distribution</span>"
    ]
  },
  {
    "objectID": "t-stats-and-tests.html#the-z-score-and-probability",
    "href": "t-stats-and-tests.html#the-z-score-and-probability",
    "title": "\n16  The t-statistic and t-distribution\n",
    "section": "\n16.2 The Z-score and probability",
    "text": "16.2 The Z-score and probability\nBefore talking about the t-distribution and t-scores, lets review the Z-score, its relation to the normal distribution, and probability.\nThe Z-score is defined as:\n\\[Z = \\frac{x - \\mu}{\\sigma} \\tag{16.1}\\]\nwhere \\(\\mu\\) is a the population mean from which \\(x\\) is drawn and \\(\\sigma\\) is the population standard deviation (taken as known, not estimated from the data).\nThe probability of observing a \\(Z\\) score of \\(z\\) or greater can be calculated by \\(pnorm(z,\\mu,\\sigma)\\).\nFor example, let’s assume that our “population” is known and it truly has a mean 0 and standard deviation 1. If we have observations drawn from that population, we can assign a probability of seeing that observation by random chance under the assumption that the null hypothesis is TRUE.\n\nzscore = seq(-5,5,1)\n\nFor each value of zscore, let’s calculate the p-value and put the results in a data.frame.\n\ndf = data.frame(\n    zscore = zscore,\n    pval   = pnorm(zscore, 0, 1)\n)\ndf\n\n   zscore         pval\n1      -5 2.866516e-07\n2      -4 3.167124e-05\n3      -3 1.349898e-03\n4      -2 2.275013e-02\n5      -1 1.586553e-01\n6       0 5.000000e-01\n7       1 8.413447e-01\n8       2 9.772499e-01\n9       3 9.986501e-01\n10      4 9.999683e-01\n11      5 9.999997e-01\n\n\nWhy is the p-value of something 5 population standard deviations away from the mean (zscore=5) nearly 1 in this calculation? What is the default for pnorm with respect to being one-sided or two-sided?\nLet’s plot the values of probability vs z-score:\n\nplot(df$zscore, df$pval, type='b')\n\n\n\n\n\n\n\nThis plot is the empirical cumulative density function (cdf) for our data. How can we use it? If we know the z-score, we can look up the probability of observing that value. Since we have constructed our experiment to follow the standard normal distribution, this cdf also represents the cdf of the standard normal distribution.\n\n16.2.1 Small diversion: two-sided pnorm function\nThe pnorm function returns the “one-sided” probability of having a value at least as extreme as the observed \\(x\\) and uses the “lower” tail by default. Let’s create a function that computes two-sided p-values.\n\nTake the absolute value of x\nCompute pnorm with lower.tail=FALSE so we get lower p-values with larger values of \\(x\\).\nSince we want to include both tails, we need to multiply the area (probability) returned by pnorm by 2.\n\n\ntwosidedpnorm = function(x,mu=0,sd=1) {\n    2*pnorm(abs(x),mu,sd,lower.tail=FALSE)\n}\n\nAnd we can test this to see how likely it is to be 2 or 3 standard deviations from the mean:\n\ntwosidedpnorm(2)\n\n[1] 0.04550026\n\ntwosidedpnorm(3)\n\n[1] 0.002699796",
    "crumbs": [
      "Home",
      "statististics",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>The t-statistic and t-distribution</span>"
    ]
  },
  {
    "objectID": "t-stats-and-tests.html#the-t-distribution",
    "href": "t-stats-and-tests.html#the-t-distribution",
    "title": "\n16  The t-statistic and t-distribution\n",
    "section": "\n16.3 The t-distribution",
    "text": "16.3 The t-distribution\nWe spent time above working with z-scores and probability. An important aspect of working with the normal distribution is that we MUST assume that we know the standard deviation. Remember that the Z-score is defined as:\n\\[Z = \\frac{x - \\mu}{\\sigma}\\]\nThe formula for the population standard deviation is:\n\\[\\sigma = \\sqrt{\\frac{1}{N}\\sum_{i=1}^{N}({xi - \\mu)^2}} \\tag{16.2}\\]\nIn general, the population standard deviation is taken as “known” as we did above.\nIf we do not but only have a sample from the population, instead of using the Z-score, we use the t-score defined as:\n\\[t = \\frac{x - \\bar{x}}{s} \\tag{16.3}\\]\nThis looks quite similar to the formula for Z-score, but here we have to estimate the standard deviation, \\(s\\) from the data. The formula for \\(s\\) is:\n\\[s = \\sqrt{\\frac{1}{N-1}\\sum_{i=1}^{N}({x_{i} - \\bar{x})^2}} \\tag{16.4}\\]\nSince we are estimating the standard deviation from the data, this leads to extra variability that shows up as “fatter tails” for smaller sample sizes than for larger sample sizes. We can see this by comparing the t-distribution for various numbers of degrees of freedom (sample sizes).\nWe can look at the effect of sample size on the distributions graphically by looking at the densities for 3, 5, 10, 20 degrees of freedom and the normal distribution:\n\nlibrary(dplyr)\nlibrary(ggplot2)\nt_values = seq(-6,6,0.01)\ndf = data.frame(\n    value = t_values,\n    t_3   = dt(t_values,3),\n    t_6   = dt(t_values,6),\n    t_10  = dt(t_values,10),\n    t_20  = dt(t_values,20),\n    Normal= dnorm(t_values)\n) |&gt;\n    tidyr::gather(\"Distribution\", \"density\", -value)\nggplot(df, aes(x=value, y=density, color=Distribution)) + \n    geom_line()\n\n\n\n\n\n\nFigure 16.1: t-distributions for various degrees of freedom. Note that the tails are fatter for smaller degrees of freedom, which is a result of estimating the standard deviation from the data.\n\n\n\n\nThe dt and dnorm functions give the density of the distributions for each point.\n\ndf2 = df |&gt; \n    group_by(Distribution) |&gt;\n    arrange(value) |&gt; \n    mutate(cdf=cumsum(density))\nggplot(df2, aes(x=value, y=cdf, color=Distribution)) + \n    geom_line()\n\n\n\n\n\n\n\n\n16.3.1 p-values based on Z vs t\nWhen we have a “sample” of data and want to compute the statistical significance of the difference of the mean from the population mean, we calculate the standard deviation of the sample means (standard error).\n\\[z = \\frac{x - \\mu}{\\sigma/\\sqrt{n}}\\]\nLet’s look at the relationship between the p-values of Z (from the normal distribution) vs t for a sample of data.\n\nset.seed(5432)\nsamp = rnorm(5,mean = 0.5)\nz = sqrt(length(samp)) * mean(samp) #simplifying assumption (sigma=1, mu=0)\n\nAnd the p-value if we assume we know the standard deviation:\n\npnorm(z, lower.tail = FALSE)\n\n[1] 0.02428316\n\n\nIn reality, we don’t know the standard deviation, so we have to estimate it from the data. We can do this by calculating the sample standard deviation:\n\nts = sqrt(length(samp)) * mean(samp) / sd(samp)\npnorm(ts, lower.tail = FALSE)\n\n[1] 0.0167297\n\npt(ts,df = length(samp)-1, lower.tail = FALSE)\n\n[1] 0.0503001\n\n\n\n16.3.2 Experiment\nWhen sampling from a normal distribution, we often calculate p-values to test hypotheses or determine the statistical significance of our results. The p-value represents the probability of obtaining a test statistic as extreme or more extreme than the one observed, under the null hypothesis.\nIn a typical scenario, we assume that the population mean and standard deviation are known. However, in many real-life situations, we don’t know the true population standard deviation, and we have to estimate it using the sample standard deviation (Equation 16.4). This estimation introduces some uncertainty into our calculations, which affects the p-values. When we include an estimate of the standard deviation, we switch from using the standard normal (z) distribution to the t-distribution for calculating p-values.\nWhat would happen if we used the normal distribution to calculate p-values when we use the sample standard deviation? Let’s find out!\n\nSimulate a bunch of samples of size n from the standard normal distribution\nCalculate the p-value distribution for those samples based on the normal.\nCalculate the p-value distribution for those samples based on the normal, but with the estimated standard deviation.\nCalculate the p-value distribution for those samples based on the t-distribution.\n\nCreate a function that draws a sample of size n from the standard normal distribution.\n\nzf = function(n) {\n    samp = rnorm(n)\n    z = sqrt(length(samp)) * mean(samp) / 1 #simplifying assumption (sigma=1, mu=0)\n    z\n}\n\nAnd give it a try:\n\nzf(5)\n\n[1] 0.7406094\n\n\nPerform 10000 replicates of our sampling and z-scoring. We are using the assumption that we know the population standard deviation; in this case, we do know since we are sampling from the standard normal distribution.\n\nz10k = replicate(10000,zf(5))\nhist(pnorm(z10k))\n\n\n\n\n\n\n\nAnd do the same, but now creating a t-score function. We are using the assumption that we don’t know the population standard deviation; in this case, we must estimate it from the data. Note the difference in the calculation of the t-score (ts) as compared to the z-score (z).\n\ntf = function(n) {\n    samp = rnorm(n)\n    # now, using the sample standard deviation since we \n    # \"don't know\" the population standard deviation\n    ts = sqrt(length(samp)) * mean(samp) / sd(samp)\n    ts\n}\n\nIf we use those t-scores and calculate the p-values based on the normal distribution, the histogram of those p-values looks like:\n\nt10k = replicate(10000,tf(5))\nhist(pnorm(t10k))\n\n\n\n\n\n\n\nSince we are using the normal distribution to calculate the p-values, we are, in effect, assuming that we know the population standard deviation. This assumption is incorrect, and we can see that the p-values are not uniformly distributed between 0 and 1.\nIf we use those t-scores and calculate the p-values based on the t-distribution, the histogram of those p-values looks like:\n\nhist(pt(t10k,5))\n\n\n\n\n\n\n\nNow, the p-values are uniformly distributed between 0 and 1, as expected.\nWhat is a qqplot and how do we use it? A qqplot is a plot of the quantiles of two distributions against each other. If the two distributions are identical, the points will fall on a straight line. If the two distributions are different, the points will deviate from the straight line. We can use a qqplot to compare the t-distribution to the normal distribution. If the t-distribution is identical to the normal distribution, the points will fall on a straight line. If the t-distribution is different from the normal distribution, the points will deviate from the straight line. In this case, we can see that the t-distribution is different from the normal distribution, as the points deviate from the straight line. What would happen if we increased the sample size? The t-distribution would approach the normal distribution, and the points would fall closer and closer to the straight line.\n\nqqplot(z10k,t10k)\nabline(0,1)",
    "crumbs": [
      "Home",
      "statististics",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>The t-statistic and t-distribution</span>"
    ]
  },
  {
    "objectID": "t-stats-and-tests.html#summary-of-t-distribution-vs-normal-distribution",
    "href": "t-stats-and-tests.html#summary-of-t-distribution-vs-normal-distribution",
    "title": "\n16  The t-statistic and t-distribution\n",
    "section": "\n16.4 Summary of t-distribution vs normal distribution",
    "text": "16.4 Summary of t-distribution vs normal distribution\nThe t-distribution is a family of probability distributions that depends on a parameter called degrees of freedom, which is related to the sample size. The t-distribution approaches the standard normal distribution as the sample size increases but has heavier tails for smaller sample sizes. This means that the t-distribution is more conservative in calculating p-values for small samples, making it harder to reject the null hypothesis. Including an estimate of the standard deviation changes the way we calculate p-values by switching from the standard normal distribution to the t-distribution, which accounts for the uncertainty introduced by estimating the population standard deviation from the sample. This adjustment is particularly important for small sample sizes, as it provides a more accurate assessment of the statistical significance of our results.",
    "crumbs": [
      "Home",
      "statististics",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>The t-statistic and t-distribution</span>"
    ]
  },
  {
    "objectID": "t-stats-and-tests.html#t.test",
    "href": "t-stats-and-tests.html#t.test",
    "title": "\n16  The t-statistic and t-distribution\n",
    "section": "\n16.5 t.test",
    "text": "16.5 t.test\n\n16.5.1 One-sample\nWe are going to use the t.test function to perform a one-sample t-test. The t.test function takes a vector of values as input that represents the sample values. In this case, we’ll simulate our sample using the rnorm function and presume that our “effect-size” is 1.\n\nx = rnorm(20,1)\n# small sample\n# Just use the first 5 values of the sample\nt.test(x[1:5])\n\n\n    One Sample t-test\n\ndata:  x[1:5]\nt = 0.97599, df = 4, p-value = 0.3843\nalternative hypothesis: true mean is not equal to 0\n95 percent confidence interval:\n -1.029600  2.145843\nsample estimates:\nmean of x \n0.5581214 \n\n\nIn this case, we set up the experiment so that the null hypothesis is true (the true mean is not zero, but actually 1). However, we only have a small sample size that leads to a modest p-value.\nIncreasing the sample size allows us to see the effect more clearly.\n\nt.test(x[1:20])\n\n\n    One Sample t-test\n\ndata:  x[1:20]\nt = 3.8245, df = 19, p-value = 0.001144\nalternative hypothesis: true mean is not equal to 0\n95 percent confidence interval:\n 0.3541055 1.2101894\nsample estimates:\nmean of x \n0.7821474 \n\n\n\n16.5.2 two-sample\n\nx = rnorm(10,0.5)\ny = rnorm(10,-0.5)\nt.test(x,y)\n\n\n    Welch Two Sample t-test\n\ndata:  x and y\nt = 3.4296, df = 17.926, p-value = 0.003003\nalternative hypothesis: true difference in means is not equal to 0\n95 percent confidence interval:\n 0.5811367 2.4204048\nsample estimates:\n mean of x  mean of y \n 0.7039205 -0.7968502 \n\n\n\n16.5.3 from a data.frame\nIn some situations, you may have data and groups as columns in a data.frame. See the following data.frame, for example\n\ndf = data.frame(value=c(x,y),group=as.factor(rep(c('g1','g2'),each=10)))\ndf\n\n         value group\n1   1.12896674    g1\n2  -1.26838101    g1\n3   1.04577597    g1\n4   1.69075585    g1\n5   0.18672204    g1\n6   1.99715092    g1\n7   1.15424947    g1\n8   0.37671442    g1\n9  -0.09565723    g1\n10  0.82290783    g1\n11 -1.48530261    g2\n12 -1.29200440    g2\n13 -0.18778362    g2\n14  0.59205742    g2\n15 -2.10065248    g2\n16 -0.29961560    g2\n17 -0.38985115    g2\n18 -2.47126235    g2\n19 -0.63654380    g2\n20  0.30245611    g2\n\n\nR allows us to perform a t-test using the formula notation.\n\nt.test(value ~ group, data=df)\n\n\n    Welch Two Sample t-test\n\ndata:  value by group\nt = 3.4296, df = 17.926, p-value = 0.003003\nalternative hypothesis: true difference in means between group g1 and group g2 is not equal to 0\n95 percent confidence interval:\n 0.5811367 2.4204048\nsample estimates:\nmean in group g1 mean in group g2 \n       0.7039205       -0.7968502 \n\n\nYou read that as value is a function of group. In practice, this will do a t-test between the values in g1 vs g2.\n\n16.5.4 Equivalence to linear model\n\nt.test(value ~ group, data=df, var.equal=TRUE)\n\n\n    Two Sample t-test\n\ndata:  value by group\nt = 3.4296, df = 18, p-value = 0.002989\nalternative hypothesis: true difference in means between group g1 and group g2 is not equal to 0\n95 percent confidence interval:\n 0.5814078 2.4201337\nsample estimates:\nmean in group g1 mean in group g2 \n       0.7039205       -0.7968502 \n\n\nThis is equivalent to:\n\nres = lm(value ~ group, data=df)\nsummary(res)\n\n\nCall:\nlm(formula = value ~ group, data = df)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-1.9723 -0.5600  0.2511  0.5252  1.3889 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)   \n(Intercept)   0.7039     0.3094   2.275  0.03538 * \ngroupg2      -1.5008     0.4376  -3.430  0.00299 **\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.9785 on 18 degrees of freedom\nMultiple R-squared:  0.3952,    Adjusted R-squared:  0.3616 \nF-statistic: 11.76 on 1 and 18 DF,  p-value: 0.002989",
    "crumbs": [
      "Home",
      "statististics",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>The t-statistic and t-distribution</span>"
    ]
  },
  {
    "objectID": "t-stats-and-tests.html#power-calculations",
    "href": "t-stats-and-tests.html#power-calculations",
    "title": "\n16  The t-statistic and t-distribution\n",
    "section": "\n16.6 Power calculations",
    "text": "16.6 Power calculations\nThe power of a statistical test is the probability that the test will reject the null hypothesis when the alternative hypothesis is true. In other words, the power of a statistical test is the probability of not making a Type II error. The power of a statistical test depends on the significance level (alpha), the sample size, and the effect size.\nThe power.t.test function can be used to calculate the power of a one-sample t-test.\nLooking at help(\"power.t.test\"), we see that the function takes the following arguments:\n\n\nn - sample size\n\ndelta - effect size\n\nsd - standard deviation of the sample\n\nsig.level - significance level\n\npower - power\n\nWe need to supply four of these arguments to calculate the fifth. For example, if we want to calculate the power of a one-sample t-test with a sample size of 5, a standard deviation of 1, and an effect size of 1, we can use the following command:\n\npower.t.test(n = 5, delta = 1, sd = 1, sig.level = 0.05)\n\n\n     Two-sample t test power calculation \n\n              n = 5\n          delta = 1\n             sd = 1\n      sig.level = 0.05\n          power = 0.2859276\n    alternative = two.sided\n\nNOTE: n is number in *each* group\n\n\nThis gives a nice summary of the power calculation. We can also extract the power value from the result:\n\npower.t.test(n = 5, delta = 1, sd = 1, \n             sig.level = 0.05, type='one.sample')$power\n\n[1] 0.4013203\n\n\n\n\n\n\n\n\nTip\n\n\n\nWhen getting results from a function that don’t look “computable” such as those from power.t.test, you can use the $ operator to extract the value you want. In this case, we want the power value from the result of power.t.test.\nHow would you know what to extract? You can use the names function or the str function to see the structure of the result. For example:\n\nnames(power.t.test(n = 5, delta = 1, sd = 1, \n             sig.level = 0.05, type='one.sample'))\n\n[1] \"n\"           \"delta\"       \"sd\"          \"sig.level\"   \"power\"      \n[6] \"alternative\" \"note\"        \"method\"     \n\n# or \nstr(power.t.test(n = 5, delta = 1, sd = 1, \n             sig.level = 0.05, type='one.sample'))\n\nList of 8\n $ n          : num 5\n $ delta      : num 1\n $ sd         : num 1\n $ sig.level  : num 0.05\n $ power      : num 0.401\n $ alternative: chr \"two.sided\"\n $ note       : NULL\n $ method     : chr \"One-sample t test power calculation\"\n - attr(*, \"class\")= chr \"power.htest\"\n\n\n\n\nAlternatively, we may know a lot about our experimental system and want to calculate the sample size needed to achieve a certain power. For example, if we want to achieve a power of 0.8 with a standard deviation of 1 and an effect size of 1, we can use the following command:\n\npower.t.test(delta = 1, sd = 1, sig.level = 0.05, power = 0.8, type = \"one.sample\")\n\n\n     One-sample t test power calculation \n\n              n = 9.937864\n          delta = 1\n             sd = 1\n      sig.level = 0.05\n          power = 0.8\n    alternative = two.sided\n\n\nThe power.t.test function is convenient and quite fast. As we’ve seen before, though, sometimes the distribution of the test statistics is now easily calculated. In those cases, we can use simulation to calculate the power of a statistical test. For example, if we want to calculate the power of a one-sample t-test with a sample size of 5, a standard deviation of 1, and an effect size of 1, we can use the following command:\n\nsim_t_test_pval &lt;- function(n = 5, delta = 1, sd = 1, sig.level = 0.05) {\n    x = rnorm(n, delta, sd)\n    t.test(x)$p.value &lt;= sig.level\n}\npow = mean(replicate(1000, sim_t_test_pval()))\npow\n\n[1] 0.405\n\n\nLet’s break this down. First, we define a function called sim_t_test_pval that takes the same arguments as the power.t.test function. Inside the function, we simulate a sample of size n from a normal distribution with mean delta and standard deviation sd. Then, we perform a one-sample t-test on the sample and return a logical value indicating whether the p-value is less than the significance level. Next, we use the replicate function to repeat the simulation 1000 times. Finally, we calculate the proportion of simulations in which the p-value was less than the significance level. This proportion is an estimate of the power of the one-sample t-test.\nLet’s compare the results of the power.t.test function and our simulation-based approach:\n\npower.t.test(n = 5, delta = 1, sd = 1, sig.level = 0.05, type='one.sample')$power\n\n[1] 0.4013203\n\nmean(replicate(1000, sim_t_test_pval(n = 5, delta = 1, sd = 1, sig.level = 0.05)))\n\n[1] 0.414",
    "crumbs": [
      "Home",
      "statististics",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>The t-statistic and t-distribution</span>"
    ]
  },
  {
    "objectID": "t-stats-and-tests.html#resources",
    "href": "t-stats-and-tests.html#resources",
    "title": "\n16  The t-statistic and t-distribution\n",
    "section": "\n16.7 Resources",
    "text": "16.7 Resources\nSee the pwr package for more information on power calculations.\n\n\n\n\nStudent. 1908. “The Probable Error of a Mean.” Biometrika 6 (1): 1–25. https://doi.org/10.2307/2331554.",
    "crumbs": [
      "Home",
      "statististics",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>The t-statistic and t-distribution</span>"
    ]
  },
  {
    "objectID": "kmeans.html",
    "href": "kmeans.html",
    "title": "\n17  K-means clustering\n",
    "section": "",
    "text": "17.1 History of the k-means algorithm\nThe k-means clustering algorithm was first proposed by Stuart Lloyd in 1957 as a technique for pulse-code modulation. However, it was not published until 1982. In 1965, Edward W. Forgy published an essentially identical method, which became widely known as the k-means algorithm. Since then, k-means clustering has become one of the most popular unsupervised learning techniques in data analysis and machine learning.\nK-means clustering is a method for finding patterns or groups in a dataset. It is an unsupervised learning technique, meaning that it doesn’t rely on previously labeled data for training. Instead, it identifies structures or patterns directly from the data based on the similarity between data points (see Figure 17.1).\nIn simple terms, k-means clustering aims to divide a dataset into k distinct groups or clusters, where each data point belongs to the cluster with the nearest mean (average). The goal is to minimize the variability within each cluster while maximizing the differences between clusters. This helps to reveal hidden patterns or relationships in the data that might not be apparent otherwise.",
    "crumbs": [
      "Home",
      "statististics",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>K-means clustering</span>"
    ]
  },
  {
    "objectID": "kmeans.html#history-of-the-k-means-algorithm",
    "href": "kmeans.html#history-of-the-k-means-algorithm",
    "title": "\n17  K-means clustering\n",
    "section": "",
    "text": "Figure 17.1: K-means clustering takes a dataset and divides it into k clusters.",
    "crumbs": [
      "Home",
      "statististics",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>K-means clustering</span>"
    ]
  },
  {
    "objectID": "kmeans.html#the-k-means-algorithm",
    "href": "kmeans.html#the-k-means-algorithm",
    "title": "\n17  K-means clustering\n",
    "section": "\n17.2 The k-means algorithm",
    "text": "17.2 The k-means algorithm\nThe k-means algorithm follows these general steps:\n\nChoose the number of clusters k.\nInitialize the cluster centroids randomly by selecting k data points from the dataset.\nAssign each data point to the nearest centroid.\nUpdate the centroids by computing the mean of all the data points assigned to each centroid.\nRepeat steps 3 and 4 until the centroids no longer change or a certain stopping criterion is met (e.g., a maximum number of iterations).\n\nThe algorithm converges when the centroids stabilize or no longer change significantly. The final clusters represent the underlying patterns or structures in the data. Advantages and disadvantages of k-means clustering",
    "crumbs": [
      "Home",
      "statististics",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>K-means clustering</span>"
    ]
  },
  {
    "objectID": "kmeans.html#pros-and-cons-of-k-means-clustering",
    "href": "kmeans.html#pros-and-cons-of-k-means-clustering",
    "title": "\n17  K-means clustering\n",
    "section": "\n17.3 Pros and cons of k-means clustering",
    "text": "17.3 Pros and cons of k-means clustering\nCompared to other clustering algorithms, k-means has several advantages:\n\n\nSimplicity and ease of implementation\n\nThe k-means algorithm is relatively straightforward and can be easily implemented, even for large datasets.\n\n\n\nScalability\n\nThe algorithm can be adapted for large datasets using various optimization techniques or parallel processing.\n\n\n\nSpeed\n\nK-means is generally faster than other clustering algorithms, especially when the number of clusters k is small.\n\n\n\nInterpretability\n\nThe results of k-means clustering are easy to understand, as the algorithm assigns each data point to a specific cluster based on its similarity to the cluster’s centroid.\n\n\n\nHowever, k-means clustering has several disadvantages as well:\n\n\nChoice of k\n\nSelecting the appropriate number of clusters can be challenging and often requires domain knowledge or experimentation. A poor choice of k may yield poor results.\n\n\n\nSensitivity to initial conditions\n\nThe algorithm’s results can vary depending on the initial placement of centroids. To overcome this issue, the algorithm can be run multiple times with different initializations and the best solution can be chosen based on a criterion (e.g., minimizing within-cluster variation).\n\n\n\nAssumes spherical clusters\n\nK-means assumes that clusters are spherical and evenly sized, which may not always be the case in real-world datasets. This can lead to poor performance if the underlying clusters have different shapes or densities.\n\n\n\nSensitivity to outliers\n\nThe algorithm is sensitive to outliers, which can heavily influence the position of centroids and the final clustering result. Preprocessing the data to remove or mitigate the impact of outliers can help improve the performance of k-means clustering.\n\n\n\nDespite limitations, k-means clustering remains a popular and widely used method for exploring and analyzing data, particularly in biological data analysis, where identifying patterns and relationships can provide valuable insights into complex systems and processes.",
    "crumbs": [
      "Home",
      "statististics",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>K-means clustering</span>"
    ]
  },
  {
    "objectID": "kmeans.html#an-example-of-k-means-clustering",
    "href": "kmeans.html#an-example-of-k-means-clustering",
    "title": "\n17  K-means clustering\n",
    "section": "\n17.4 An example of k-means clustering",
    "text": "17.4 An example of k-means clustering\n\n17.4.1 The data and experimental background\nThe data we are going to use are from DeRisi, Iyer, and Brown (1997). From their abstract:\n\nDNA microarrays containing virtually every gene of Saccharomyces cerevisiae were used to carry out a comprehensive investigation of the temporal program of gene expression accompanying the metabolic shift from fermentation to respiration. The expression profiles observed for genes with known metabolic functions pointed to features of the metabolic reprogramming that occur during the diauxic shift, and the expression patterns of many previously uncharacterized genes provided clues to their possible functions.\n\nThese data are available from NCBI GEO as GSE28.\nIn the case of the baker’s or brewer’s yeast Saccharomyces cerevisiae growing on glucose with plenty of aeration, the diauxic growth pattern is commonly observed in batch culture. During the first growth phase, when there is plenty of glucose and oxygen available, the yeast cells prefer glucose fermentation to aerobic respiration even though aerobic respiration is the more efficient pathway to grow on glucose. This experiment profiles gene expression for 6400 genes over a time course during which the cells are undergoing a diauxic shift.\nThe data in deRisi et al. have no replicates and are time course data. Sometimes, seeing how groups of genes behave can give biological insight into the experimental system or the function of individual genes. We can use clustering to group genes that have a similar expression pattern over time and then potentially look at the genes that do so.\nOur goal, then, is to use kmeans clustering to divide highly variable (informative) genes into groups and then to visualize those groups.",
    "crumbs": [
      "Home",
      "statististics",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>K-means clustering</span>"
    ]
  },
  {
    "objectID": "kmeans.html#getting-data",
    "href": "kmeans.html#getting-data",
    "title": "\n17  K-means clustering\n",
    "section": "\n17.5 Getting data",
    "text": "17.5 Getting data\nThese data were deposited at NCBI GEO back in 2002. GEOquery can pull them out easily.\n\nlibrary(GEOquery)\ngse = getGEO(\"GSE28\")[[1]]\nclass(gse)\n\n[1] \"ExpressionSet\"\nattr(,\"package\")\n[1] \"Biobase\"\n\n\nGEOquery is a little dated and was written before the SummarizedExperiment existed. However, Bioconductor makes a conversion from the old ExpressionSet that GEOquery uses to the SummarizedExperiment that we see so commonly used now.\n\nlibrary(SummarizedExperiment)\ngse = as(gse, \"SummarizedExperiment\")\ngse\n\nclass: SummarizedExperiment \ndim: 6400 7 \nmetadata(3): experimentData annotation protocolData\nassays(1): exprs\nrownames(6400): 1 2 ... 6399 6400\nrowData names(20): ID ORF ... FAILED IS_CONTAMINATED\ncolnames(7): GSM887 GSM888 ... GSM892 GSM893\ncolData names(33): title geo_accession ... supplementary_file\n  data_row_count\n\n\nTaking a quick look at the colData(), it might be that we want to reorder the columns a bit.\n\ncolData(gse)$title\n\n[1] \"diauxic shift timecourse: 15.5 hr\" \"diauxic shift timecourse: 0 hr\"   \n[3] \"diauxic shift timecourse: 18.5 hr\" \"diauxic shift timecourse: 9.5 hr\" \n[5] \"diauxic shift timecourse: 11.5 hr\" \"diauxic shift timecourse: 13.5 hr\"\n[7] \"diauxic shift timecourse: 20.5 hr\"\n\n\nSo, we can reorder by hand to get the time course correct:\n\ngse = gse[, c(2,4,5,6,1,3,7)]",
    "crumbs": [
      "Home",
      "statististics",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>K-means clustering</span>"
    ]
  },
  {
    "objectID": "kmeans.html#preprocessing",
    "href": "kmeans.html#preprocessing",
    "title": "\n17  K-means clustering\n",
    "section": "\n17.6 Preprocessing",
    "text": "17.6 Preprocessing\nIn gene expression data analysis, the primary objective is often to identify genes that exhibit significant differences in expression levels across various conditions, such as diseased vs. healthy samples or different time points in a time-course experiment. However, gene expression datasets are typically large, noisy, and contain numerous genes that do not exhibit substantial changes in expression levels. Analyzing all genes in the dataset can be computationally intensive and may introduce noise or false positives in the results.\nOne common approach to reduce the complexity of the dataset and focus on the most informative genes is to subset the genes based on their standard deviation in expression levels across the samples. The standard deviation is a measure of dispersion or variability in the data, and genes with high standard deviations have more variation in their expression levels across the samples.\nBy selecting genes with high standard deviations, we focus on genes that show relatively large changes in expression levels across different conditions. These genes are more likely to be biologically relevant and involved in the underlying processes or pathways of interest. In contrast, genes with low standard deviations exhibit little or no change in expression levels and are less likely to be informative for the analysis. It turns out that applying filtering based on criteria such as standard deviation can also increase power and reduce false positives in the analysis (Bourgon, Gentleman, and Huber 2010).\nTo subset the genes for analysis based on their standard deviation, the following steps can be followed: Calculate the standard deviation of each gene’s expression levels across all samples. Set a threshold for the standard deviation, which can be determined based on domain knowledge, data distribution, or a specific percentile of the standard deviation values (e.g., selecting the top 10% or 25% of genes with the highest standard deviations). Retain only the genes with a standard deviation above the chosen threshold for further analysis.\nBy subsetting the genes based on their standard deviation, we can reduce the complexity of the dataset, speed up the subsequent analysis, and increase the likelihood of detecting biologically meaningful patterns and relationships in the gene expression data. The threshold for the standard deviation cutoff is rather arbitrary, so it may be beneficial to try a few to check for sensitivity of findings.\n\nsds = apply(assays(gse)[[1]], 1, sd)\nhist(sds)\n\n\n\n\n\n\nFigure 17.2: Histogram of standard deviations for all genes in the deRisi dataset.\n\n\n\n\nExamining the plot, we can see that the most highly variable genes have an sd &gt; 0.8 or so (arbitrary). We can, for convenience, create a new SummarizedExperiment that contains only our most highly variable genes.\n\nidx = sds&gt;0.8 & !is.na(sds)\ngse_sub = gse[idx,]",
    "crumbs": [
      "Home",
      "statististics",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>K-means clustering</span>"
    ]
  },
  {
    "objectID": "kmeans.html#clustering",
    "href": "kmeans.html#clustering",
    "title": "\n17  K-means clustering\n",
    "section": "\n17.7 Clustering",
    "text": "17.7 Clustering\nNow, gse_sub contains a subset of our data.\nThe kmeans function takes a matrix and the number of clusters as arguments.\n\nk = 4\nkm = kmeans(assays(gse_sub)[[1]], 4)\n\nThe km kmeans result contains a vector, km$cluster, which gives the cluster associated with each gene. We can plot the genes for each cluster to see how these different genes behave.\n\nexpression_values = assays(gse_sub)[[1]]\npar(mfrow=c(2,2), mar=c(3,4,1,2)) # this allows multiple plots per page\nfor(i in 1:k) {\n    matplot(t(expression_values[km$cluster==i, ]), type='l', ylim=c(-3,3),\n            ylab = paste(\"cluster\", i))\n}\n\n\n\n\n\n\nFigure 17.3: Gene expression profiles for the four clusters identified by k-means clustering. Each line represents a gene in the cluster, and each column represents a time point in the experiment. Each cluster shows a distinct trend where the genes in the cluster are potentially co-regulated.\n\n\n\n\nTry this with different size k. Perhaps go back to choose more genes (using a smaller cutoff for sd).",
    "crumbs": [
      "Home",
      "statististics",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>K-means clustering</span>"
    ]
  },
  {
    "objectID": "kmeans.html#summary",
    "href": "kmeans.html#summary",
    "title": "\n17  K-means clustering\n",
    "section": "\n17.8 Summary",
    "text": "17.8 Summary\nIn this lesson, we have learned how to use k-means clustering to identify groups of genes that behave similarly over time. We have also learned how to subset our data to focus on the most informative genes.\n\n\n\n\nBourgon, Richard, Robert Gentleman, and Wolfgang Huber. 2010. “Independent Filtering Increases Detection Power for High-Throughput Experiments.” Proceedings of the National Academy of Sciences 107 (21): 9546–51. https://doi.org/10.1073/pnas.0914005107.\n\n\nDeRisi, J. L., V. R. Iyer, and P. O. Brown. 1997. “Exploring the Metabolic and Genetic Control of Gene Expression on a Genomic Scale.” Science (New York, N.Y.) 278 (5338): 680–86. https://doi.org/10.1126/science.278.5338.680.",
    "crumbs": [
      "Home",
      "statististics",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>K-means clustering</span>"
    ]
  },
  {
    "objectID": "machine_learning/intro.html",
    "href": "machine_learning/intro.html",
    "title": "\n18  Introduction\n",
    "section": "",
    "text": "18.1 Types of Machine Learning\nMachine learning represents a fundamental shift in how we approach problem-solving with computers. Rather than explicitly programming every rule and decision path, machine learning allows algorithms to discover patterns in data and make predictions or decisions based on these learned patterns. This approach has proven particularly powerful for complex problems where traditional rule-based programming becomes unwieldy or where the underlying patterns are too subtle for humans to easily codify.\nAt its core, machine learning is about generalization. We want to build models that can take what they’ve learned from historical data and apply that knowledge to new, previously unseen situations. This capability makes machine learning invaluable across diverse fields, from predicting stock prices and diagnosing diseases to recognizing speech and recommending movies.\nMachine learning in biology is a really broad topic. Greener et al. (2022) present a nice overview of the different types of machine learning methods that are used in biology. Libbrecht and Noble (2015) also present an early review of machine learning in genetics and genomics.\nThe field of machine learning encompasses several distinct approaches, each suited to different types of problems and data structures. Understanding these categories helps practitioners choose appropriate methods and set realistic expectations for their projects.\nSupervised learning forms the foundation of most practical machine learning applications. In supervised learning, we have access to both input features and the correct answers (labels or targets) for our training examples. The algorithm learns to map inputs to outputs by studying these example pairs. Classification problems, where we predict discrete categories, and regression problems, where we predict continuous numerical values, both fall under supervised learning. For instance, predicting whether an email is spam (classification) or forecasting house prices (regression) are classic supervised learning tasks.\nUnsupervised learning tackles scenarios where we have input data but no predetermined correct answers. Instead of learning to predict specific outputs, unsupervised algorithms seek to discover hidden structures or patterns within the data itself. Clustering algorithms group similar data points together, while dimensionality reduction techniques identify the most important underlying factors that explain variation in the data. These methods often serve as exploratory tools, helping analysts understand their data better before applying supervised techniques.\nReinforcement learning takes a different approach entirely, focusing on learning through interaction with an environment. Rather than learning from fixed examples, reinforcement learning agents take actions and receive rewards or penalties, gradually improving their decision-making through trial and error. This approach has achieved remarkable success in game-playing scenarios and robotics, though it requires specialized techniques and considerable computational resources.\nWhen thinking about machine learning, it can help to have a simple framework in mind. In Figure 18.1, we present a simple view of machine learning according to the scikit-learn package.",
    "crumbs": [
      "Home",
      "Machine Learning",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "machine_learning/intro.html#types-of-machine-learning",
    "href": "machine_learning/intro.html#types-of-machine-learning",
    "title": "\n18  Introduction\n",
    "section": "",
    "text": "Learning Type\nData Requirements\nGoal\nCommon Applications\n\n\n\nSupervised\nInput-output pairs\nPredict labels/values\nClassification, regression\n\n\nUnsupervised\nInput data only\nDiscover patterns\nClustering, dimensionality reduction\n\n\nReinforcement\nEnvironment interaction\nOptimize decisions\nGame playing, robotics\n\n\n\n\n\n\n\n\n\nFigure 18.1: A simple view of machine learning according the sklearn.\n\n\n\n\n\n\n\n\nTerminology and Concepts\n\n\n\n\n\nData\n\nData is the foundation of machine learning and can be structured (tabular) or unstructured (text, images, audio). It is usually divided into training, validation, and testing sets for model development and evaluation.\n\n\n\nFeatures\n\nFeatures are the variables or attributes used to describe the data points. Feature engineering and selection are crucial steps in machine learning to improve model performance and interpretability.\n\n\n\nModels and Algorithms\n\nModels are mathematical representations of the relationship between features and the target variable(s). Algorithms are the methods used to train models, such as linear regression, decision trees, and neaural networks.\n\n\n\nHyperparameters and Tuning\n\nHyperparameters are adjustable parameters that control the learning process of an algorithm. Tuning involves finding the optimal set of hyperparameters to improve model performance.\n\n\n\nEvaluation Metrics\n\nEvaluation metrics quantify the performance of a model, such as accuracy, precision, recall, F1-score (for classification), and mean squared error, R-squared (for regression).",
    "crumbs": [
      "Home",
      "Machine Learning",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "machine_learning/intro.html#the-machine-learning-workflow",
    "href": "machine_learning/intro.html#the-machine-learning-workflow",
    "title": "\n18  Introduction\n",
    "section": "\n18.2 The Machine Learning Workflow",
    "text": "18.2 The Machine Learning Workflow\nSuccessful machine learning projects follow a structured workflow that ensures robust, reliable results. This process begins long before any algorithms are trained and extends well beyond initial model development.\n\n\n\n\n\n\nThe Machine Learning Workflow\n\n\n\nIn a nutshell, the machine learning workflow consists of the following steps:\n\n\nProblem Definition: Clearly define the problem and determine if machine learning is the right approach.\n\nData Collection and Preparation: Gather relevant data and preprocess it to make it suitable for modeling.\n\nData Splitting: Divide the data into training, validation, and test sets to ensure unbiased evaluation.\n\nModel Selection: Choose the appropriate machine learning algorithm and configure its hyperparameters.\n\nTraining: Fit the model to the training data, allowing it to learn patterns and relationships.\n\nEvaluation: Assess model performance using validation data and appropriate metrics.\n\nDeployment: Integrate the model into production systems for real-world use.\n\n\n\nThe journey starts with problem definition, where practitioners must clearly articulate what they’re trying to achieve and whether machine learning is the appropriate solution. Not every problem requires machine learning; sometimes simpler statistical methods or rule-based systems provide better solutions with less complexity and maintenance overhead.\nData collection and preparation typically consume the majority of time in real-world projects. Raw data rarely arrives in a format suitable for machine learning algorithms. Common preprocessing steps include handling missing values, encoding categorical variables, scaling numerical features, and addressing outliers. The quality of this preprocessing often determines the success or failure of the entire project.\nData splitting deserves special attention because it directly impacts the reliability of your results. The training set teaches the algorithm, but if we evaluate performance on the same data used for training, we get an overly optimistic view of how well our model will perform on new data. This is analogous to letting students see exam questions while studying and then using the same questions for the actual exam.\n\n\n\n\n\n\nThe Critical Importance of Data Splitting\n\n\n\nOne of the most crucial steps in the machine learning workflow is properly splitting your data into separate sets for training, validation, and testing. This separation serves as the foundation for honest evaluation and prevents overfitting.\n\n\nTraining data is used to fit the model parameters.\n\nValidation data helps select the best model architecture and hyperparameters.\n\nTest data provides a final, unbiased estimate of model performance on new data.\n\nThe golden rule: never use test data for any decision-making during model development. Reserve it exclusively for final evaluation.\n\n\nA common approach involves splitting data into 80% for training and 20% for testing. For more complex projects, a three-way split might use 60% for training, 20% for validation (model selection), and 20% for final testing. Cross-validation provides an alternative approach where the training data is repeatedly split into smaller training and validation sets, providing more robust estimates of model performance while still preserving an untouched test set.\n\n\n\n\n\nFigure 18.2: Data splitting and train/validate/test paradigm. For models without “hyperparameters”, only the training/testing sets are necessary. For models that require learning model structure (such as the k in k-nearest-neighbor) as well as model parameters (like betas in linear regression), a separate validation set is essential.\n\n\nModel selection involves choosing both the type of algorithm and its specific configuration. Different algorithms make different assumptions about the data and excel in different scenarios. Linear models work well when relationships are approximately linear and interpretability is important. Tree-based methods handle nonlinear relationships and interactions naturally but may overfit with limited data. Neural networks can model complex patterns but require large datasets and careful tuning. Hyperparameters are the parameters that describe the details of the model such as the depth of a decision tree, the choice of k in k-nearest neighbors, or the learning rate in gradient descent. Hyperparameter tuning is the process of finding the best values for these parameters, often using techniques like grid search or random search combined with cross-validation.\nTraining is where the algorithm actually learns from the data. During this phase, the model adjusts its internal parameters to minimize prediction errors on the training set. However, the goal isn’t to achieve perfect performance on training data. Models that fit training data too closely often fail to generalize to new examples, a problem known as overfitting.\nEvaluation determines whether our model is ready for deployment. This involves multiple metrics beyond simple accuracy, including precision, recall, F1-score for classification, or mean squared error and R-squared for regression. We also examine model behavior across different subgroups in our data to ensure fair and consistent performance.",
    "crumbs": [
      "Home",
      "Machine Learning",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "machine_learning/intro.html#understanding-overfitting-and-underfitting",
    "href": "machine_learning/intro.html#understanding-overfitting-and-underfitting",
    "title": "\n18  Introduction\n",
    "section": "\n18.3 Understanding Overfitting and Underfitting",
    "text": "18.3 Understanding Overfitting and Underfitting\nRecall that the goal of machine learning is to build models that generalize well to new, unseen data. Any model that is complex enough can perfectly model data given to it. However, achieving this balance between fitting the training data and maintaining good performance on new examples (ie., generalization) is the goal, not just performing well on the training data.\nThe concept of overfitting represents one of the central challenges in machine learning. When a model overfits, it learns the training data too well, memorizing specific examples rather than generalizing patterns. This results in excellent performance on training data but poor performance on new, unseen data.\nImagine teaching someone to recognize cats by showing them 100 photos. An overfitted learner might memorize every pixel of those specific photos rather than learning general features like whiskers, pointed ears, and fur patterns. When shown new cat photos, they would struggle because they focused on irrelevant details specific to the training images.\nUnderfitting represents the opposite problem. An underfitted model is too simple to capture the underlying patterns in the data. It performs poorly on both training and test data because it lacks the complexity needed to model the relationships present in the data. Continuing our cat recognition analogy, an underfitted model might only look at image brightness and miss all the important visual features that distinguish cats from other animals.\n\nCodeset.seed(123)\nsinsim &lt;- function(n,sd=0.1) {\n  x &lt;- seq(0,1,length.out=n)\n  y &lt;- sin(2*pi*x) + rnorm(n,0,sd)\n  return(data.frame(x=x,y=y))\n}\ndat &lt;- sinsim(100,0.25)\nlibrary(ggplot2)\nlibrary(patchwork)\np_base &lt;- ggplot(dat,aes(x=x,y=y)) +\n geom_point(alpha=0.7) +\n theme_bw()\np_lm &lt;- p_base + \n geom_smooth(method=\"lm\", se=FALSE, alpha=0.6, formula = y ~ x) \np_lmsin &lt;- p_base +\n geom_smooth(method=\"lm\",formula=y~sin(2*pi*x), se=FALSE, alpha=0.6) \np_loess_wide &lt;- p_base +\n  geom_smooth(method=\"loess\",span=0.5, se=FALSE, alpha=0.6, formula = y ~ x) \np_loess_narrow &lt;- p_base + \n geom_smooth(method=\"loess\",span=0.05, se=FALSE, alpha=0.6, formula = y ~ x) \np_lm + p_lmsin + p_loess_wide + p_loess_narrow + plot_layout(ncol=2) +\n  plot_annotation(tag_levels = 'A') & \n  theme(plot.tag = element_text(size = 8))\n\nWarning in simpleLoess(y, x, w, span, degree = degree, parametric = parametric,\n: k-d tree limited by memory. ncmax= 200\n\n\nWarning in sqrt(sum.squares/one.delta): NaNs produced\n\n\n\n\n\n\n\nFigure 18.3: Data simulated according to the function \\(f(x) = sin(2 \\pi x) + N(0,0.25)\\) fitted with four different models. A) A simple linear model demonstrates underfitting. B) A linear model with a sin function (\\(y = sin(2 \\pi x)\\)) and C) a loess model with a wide span (0.5) demonstrate good fits. D) A loess model with a narrow span (0.05) is a good example of overfitting.\n\n\n\n\nIn Figure 18.3, we simulate data according to the function \\(f(x) = sin(2 \\pi x) + N(0,0.25)\\) and fit four different models. Choosing a model that is too simple (A) will result in underfitting the data, while choosing a model that is too complex (D) will result in overfitting the data. For model (D), the loess model with a narrow span, the fitted line follows the noise in the data too closely, capturing random fluctuations rather than the underlying trend. In contrast, models (B) and (C) demonstrate good fits, capturing the essential pattern without being overly complex.\nThe relationship between model complexity and performance often follows a characteristic U-shaped curve. Very simple models underperform due to underfitting. As complexity increases, performance improves as the model captures more relevant patterns. However, beyond an optimal point, additional complexity leads to overfitting and degraded performance on new data.\n\n\n\n\n\n\nDetecting Overfitting\n\n\n\nOverfitting reveals itself through a growing gap between training and validation performance. If your model achieves 95% accuracy on training data but only 70% on validation data, you’re likely overfitting.\nMonitor both training and validation metrics throughout model development. The best models show similar performance on both sets, indicating good generalization.\n\n\nSeveral strategies help combat overfitting. Regularization techniques add penalties for model complexity, encouraging simpler solutions. Cross-validation provides more robust estimates of model performance by training and validating on multiple data splits. Early stopping halts training when validation performance begins to degrade, even if training performance continues improving. Feature selection reduces the number of input variables, focusing the model on the most relevant information.\nThe bias-variance tradeoff provides a theoretical framework for understanding these phenomena. Bias refers to systematic errors due to overly simple models, while variance refers to sensitivity to small changes in training data. High-bias models underfit, while high-variance models overfit. The optimal model balances these competing sources of error.",
    "crumbs": [
      "Home",
      "Machine Learning",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "machine_learning/intro.html#cross-validation-and-model-selection",
    "href": "machine_learning/intro.html#cross-validation-and-model-selection",
    "title": "\n18  Introduction\n",
    "section": "\n18.4 Cross-Validation and Model Selection",
    "text": "18.4 Cross-Validation and Model Selection\nCross-validation addresses a fundamental challenge in machine learning: how do we reliably estimate model performance when we have limited data? Simple train-test splits can be misleading, especially with small datasets, because performance estimates depend heavily on which specific examples end up in each set.\nK-fold cross-validation provides a more robust solution. The training data is divided into k equal-sized subsets (folds). The model is trained k times, each time using k-1 folds for training and the remaining fold for validation. This process yields k performance estimates, which can be averaged to get a more stable assessment of model quality.\nFive-fold and ten-fold cross-validation are common choices, providing good balance between computational efficiency and reliable estimates. Leave-one-out cross-validation represents an extreme case where k equals the number of training examples. While this maximizes the use of training data, it can be computationally expensive and may provide overly optimistic estimates for some types of models.\nStratified cross-validation ensures that each fold maintains the same proportion of examples from each class, which is particularly important for classification problems with imbalanced datasets. Time series data requires special consideration, as temporal order matters. Time series cross-validation uses only past data to predict future values, respecting the temporal structure.\nCross-validation serves multiple purposes in the model development process. It helps select the best algorithm from a set of candidates, tune hyperparameters within a chosen algorithm, and provide realistic estimates of expected performance on new data. However, it’s crucial to remember that cross-validation results still come from the training data. Final model evaluation should always use a completely separate test set.\n\n\n\n\nGreener, Joe G., Shaun M. Kandathil, Lewis Moffat, and David T. Jones. 2022. “A Guide to Machine Learning for Biologists.” Nature Reviews Molecular Cell Biology 23 (1): 40–55. https://doi.org/10.1038/s41580-021-00407-0.\n\n\nLibbrecht, Maxwell W., and William Stafford Noble. 2015. “Machine Learning Applications in Genetics and Genomics.” Nature Reviews Genetics 16 (6): 321–32. https://doi.org/10.1038/nrg3920.",
    "crumbs": [
      "Home",
      "Machine Learning",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "machine_learning/models.html",
    "href": "machine_learning/models.html",
    "title": "\n19  Supervised Learning\n",
    "section": "",
    "text": "19.1 A Framework for Understanding Supervised Algorithms\nIn the previous chapter, we introduced the three main paradigms of machine learning: supervised, unsupervised, and reinforcement learning. We also highlighted the importance of a structured workflow, particularly the critical step of data splitting to prevent overfitting. Now, we’ll dive deeper into the world of supervised learning, the most common and widely applied category of machine learning.\nThis chapter provides a practical framework for understanding how different supervised learning algorithms work. Instead of viewing each algorithm as a completely separate entity, we’ll see that they all share a common conceptual structure. By grasping this structure, you can more easily understand, compare, and select the right algorithm for your problem. We will then use this framework to explore some of the most fundamental and popular algorithms for both regression and classification tasks.\nAt a high level, every supervised learning algorithm can be understood as having three core components: the hypothesis space, the objective function, and the learning algorithm. Thinking about new algorithms in terms of these three parts can demystify how they operate and highlight their key differences.\nBy framing algorithms in this way, we can see, for example, that Ridge and Lasso regression share the same hypothesis space and learning algorithm as linear regression but differ in their objective function, which includes a penalty for complexity. This small change in the objective function has profound effects on the final model, helping to prevent overfitting.",
    "crumbs": [
      "Home",
      "Machine Learning",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Supervised Learning</span>"
    ]
  },
  {
    "objectID": "machine_learning/models.html#a-framework-for-understanding-supervised-algorithms",
    "href": "machine_learning/models.html#a-framework-for-understanding-supervised-algorithms",
    "title": "\n19  Supervised Learning\n",
    "section": "",
    "text": "The Three Pillars of a Supervised Algorithm\n\n\n\n\nHypothesis Space (The Model Family): This is the set of all possible models the algorithm is allowed to consider. For example, in linear regression, the hypothesis space is the set of all possible straight lines. For a decision tree, it’s the set of all possible branching rules. The choice of hypothesis space imposes a certain structure or bias on the solution.\nObjective Function (The Goal): This function defines what a “good” model looks like. It measures how well a particular model from the hypothesis space fits the training data. For instance, in regression, a common objective function is the Mean Squared Error (MSE), which penalizes models for making large prediction errors. The goal of learning is to find the model that minimizes this objective function.\nLearning Algorithm (The Optimizer): This is the computational procedure used to search through the hypothesis space and find the model that best minimizes the objective function. This could be a straightforward mathematical formula (as in simple linear regression) or a complex iterative process (as in training neural networks).",
    "crumbs": [
      "Home",
      "Machine Learning",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Supervised Learning</span>"
    ]
  },
  {
    "objectID": "machine_learning/models.html#supervised-learning",
    "href": "machine_learning/models.html#supervised-learning",
    "title": "\n19  Supervised Learning\n",
    "section": "\n19.2 Supervised Learning",
    "text": "19.2 Supervised Learning\n\n19.2.1 Linear regression\nIn statistics, linear regression is a linear approach for modelling the relationship between a scalar response and one or more explanatory variables (also known as dependent and independent variables). The case of one explanatory variable is called simple linear regression; for more than one, the process is called multiple linear regression. This term is distinct from multivariate linear regression, where multiple correlated dependent variables are predicted, rather than a single scalar variable.\nIn linear regression, the relationships are modeled using linear predictor functions whose unknown model parameters are estimated from the data. Such models are called linear models. Most commonly, the conditional mean of the response given the values of the explanatory variables (or predictors) is assumed to be an affine function of those values; less commonly, the conditional median or some other quantile is used. Like all forms of regression analysis, linear regression focuses on the conditional probability distribution of the response given the values of the predictors, rather than on the joint probability distribution of all of these variables, which is the domain of multivariate analysis.\nLinear regression was the first type of regression analysis to be studied rigorously, and to be used extensively in practical applications. This is because models which depend linearly on their unknown parameters are easier to fit than models which are non-linearly related to their parameters and because the statistical properties of the resulting estimators are easier to determine.\n\n19.2.2 K-nearest Neighbor\n\n\n\n\nFigure. The k-nearest neighbor algorithm can be used for regression or classification.\n\n\n\nThe k-nearest neighbors algorithm (k-NN) is a non-parametric supervised learning method first developed by Evelyn Fix and Joseph Hodges in 1951, and later expanded by Thomas Cover. It is used for classification and regression. In both cases, the input consists of the k closest training examples in a data set.\nThe k-nearest neighbor (k-NN) algorithm is a simple, yet powerful, supervised machine learning method used for classification and regression tasks. It is an instance-based, non-parametric learning method that stores the entire training dataset and makes predictions based on the similarity between data points. The underlying principle of the k-NN algorithm is that similar data points (those that are close to each other in multidimensional space) are likely to have similar outcomes or belong to the same class.\nHere’s a description of how the k-NN algorithm works:\n\nDetermine the value of k: The first step is to choose the number of nearest neighbors (k) to consider when making predictions. The value of k is a user-defined hyperparameter and can significantly impact the algorithm’s performance. A small value of k can lead to overfitting, while a large value may result in underfitting.\nCompute distance: Calculate the distance between the new data point (query point) and each data point in the training dataset. The most common distance metrics used are Euclidean, Manhattan, and Minkowski distance. The choice of distance metric depends on the problem and the nature of the data.\nFind k-nearest neighbors: Identify the k data points in the training dataset that are closest to the query point, based on the chosen distance metric.\nMake predictions: Once the k-nearest neighbors are identified, the final step is to make predictions. The prediction for the query point can be made in two ways:\n\nFor classification, determine the class labels of the k-nearest neighbors and assign the class label with the highest frequency (majority vote) to the query point. In case of a tie, one can choose the class with the smallest average distance to the query point or randomly select one among the tied classes.\nFor regression tasks, the k-NN algorithm follows a similar process, but instead of majority voting, it calculates the mean (or median) of the target values of the k-nearest neighbors and assigns it as the prediction for the query point.\n\n\n\nThe k-NN algorithm is known for its simplicity, ease of implementation, and ability to handle multi-class problems. However, it has some drawbacks, such as high computational cost (especially for large datasets), sensitivity to the choice of k and distance metric, and poor performance with high-dimensional or noisy data. Scaling and preprocessing the data, as well as using dimensionality reduction techniques, can help mitigate some of these issues.\n\nIn k-NN classification, the output is a class membership. An object is classified by a plurality vote of its neighbors, with the object being assigned to the class most common among its k nearest neighbors (k is a positive integer, typically small). If k = 1, then the object is simply assigned to the class of that single nearest neighbor.\nIn k-NN regression, the output is the property value for the object. This value is the average of the values of k nearest neighbors.\n\nk-NN is a type of classification where the function is only approximated locally and all computation is deferred until function evaluation. Since this algorithm relies on distance for classification, if the features represent different physical units or come in vastly different scales then normalizing the training data can improve its accuracy dramatically.\nBoth for classification and regression, a useful technique can be to assign weights to the contributions of the neighbors, so that the nearer neighbors contribute more to the average than the more distant ones. For example, a common weighting scheme consists in giving each neighbor a weight of 1/d, where d is the distance to the neighbor.\nThe neighbors are taken from a set of objects for which the class (for k-NN classification) or the object property value (for k-NN regression) is known. This can be thought of as the training set for the algorithm, though no explicit training step is required.",
    "crumbs": [
      "Home",
      "Machine Learning",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Supervised Learning</span>"
    ]
  },
  {
    "objectID": "machine_learning/models.html#penalized-regression",
    "href": "machine_learning/models.html#penalized-regression",
    "title": "\n19  Supervised Learning\n",
    "section": "\n19.3 Penalized regression",
    "text": "19.3 Penalized regression\nAdapted from http://www.sthda.com/english/articles/37-model-selection-essentials-in-r/153-penalized-regression-essentials-ridge-lasso-elastic-net/.\nPenalized regression is a type of regression analysis that introduces a penalty term to the loss function in order to prevent overfitting and improve the model’s ability to generalize. Remember that in regression, the loss function is the sum of squares Equation 19.1.\n\\[L = \\sum_{i=0}^{n}{(\\hat{y}_i - y_i)}^2 \\tag{19.1}\\]\nIn Equation 19.1, \\(\\hat{y}_i\\) is the predicted output, \\(y_i\\) is the actual output, and n is the number of observations. The goal of regression is to minimize the loss function by finding the optimal values of the model parameters or coefficients. The model parameters are estimated using the training data. The model is then evaluated using the test data. If the model performs well on the training data but poorly on the test data, it is said to be overfit. Overfitting occurs when the model learns the training data too well, including the noise, and is not able to generalize well to new data. This is a common problem in machine learning, particularly when there are a large number of predictors compared to the number of observations, and can be addressed by penalized regression.\nThe two most common types of penalized regression are Ridge Regression (L2 penalty) and LASSO Regression (L1 penalty). Both Ridge and LASSO help to reduce model complexity and prevent over-fitting which may result from simple linear regression. However, the choice between Ridge and LASSO depends on the situation and the dataset at hand. If feature selection is important for the interpretation of the model, LASSO might be preferred. If the goal is prediction accuracy and the model needs to retain all features, Ridge might be the better choice.\n\n19.3.1 Ridge regression\nRidge regression shrinks the regression coefficients, so that variables, with minor contribution to the outcome, have their coefficients close to zero. The shrinkage of the coefficients is achieved by penalizing the regression model with a penalty term called L2-norm, which is the sum of the squared coefficients. The amount of the penalty can be fine-tuned using a constant called lambda (λ). Selecting a good value for λ is critical. When λ=0, the penalty term has no effect, and ridge regression will produce the classical least square coefficients. However, as λ increases to infinite, the impact of the shrinkage penalty grows, and the ridge regression coefficients will get close zero. The loss function for Ridge Regression is:\n\\[L = \\sum_{i=0}^{n}{(\\hat{y}_i - y_i)}^2 + \\lambda\\sum_{j=0}^{k} \\beta_j^2 \\tag{19.2}\\]\nHere, \\(\\hat{y}_i\\) is the predicted output, \\(y_i\\) is the actual output, \\({\\beta_j}\\) represents the model parameters or coefficients, and λ is the regularization parameter. The second term, λ∑βj^2, is the penalty term where all parameters are squared and summed. Ridge regression tends to shrink the coefficients but doesn’t necessarily zero them.\nNote that, in contrast to the ordinary least square regression, ridge regression is highly affected by the scale of the predictors. Therefore, it is better to standardize (i.e., scale) the predictors before applying the ridge regression (James et al. 2014), so that all the predictors are on the same scale. The standardization of a predictor x, can be achieved using the formula \\(x' = \\frac{x}{sd(x)}\\), where \\(sd(x)\\) is the standard deviation of \\(x\\). The consequence of this is that, all standardized predictors will have a standard deviation of one allowing the final fit to not depend on the scale on which the predictors are measured.\nOne important advantage of the ridge regression, is that it still performs well, compared to the ordinary least square method (see Equation 19.1), in a situation where you have a large multivariate data with the number of predictors (p) larger than the number of observations (n). One disadvantage of the ridge regression is that, it will include all the predictors in the final model, unlike the stepwise regression methods, which will generally select models that involve a reduced set of variables. Ridge regression shrinks the coefficients towards zero, but it will not set any of them exactly to zero. The LASSO regression is an alternative that overcomes this drawback.\n\n19.3.2 LASSO regression\nLASSO stands for Least Absolute Shrinkage and Selection Operator. It shrinks the regression coefficients toward zero by penalizing the regression model with a penalty term called L1-norm, which is the sum of the absolute coefficients. In the case of LASSO regression, the penalty has the effect of forcing some of the coefficient estimates, with a minor contribution to the model, to be exactly equal to zero. This means that, LASSO can be also seen as an alternative to the subset selection methods for performing variable selection in order to reduce the complexity of the model. As in ridge regression, selecting a good value of \\(\\lambda\\) for the LASSO is critical. The loss function for LASSO Regression is:\n\\[L = \\sum_{i=0}^{n}{(\\hat{y}_i - y_i)}^2 + \\lambda\\sum_{j=0}^{k} |\\beta_j| \\tag{19.3}\\]\nSimilar to Ridge, ŷi is the predicted output, yi is the actual output, βj represents the model parameters or coefficients, and λ is the regularization parameter. The second term, λ∑|βj|, is the penalty term where the absolute values of all parameters are summed. LASSO regression tends to shrink the coefficients and can zero out some of them, effectively performing variable selection.\nOne obvious advantage of LASSO regression over ridge regression, is that it produces simpler and more interpretable models that incorporate only a reduced set of the predictors. However, neither ridge regression nor the LASSO will universally dominate the other. Generally, LASSO might perform better in a situation where some of the predictors have large coefficients, and the remaining predictors have very small coefficients. Ridge regression will perform better when the outcome is a function of many predictors, all with coefficients of roughly equal size (James et al. 2014).\nCross-validation methods can be used for identifying which of these two techniques is better on a particular data set.\n\n19.3.3 Elastic Net\nElastic Net produces a regression model that is penalized with both the L1-norm and L2-norm. The consequence of this is to effectively shrink coefficients (like in ridge regression) and to set some coefficients to zero (as in LASSO).\n\n19.3.4 Classification and Regression Trees (CART)\nDecision Tree Learning is supervised learning approach used in statistics, data mining and machine learning. In this formalism, a classification or regression decision tree is used as a predictive model to draw conclusions about a set of observations. Decision trees are a popular machine learning method used for both classification and regression tasks. They are hierarchical, tree-like structures that model the relationship between features and the target variable by recursively splitting the data into subsets based on the feature values. Each internal node in the tree represents a decision or test on a feature, and each branch represents the outcome of that test. The leaf nodes contain the final prediction, which is the majority class for classification tasks or the mean/median of the target values for regression tasks.\n\n\n\n\n\nFigure 19.1: Graphical representation of a decision tree.\n\n\nHere’s an overview of the decision tree learning process:\n\nSelect the best feature and split value: Start at the root node and choose the feature and split value that results in the maximum reduction of impurity (or increase in information gain) in the child nodes. For classification tasks, impurity measures like Gini index or entropy are commonly used, while for regression tasks, mean squared error (MSE) or mean absolute error (MAE) can be used.\nSplit the data: Partition the dataset into subsets based on the chosen feature and split value.\nRecursion: Repeat steps 1 and 2 for each subset until a stopping criterion is met. Stopping criteria can include reaching a maximum tree depth, a minimum number of samples per leaf, or no further improvement in impurity.\nPrune the tree (optional): To reduce overfitting, decision trees can be pruned by removing branches that do not significantly improve the model’s performance on the validation dataset. This can be done using techniques like reduced error pruning or cost-complexity pruning.\n\nDecision trees have several advantages, such as:\n\n\nInterpretability\n\nThey are easy to understand, visualize, and explain, even for non-experts.\n\n\n\nMinimal data preprocessing\n\nDecision trees can handle both numerical and categorical data, and they are robust to outliers and missing values.\n\n\n\nNon-linear relationships\n\nThey can capture complex non-linear relationships between features and the target variable.\n\n\n\nHowever, decision trees also have some drawbacks:\n\n\nOverfitting\n\nThey are prone to overfitting, especially when the tree is deep or has few samples per leaf. Pruning and setting stopping criteria can help mitigate this issue.\n\n\n\nInstability\n\nSmall changes in the data can lead to different tree structures. This can be addressed by using ensemble methods like random forests or gradient boosting machines (GBMs).\n\n\n\nGreedy learning\n\nDecision tree algorithms use a greedy approach, meaning they make locally optimal choices at each node. This may not always result in a globally optimal tree.\n\n\n\nDespite these limitations, decision trees are widely used in various applications due to their simplicity, interpretability, and ability to handle diverse data types.\n\n19.3.5 RandomForest\nRandom forests or random decision forests is an ensemble learning method for classification, regression and other tasks that operates by constructing a multitude of decision trees at training time. For classification tasks, the output of the random forest is the class selected by most trees. For regression tasks, the mean or average prediction of the individual trees is returned. Random decision forests correct for decision trees’ habit of overfitting to their training set. Random forests generally outperform decision trees, but their accuracy is lower than gradient boosted trees[citation needed]. However, data characteristics can affect their performance.\nThe first algorithm for random decision forests was created in 1995 by Tin Kam Ho using the random subspace method, which, in Ho’s formulation, is a way to implement the “stochastic discrimination” approach to classification proposed by Eugene Kleinberg.\nAn extension of the algorithm was developed by Leo Breiman and Adele Cutler, who registered “Random Forests” as a trademark in 2006 (as of 2019[update], owned by Minitab, Inc.). The extension combines Breiman’s “bagging” idea and random selection of features, introduced first by Ho and later independently by Amit and Geman in order to construct a collection of decision trees with controlled variance.\nRandom forests are frequently used as “blackbox” models in businesses, as they generate reasonable predictions across a wide range of data while requiring little configuration.\n\n\n\n\n\nFigure 19.2: Graphical representation of random forests.",
    "crumbs": [
      "Home",
      "Machine Learning",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Supervised Learning</span>"
    ]
  },
  {
    "objectID": "machine_learning/mlr3verse_intro.html",
    "href": "machine_learning/mlr3verse_intro.html",
    "title": "\n20  The mlr3verse\n",
    "section": "",
    "text": "20.1 The Philosophy of mlr3verse\nThe R ecosystem offers numerous packages for machine learning, each with its own interface, conventions, and capabilities. While this diversity provides flexibility, it also creates challenges for practitioners who must learn different APIs for different algorithms and spend time on repetitive data preparation tasks. The mlr3verse addresses these challenges by providing a unified, consistent framework for machine learning in R.\nThe mlr3verse follows a modular design philosophy that separates different aspects of the machine learning workflow into distinct, interchangeable components. This separation of concerns makes it easier to experiment with different combinations of preprocessing steps, algorithms, and evaluation strategies without rewriting code.\nRather than monolithic functions that handle everything from data preprocessing to model evaluation, mlr3verse provides specialized objects for each component of the machine learning pipeline. Tasks define the problem and data, learners implement algorithms, measures specify evaluation metrics, and resamplings control validation strategies. This modular approach promotes code reusability and makes it easier to understand and maintain complex machine learning workflows.\nThe framework emphasizes object-oriented design, leveraging R6 classes to provide consistent interfaces across different components. This design ensures that similar operations work the same way regardless of which specific algorithm or evaluation metric you’re using. Once you learn the basic patterns, you can easily work with new algorithms and techniques.\nReproducibility receives special attention in mlr3verse. All random operations can be controlled through seed settings, and the framework provides tools to track and reproduce experimental results. This focus on reproducibility is essential for scientific applications and helps practitioners debug and iterate on their models.",
    "crumbs": [
      "Home",
      "Machine Learning",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>The mlr3verse</span>"
    ]
  },
  {
    "objectID": "machine_learning/mlr3verse_intro.html#the-mlr3verse-ecosystem",
    "href": "machine_learning/mlr3verse_intro.html#the-mlr3verse-ecosystem",
    "title": "\n20  The mlr3verse\n",
    "section": "\n20.2 The mlr3verse Ecosystem",
    "text": "20.2 The mlr3verse Ecosystem\nThe mlr3verse consists of several interconnected packages, each focused on specific aspects of machine learning. Understanding these components helps practitioners choose the right tools for their projects and leverage the full power of the ecosystem.\n\n\n\n\n\n\n\nPackage\nPurpose\nKey Features\n\n\n\nmlr3\nCore framework\nTasks, learners, measures, resampling\n\n\nmlr3learners\nExtended algorithms\nAdditional ML algorithms beyond base mlr3\n\n\nmlr3pipelines\nPreprocessing & pipelines\nFeature engineering, model stacking\n\n\nmlr3tuning\nHyperparameter optimization\nGrid search, random search, Bayesian optimization\n\n\nmlr3measures\nAdditional metrics\nExtended evaluation measures\n\n\nmlr3viz\nVisualization\nPlotting utilities for models and results\n\n\nmlr3filters\nFeature selection\nFilter-based feature selection methods\n\n\n\nThe core mlr3 package provides the foundation, implementing basic tasks, learners, and evaluation procedures. It includes common algorithms like linear regression, logistic regression, and decision trees, along with fundamental evaluation metrics and resampling strategies.\nmlr3learners extends the available algorithms significantly, providing interfaces to popular R packages like randomForest, glmnet, and xgboost. This extension allows practitioners to access state-of-the-art algorithms while maintaining the consistent mlr3 interface.\nmlr3pipelines introduces powerful preprocessing and model composition capabilities. It allows users to chain together multiple preprocessing steps, combine different algorithms, and create complex model architectures. This package particularly shines in scenarios requiring sophisticated feature engineering or ensemble methods.\nmlr3tuning automates the search for optimal hyperparameters, implementing various optimization strategies from simple grid search to sophisticated Bayesian optimization. Hyperparameter tuning can dramatically improve model performance, but it requires careful validation to avoid overfitting.",
    "crumbs": [
      "Home",
      "Machine Learning",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>The mlr3verse</span>"
    ]
  },
  {
    "objectID": "machine_learning/mlr3verse_intro.html#core-mlr3-concepts-and-objects",
    "href": "machine_learning/mlr3verse_intro.html#core-mlr3-concepts-and-objects",
    "title": "\n20  The mlr3verse\n",
    "section": "\n20.3 Core mlr3 Concepts and Objects",
    "text": "20.3 Core mlr3 Concepts and Objects\nThe mlr3 framework organizes machine learning workflows around four fundamental object types:\n\n\nTasks provide the data and problem definition.\n\nLearners implement the algorithms.\n\nMeasures evaluate performance.\n\nResamplings ensure robust validation.\n\nUnderstanding these objects and how they interact forms the foundation for effective use of the mlr3verse. The goal is to provide a consistent, reusable interface for machine learning tasks, allowing practitioners to focus on solving problems rather than learning different APIs.\nTasks encapsulate the machine learning problem, combining data with metadata about the prediction target and feature types. Classification tasks specify categorical targets, while regression tasks involve continuous targets. Tasks handle many routine data management operations automatically, such as identifying feature types and managing factor levels. They also provide methods for data manipulation, including filtering rows, selecting features, and creating subsets.\n\n\nLoading required package: mlr3\n\n\n\n# Create a classification task with Palmer Penguins data\ntask &lt;- as_task_classif(penguins, target = \"species\")\n\n# Examine task properties\ntask$nrow  # Number of observations\n\n[1] 333\n\ntask$ncol  # Number of features\n\n[1] 8\n\ntask$feature_names  # Names of predictor variables\n\n[1] \"bill_depth_mm\"     \"bill_length_mm\"    \"body_mass_g\"      \n[4] \"flipper_length_mm\" \"island\"            \"sex\"              \n[7] \"year\"             \n\ntask$target_names  # Name of target variable\n\n[1] \"species\"\n\n\nLearners implement machine learning algorithms, providing a consistent interface regardless of the underlying implementation. Each learner specifies its capabilities, including which task types it supports, whether it can handle missing values, and what hyperparameters are available for tuning. Learners maintain information about their training state and can generate predictions on new data.\nThe learner registry provides a convenient way to discover available algorithms. You can query the registry to find learners for specific task types or search for algorithms with particular capabilities.\n\n# Explore available classification learners\nclassif_learners &lt;- mlr_learners$keys(\"classif\")\nhead(classif_learners, 10)\n\n [1] \"classif.cv_glmnet\"   \"classif.debug\"       \"classif.featureless\"\n [4] \"classif.glmnet\"      \"classif.kknn\"        \"classif.lda\"        \n [7] \"classif.log_reg\"     \"classif.multinom\"    \"classif.naive_bayes\"\n[10] \"classif.nnet\"       \n\n# Examine a specific learner\nrpart_learner &lt;- lrn(\"classif.rpart\")\nrpart_learner$param_set$ids()  # Available hyperparameters\n\n [1] \"cp\"             \"keep_model\"     \"maxcompete\"     \"maxdepth\"      \n [5] \"maxsurrogate\"   \"minbucket\"      \"minsplit\"       \"surrogatestyle\"\n [9] \"usesurrogate\"   \"xval\"          \n\n\n\n\n\n\n\n\nLearner Naming Convention\n\n\n\nmlr3 uses a consistent naming scheme for learners: [task_type].[algorithm]. For example, classif.rpart implements decision trees for classification, while regr.lm provides linear regression. This naming makes it easy to find appropriate algorithms for your task type.\n\n\nMeasures define evaluation metrics for assessing model performance. Different measures emphasize different aspects of model quality, and the choice of measure can significantly impact model selection and hyperparameter tuning. Classification measures include accuracy, precision, recall, and F1-score, while regression measures encompass mean squared error, mean absolute error, and R-squared.\n\n# Common classification measures\nacc_measure &lt;- msr(\"classif.acc\")  # Accuracy\nce_measure &lt;- msr(\"classif.ce\")    # Classification error\nauc_measure &lt;- msr(\"classif.auc\")  # Area under ROC curve\n\nResamplings control how data is split for training and validation. They implement various strategies including simple holdout splits, k-fold cross-validation, and bootstrap sampling. The choice of resampling strategy affects both the reliability of performance estimates and computational requirements.\n\n# Different resampling strategies\nholdout &lt;- rsmp(\"holdout\", ratio = 0.8)  # 80/20 split\ncv5 &lt;- rsmp(\"cv\", folds = 5)             # 5-fold cross-validation\nbootstrap &lt;- rsmp(\"bootstrap\", repeats = 30)  # Bootstrap sampling\n\n# Examine resampling properties\ncv5$param_set\n\n&lt;ParamSet(1)&gt;\n       id    class lower upper nlevels        default  value\n   &lt;char&gt;   &lt;char&gt; &lt;int&gt; &lt;num&gt;   &lt;num&gt;         &lt;list&gt; &lt;list&gt;\n1:  folds ParamInt     2   Inf     Inf &lt;NoDefault[0]&gt;      5\n\n\nThese four object types work together to create complete machine learning workflows. Tasks provide the data and problem definition, learners implement the algorithms, measures evaluate performance, and resamplings ensure robust validation. This modular design allows practitioners to mix and match components easily, experimenting with different combinations to find optimal solutions for their specific problems.\nThe interaction between these objects follows predictable patterns. Learners train on tasks, producing fitted models that can generate predictions. These predictions are evaluated using measures, with resampling strategies ensuring that evaluation results generalize beyond the specific training data used. Understanding these interactions enables practitioners to build sophisticated machine learning pipelines while maintaining clear, readable code.\n\n\n\n\n\n\nKey Advantages of mlr3verse\n\n\n\nThe mlr3verse provides several crucial advantages for machine learning practitioners:\nConsistency: All algorithms use the same interface, reducing learning overhead and making code more maintainable.\nFlexibility: Modular design allows easy experimentation with different combinations of preprocessing, algorithms, and evaluation strategies.\nReproducibility: Built-in support for controlling randomness and tracking experimental results.\nExtensibility: Easy to add new algorithms, measures, and preprocessing steps while maintaining compatibility with existing code.\nBest Practices: Framework encourages proper practices like train/test splitting and cross-validation through its core design.\n\n\nThis foundation in mlr3verse concepts prepares practitioners to tackle real machine learning problems with confidence. The consistent interfaces and modular design reduce the cognitive load associated with learning new algorithms, allowing focus on the more important aspects of problem-solving: understanding the data, choosing appropriate methods, and interpreting results. In the hands-on sections that follow, these concepts will come together to demonstrate how mlr3verse facilitates efficient, robust machine learning workflows.",
    "crumbs": [
      "Home",
      "Machine Learning",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>The mlr3verse</span>"
    ]
  },
  {
    "objectID": "machine_learning/machine_learning_mlr3.html",
    "href": "machine_learning/machine_learning_mlr3.html",
    "title": "\n21  Examples\n",
    "section": "",
    "text": "21.1 Overview\nIn this chapter, we focus on practical aspects of machine learning. The goal is to provide a hands-on introduction to the application of machine learning techniques to real-world data. While the theoretical foundations of machine learning are important, the ability to apply these techniques to solve practical problems is equally crucial. In this chapter, we will use the mlr3 package in R to build and evaluate machine learning models for classification and regression tasks.\nWe will use three examples to illustrate the machine learning workflow:\nWe’ll be applying knn, decision trees, and random forests, linear regression, and penalized regression models to these datasets.\nThe mlr3 R package is a modern, object-oriented machine learning framework in R that builds on the success of its predecessor, the mlr package. It provides a flexible and extensible platform for handling common machine learning tasks such as data preprocessing, model training, hyperparameter tuning, and model evaluation Figure 21.1. The package is designed to guide and standardize the process of using complex machine learning pipelines.\nFigure 21.1: The mlr3 ecosystem.",
    "crumbs": [
      "Home",
      "Machine Learning",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Examples</span>"
    ]
  },
  {
    "objectID": "machine_learning/machine_learning_mlr3.html#overview",
    "href": "machine_learning/machine_learning_mlr3.html#overview",
    "title": "\n21  Examples\n",
    "section": "",
    "text": "Cancer types classification: We will classify different types of cancer based on gene expression data.\n\nAge prediction from DNA methylation: We will predict the chronological age of individuals based on DNA methylation patterns.\n\nGene expression prediction: We will predict gene expression levels based on histone modification data.\n\n\n\n\n\n21.1.1 Key features of mlr3\n\n\nTask abstraction\n\nmlr3 encapsulates different types of learning problems like classification, regression, and survival analysis into “Task” objects, making it easier to handle various learning scenarios. Examples of tasks include classification tasks, regression tasks, and survival tasks.\n\n\n\nModular design\n\nThe package follows a modular design, allowing users to quickly swap out different components such as learners (algorithms), measures (performance metrics), and resampling strategies. Examples of learners include linear regression, logistic regression, and random forests. Examples of measures include accuracy, precision, recall, and F1 score. Examples of resampling strategies include cross-validation, bootstrapping, and holdout validation.\n\n\n\nExtensibility\n\nUsers can extend the functionality of mlr3 by adding custom components like learners, measures, and preprocessing steps via the R6 object-oriented system.\n\n\n\nPreprocessing\n\nmlr3 provides a flexible way to preprocess data using “PipeOps” (pipeline operations), allowing users to create reusable preprocessing pipelines.\n\n\n\nTuning and model selection\n\nmlr3 supports hyperparameter tuning and model selection using various search strategies like grid search, random search, and Bayesian optimization.\n\n\n\nParallelization\n\nThe package allows for parallelization of model training and evaluation, making it suitable for large-scale machine learning tasks.\n\n\n\nBenchmarking\n\nmlr3 facilitates benchmarking of multiple algorithms on multiple tasks, simplifying the process of comparing and selecting the best models.\n\n\n\nYou can find more information, including tutorials and examples, on the official mlr3 GitHub repository1 and the mlr3 book2.\n1 https://github.com/mlr-org/mlr32 https://mlr3book.mlr-org.com/",
    "crumbs": [
      "Home",
      "Machine Learning",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Examples</span>"
    ]
  },
  {
    "objectID": "machine_learning/machine_learning_mlr3.html#the-mlr3-workflow",
    "href": "machine_learning/machine_learning_mlr3.html#the-mlr3-workflow",
    "title": "\n21  Examples\n",
    "section": "\n21.2 The mlr3 workflow",
    "text": "21.2 The mlr3 workflow\nThe mlr3 package is designed to simplify the process of creating and deploying complex machine learning pipelines. The package follows a modular design, which means that users can quickly swap out different components such as learners (algorithms), measures (performance metrics), and resampling strategies. The package also supports parallelization of model training and evaluation, making it suitable for large-scale machine learning tasks.\n\n\n\n\n\ngraph TD\n  A[Load data]\n  B[Create a task\\nregression, classification, etc.]\n  B --&gt; learner[Choose learner]\n  A --&gt; C[Split data]\n  C --&gt; trainset(Training set)\n  C --&gt; testset[Test set]\n  trainset --&gt; train[Train model]\n  learner --&gt; train\n  testset --&gt; testing[Test model]\n  train --&gt; testing\n  testing --&gt; eval[Evaluate &\\nInterpret]\n\n\n\n\nFigure 21.2: The simplified workflow of a machine learning pipeline using mlr3.\n\n\n\n\nThe following sections describe each of these steps in detail.\n\n21.2.1 The machine learning Task\nImagine you want to teach a computer how to make predictions or decisions, similar to how you might teach a student. To do this, you need to clearly define what you want the computer to learn and work on. This is called defining a “task.” Let’s break down what this involves and why it’s important.\n\n21.2.1.1 Step 1: Understand the Problem\nFirst, think about what problem you want to solve or what question you want the computer to answer. For example: - Do you want to predict the weather for tomorrow? - Are you trying to figure out if an email is spam or not? - Do you want to know how much a house might sell for?\nThese questions define your task type. In machine learning, there are several common task types:\n\n\nClassification: Deciding which category something belongs to (e.g., spam or not spam).\n\nRegression: Predicting a number (e.g., the price of a house).\n\nClustering: Grouping similar items together (e.g., customer segmentation).\n\n21.2.1.2 Step 2: Choose Your Data\nNext, you need data that is related to your problem. Think of data as the information or examples you’ll use to teach the computer. For instance, if your task is to predict house prices, your data might include:\n\nThe size of the house\nThe number of bedrooms\nThe location of the house\nThe age of the house\n\nThese pieces of information are called features. Features are the input that the computer uses to make predictions.\n\n21.2.1.3 Step 3: Define the Target\nAlong with features, you need to define the target. The target is what you want to predict or decide. In the house price example, the target would be the actual price of the house.\n\n21.2.1.4 Step 4: Create the Task\nNow that you have your problem, data, and target, you can create the task. In mlr3, a task brings together the type of problem (task type), the features (input data), and the target (what you want to predict).\nHere’s a simple summary:\n\n\nTask Type: What kind of problem are you solving? (e.g., classification, regression)\n\nFeatures: What information do you have to make the prediction? (e.g., size, location)\n\nTarget: What are you trying to predict? (e.g., house price)\n\nBy clearly defining these elements, you set a solid foundation for the machine learning process. This helps ensure that the computer can learn effectively and make accurate predictions.\n\n21.2.1.5 mlr3 and Tasks\nThe mlr3 package uses the concept of “Tasks” to encapsulate different types of learning problems like classification, regression, and survival analysis. A Task contains the data (features and target variable) and additional metadata to define the machine learning problem. For example, in a classification task, the target variable is a label (stored as a character or factor), while in a regression task, the target variable is a numeric quantity (stored as an integer or numeric).\nThere are a number of Task Types that are supported by mlr3. To create a task from a data.frame(), data.table() or Matrix(), you first need to select the right task type:\n\nClassification Task: The target is a label (stored as character or factor) with only relatively few distinct values → TaskClassif.\nRegression Task: The target is a numeric quantity (stored as integer or numeric) → TaskRegr.\nSurvival Task: The target is the (right-censored) time to an event. More censoring types are currently in development → mlr3proba::TaskSurv in add-on package mlr3proba.\nDensity Task: An unsupervised task to estimate the density → mlr3proba::TaskDens in add-on package mlr3proba.\nCluster Task: An unsupervised task type; there is no target and the aim is to identify similar groups within the feature space → mlr3cluster::TaskClust in add-on package mlr3cluster.\nSpatial Task: Observations in the task have spatio-temporal information (e.g. coordinates) → mlr3spatiotempcv::TaskRegrST or mlr3spatiotempcv::TaskClassifST in add-on package mlr3spatiotempcv.\nOrdinal Regression Task: The target is ordinal → TaskOrdinal in add-on package mlr3ordinal (still in development).\n\n21.2.2 The “Learner” in Machine Learning\nAfter you’ve defined your task, the next step in teaching a computer to make predictions or decisions is to choose a “learner.” Let’s explore what a learner is and how it fits into the mlr3 package.\n\n21.2.2.1 What is a “Learner”?\nThink of a learner as the method or tool that the computer uses to learn from the data. Another common name for a “learner” is a “model.” It’s similar to choosing a tutor or a teacher for a student. Different learners have different ways of understanding and processing information. For example:\n\nSome learners might be great at recognizing patterns in data, like a tutor who is excellent at spotting trends.\nOthers might be good at making decisions based on rules, like a tutor who uses step-by-step logic.\n\nIn machine learning, there are many types of learners, each with its own strengths and weaknesses. Here are a few examples:\n\n\nDecision Trees: These learners make decisions by asking a series of questions, like “Is the house larger than 1000 square feet?” and “Does it have more than 3 bedrooms?”\n\nk-Nearest Neighbors: These learners make predictions based on the similarity of new data points to existing data points.\n\nLinear Regression: This learner tries to fit a straight line through the data points to make predictions about numbers.\n\nRandom Forests: These are like a group of decision trees working together to make more accurate predictions.\n\nSupport Vector Machines: These learners find the best boundary that separates different categories in the data.\n\n21.2.2.2 Choosing the Right Learner\nSelecting the right learner is crucial because different learners work better for different types of tasks and data. For example:\n\nIf your task is to classify emails as spam or not spam, a decision tree or a support vector machine might work well.\nIf you’re predicting house prices, linear regression or random forests could be good choices.\n\nThe goal is to find a learner that can understand the patterns in your data and make accurate predictions. This is where the mlr3 package comes in handy. It provides a wide range of learners that you can choose from, making it easier to experiment and find the best learner for your task.\n\n21.2.2.3 Learners in mlr3\nIn the mlr3 package, learners are pre-built tools that you can easily use for your tasks. Here’s how it works:\n\n\nSelect a Learner: mlr3 provides a variety of learners to choose from, like decision trees, linear regression, and more.\n\nTrain the Learner: Once you’ve selected a learner, you provide it with your task (the problem, data, and target). The learner uses this information to learn and make predictions.\n\nEvaluate and Improve: After training, you can test how well the learner performs and make adjustments if needed, such as trying a different learner or fine-tuning the current one.\n\n21.2.2.4 mlr3 and Learners\nObjects of class Learner provide a unified interface to many popular machine learning algorithms in R. They consist of methods to train and predict a model for a Task and provide meta-information about the learners, such as the hyperparameters (which control the behavior of the learner) you can set.\nThe base class of each learner is Learner, specialized for regression as LearnerRegr and for classification as LearnerClassif. Other types of learners, provided by extension packages, also inherit from the Learner base class, e.g. mlr3proba::LearnerSurv or mlr3cluster::LearnerClust.\nAll Learners work in a two-stage procedure:\n\n\n\n\n\nFigure 21.3: Two stages of a learner. Top: data (features and a target) are passed to an (untrained) learner. Bottom: new data are passed to the trained model which makes predictions for the ‘missing’ target column.\n\n\n\n\nTraining stage: The training data (features and target) is passed to the Learner’s $train() function which trains and stores a model, i.e. the relationship of the target and features.\n\nPredict stage: The new data, usually a different slice of the original data than used for training, is passed to the $predict() method of the Learner. The model trained in the first step is used to predict the missing target, e.g. labels for classification problems or the numerical value for regression problems.\n\nThere are a number of predefined learners. The mlr3 package ships with the following set of classification and regression learners. We deliberately keep this small to avoid unnecessary dependencies:\n\n\nclassif.featureless: Simple baseline classification learner. The default is to always predict the label that is most frequent in the training set. While this is not very useful by itself, it can be used as a “fallback learner” to make predictions in case another, more sophisticated, learner failed for some reason.\n\nregr.featureless: Simple baseline regression learner. The default is to always predict the mean of the target in training set. Similar to mlr_learners_classif.featureless, it makes for a good “fallback learner”\n\nclassif.rpart: Single classification tree from package rpart.\n\nregr.rpart: Single regression tree from package rpart.\n\nThis set of baseline learners is usually insufficient for a real data analysis. Thus, we have cherry-picked implementations of the most popular machine learning method and collected them in the mlr3learners package:\n\nLinear (regr.lm) and logistic (classif.log_reg) regression\nPenalized Generalized Linear Models (regr.glmnet, classif.glmnet), possibly with built-in optimization of the penalization parameter (regr.cv_glmnet, classif.cv_glmnet)\n(Kernelized) k-Nearest Neighbors regression (regr.kknn) and classification (classif.kknn).\nKriging / Gaussian Process Regression (regr.km)\nLinear (classif.lda) and Quadratic (classif.qda) Discriminant Analysis\nNaive Bayes Classification (classif.naive_bayes)\nSupport-Vector machines (regr.svm, classif.svm)\nGradient Boosting (regr.xgboost, classif.xgboost)\nRandom Forests for regression and classification (regr.ranger, classif.ranger)\n\nMore machine learning methods and alternative implementations are collected in the mlr3extralearners repository.",
    "crumbs": [
      "Home",
      "Machine Learning",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Examples</span>"
    ]
  },
  {
    "objectID": "machine_learning/machine_learning_mlr3.html#setup",
    "href": "machine_learning/machine_learning_mlr3.html#setup",
    "title": "\n21  Examples\n",
    "section": "\n21.3 Setup",
    "text": "21.3 Setup\n\nlibrary(mlr3verse)\nlibrary(GEOquery)\nlibrary(mlr3learners) # for knn\nlibrary(ranger) # for randomforest\nset.seed(789)",
    "crumbs": [
      "Home",
      "Machine Learning",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Examples</span>"
    ]
  },
  {
    "objectID": "machine_learning/machine_learning_mlr3.html#example-cancer-types",
    "href": "machine_learning/machine_learning_mlr3.html#example-cancer-types",
    "title": "\n21  Examples\n",
    "section": "\n21.4 Example: Cancer types",
    "text": "21.4 Example: Cancer types\nIn this exercise, we will be classifying cancer types based on gene expression data. The data we are going to access are from Brouwer-Visser et al. (2018).\nThe data are from the Gene Expression Omnibus (GEO) database, a public repository of functional genomics data. The data are from a study that aimed to identify gene expression signatures that can distinguish between different types of cancer. The data include gene expression profiles from patients with different types of cancer. The goal is to build a machine learning model that can predict the cancer type based on the gene expression data.\n\n21.4.1 Understanding the Problem\nBefore we start building a machine learning model, it’s important to understand the problem we are trying to solve. Here are some key questions to consider:\n\nWhat are the features?\nWhat is the target variable?\nWhat type of machine learning task is this (classification, regression, clustering)?\nWhat is the goal of the analysis?\n\n21.4.2 Data Preparation\nUse the [GEOquery] package to fetch data about [GSE103512].\n\nlibrary(GEOquery)\ngse = getGEO(\"GSE103512\")[[1]]\n\nThe first step, a detail, is to convert from the older Bioconductor data structure (GEOquery was written in 2007), the ExpressionSet, to the newer SummarizedExperiment.\n\nlibrary(SummarizedExperiment)\nse = as(gse, \"SummarizedExperiment\")\n\nExamine two variables of interest, cancer type and tumor/normal status.\n\nwith(colData(se),table(`cancer.type.ch1`,`normal.ch1`))\n\n               normal.ch1\ncancer.type.ch1 no yes\n          BC    65  10\n          CRC   57  12\n          NSCLC 60   9\n          PCA   60   7\n\n\nBefore embarking on a machine learning analysis, we need to make sure that we understand the data. Things like missing values, outliers, and other problems can cause problems for machine learning algorithms.\nIn R, plotting, summaries, and other exploratory data analysis tools are available. PCA analysis, clustering, and other methods can also be used to understand the data. It is worth spending time on this step, as it can save time later.\n\n21.4.3 Feature selection and data cleaning\nWhile we could use all genes in the analysis, we will select the most informative genes using the variance of gene expression across samples. Other methods for feature selection are available, including those based on correlation with the outcome variable.\n\n\n\n\n\n\nFeature selection\n\n\n\nFeature selection should be done on the training data only, not the test data to avoid overfitting. The test data should be used only for evaluation. In other words, the test data should be “unseen” by the model until the final evaluation.\n\n\nRemember that the apply function applies a function to each row or column of a matrix. Here, we apply the sd function to each row of the expression matrix to get a vector of stan\n\nsds = apply(assay(se, 'exprs'),1,sd)\n## filter out normal tissues\nse_small = se[order(sds,decreasing = TRUE)[1:200],\n              colData(se)$characteristics_ch1.1=='normal: no']\n# remove genes with no gene symbol\nse_small = se_small[rowData(se_small)$Gene.Symbol!='',]\n\nTo make the data easier to work with, we will use the opportunity to use one of the rowData columns as the rownames of the data frame. The make.names function is used to make sure that the rownames are valid R variable names and unique.\n\n## convert to matrix for later use\ndat = assay(se_small, 'exprs')\nrownames(dat) = make.names(rowData(se_small)$Gene.Symbol)\n\nWe also need to transpose the data so that the rows are the samples and the columns are the features in order to use the data with mlr3.\n\nfeat_dat = t(dat)\ntumor = data.frame(tumor_type = colData(se_small)$cancer.type.ch1, feat_dat)\n\nThis is another good time to check the data. Make sure that the data is in the format that you expect. Check the dimensions, the column names, and the data types.\n\n21.4.4 Creating the “task”\nThe first step in using mlr3 is to create a task. A task is a data set with a target variable. In this case, the target variable is the cancer type. The mlr3 package provides a function to convert a data frame into a task. These tasks can be used with any machine learning algorithm in mlr3.\nThis is a classification task, so we will use the as_task_classif function to create the task. The classification task requires a target variable that is categorical.\n\nlibrary(mlr3)\ntumor$tumor_type = as.factor(tumor$tumor_type)\ntask = as_task_classif(tumor,target='tumor_type')\n\n\n21.4.5 Splitting the data\nHere, we randomly divide the data into 2/3 training data and 1/3 test data. This is a common split, but other splits can be used. The training data is used to train the model, and the test data is used to evaluate the trained model.\n\nset.seed(7)\ntrain_set = sample(task$row_ids, 0.67 * task$nrow)\ntest_set = setdiff(task$row_ids, train_set)\n\n\n\n\n\n\n\nImportant\n\n\n\nTraining and testing on the same data is a common mistake. We want to test the model on data that it has not seen before. This is the only way to know if the model is overfitting and to get an accurate estimate of the model’s performance.\n\n\nIn the next sections, we will train and evaluate three different models on the data: k-nearest-neighbor, classification tree, and random forest. Remember that the goal is to predict the cancer type based on the gene expression data. The mlr3 package uses the concept of “learners” to encapsulate different machine learning algorithms.\n\n21.4.6 Example learners\n\n21.4.6.1 K-nearest-neighbor\nThe first model we will use is the k-nearest-neighbor model. This model is based on the idea that similar samples have similar outcomes. The number of neighbors to use is a parameter that can be tuned. We’ll use the default value of 7, but you can try other values to see how they affect the results. In fact, mlr3 provides the ability to tune parameters automatically, but we won’t cover that here.\n\n21.4.6.1.1 Create the learner\nIn mlr3, the machine learning algorithms are called learners. To create a learner, we use the lrn function. The lrn function takes the name of the learner as an argument. The lrn function also takes other arguments that are specific to the learner. In this case, we will use the default values for the arguments.\n\nlibrary(mlr3learners)\nlearner = lrn(\"classif.kknn\")\n\nYou can get a list of all the learners available in mlr3 by using the lrn() function without any arguments.\n\nlrn()\n\n&lt;DictionaryLearner&gt; with 51 stored values\nKeys: classif.cv_glmnet, classif.debug, classif.featureless,\n  classif.glmnet, classif.kknn, classif.lda, classif.log_reg,\n  classif.multinom, classif.naive_bayes, classif.nnet, classif.qda,\n  classif.ranger, classif.rpart, classif.svm, classif.xgboost,\n  clust.agnes, clust.ap, clust.bico, clust.birch, clust.cmeans,\n  clust.cobweb, clust.dbscan, clust.dbscan_fpc, clust.diana, clust.em,\n  clust.fanny, clust.featureless, clust.ff, clust.hclust,\n  clust.hdbscan, clust.kkmeans, clust.kmeans, clust.MBatchKMeans,\n  clust.mclust, clust.meanshift, clust.optics, clust.pam,\n  clust.SimpleKMeans, clust.xmeans, regr.cv_glmnet, regr.debug,\n  regr.featureless, regr.glmnet, regr.kknn, regr.km, regr.lm,\n  regr.nnet, regr.ranger, regr.rpart, regr.svm, regr.xgboost\n\n\n\n21.4.6.1.2 Train\nTo train the model, we use the train function. The train function takes the task and the row ids of the training data as arguments.\n\nlearner$train(task, row_ids = train_set)\n\nHere, we can look at the trained model:\n\n# output is large, so do this on your own\nlearner$model\n\n\n21.4.6.1.3 Predict\nLets use our trained model works to predict the classes of the training data. Of course, we already know the classes of the training data, but this is a good way to check that the model is working as expected. It also gives us a measure of performance on the training data that we can compare to the test data to look for overfitting.\n\npred_train = learner$predict(task, row_ids=train_set)\n\nAnd check on the test data:\n\npred_test = learner$predict(task, row_ids=test_set)\n\n\n21.4.6.1.4 Assess\nIn this section, we can look at the accuracy and performance of our model on the training data and the test data. We can also look at the confusion matrix to see which classes are being confused with each other.\n\npred_train$confusion\n\n        truth\nresponse BC CRC NSCLC PCA\n   BC    42   0     0   0\n   CRC    0  40     0   0\n   NSCLC  1   0    44   0\n   PCA    0   0     0  35\n\n\nThis is a multi-class confusion matrix. The rows are the true classes and the columns are the predicted classes. The diagonal shows the number of samples that were correctly classified. The off-diagonal elements show the number of samples that were misclassified.\nWe can also look at the accuracy of the model on the training data and the test data. The accuracy is the number of correctly classified samples divided by the total number of samples.\n\nmeasures = msrs(c('classif.acc'))\npred_train$score(measures)\n\nclassif.acc \n  0.9938272 \n\n\n\npred_test$confusion\n\n        truth\nresponse BC CRC NSCLC PCA\n   BC    22   0     0   0\n   CRC    0  17     1   0\n   NSCLC  0   0    15   0\n   PCA    0   0     0  25\n\npred_test$score(measures)\n\nclassif.acc \n     0.9875 \n\n\nCompare the accuracy on the training data to the accuracy on the test data. Do you see any evidence of overfitting?\n\n21.4.6.2 Classification tree\nWe are going to use a classification tree to classify the data. A classification tree is a series of yes/no questions that are used to classify the data. The questions are based on the features in the data. The classification tree is built by finding the feature that best separates the data into the different classes. Then, the data is split based on the value of that feature. The process is repeated until the data is completely separated into the different classes.\n\n21.4.6.2.1 Train\n\n# in this case, we want to keep the model\n# so we can look at it later\nlearner = lrn(\"classif.rpart\", keep_model = TRUE)\n\n\nlearner$train(task, row_ids = train_set)\n\nWe can take a look at the model.\n\nlearner$model\n\nn= 162 \n\nnode), split, n, loss, yval, (yprob)\n      * denotes terminal node\n\n 1) root 162 118 NSCLC (0.26543210 0.24691358 0.27160494 0.21604938)  \n   2) CDHR5&gt;=5.101625 40   0 CRC (0.00000000 1.00000000 0.00000000 0.00000000) *\n   3) CDHR5&lt; 5.101625 122  78 NSCLC (0.35245902 0.00000000 0.36065574 0.28688525)  \n     6) ACPP&lt; 6.088431 87  43 NSCLC (0.49425287 0.00000000 0.50574713 0.00000000)  \n      12) GATA3&gt;=4.697803 41   1 BC (0.97560976 0.00000000 0.02439024 0.00000000) *\n      13) GATA3&lt; 4.697803 46   3 NSCLC (0.06521739 0.00000000 0.93478261 0.00000000) *\n     7) ACPP&gt;=6.088431 35   0 PCA (0.00000000 0.00000000 0.00000000 1.00000000) *\n\n\nDecision trees are easy to visualize if they are small. Here, we can see that the tree is very simple, with only two splits.\n\nlibrary(mlr3viz)\nlibrary(ggparty)\n\nLoading required package: ggplot2\n\n\nLoading required package: partykit\n\n\nLoading required package: grid\n\n\nLoading required package: libcoin\n\n\nLoading required package: mvtnorm\n\n\n\nAttaching package: 'partykit'\n\n\nThe following object is masked from 'package:SummarizedExperiment':\n\n    width\n\n\nThe following object is masked from 'package:GenomicRanges':\n\n    width\n\n\nThe following object is masked from 'package:IRanges':\n\n    width\n\n\nThe following object is masked from 'package:S4Vectors':\n\n    width\n\n\nThe following object is masked from 'package:BiocGenerics':\n\n    width\n\nautoplot(learner, type='ggparty')\n\n\n\n\n\n\n\n\n21.4.6.2.2 Predict\nNow that we have trained the model on the training data, we can use it to predict the classes of the training data and the test data. The $predict method takes a task and produces a prediction based on the trained model, in this case, called learner.\n\npred_train = learner$predict(task, row_ids=train_set)\n\nRemember that we split the data into training and test sets. We can use the trained model to predict the classes of the test data. Since the test data was not used to train the model, it is not “cheating” like what we just did where we did the prediction on the training data.\n\npred_test = learner$predict(task, row_ids=test_set)\n\n\n21.4.6.2.3 Assess\nFor classification tasks, we often look at a confusion matrix of the truth vs the predicted classes for the samples.\n\n\n\n\n\n\nImportant\n\n\n\nAssessing the performance of a model should always be reported from assessment on an independent test set.\n\n\n\npred_train$confusion\n\n        truth\nresponse BC CRC NSCLC PCA\n   BC    40   0     1   0\n   CRC    0  40     0   0\n   NSCLC  3   0    43   0\n   PCA    0   0     0  35\n\n\n\nWhat does this confusion matrix tell you?\n\nWe can also ask for several “measures” of the performance of the model. Here, we ask for the accuracy of the model. To get a complete list of measures, use msr().\n\nmeasures = msrs(c('classif.acc'))\npred_train$score(measures)\n\nclassif.acc \n  0.9753086 \n\n\n\nHow does the accuracy compare to the confusion matrix?\nHow does this accuracy compare to the accuracy of the k-nearest-neighbor model?\nHow about the decision tree model?\n\n\npred_test$confusion\n\n        truth\nresponse BC CRC NSCLC PCA\n   BC    20   0     1   0\n   CRC    0  17     3   0\n   NSCLC  2   0    12   0\n   PCA    0   0     0  25\n\npred_test$score(measures)\n\nclassif.acc \n      0.925 \n\n\n\nWhat does the confusion matrix in the test set tell you?\nHow do the assessments of the test and training sets differ?\n\n\n\n\n\n\n\nOverfitting\n\n\n\nWhen the assessment of the test set is worse than the evaluation of the training set, the model may be overfit. How to address overfitting varies by model type, but it is a sign that you should pay attention to model selection and parameters.\n\n\n\n21.4.6.3 RandomForest\n\nlearner = lrn(\"classif.ranger\", importance = \"impurity\")\n\n\n21.4.6.3.1 Train\n\nlearner$train(task, row_ids = train_set)\n\nAgain, you can look at the model that was trained.\n\nlearner$model\n\nRanger result\n\nCall:\n ranger::ranger(dependent.variable.name = task$target_names, data = task$data(),      probability = self$predict_type == \"prob\", importance = \"impurity\",      num.threads = 1L) \n\nType:                             Classification \nNumber of trees:                  500 \nSample size:                      162 \nNumber of independent variables:  192 \nMtry:                             13 \nTarget node size:                 1 \nVariable importance mode:         impurity \nSplitrule:                        gini \nOOB prediction error:             0.62 % \n\n\nFor more details, the mlr3 random forest approach is based ont he ranger package. You can look at the ranger documentation.\n\nWhat is the OOB error in the output?\n\nRandom forests are a collection of decision trees. Since predictors enter the trees in a random order, the trees are different from each other. The random forest procedure gives us a measure of the “importance” of each variable.\n\nhead(learner$importance(), 15)\n\n   CDHR5  TRPS1.1    FABP1   EPS8L3    KRT20    EFHD1   LGALS4    TRPS1 \n4.791870 3.918063 3.692649 3.651422 3.340382 3.314491 2.952969 2.926175 \n   SFTPB  SFTPB.1    GATA3  GATA3.1  TMPRSS2    MUC12    POF1B \n2.805811 2.681004 2.344603 2.271845 2.248734 2.207347 1.806906 \n\n\nMore “important” variables are those that are more often used in the trees. Are the most important variables the same as the ones that were important in the decision tree?\nIf you are interested, look up a few of the important variables in the model to see if they make biological sense.\n\n21.4.6.3.2 Predict\nAgain, we can use the trained model to predict the classes of the training data and the test data.\n\npred_train = learner$predict(task, row_ids=train_set)\n\n\npred_test = learner$predict(task, row_ids=test_set)\n\n\n21.4.6.3.3 Assess\n\npred_train$confusion\n\n        truth\nresponse BC CRC NSCLC PCA\n   BC    43   0     0   0\n   CRC    0  40     0   0\n   NSCLC  0   0    44   0\n   PCA    0   0     0  35\n\n\n\nmeasures = msrs(c('classif.acc'))\npred_train$score(measures)\n\nclassif.acc \n          1 \n\n\n\npred_test$confusion\n\n        truth\nresponse BC CRC NSCLC PCA\n   BC    22   0     0   0\n   CRC    0  17     0   0\n   NSCLC  0   0    16   0\n   PCA    0   0     0  25\n\npred_test$score(measures)\n\nclassif.acc \n          1",
    "crumbs": [
      "Home",
      "Machine Learning",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Examples</span>"
    ]
  },
  {
    "objectID": "machine_learning/machine_learning_mlr3.html#example-predicting-age-from-dna-methylation",
    "href": "machine_learning/machine_learning_mlr3.html#example-predicting-age-from-dna-methylation",
    "title": "\n21  Examples\n",
    "section": "\n21.5 Example Predicting age from DNA methylation",
    "text": "21.5 Example Predicting age from DNA methylation\nWe will be building a regression model for chronological age prediction, based on DNA methylation. This is based on the work of Jana Naue et al. 2017, in which biomarkers are examined to predict the chronological age of humans by analyzing the DNA methylation patterns. Different machine learning algorithms are used in this study to make an age prediction.\nIt has been recognized that within each individual, the level of DNA methylation changes with age. This knowledge is used to select useful biomarkers from DNA methylation datasets. The CpG sites with the highest correlation to age are selected as the biomarkers (and therefore features for building a regression model). In this tutorial, specific biomarkers are analyzed by machine learning algorithms to create an age prediction model.\nThe data are taken from this tutorial.\n\nlibrary(data.table)\nmeth_age = rbind(\n    fread('https://zenodo.org/record/2545213/files/test_rows_labels.csv'),\n    fread('https://zenodo.org/record/2545213/files/train_rows.csv')\n)\n\nLet’s take a quick look at the data.\n\nhead(meth_age)\n\n   RPA2_3 ZYG11A_4  F5_2 HOXC4_1 NKIRAS2_2 MEIS1_1 SAMD10_2 GRM2_9 TRIM59_5\n    &lt;num&gt;    &lt;num&gt; &lt;num&gt;   &lt;num&gt;     &lt;num&gt;   &lt;num&gt;    &lt;num&gt;  &lt;num&gt;    &lt;num&gt;\n1:  65.96    18.08 41.57   55.46     30.69   63.42    40.86  68.88    44.32\n2:  66.83    20.27 40.55   49.67     29.53   30.47    37.73  53.30    50.09\n3:  50.30    11.74 40.17   33.85     23.39   58.83    38.84  35.08    35.90\n4:  65.54    15.56 33.56   36.79     20.23   56.39    41.75  50.37    41.46\n5:  59.01    14.38 41.95   30.30     24.99   54.40    37.38  30.35    31.28\n6:  81.30    14.68 35.91   50.20     26.57   32.37    32.30  55.19    42.21\n   LDB2_3 ELOVL2_6 DDO_1 KLF14_2   Age\n    &lt;num&gt;    &lt;num&gt; &lt;num&gt;   &lt;num&gt; &lt;int&gt;\n1:  56.17    62.29 40.99    2.30    40\n2:  58.40    61.10 49.73    1.07    44\n3:  58.81    50.38 63.03    0.95    28\n4:  58.05    50.58 62.13    1.99    37\n5:  65.80    48.74 41.88    0.90    24\n6:  70.15    61.36 33.62    1.87    43\n\n\nAs before, we create the task object, but this time we use as_task_regr() to create a regression task.\n\nWhy is this a regression task?\n\n\ntask = as_task_regr(meth_age,target = 'Age')\n\n\nset.seed(7)\ntrain_set = sample(task$row_ids, 0.67 * task$nrow)\ntest_set = setdiff(task$row_ids, train_set)\n\n\n21.5.1 Example learners\n\n21.5.1.1 Linear regression\nWe will start with a simple linear regression model.\n\nlearner = lrn(\"regr.lm\")\n\n\n21.5.1.1.1 Train\n\nlearner$train(task, row_ids = train_set)\n\nWhen you train a linear regression model, we can evaluate some of the diagnostic plots to see if the model is appropriate (Figure 21.4).\n\npar(mfrow=c(2,2))\nplot(learner$model)\n\n\n\n\n\n\nFigure 21.4: Regression diagnostic plots. The top left plot shows the residuals vs. fitted values. The top right plot shows the normal Q-Q plot. The bottom left plot shows the scale-location plot. The bottom right plot shows the residuals vs. leverage.\n\n\n\n\n\n21.5.1.1.2 Predict\n\npred_train = learner$predict(task, row_ids=train_set)\n\n\npred_test = learner$predict(task, row_ids=test_set)\n\n\n21.5.1.1.3 Assess\n\npred_train\n\n&lt;PredictionRegr&gt; for 209 observations:\n row_ids truth response\n     298    29 31.40565\n     103    58 56.26019\n     194    53 48.96480\n     ---   ---      ---\n     312    48 52.61195\n     246    66 67.66312\n     238    38 39.38414\n\n\nWe can plot the relationship between the truth and response, or predicted value to see visually how our model performs.\n\nlibrary(ggplot2)\nggplot(pred_train,aes(x=truth, y=response)) +\n    geom_point() +\n    geom_smooth(method='lm')\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\nWe can use the r-squared of the fit to roughly compare two models.\n\nmeasures = msrs(c('regr.rsq'))\npred_train$score(measures)\n\n      rsq \n0.9376672 \n\n\n\npred_test\n\n&lt;PredictionRegr&gt; for 103 observations:\n row_ids truth response\n       4    37 37.64301\n       5    24 28.34777\n       7    34 33.22419\n     ---   ---      ---\n     306    42 41.65864\n     307    63 58.68486\n     309    68 70.41987\n\npred_test$score(measures)\n\n      rsq \n0.9363526 \n\n\n\n21.5.1.2 Regression tree\n\nlearner = lrn(\"regr.rpart\", keep_model = TRUE)\n\n\n21.5.1.2.1 Train\n\nlearner$train(task, row_ids = train_set)\n\n\nlearner$model\n\nn= 209 \n\nnode), split, n, deviance, yval\n      * denotes terminal node\n\n 1) root 209 45441.4500 43.27273  \n   2) ELOVL2_6&lt; 56.675 98  5512.1220 30.24490  \n     4) ELOVL2_6&lt; 47.24 47   866.4255 24.23404  \n       8) GRM2_9&lt; 31.3 34   289.0588 22.29412 *\n       9) GRM2_9&gt;=31.3 13   114.7692 29.30769 *\n     5) ELOVL2_6&gt;=47.24 51  1382.6270 35.78431  \n      10) F5_2&gt;=39.295 35   473.1429 33.28571 *\n      11) F5_2&lt; 39.295 16   213.0000 41.25000 *\n   3) ELOVL2_6&gt;=56.675 111  8611.3690 54.77477  \n     6) ELOVL2_6&lt; 65.365 63  3101.2700 49.41270  \n      12) KLF14_2&lt; 3.415 37  1059.0270 46.16216 *\n      13) KLF14_2&gt;=3.415 26  1094.9620 54.03846 *\n     7) ELOVL2_6&gt;=65.365 48  1321.3120 61.81250 *\n\n\nWhat is odd about using a regression tree here is that we end up with only a few discrete estimates of age. Each “leaf” has a value.\n\n21.5.1.2.2 Predict\n\npred_train = learner$predict(task, row_ids=train_set)\n\n\npred_test = learner$predict(task, row_ids=test_set)\n\n\n21.5.1.2.3 Assess\n\npred_train\n\n&lt;PredictionRegr&gt; for 209 observations:\n row_ids truth response\n     298    29 33.28571\n     103    58 61.81250\n     194    53 46.16216\n     ---   ---      ---\n     312    48 54.03846\n     246    66 61.81250\n     238    38 41.25000\n\n\nWe can see the effect of the discrete values much more clearly here.\n\nlibrary(ggplot2)\nggplot(pred_train,aes(x=truth, y=response)) +\n    geom_point() +\n    geom_smooth(method='lm')\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\nAnd the r-squared values for this model prediction shows quite a bit of difference from the linear regression above.\n\nmeasures = msrs(c('regr.rsq'))\npred_train$score(measures)\n\n      rsq \n0.8995351 \n\n\n\npred_test\n\n&lt;PredictionRegr&gt; for 103 observations:\n row_ids truth response\n       4    37 41.25000\n       5    24 33.28571\n       7    34 33.28571\n     ---   ---      ---\n     306    42 46.16216\n     307    63 61.81250\n     309    68 61.81250\n\npred_test$score(measures)\n\n      rsq \n0.8545402 \n\n\n\n21.5.1.3 RandomForest\nRandomforest is also tree-based, but unlike the single regression tree above, randomforest is a “forest” of trees which will eliminate the discrete nature of a single tree.\n\nlearner = lrn(\"regr.ranger\", mtry=2, min.node.size=20)\n\n\n21.5.1.3.1 Train\n\nlearner$train(task, row_ids = train_set)\n\n\nlearner$model\n\nRanger result\n\nCall:\n ranger::ranger(dependent.variable.name = task$target_names, data = task$data(),      min.node.size = 20L, mtry = 2L, num.threads = 1L) \n\nType:                             Regression \nNumber of trees:                  500 \nSample size:                      209 \nNumber of independent variables:  13 \nMtry:                             2 \nTarget node size:                 20 \nVariable importance mode:         none \nSplitrule:                        variance \nOOB prediction error (MSE):       18.85364 \nR squared (OOB):                  0.9137009 \n\n\n\n21.5.1.3.2 Predict\n\npred_train = learner$predict(task, row_ids=train_set)\n\n\npred_test = learner$predict(task, row_ids=test_set)\n\n\n21.5.1.3.3 Assess\n\npred_train\n\n&lt;PredictionRegr&gt; for 209 observations:\n row_ids truth response\n     298    29 30.62154\n     103    58 58.05445\n     194    53 48.25661\n     ---   ---      ---\n     312    48 51.49846\n     246    66 64.39315\n     238    38 38.18038\n\n\n\nggplot(pred_train,aes(x=truth, y=response)) +\n    geom_point() +\n    geom_smooth(method='lm')\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\nmeasures = msrs(c('regr.rsq'))\npred_train$score(measures)\n\n     rsq \n0.960961 \n\n\n\npred_test\n\n&lt;PredictionRegr&gt; for 103 observations:\n row_ids truth response\n       4    37 37.79631\n       5    24 29.18371\n       7    34 33.26780\n     ---   ---      ---\n     306    42 40.29101\n     307    63 58.26534\n     309    68 63.15481\n\npred_test$score(measures)\n\n      rsq \n0.9208394",
    "crumbs": [
      "Home",
      "Machine Learning",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Examples</span>"
    ]
  },
  {
    "objectID": "machine_learning/machine_learning_mlr3.html#example-expression-prediction-from-histone-modification-data",
    "href": "machine_learning/machine_learning_mlr3.html#example-expression-prediction-from-histone-modification-data",
    "title": "\n21  Examples\n",
    "section": "\n21.6 Example: Expression prediction from histone modification data",
    "text": "21.6 Example: Expression prediction from histone modification data\nIn this little set of exercises, you will be using histone marks near a gene to predict its expression (Figure 21.5).\n\\[\ny ~ h1 + h2 + h3 + ...\n\\tag{21.1}\\]\n\n\n\n\n\nFigure 21.5: What is the combined effect of histone marks on gene expression?\n\n\nThe data are from a study that aimed to predict gene expression from histone modification data. The data include gene expression levels and histone modification data for a set of genes. The goal is to build a machine learning model that can predict gene expression levels based on the histone modification data. The histone modification data are simply summaries of the histone marks within the promoter, defined as the region 2kb upstream of the transcription start site for this exercise.\nWe will try a couple of different approaches:\n\nPenalized regression\nRandomForest\n\n\n21.6.1 The Data\nThe data in this exercise were developed by Anshul Kundaje. We are not going to focus on the details of the data collection, etc. Instead, this is\n\nfullFeatureSet &lt;- read.table(\"http://seandavi.github.io/ITR/expression-prediction/features.txt\");\n\nWhat are the column names of the predictor variables?\n\ncolnames(fullFeatureSet)\n\n [1] \"Control\"  \"Dnase\"    \"H2az\"     \"H3k27ac\"  \"H3k27me3\" \"H3k36me3\"\n [7] \"H3k4me1\"  \"H3k4me2\"  \"H3k4me3\"  \"H3k79me2\" \"H3k9ac\"   \"H3k9me1\" \n[13] \"H3k9me3\"  \"H4k20me1\"\n\n\nThese are going to be predictors combined into a model. Some of our learners will rely on predictors being on a similar scale. Are our data already there?\nTo perform centering and scaling by column, we can convert to a matrix and then use scale.\n\npar(mfrow=c(1,2))\nscaled_features &lt;- scale(as.matrix(fullFeatureSet))\nboxplot(fullFeatureSet, title='Original data')\nboxplot(scaled_features, title='Centered and scaled data')\n\n\n\n\n\n\nFigure 21.6: Boxplots of original and scaled data.\n\n\n\n\nThere is a row for each gene and a column for each histone mark and we can see that the data are centered and scaled by column. We can also see some patterns in the data (see Figure 21.7).\n\nsampled_features &lt;- fullFeatureSet[sample(nrow(scaled_features), 500),]\nlibrary(ComplexHeatmap)\n\n========================================\nComplexHeatmap version 2.22.0\nBioconductor page: http://bioconductor.org/packages/ComplexHeatmap/\nGithub page: https://github.com/jokergoo/ComplexHeatmap\nDocumentation: http://jokergoo.github.io/ComplexHeatmap-reference\n\nIf you use it in published research, please cite either one:\n- Gu, Z. Complex Heatmap Visualization. iMeta 2022.\n- Gu, Z. Complex heatmaps reveal patterns and correlations in multidimensional \n    genomic data. Bioinformatics 2016.\n\n\nThe new InteractiveComplexHeatmap package can directly export static \ncomplex heatmaps into an interactive Shiny app with zero effort. Have a try!\n\nThis message can be suppressed by:\n  suppressPackageStartupMessages(library(ComplexHeatmap))\n========================================\n\nHeatmap(sampled_features, name='histone marks', show_row_names=FALSE)\n\nWarning: The input is a data frame-like object, convert it to a matrix.\n\n\n\n\n\n\n\nFigure 21.7: Heatmap of 500 randomly sampled rows of the data. Columns are histone marks and there is a row for each gene.\n\n\n\n\nNow, we can read in the associated gene expression measures that will become our “target” for prediction.\n\ntarget &lt;- scan(url(\"http://seandavi.github.io/ITR/expression-prediction/target.txt\"), skip=1)\n# make into a dataframe\nexp_pred_data &lt;- data.frame(gene_expression=target, scaled_features)\n\nAnd the first few rows of the target data frame using:\n\nhead(exp_pred_data,3)\n\n                            gene_expression    Control      Dnase       H2az\nENSG00000000419.7.49575069         6.082343  0.7452926  0.7575546  1.0728432\nENSG00000000457.8.169863093        2.989145  1.9509786  1.0216546  0.3702787\nENSG00000000938.7.27961645        -5.058894 -0.3505542 -1.4482958 -1.0390775\n                               H3k27ac   H3k27me3   H3k36me3    H3k4me1\nENSG00000000419.7.49575069   1.0950440 -0.5125312  1.1334793  0.4127984\nENSG00000000457.8.169863093  0.7142157 -0.4079244  0.8739005  1.1649282\nENSG00000000938.7.27961645  -1.0173283  1.4117293 -0.5157582 -0.5017450\n                               H3k4me2    H3k4me3   H3k79me2     H3k9ac\nENSG00000000419.7.49575069   1.2136176  1.1202901  1.5155803  1.2468256\nENSG00000000457.8.169863093  0.6456572  0.6508561  0.7976487  0.5792891\nENSG00000000938.7.27961645  -0.1878255 -0.6560973 -1.3803974 -1.0067972\n                              H3k9me1   H3k9me3   H4k20me1\nENSG00000000419.7.49575069  0.1426980  1.185622  1.9599992\nENSG00000000457.8.169863093 0.3630902  1.014923 -0.2695111\nENSG00000000938.7.27961645  0.6564520 -1.370871 -1.8773178\n\n\n\n21.6.2 Create task\n\nexp_pred_task = as_task_regr(exp_pred_data, target='gene_expression')\n\nPartition the data into test and training sets. We will use \\(\\frac{1}{3}\\) and \\(\\frac{2}{3}\\) of the data for testing.\n\nsplit = partition(exp_pred_task)\n\n\n21.6.3 Example learners\n\n21.6.3.1 Linear regression\n\nlearner = lrn(\"regr.lm\")\n\n\n21.6.3.1.1 Train\n\nlearner$train(exp_pred_task, split$train)\n\n\n21.6.3.1.2 Predict\n\npred_train = learner$predict(exp_pred_task, split$train)\npred_test = learner$predict(exp_pred_task, split$test)\n\n\n21.6.3.1.3 Assess\n\npred_train\n\n&lt;PredictionRegr&gt; for 5789 observations:\n row_ids     truth  response\n       1  6.082343  5.182373\n       2  2.989145  2.970278\n       3 -5.058894 -5.283509\n     ---       ---       ---\n    8637 -5.058894 -3.955237\n    8638  6.089159  4.809801\n    8640 -3.114148 -4.723061\n\nplot(pred_train)\n\n\n\n\n\n\n\nFor the training data:\n\nmeasures = msrs(c('regr.rsq'))\npred_train$score(measures)\n\n      rsq \n0.7505366 \n\n\nAnd the test data:\n\npred_test$score(measures)\n\n      rsq \n0.7510334 \n\n\nAnd the plot of the test data predictions:\n\nplot(pred_test)\n\n\n\n\n\n\n\n\n21.6.3.2 Penalized regression\nImagine you want to teach a computer to predict house prices based on various features like size, number of bedrooms, and location. You decide to use regression, which finds a relationship between these features and the house prices. But what if your model becomes too complicated? This is where penalized regression comes in.\n\n21.6.3.2.1 The Problem with Overfitting\nSometimes, the model tries too hard to fit every single data point perfectly. This can make the model very complex, like trying to draw a perfect line through a very bumpy path. This problem is called overfitting. An overfitted model works well on the data it has seen (training data) but performs poorly on new, unseen data (testing data).\n\n21.6.3.2.2 Introducing Penalized Regression\nPenalized regression helps prevent overfitting by adding a “penalty” to the model for being too complex. Think of it as a way to encourage the model to be simpler and more general. There are three common types of penalized regression:\n\n\nRidge Regression (L2 Penalty):\n\nAdds a penalty based on the size of the coefficients. It tries to keep all coefficients small.\nIf the model’s equation looks too complicated, Ridge Regression will push it towards a simpler form by shrinking the coefficients.\nImagine you have a rubber band that pulls the coefficients towards zero, making the model less likely to overfit.\n\n\n\nLasso Regression (L1 Penalty):\n\nAdds a penalty that can shrink some coefficients all the way to zero.\nThis means Lasso Regression can completely remove some features from the model, making it simpler.\nImagine you have a pair of scissors that can cut off the least important features, leaving only the most important ones.\n\n\n\nElastic Net:\n\nCombines both Ridge and Lasso penalties. It adds penalties for both the size and the number of coefficients.\nThis method balances between shrinking coefficients and eliminating some altogether, offering the benefits of both Ridge and Lasso.\nThink of Elastic Net as using both the rubber band (Ridge) and scissors (Lasso) to simplify the model.\n\n\n\nWith our data, the number of predictors is not huge, but we might be interested in 1) reducing overfitting, 2) improving interpretability, or 3) both by minimizing the number of predictors in our model without drastically affecting our prediction accuracy. Without penalized regression, the model might come up with a very complex equation. With Ridge, Lasso, or Elastic Net, the model simplifies this equation by either shrinking the coefficients (Ridge), removing some of them (Lasso), or balancing both (Elastic Net).\nHere’s a simple summary:\n\n\nRidge Regression: Reduces the impact of less important features by shrinking their coefficients.\n\nLasso Regression: Can eliminate some features entirely by setting their coefficients to zero.\n\nElastic Net: Combines the effects of Ridge and Lasso, shrinking some coefficients and eliminating others.\n\nUsing penalized regression in machine learning ensures that your model:\n\n\nPerforms Better on New Data: By avoiding overfitting, the model can make more accurate predictions on new, unseen data.\n\nIs Easier to Interpret: A simpler model with fewer features is easier to understand and explain.\n\n21.6.3.3 Penalized Regression with mlr3\nIn the mlr3 package, you can easily apply penalized regression methods to your tasks. Here’s how:\n\n\nSelect Penalized Regression Learners: mlr3 provides learners for Ridge, Lasso, and Elastic Net Regression.\n\nTrain the Learner: Use your data to train the chosen penalized regression model.\n\nEvaluate and Adjust: Check how well the model performs and make adjustments if needed.\n\nThis description explains penalized regression, including Ridge, Lasso, and Elastic Net, in an intuitive way, highlighting their benefits and how they work, while relating them to familiar concepts and the mlr3 package.\nRecall that we can use penalized regression to select the most important predictors from a large set of predictors. In this case, we will use the glmnet package to perform penalized regression, but we will use the mlr interface to glmnet to make it easier to use.\nThe nfolds parameter is the number of folds to use in the cross-validation procedure.\nWhat is Cross-Validation? Cross-validation is a technique used to assess how well a model will perform on unseen data. It involves splitting the data into multiple parts, training the model on some of these parts, and validating it on the remaining parts. This process is repeated several times to ensure the model’s performance is consistent.\nWhy Use Cross-Validation? Cross-validation helps to:\n\nAvoid Overfitting: By testing the model on different subsets of the data, cross-validation helps ensure that the model does not memorize the training data but learns to generalize from it.\nSelect the Best Model Parameters: Penalized regression models, such as those trained with glmnet, have parameters that control the strength of the penalty (e.g., lambda). Cross-validation helps find the best values for these parameters.\n\nWhen using the glmnet package, cross-validation can be performed using the cv.glmnet function. Here’s how the process works:\n\nSplit the Data: The data is divided into 𝑘 k folds (common choices are 5 or 10 folds). Each fold is a subset of the data.\nTrain and Validate: The model is trained 𝑘 k times. In each iteration, 𝑘 − 1 k−1 folds are used for training, and the remaining fold is used for validation. This process is repeated until each fold has been used as the validation set exactly once.\nCalculate Performance: The performance of the model (e.g., mean squared error for regression) is calculated for each fold. The average performance across all folds is computed to get an overall measure of how well the model is expected to perform on unseen data.\nSelect the Best Parameters: The cv.glmnet function evaluates different values of the penalty parameter (lambda). It selects the lambda value that results in the best average performance across the folds.\n\nIn this case, we will use the cv_glmnet learner, which will automatically select the best value of the penalization parameters. When the alpha parameter is set to 0, the model is a Ridge regression model. When the alpha parameter is set to 1, the model is a Lasso regression model.\n\nlearner = lrn(\"regr.cv_glmnet\", nfolds=10, alpha=0)\n\n\n21.6.3.3.1 Train\n\nlearner$train(exp_pred_task)\n\n\nmeasures = msrs(c('regr.rsq', 'regr.mse', 'regr.rmse'))\npred_train$score(measures)\n\n      rsq  regr.mse regr.rmse \n0.7505366 4.8335493 2.1985334 \n\n\nIn the case of the penalized regression, we can also look at the coefficients of the model.\n\ncoef(learner$model)\n\n15 x 1 sparse Matrix of class \"dgCMatrix\"\n                     s0\n(Intercept)  0.10173828\nControl     -0.07049573\nDnase        0.87495827\nH2az         0.33778155\nH3k27ac      0.19454755\nH3k27me3    -0.26196627\nH3k36me3     0.70067015\nH3k4me1     -0.06753216\nH3k4me2      0.15796965\nH3k4me3      0.38806405\nH3k79me2     0.94399114\nH3k9ac       0.50694074\nH3k9me1     -0.07422291\nH3k9me3     -0.17013775\nH4k20me1     0.11617186\n\n\nNote that the coefficients are all zero for the histone marks that were not selected by the model. In this case, we can use the model not to predict new data, but to help us understand the data.\n\npred_train = learner$predict(exp_pred_task, split$train)\npred_test = learner$predict(exp_pred_task, split$test)\n\n\n21.6.3.3.2 Assess\n\npred_train\n\n&lt;PredictionRegr&gt; for 5789 observations:\n row_ids     truth  response\n       1  6.082343  4.892642\n       2  2.989145  2.932909\n       3 -5.058894 -4.518292\n     ---       ---       ---\n    8637 -5.058894 -4.341742\n    8638  6.089159  4.696146\n    8640 -3.114148 -4.153795\n\nplot(pred_train)\n\n\n\n\n\n\n\nFor the training data:\n\nmeasures = msrs(c('regr.rsq'))\npred_train$score(measures)\n\n     rsq \n0.742853 \n\n\nAnd the test data:\n\npred_test$score(measures)\n\n      rsq \n0.7428019 \n\n\nAnd the plot of the test data predictions:\n\nplot(pred_test)\n\n\n\n\n\n\n\n\n# Calculate the R-squared value\ntruth &lt;- pred_test$truth\npredicted &lt;- pred_test$response\nrss &lt;- sum((truth - predicted)^2)  # Residual sum of squares\ntss &lt;- sum((truth - mean(truth))^2)  # Total sum of squares\nr_squared &lt;- 1 - (rss / tss)\n\n\n\n\n\nBrouwer-Visser, Jurriaan, Wei-Yi Cheng, Anna Bauer-Mehren, Daniela Maisel, Katharina Lechner, Emilia Andersson, Joel T. Dudley, and Francesca Milletti. 2018. “Regulatory T-Cell Genes Drive Altered Immune Microenvironment in Adult Solid Cancers and Allow for Immune Contextual Patient Subtyping.” Cancer Epidemiology, Biomarkers & Prevention 27 (1): 103–12. https://doi.org/10.1158/1055-9965.EPI-17-0461.",
    "crumbs": [
      "Home",
      "Machine Learning",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Examples</span>"
    ]
  },
  {
    "objectID": "geoquery.html",
    "href": "geoquery.html",
    "title": "\n22  Accessing and working with public omics data\n",
    "section": "",
    "text": "22.1 Background\nThe data we are going to access are from this paper.\nIn this little exercise, we will:",
    "crumbs": [
      "Home",
      "Bioconductor",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>Accessing and working with public omics data</span>"
    ]
  },
  {
    "objectID": "geoquery.html#background",
    "href": "geoquery.html#background",
    "title": "\n22  Accessing and working with public omics data\n",
    "section": "",
    "text": "Background: The tumor microenvironment is an important factor in cancer immunotherapy response. To further understand how a tumor affects the local immune system, we analyzed immune gene expression differences between matching normal and tumor tissue.Methods: We analyzed public and new gene expression data from solid cancers and isolated immune cell populations. We also determined the correlation between CD8, FoxP3 IHC, and our gene signatures.Results: We observed that regulatory T cells (Tregs) were one of the main drivers of immune gene expression differences between normal and tumor tissue. A tumor-specific CD8 signature was slightly lower in tumor tissue compared with normal of most (12 of 16) cancers, whereas a Treg signature was higher in tumor tissue of all cancers except liver. Clustering by Treg signature found two groups in colorectal cancer datasets. The high Treg cluster had more samples that were consensus molecular subtype 1/4, right-sided, and microsatellite-instable, compared with the low Treg cluster. Finally, we found that the correlation between signature and IHC was low in our small dataset, but samples in the high Treg cluster had significantly more CD8+ and FoxP3+ cells compared with the low Treg cluster.Conclusions: Treg gene expression is highly indicative of the overall tumor immune environment.Impact: In comparison with the consensus molecular subtype and microsatellite status, the Treg signature identifies more colorectal tumors with high immune activation that may benefit from cancer immunotherapy.\n\n\n\nAccess public omics data using the GEOquery package\nGet an opportunity to work with another SummarizedExperiment object.\nPerform a simple unsupervised analysis to visualize these public data.",
    "crumbs": [
      "Home",
      "Bioconductor",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>Accessing and working with public omics data</span>"
    ]
  },
  {
    "objectID": "geoquery.html#geoquery-to-pca",
    "href": "geoquery.html#geoquery-to-pca",
    "title": "\n22  Accessing and working with public omics data\n",
    "section": "\n22.2 GEOquery to PCA",
    "text": "22.2 GEOquery to PCA\nThe first step is to install the R package GEOquery. This package allows us to access data from the Gene Expression Omnibus (GEO) database. GEO is a public repository of omics data.\n\nBiocManager::install(\"GEOquery\")\n\nGEOquery has only one commonly used function, getGEO() which takes a GEO accession number as an argument. The GEO accession number is a unique identifier for a dataset.\nUse the GEOquery package to fetch data about GSE103512.\n\nlibrary(GEOquery)\ngse = getGEO(\"GSE103512\")[[1]]\n\nYou might ask why we are using [[1]] at the end of the getGEO() function. The reason is that getGEO() returns a list of GSE objects. We are only interested in the first one (and in this case, the only one). We return a list of GSE objects because in the early days, it was not unusual to have a single GEO accession number represent multiple datasets. While uncommon now, we’ve kept the convention since lots of “older” data is still quite useful.\nAgain, a historically-derived detail, is to convert from the older Bioconductor data structure (GEOquery was written in 2007), the ExpressionSet, to the newer SummarizedExperiment.\n\nlibrary(SummarizedExperiment)\nse = as(gse, \"SummarizedExperiment\")\n\nUse some code to determine the answers to the following:\n\nWhat is the class of se?\nWhat are the dimensions of se?\nWhat are the dimensions of the assay slot of se?\nWhat are the dimensions of the colData slot of se?\nWhat variables are in the colData slot of se?\n\nExamine two variables of interest, cancer type and tumor/normal status. The with function is a convenience to allow us to access variables in a data frame by name (rather than having to do dataframe$variable_name. Recalling that the table function is a convenient way to summarize the counts of unique values in a vector, we can use with to access the variables of interest and table to summarize the counts of unique values.\n\nwith(colData(se),table(`cancer.type.ch1`,`normal.ch1`))\n\n               normal.ch1\ncancer.type.ch1 no yes\n          BC    65  10\n          CRC   57  12\n          NSCLC 60   9\n          PCA   60   7\n\n\n\nHow many samples are there of each cancer type?\nHow many samples are there of each tumor/normal status?\n\nWhen performing unsupervised analysis, it is common to filter genes by variance to find the most informative genes. It is common practice to filter genes by standard deviation or some other measure of variability and keep the top X percent of them when performing dimensionality reduction. There is not a single right answer to what percentage to use, so try a few to see what happens. In the example code, I chose to use the top 500 genes by standard deviation, but you can play with the threshold to see what happens.\nRecall that the assay function is used to access the data matrix of the SummarizedExperiment object.\nThink through the code below and then run it.\n\nsds = apply(assay(se, 'exprs'),1,sd)\ndat = assay(se, 'exprs')[order(sds,decreasing = TRUE)[1:500],]\n\nIf you don’t recognize the function apply, it is a function that applies a function to each row or column of a matrix. In this case, we are applying the sd function to each row of the data matrix. The order function is used to sort the standard deviations in decreasing order (when decreasing=TRUE). And the [1:500] is used to subset the data matrix to the top 500 genes by standard deviation.\nPerform PCA and prepare for plotting. We will be using ggplot2, so we need to make a data.frame before plotting.\n\npca_results &lt;- prcomp(t(dat))\npca_df = as.data.frame(pca_results$x)\npca_df$Type=factor(colData(se)[,'cancer.type.ch1'])\npca_df$Normal = factor(colData(se)[,'normal.ch1'])\n\nWe can take a look at the variance explained by each principal component as a plot.\n\nlibrary(ggplot2)\npca_var &lt;- pca_results$sdev^2 / sum(pca_results $sdev^2)\nplot(pca_var, xlab = \"Principal Component\", ylab = \"Variance Explained\", \n     main = \"Variance Explained by Principal Components\", type = \"b\")\n\n\n\n\n\n\n\nNow, we are going to plot the results of the PCA, coloring the points by cancer type and using different shapes for normal and tumor samples.\n\nlibrary(ggplot2)\nggplot(pca_df, aes(x=PC1,y=PC2,shape=Normal,color=Type)) + \n    geom_point( alpha=0.6) + theme(text=element_text(size = 18))\n\n\n\n\n\n\n\nIn this case, the x-axis is the first principal component and the y-axis is the second principal component.\n\nWhat do you see?\nWhat about additional principal components?\nBonus: Try using the GGally package to plot principal components (using the ggpairs function).\nBonus: Calculate the variance explained by each principal component and plot the results.",
    "crumbs": [
      "Home",
      "Bioconductor",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>Accessing and working with public omics data</span>"
    ]
  },
  {
    "objectID": "bioc-summarizedexperiment.html",
    "href": "bioc-summarizedexperiment.html",
    "title": "\n23  Introduction to SummarizedExperiment\n",
    "section": "",
    "text": "23.1 Anatomy of a SummarizedExperiment\nThe SummarizedExperiment class is used to store rectangular matrices of experimental results, which are commonly produced by sequencing and microarray experiments. Each object stores observations of one or more samples, along with additional meta-data describing both the observations (features) and samples (phenotypes).\nA key aspect of the SummarizedExperiment class is the coordination of the meta-data and assays when subsetting. For example, if you want to exclude a given sample you can do for both the meta-data and assay in one operation, which ensures the meta-data and observed data will remain in sync. Improperly accounting for meta and observational data has resulted in a number of incorrect results and retractions so this is a very desirable property.\nSummarizedExperiment is in many ways similar to the historical ExpressionSet, the main distinction being that SummarizedExperiment is more flexible in it’s row information, allowing both GRanges based as well as those described by arbitrary DataFrames. This makes it ideally suited to a variety of experiments, particularly sequencing based experiments such as RNA-Seq and ChIp-Seq.\nThe SummarizedExperiment package contains two classes: SummarizedExperiment and RangedSummarizedExperiment.\nSummarizedExperiment is a matrix-like container where rows represent features of interest (e.g. genes, transcripts, exons, etc.) and columns represent samples. The objects contain one or more assays, each represented by a matrix-like object of numeric or other mode. The rows of a SummarizedExperiment object represent features of interest. Information about these features is stored in a DataFrame object, accessible using the function rowData(). Each row of the DataFrame provides information on the feature in the corresponding row of the SummarizedExperiment object. Columns of the DataFrame represent different attributes of the features of interest, e.g., gene or transcript IDs, etc.\nRangedSummarizedExperiment is the “child”” of the SummarizedExperiment class which means that all the methods on SummarizedExperiment also work on a RangedSummarizedExperiment.\nThe fundamental difference between the two classes is that the rows of a RangedSummarizedExperiment object represent genomic ranges of interest instead of a DataFrame of features. The RangedSummarizedExperiment ranges are described by a GRanges or a GRangesList object, accessible using the rowRanges() function.\nFigure 23.1 displays the class geometry and highlights the vertical (column) and horizontal (row) relationships.",
    "crumbs": [
      "Home",
      "Bioconductor",
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>Introduction to `SummarizedExperiment`</span>"
    ]
  },
  {
    "objectID": "bioc-summarizedexperiment.html#anatomy-of-a-summarizedexperiment",
    "href": "bioc-summarizedexperiment.html#anatomy-of-a-summarizedexperiment",
    "title": "\n23  Introduction to SummarizedExperiment\n",
    "section": "",
    "text": "Figure 23.1: Summarized Experiment. There are three main components, the colData(), the rowData() and the assays(). The accessors for the various parts of a complete SummarizedExperiment object match the names.\n\n\n\n23.1.1 Assays\nThe airway package contains an example dataset from an RNA-Seq experiment of read counts per gene for airway smooth muscles. These data are stored in a RangedSummarizedExperiment object which contains 8 different experimental and assays 64,102 gene transcripts.\n\n\nLoading required package: airway\n\n\n\nlibrary(SummarizedExperiment)\ndata(airway, package=\"airway\")\nse &lt;- airway\nse\n\nclass: RangedSummarizedExperiment \ndim: 63677 8 \nmetadata(1): ''\nassays(1): counts\nrownames(63677): ENSG00000000003 ENSG00000000005 ... ENSG00000273492\n  ENSG00000273493\nrowData names(10): gene_id gene_name ... seq_coord_system symbol\ncolnames(8): SRR1039508 SRR1039509 ... SRR1039520 SRR1039521\ncolData names(9): SampleName cell ... Sample BioSample\n\n\nTo retrieve the experiment data from a SummarizedExperiment object one can use the assays() accessor. An object can have multiple assay datasets each of which can be accessed using the $ operator. The airway dataset contains only one assay (counts). Here each row represents a gene transcript and each column one of the samples.\n\nassays(se)$counts\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSRR1039508\nSRR1039509\nSRR1039512\nSRR1039513\nSRR1039516\nSRR1039517\nSRR1039520\nSRR1039521\n\n\n\nENSG00000000003\n679\n448\n873\n408\n1138\n1047\n770\n572\n\n\nENSG00000000005\n0\n0\n0\n0\n0\n0\n0\n0\n\n\nENSG00000000419\n467\n515\n621\n365\n587\n799\n417\n508\n\n\nENSG00000000457\n260\n211\n263\n164\n245\n331\n233\n229\n\n\nENSG00000000460\n60\n55\n40\n35\n78\n63\n76\n60\n\n\nENSG00000000938\n0\n0\n2\n0\n1\n0\n0\n0\n\n\nENSG00000000971\n3251\n3679\n6177\n4252\n6721\n11027\n5176\n7995\n\n\nENSG00000001036\n1433\n1062\n1733\n881\n1424\n1439\n1359\n1109\n\n\nENSG00000001084\n519\n380\n595\n493\n820\n714\n696\n704\n\n\nENSG00000001167\n394\n236\n464\n175\n658\n584\n360\n269\n\n\n\n\n\n\n23.1.2 ‘Row’ (regions-of-interest) data\nThe rowRanges() accessor is used to view the range information for a RangedSummarizedExperiment. (Note if this were the parent SummarizedExperiment class we’d use rowData()). The data are stored in a GRangesList object, where each list element corresponds to one gene transcript and the ranges in each GRanges correspond to the exons in the transcript.\n\nrowRanges(se)\n\nGRangesList object of length 63677:\n$ENSG00000000003\nGRanges object with 17 ranges and 2 metadata columns:\n       seqnames            ranges strand |   exon_id       exon_name\n          &lt;Rle&gt;         &lt;IRanges&gt;  &lt;Rle&gt; | &lt;integer&gt;     &lt;character&gt;\n   [1]        X 99883667-99884983      - |    667145 ENSE00001459322\n   [2]        X 99885756-99885863      - |    667146 ENSE00000868868\n   [3]        X 99887482-99887565      - |    667147 ENSE00000401072\n   [4]        X 99887538-99887565      - |    667148 ENSE00001849132\n   [5]        X 99888402-99888536      - |    667149 ENSE00003554016\n   ...      ...               ...    ... .       ...             ...\n  [13]        X 99890555-99890743      - |    667156 ENSE00003512331\n  [14]        X 99891188-99891686      - |    667158 ENSE00001886883\n  [15]        X 99891605-99891803      - |    667159 ENSE00001855382\n  [16]        X 99891790-99892101      - |    667160 ENSE00001863395\n  [17]        X 99894942-99894988      - |    667161 ENSE00001828996\n  -------\n  seqinfo: 722 sequences (1 circular) from an unspecified genome\n\n...\n&lt;63676 more elements&gt;\n\n\n\n23.1.3 ‘Column’ (sample) data\nSample meta-data describing the samples can be accessed using colData(), and is a DataFrame that can store any number of descriptive columns for each sample row.\n\ncolData(se)\n\nDataFrame with 8 rows and 9 columns\n           SampleName     cell      dex    albut        Run avgLength\n             &lt;factor&gt; &lt;factor&gt; &lt;factor&gt; &lt;factor&gt;   &lt;factor&gt; &lt;integer&gt;\nSRR1039508 GSM1275862  N61311     untrt    untrt SRR1039508       126\nSRR1039509 GSM1275863  N61311     trt      untrt SRR1039509       126\nSRR1039512 GSM1275866  N052611    untrt    untrt SRR1039512       126\nSRR1039513 GSM1275867  N052611    trt      untrt SRR1039513        87\nSRR1039516 GSM1275870  N080611    untrt    untrt SRR1039516       120\nSRR1039517 GSM1275871  N080611    trt      untrt SRR1039517       126\nSRR1039520 GSM1275874  N061011    untrt    untrt SRR1039520       101\nSRR1039521 GSM1275875  N061011    trt      untrt SRR1039521        98\n           Experiment    Sample    BioSample\n             &lt;factor&gt;  &lt;factor&gt;     &lt;factor&gt;\nSRR1039508  SRX384345 SRS508568 SAMN02422669\nSRR1039509  SRX384346 SRS508567 SAMN02422675\nSRR1039512  SRX384349 SRS508571 SAMN02422678\nSRR1039513  SRX384350 SRS508572 SAMN02422670\nSRR1039516  SRX384353 SRS508575 SAMN02422682\nSRR1039517  SRX384354 SRS508576 SAMN02422673\nSRR1039520  SRX384357 SRS508579 SAMN02422683\nSRR1039521  SRX384358 SRS508580 SAMN02422677\n\n\nThis sample metadata can be accessed using the $ accessor which makes it easy to subset the entire object by a given phenotype.\n\n# subset for only those samples treated with dexamethasone\nse[, se$dex == \"trt\"]\n\nclass: RangedSummarizedExperiment \ndim: 63677 4 \nmetadata(1): ''\nassays(1): counts\nrownames(63677): ENSG00000000003 ENSG00000000005 ... ENSG00000273492\n  ENSG00000273493\nrowData names(10): gene_id gene_name ... seq_coord_system symbol\ncolnames(4): SRR1039509 SRR1039513 SRR1039517 SRR1039521\ncolData names(9): SampleName cell ... Sample BioSample\n\n\n\n23.1.4 Experiment-wide metadata\nMeta-data describing the experimental methods and publication references can be accessed using metadata().\n\nmetadata(se)\n\n[[1]]\nExperiment data\n  Experimenter name: Himes BE \n  Laboratory: NA \n  Contact information:  \n  Title: RNA-Seq transcriptome profiling identifies CRISPLD2 as a glucocorticoid responsive gene that modulates cytokine function in airway smooth muscle cells. \n  URL: http://www.ncbi.nlm.nih.gov/pubmed/24926665 \n  PMIDs: 24926665 \n\n  Abstract: A 226 word abstract is available. Use 'abstract' method.\n\n\nNote that metadata() is just a simple list, so it is appropriate for any experiment wide metadata the user wishes to save, such as storing model formulas.\n\nmetadata(se)$formula &lt;- counts ~ dex + albut\n\nmetadata(se)\n\n[[1]]\nExperiment data\n  Experimenter name: Himes BE \n  Laboratory: NA \n  Contact information:  \n  Title: RNA-Seq transcriptome profiling identifies CRISPLD2 as a glucocorticoid responsive gene that modulates cytokine function in airway smooth muscle cells. \n  URL: http://www.ncbi.nlm.nih.gov/pubmed/24926665 \n  PMIDs: 24926665 \n\n  Abstract: A 226 word abstract is available. Use 'abstract' method.\n\n$formula\ncounts ~ dex + albut",
    "crumbs": [
      "Home",
      "Bioconductor",
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>Introduction to `SummarizedExperiment`</span>"
    ]
  },
  {
    "objectID": "bioc-summarizedexperiment.html#common-operations-on-summarizedexperiment",
    "href": "bioc-summarizedexperiment.html#common-operations-on-summarizedexperiment",
    "title": "\n23  Introduction to SummarizedExperiment\n",
    "section": "\n23.2 Common operations on SummarizedExperiment\n",
    "text": "23.2 Common operations on SummarizedExperiment\n\n\n23.2.1 Subsetting\n\n\n[ Performs two dimensional subsetting, just like subsetting a matrix or data frame.\n\n\n# subset the first five transcripts and first three samples\nse[1:5, 1:3]\n\nclass: RangedSummarizedExperiment \ndim: 5 3 \nmetadata(2): '' formula\nassays(1): counts\nrownames(5): ENSG00000000003 ENSG00000000005 ENSG00000000419\n  ENSG00000000457 ENSG00000000460\nrowData names(10): gene_id gene_name ... seq_coord_system symbol\ncolnames(3): SRR1039508 SRR1039509 SRR1039512\ncolData names(9): SampleName cell ... Sample BioSample\n\n\n\n\n$ operates on colData() columns, for easy sample extraction.\n\n\nse[, se$cell == \"N61311\"]\n\nclass: RangedSummarizedExperiment \ndim: 63677 2 \nmetadata(2): '' formula\nassays(1): counts\nrownames(63677): ENSG00000000003 ENSG00000000005 ... ENSG00000273492\n  ENSG00000273493\nrowData names(10): gene_id gene_name ... seq_coord_system symbol\ncolnames(2): SRR1039508 SRR1039509\ncolData names(9): SampleName cell ... Sample BioSample\n\n\n\n23.2.2 Getters and setters\n\n\nrowRanges() / (rowData()), colData(), metadata()\n\n\n\ncounts &lt;- matrix(1:15, 5, 3, dimnames=list(LETTERS[1:5], LETTERS[1:3]))\n\ndates &lt;- SummarizedExperiment(assays=list(counts=counts),\n                              rowData=DataFrame(month=month.name[1:5], day=1:5))\n\n# Subset all January assays\ndates[rowData(dates)$month == \"January\", ]\n\nclass: SummarizedExperiment \ndim: 1 3 \nmetadata(0):\nassays(1): counts\nrownames(1): A\nrowData names(2): month day\ncolnames(3): A B C\ncolData names(0):\n\n\n\n\nassay() versus assays() There are two accessor functions for extracting the assay data from a SummarizedExperiment object. assays() operates on the entire list of assay data as a whole, while assay() operates on only one assay at a time. assay(x, i) is simply a convenience function which is equivalent to assays(x)[[i]].\n\n\nassays(se)\n\nList of length 1\nnames(1): counts\n\nassays(se)[[1]][1:5, 1:5]\n\n                SRR1039508 SRR1039509 SRR1039512 SRR1039513 SRR1039516\nENSG00000000003        679        448        873        408       1138\nENSG00000000005          0          0          0          0          0\nENSG00000000419        467        515        621        365        587\nENSG00000000457        260        211        263        164        245\nENSG00000000460         60         55         40         35         78\n\n# assay defaults to the first assay if no i is given\nassay(se)[1:5, 1:5]\n\n                SRR1039508 SRR1039509 SRR1039512 SRR1039513 SRR1039516\nENSG00000000003        679        448        873        408       1138\nENSG00000000005          0          0          0          0          0\nENSG00000000419        467        515        621        365        587\nENSG00000000457        260        211        263        164        245\nENSG00000000460         60         55         40         35         78\n\nassay(se, 1)[1:5, 1:5]\n\n                SRR1039508 SRR1039509 SRR1039512 SRR1039513 SRR1039516\nENSG00000000003        679        448        873        408       1138\nENSG00000000005          0          0          0          0          0\nENSG00000000419        467        515        621        365        587\nENSG00000000457        260        211        263        164        245\nENSG00000000460         60         55         40         35         78\n\n\n\n23.2.3 Range-based operations\n\n\nsubsetByOverlaps() SummarizedExperiment objects support all of the findOverlaps() methods and associated functions. This includes subsetByOverlaps(), which makes it easy to subset a SummarizedExperiment object by an interval.\n\nIn tne next code block, we define a region of interest (or many regions of interest) and then subset our SummarizedExperiment by overlaps with this region.\n\n# Subset for only rows which are in the interval 100,000 to 110,000 of\n# chromosome 1\nroi &lt;- GRanges(seqnames=\"1\", ranges=100000:1100000)\nsub_se = subsetByOverlaps(se, roi)\nsub_se\n\nclass: RangedSummarizedExperiment \ndim: 74 8 \nmetadata(2): '' formula\nassays(1): counts\nrownames(74): ENSG00000131591 ENSG00000177757 ... ENSG00000272512\n  ENSG00000273443\nrowData names(10): gene_id gene_name ... seq_coord_system symbol\ncolnames(8): SRR1039508 SRR1039509 ... SRR1039520 SRR1039521\ncolData names(9): SampleName cell ... Sample BioSample\n\ndim(sub_se)\n\n[1] 74  8",
    "crumbs": [
      "Home",
      "Bioconductor",
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>Introduction to `SummarizedExperiment`</span>"
    ]
  },
  {
    "objectID": "bioc-summarizedexperiment.html#constructing-a-summarizedexperiment",
    "href": "bioc-summarizedexperiment.html#constructing-a-summarizedexperiment",
    "title": "\n23  Introduction to SummarizedExperiment\n",
    "section": "\n23.3 Constructing a SummarizedExperiment\n",
    "text": "23.3 Constructing a SummarizedExperiment\n\nTo construct a SummarizedExperiment object, you need to provide the following components: - assays: A list of matrices or matrix-like objects containing the data. - rowData: A DataFrame containing information about the features (rows). - colData: A DataFrame containing information about the samples (columns). - +/- metadata: A list containing additional metadata about the experiment.\nFor a nearly real example, we will use the DeRisi dataset. We’ll start with the original data, which is a data.frame with the first couple of columns containing the gene information and the rest of the columns containing the data.\n\n# Load the DeRisi dataset\ndeRisi &lt;- read.csv(\"https://raw.githubusercontent.com/seandavi/RBiocBook/refs/heads/main/data/derisi.csv\")\nhead(deRisi)\n\n      ORF  Name    R1    R2    R3    R4   R5   R6    R7 R1.Bkg R2.Bkg R3.Bkg\n1 YHR007C ERG11  7896  7484 14679 14617 9853 7599  6490   1155   1984   1323\n2 YBR218C  PYC2 12144 11177 10241  4820 4950 7047 17035   1074   1694   1243\n3 YAL051W FUN43  4478  6435  6230  6848 5111 7180  4497   1140   1950   1649\n4 YAL053W        6343  8243  6743  3304 3556 4694  3849   1020   1897   1196\n5 YAL054C  ACS1  1542  3044  2076  1695 1753 4806 10802   1082   1940   1504\n6 YAL055W        1769  3243  2094  1367 1853 3580  1956    975   1821   1185\n  R4.Bkg R5.Bkg R6.Bkg R7.Bkg    G1    G2    G3    G4    G5    G6    G7 G1.Bkg\n1   1171    914   2445    981  8432  7173 11736 16798 12315 16111 13931   2404\n2    876   1211   2444    742 11509 10226 13372  6500  6255  9024  6904   2148\n3   1183    898   2637    927  5865  5895  5345  6302  5400  7933  5026   2422\n4    881   1045   2518    697  6762  7454  6323  3595  4689  5660  4145   2107\n5   1108    902   2610    980  3138  3785  2419  2114  2763  3561  1897   2405\n6    851   1047   2536    698  2844  4069  2583  1651  2530  3484  1550   1674\n  G2.Bkg G3.Bkg G4.Bkg G5.Bkg G6.Bkg G7.Bkg\n1   2561   1598   1506   1696   2667   1244\n2   2527   1641   1196   1553   2569    848\n3   2496   1902   1501   1644   2808   1154\n4   2663   1607   1162   1577   2544    857\n5   2528   1847   1445   1713   2767   1142\n6   2648   1591   1114   1528   2668    870\n\n\nTo convert this to a SummarizedExperiment, we need to extract the assay data, row data, and column data. The assay data will be the numeric values in the data frame, the row data will be the gene information, and the column data will be the sample information.\n\n23.3.1 rowData, or feature information\nLet’s start with the rowData, which will be a DataFrame containing the gene information. We can use the first two columns of the data frame for this purpose.\n\nrdata &lt;- deRisi[, 1:2]\nhead(rdata)\n\n      ORF  Name\n1 YHR007C ERG11\n2 YBR218C  PYC2\n3 YAL051W FUN43\n4 YAL053W      \n5 YAL054C  ACS1\n6 YAL055W      \n\n\n\n23.3.2 colData, or sample information\nNext, we will create the colData, which will be a DataFrame containing the sample information. Since the sample information really isn’t in the dataset, we will create a simple DataFrame with sample names.\n\ncdata &lt;- DataFrame(sample=paste(\"Sample\", 0:6), timepoint = 0:6,\n    hours = c(0, 9.5,11.5,13.5,15.5,18.5,20.5))\nhead(cdata)\n\nDataFrame with 6 rows and 3 columns\n       sample timepoint     hours\n  &lt;character&gt; &lt;integer&gt; &lt;numeric&gt;\n1    Sample 0         0       0.0\n2    Sample 1         1       9.5\n3    Sample 2         2      11.5\n4    Sample 3         3      13.5\n5    Sample 4         4      15.5\n6    Sample 5         5      18.5\n\n\n\n23.3.3 assays, or the data\nRemember that the DeRisi dataset has four different assays,\n\n\nassay\ndescription\n\n\n\nR\nRed fluorescence\n\n\nG\nGreen luorescence\n\n\nRb\nRed background fluorescence\n\n\nGb\nGreen background fluorescence\n\n\n\nWe will create a list of matrices, one for each assay. The matrices will be the numeric values in the data frame, excluding the first two columns.\n\nR &lt;- as.matrix(deRisi[, 3:9])\nG &lt;- as.matrix(deRisi[, 10:16])\nRb &lt;- as.matrix(deRisi[, 17:23])\nGb &lt;- as.matrix(deRisi[, 24:30])\n\nWhen we create a SummarizedExperiment object, the “constructor” will check to see that the colnames of the matrices in the list are the same as the rownames of the colData DataFrame, and that the rownames of the matrices in the list are the same as the rownames of the rowData DataFrame.\nSo, we need to fix that all up. Let’s start wit the rownames of the rowData DataFrame:\n\nrownames(rdata) &lt;- rdata$ORF\n\nNow, let’s set the rownames of the coldata DataFrame to the sample names:\n\nrownames(cdata) &lt;- cdata$sample\n\nNow, we can fix the rownames and colnames of the matrices for our R, G, Rb, and Gb assays:\n\nrownames(R) &lt;- rdata$ORF\nrownames(G) &lt;- rdata$ORF\nrownames(Rb) &lt;- rdata$ORF\nrownames(Gb) &lt;- rdata$ORF\ncolnames(R) &lt;- cdata$sample\ncolnames(G) &lt;- cdata$sample\ncolnames(Rb) &lt;- cdata$sample\ncolnames(Gb) &lt;- cdata$sample\n\nTake a look at the matrices to make sure they look right.\n\n23.3.4 Putting it all together\n\nse &lt;- SummarizedExperiment(assays=list(R=R, G=G, Rb=Rb, Gb=Gb), rowData=rdata, colData=cdata)\n\n\n23.3.5 Getting logRatios\nNow that we have a SummarizedExperiment object, we can easily compute the log ratios of the Red and Green foreground fluorescence. This is a common operation in microarray data analysis.\n\nlogRatios &lt;- log2(\n    (assay(se, \"R\") - assay(se, \"Rb\")) / (assay(se, \"G\") - assay(se, \"Gb\"))\n)\n\nWarning: NaNs produced\n\nassays(se)$logRatios &lt;- logRatios\n\nNow, we’ve added a new assay to the SummarizedExperiment object called logRatios. This assay contains the log ratios of the Red and Green foreground fluorescence.\n\nassays(se)\n\nList of length 5\nnames(5): R G Rb Gb logRatios\n\n\nAnd if we want to access the log ratios, we can do so using the assay() method:\n\nhead(assay(se, \"logRatios\"))\n\n          Sample 0     Sample 1  Sample 2   Sample 3   Sample 4 Sample 5\nYHR007C -1.2204686          NaN       NaN 2.70275677  1.6545902 5.260867\nYBR218C        NaN          NaN 2.9757832 2.39231742  1.9319816 3.983313\nYAL051W  0.1135715          NaN       NaN        NaN -1.3681061 2.138654\nYAL053W -1.3753298          NaN       NaN 0.05044902  1.0906497 5.215440\nYAL054C  0.2706476  0.333657387 0.0000000 0.31420165  0.3165815      NaN\nYAL055W  0.6209723 -0.001745548 0.2683547 0.11082813  0.4931189      NaN\n         Sample 6\nYHR007C 4.8223618\nYBR218C       NaN\nYAL051W 1.2205754\nYAL053W 0.8875253\nYAL054C       NaN\nYAL055W       NaN\n\n## OR\nhead(assays(se)$logRatios)\n\n          Sample 0     Sample 1  Sample 2   Sample 3   Sample 4 Sample 5\nYHR007C -1.2204686          NaN       NaN 2.70275677  1.6545902 5.260867\nYBR218C        NaN          NaN 2.9757832 2.39231742  1.9319816 3.983313\nYAL051W  0.1135715          NaN       NaN        NaN -1.3681061 2.138654\nYAL053W -1.3753298          NaN       NaN 0.05044902  1.0906497 5.215440\nYAL054C  0.2706476  0.333657387 0.0000000 0.31420165  0.3165815      NaN\nYAL055W  0.6209723 -0.001745548 0.2683547 0.11082813  0.4931189      NaN\n         Sample 6\nYHR007C 4.8223618\nYBR218C       NaN\nYAL051W 1.2205754\nYAL053W 0.8875253\nYAL054C       NaN\nYAL055W       NaN\n\n\n\nhist(assays(se)$logRatios, breaks=50, main=\"Log Ratios of Red and Green Foreground Fluorescence\", xlab=\"Log Ratio\")",
    "crumbs": [
      "Home",
      "Bioconductor",
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>Introduction to `SummarizedExperiment`</span>"
    ]
  },
  {
    "objectID": "genomic_ranges_tutorial.html",
    "href": "genomic_ranges_tutorial.html",
    "title": "\n24  Genomic Ranges Introduction\n",
    "section": "",
    "text": "24.1 Introduction\nGenomic ranges are fundamental data structures in bioinformatics that represent intervals on chromosomes. They are essential for analyzing ChIP-seq peaks, gene annotations, regulatory elements, and other genomic features. In this tutorial, we’ll explore the BED file format and demonstrate practical genomic range operations using R’s rtracklayer package.\nThis tutorial will cover:",
    "crumbs": [
      "Home",
      "Bioconductor",
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>Genomic Ranges Introduction</span>"
    ]
  },
  {
    "objectID": "genomic_ranges_tutorial.html#introduction",
    "href": "genomic_ranges_tutorial.html#introduction",
    "title": "\n24  Genomic Ranges Introduction\n",
    "section": "",
    "text": "Understanding the BED file format\nLoading genomic ranges from BED files\nBasic exploration of genomic ranges\nAccessing and manipulating genomic coordinates",
    "crumbs": [
      "Home",
      "Bioconductor",
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>Genomic Ranges Introduction</span>"
    ]
  },
  {
    "objectID": "genomic_ranges_tutorial.html#the-dataset",
    "href": "genomic_ranges_tutorial.html#the-dataset",
    "title": "\n24  Genomic Ranges Introduction\n",
    "section": "\n24.2 The dataset",
    "text": "24.2 The dataset\nWe’ll use CTCF ChIP-seq peak data from the ENCODE project. CTCF (CCCTC-binding factor) is a key architectural protein that helps organize chromatin structure. The data is available in BED format, which we will load and analyze. Each peak represents the results of a ChIP-seq experiment, indicating regions where CTCF binds to DNA. The sample was sequenced, aligned, and peaks were called using standard ChIP-seq analysis pipelines. The peaks are stored in a BED file, which we will import into R for analysis.",
    "crumbs": [
      "Home",
      "Bioconductor",
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>Genomic Ranges Introduction</span>"
    ]
  },
  {
    "objectID": "genomic_ranges_tutorial.html#the-bed-file-format",
    "href": "genomic_ranges_tutorial.html#the-bed-file-format",
    "title": "\n24  Genomic Ranges Introduction\n",
    "section": "\n24.3 The BED File Format",
    "text": "24.3 The BED File Format\nThe Browser Extensible Data (BED) format is a standard way to represent genomic intervals. BED files are tab-delimited text files where each line represents a genomic feature.\n\n24.3.1 BED Format Structure\nThe BED format has several required and optional fields:\nRequired fields (BED3):\n\n\nchrom: Chromosome name (e.g., chr1, chr2, chrX)\n\nchromStart: Start position (0-based, inclusive)\n\nchromEnd: End position (0-based, exclusive)\n\nOptional fields:\n\n\nname: Feature name/identifier\n\nscore: Score (0-1000)\n\nstrand: Strand (+ or - OR ’*’)\n\nthickStart: Start of thick drawing\n\nthickEnd: End of thick drawing\n\nitemRgb: RGB color values\n\nblockCount: Number of blocks\n\nblockSizes: Block sizes\n\nblockStarts: Block start positions\n\n24.3.2 Key Concepts\n\n\n0-based coordinate system: BED uses 0-based coordinates where the first base is position 0\n\nHalf-open intervals: chromStart is inclusive, chromEnd is exclusive\n\nWidth calculation: Width = chromEnd - chromStart\n\nExample BED entry:\nchr1    1000    2000    peak1    500    +\nThis represents a feature named “peak1” on chromosome 1, from position 1000 to 1999 (width = 1000 bp).",
    "crumbs": [
      "Home",
      "Bioconductor",
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>Genomic Ranges Introduction</span>"
    ]
  },
  {
    "objectID": "genomic_ranges_tutorial.html#loading-required-libraries",
    "href": "genomic_ranges_tutorial.html#loading-required-libraries",
    "title": "\n24  Genomic Ranges Introduction\n",
    "section": "\n24.4 Loading Required Libraries",
    "text": "24.4 Loading Required Libraries\n\n# Load required libraries\nlibrary(rtracklayer)\nlibrary(GenomicRanges)\nlibrary(ggplot2)\nlibrary(dplyr)\n\n# Set up plotting theme\ntheme_set(theme_minimal())",
    "crumbs": [
      "Home",
      "Bioconductor",
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>Genomic Ranges Introduction</span>"
    ]
  },
  {
    "objectID": "genomic_ranges_tutorial.html#loading-ctcf-chip-seq-data",
    "href": "genomic_ranges_tutorial.html#loading-ctcf-chip-seq-data",
    "title": "\n24  Genomic Ranges Introduction\n",
    "section": "\n24.5 Loading CTCF ChIP-seq Data",
    "text": "24.5 Loading CTCF ChIP-seq Data\nWe’ll work with CTCF ChIP-seq peak data from the ENCODE project. CTCF (CCCTC-binding factor) is a key architectural protein that helps organize chromatin structure.\n\n# URL for the CTCF ChIP-seq BED file\nbed_url &lt;- \"https://www.encodeproject.org/files/ENCFF960ZGP/@@download/ENCFF960ZGP.bed.gz\"\n\n# Let's first load this file using readr::read_table to check its structure\nctcf_peaks_raw &lt;- readr::read_table(bed_url, col_names=FALSE)\n\n\n── Column specification ────────────────────────────────────────────────────────\ncols(\n  X1 = col_character(),\n  X2 = col_double(),\n  X3 = col_double(),\n  X4 = col_character(),\n  X5 = col_double(),\n  X6 = col_character(),\n  X7 = col_double(),\n  X8 = col_double(),\n  X9 = col_double(),\n  X10 = col_double()\n)\n\n# Display the first few rows of the raw data\nhead(ctcf_peaks_raw)\n\n# A tibble: 6 × 10\n  X1           X2        X3 X4       X5 X6       X7    X8    X9   X10\n  &lt;chr&gt;     &lt;dbl&gt;     &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1 chr2  238305323 238305539 .      1000 .      7.70    -1 0.274   108\n2 chr20  49982706  49982922 .       541 .      8.72    -1 0.466   108\n3 chr15  39921015  39921231 .       672 .      9.08    -1 0.547   108\n4 chr8    6708273   6708489 .       560 .      9.86    -1 0.662   108\n5 chr9  136645956 136646172 .       584 .     10.2     -1 0.751   108\n6 chr7   47669294  47669510 .       614 .     10.5     -1 0.807   108\n\n\nBioconductor’s rtracklayer package provides a convenient way to import BED files directly into R as GRanges objects, which are optimized for genomic range operations. We’ll use this package to load the CTCF peaks data.\n\n# Load the BED file using rtracklayer\nctcf_peaks &lt;- rtracklayer::import(bed_url, format=\"narrowPeak\")\n\nLet’s take a look at the loaded CTCF peaks data. The ctcf_peaks object is a GRanges object that contains genomic ranges representing the CTCF ChIP-seq peaks.\n\nctcf_peaks\n\nGRanges object with 43865 ranges and 6 metadata columns:\n          seqnames              ranges strand |        name     score\n             &lt;Rle&gt;           &lt;IRanges&gt;  &lt;Rle&gt; | &lt;character&gt; &lt;numeric&gt;\n      [1]     chr2 238305324-238305539      * |        &lt;NA&gt;      1000\n      [2]    chr20   49982707-49982922      * |        &lt;NA&gt;       541\n      [3]    chr15   39921016-39921231      * |        &lt;NA&gt;       672\n      [4]     chr8     6708274-6708489      * |        &lt;NA&gt;       560\n      [5]     chr9 136645957-136646172      * |        &lt;NA&gt;       584\n      ...      ...                 ...    ... .         ...       ...\n  [43861]    chr11   66222736-66222972      * |        &lt;NA&gt;      1000\n  [43862]    chr10   75235956-75236225      * |        &lt;NA&gt;      1000\n  [43863]    chr16   57649100-57649347      * |        &lt;NA&gt;      1000\n  [43864]    chr17   37373596-37373838      * |        &lt;NA&gt;      1000\n  [43865]    chr12   53676108-53676355      * |        &lt;NA&gt;      1000\n          signalValue    pValue    qValue      peak\n            &lt;numeric&gt; &lt;numeric&gt; &lt;numeric&gt; &lt;integer&gt;\n      [1]     7.70385        -1   0.27378       108\n      [2]     8.71626        -1   0.46571       108\n      [3]     9.07638        -1   0.54697       108\n      [4]     9.86234        -1   0.66189       108\n      [5]    10.15488        -1   0.75082       108\n      ...         ...       ...       ...       ...\n  [43861]     478.640        -1   4.87574       124\n  [43862]     480.183        -1   4.87574       141\n  [43863]     491.081        -1   4.87574       116\n  [43864]     491.991        -1   4.87574       127\n  [43865]     494.303        -1   4.87574       126\n  -------\n  seqinfo: 26 sequences from an unspecified genome; no seqlengths\n\n\nThe ctcf_peaks object now contains the genomic ranges of CTCF peaks, including chromosome names, start and end positions, and additional metadata such as peak scores and strand information.",
    "crumbs": [
      "Home",
      "Bioconductor",
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>Genomic Ranges Introduction</span>"
    ]
  },
  {
    "objectID": "genomic_ranges_tutorial.html#understanding-granges-objects",
    "href": "genomic_ranges_tutorial.html#understanding-granges-objects",
    "title": "\n24  Genomic Ranges Introduction\n",
    "section": "\n24.6 Understanding GRanges Objects",
    "text": "24.6 Understanding GRanges Objects\nTo get information from a GRanges object, we can use various accessor functions. For example, seqnames() retrieves the chromosome names, start() and end() get the start and end positions, and width() calculates the width of each peak.\n\n# Accessing chromosome names, start, end, and width\nlength(ctcf_peaks) # Total number of peaks\n\n[1] 43865\n\nseqnames(ctcf_peaks) # Chromosome names (or contig names, etc.)\n\nfactor-Rle of length 43865 with 41566 runs\n  Lengths:     1     1     1     1     1 ...     1     1     1     1     1\n  Values : chr2  chr20 chr15 chr8  chr9  ... chr11 chr10 chr16 chr17 chr12\nLevels(26): chr2 chr20 chr15 ... chr1_KI270714v1_random chrUn_GL000219v1\n\nhead(start(ctcf_peaks)) # Start positions (0-based)\n\n[1] 238305324  49982707  39921016   6708274 136645957  47669295\n\nhead(end(ctcf_peaks)) # End positions (1-based)\n\n[1] 238305539  49982922  39921231   6708489 136646172  47669510\n\nhead(width(ctcf_peaks)) # equivalent to end(ctcf_peaks) - start(ctcf_peaks)\n\n[1] 216 216 216 216 216 216\n\n\nWhat is the distribution of peak widths? We can visualize this using a histogram.\n\n# Create a histogram of peak widths\nhist(width(ctcf_peaks), breaks=50, main=\"CTCF Peak Widths\", xlab=\"Width (bp)\", col=\"lightblue\")\n\n\n\nHistogram of CTCF Peak Widths. Why is there a large peak at around 200 bp?\n\n\n\nThe “metadata” part of a GRanges object can be accessed using the mcols() function, which returns a data frame-like structure containing additional information about each genomic range.\n\nmcols(ctcf_peaks)\n\nDataFrame with 43865 rows and 6 columns\n             name     score signalValue    pValue    qValue      peak\n      &lt;character&gt; &lt;numeric&gt;   &lt;numeric&gt; &lt;numeric&gt; &lt;numeric&gt; &lt;integer&gt;\n1              NA      1000     7.70385        -1   0.27378       108\n2              NA       541     8.71626        -1   0.46571       108\n3              NA       672     9.07638        -1   0.54697       108\n4              NA       560     9.86234        -1   0.66189       108\n5              NA       584    10.15488        -1   0.75082       108\n...           ...       ...         ...       ...       ...       ...\n43861          NA      1000     478.640        -1   4.87574       124\n43862          NA      1000     480.183        -1   4.87574       141\n43863          NA      1000     491.081        -1   4.87574       116\n43864          NA      1000     491.991        -1   4.87574       127\n43865          NA      1000     494.303        -1   4.87574       126\n\n\nThe mcols() function returns a data frame-like structure containing additional information about each genomic range, such as peak scores and strand orientation. This metadata can be useful for filtering or annotating peaks.\n\nmcols(ctcf_peaks)[1:5, ]\n\nDataFrame with 5 rows and 6 columns\n         name     score signalValue    pValue    qValue      peak\n  &lt;character&gt; &lt;numeric&gt;   &lt;numeric&gt; &lt;numeric&gt; &lt;numeric&gt; &lt;integer&gt;\n1          NA      1000     7.70385        -1   0.27378       108\n2          NA       541     8.71626        -1   0.46571       108\n3          NA       672     9.07638        -1   0.54697       108\n4          NA       560     9.86234        -1   0.66189       108\n5          NA       584    10.15488        -1   0.75082       108\n\nmcols(ctcf_peaks)$signalValue[1:5]  # Accessing the score column directly\n\n[1]  7.70385  8.71626  9.07638  9.86234 10.15488\n\nhist(mcols(ctcf_peaks)$signalValue, breaks=50, main=\"CTCF Peak Scores\", xlab=\"Score\", col=\"lightblue\")",
    "crumbs": [
      "Home",
      "Bioconductor",
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>Genomic Ranges Introduction</span>"
    ]
  },
  {
    "objectID": "genomic_ranges_tutorial.html#exploring-peak-characteristics",
    "href": "genomic_ranges_tutorial.html#exploring-peak-characteristics",
    "title": "\n24  Genomic Ranges Introduction\n",
    "section": "\n24.7 Exploring Peak Characteristics",
    "text": "24.7 Exploring Peak Characteristics\n\n24.7.1 Basic Peak Statistics\nHow many peaks do we have? What chromosomes are represented? What is the range of peak widths? Let’s calculate some basic statistics about the CTCF peaks.\n\n# Calculate basic statistics\ncat(\"=== CTCF Peak Statistics ===\\n\")\n\n=== CTCF Peak Statistics ===\n\ncat(\"Total number of peaks:\", length(ctcf_peaks), \"\\n\")\n\nTotal number of peaks: 43865 \n\ncat(\"Number of chromosomes represented:\", length(unique(seqnames(ctcf_peaks))), \"\\n\")\n\nNumber of chromosomes represented: 26 \n\ncat(\"Peak width range:\", min(width(ctcf_peaks)), \"-\", max(width(ctcf_peaks)), \"bp\\n\")\n\nPeak width range: 54 - 442 bp\n\ncat(\"Median peak width:\", median(width(ctcf_peaks)), \"bp\\n\")\n\nMedian peak width: 216 bp\n\ncat(\"Mean peak width:\", round(mean(width(ctcf_peaks)), 1), \"bp\\n\")\n\nMean peak width: 181.1 bp\n\n# Show chromosome names\ncat(\"\\nChromosomes present:\\n\")\n\n\nChromosomes present:\n\nprint(sort(unique(as.character(seqnames(ctcf_peaks)))))\n\n [1] \"chr1\"                    \"chr1_KI270714v1_random\" \n [3] \"chr10\"                   \"chr11\"                  \n [5] \"chr12\"                   \"chr13\"                  \n [7] \"chr14\"                   \"chr15\"                  \n [9] \"chr16\"                   \"chr17\"                  \n[11] \"chr17_GL000205v2_random\" \"chr18\"                  \n[13] \"chr19\"                   \"chr2\"                   \n[15] \"chr20\"                   \"chr21\"                  \n[17] \"chr22\"                   \"chr3\"                   \n[19] \"chr4\"                    \"chr5\"                   \n[21] \"chr6\"                    \"chr7\"                   \n[23] \"chr8\"                    \"chr9\"                   \n[25] \"chrUn_GL000219v1\"        \"chrX\"                   \n\n\n\n24.7.2 Peaks Per Chromosome\n\n# Count peaks per chromosome\npeaks_per_chr &lt;- table(seqnames(ctcf_peaks))\npeaks_per_chr_df &lt;- as.data.frame(peaks_per_chr)\npeaks_per_chr_df\n\n                      Var1 Freq\n1                     chr2 3332\n2                    chr20 1236\n3                    chr15 1431\n4                     chr8 1935\n5                     chr9 1820\n6                     chr7 2140\n7                     chr4 2030\n8                    chr12 2274\n9                     chr5 2355\n10                   chr22  873\n11                    chr1 4207\n12                   chr17 2010\n13                    chr3 2808\n14                   chr11 2417\n15                   chr19 1662\n16                    chr6 2504\n17                   chr13 1030\n18                    chrX 1460\n19                   chr10 2037\n20                   chr16 1497\n21                   chr14 1461\n22                   chr21  406\n23                   chr18  932\n24 chr17_GL000205v2_random    4\n25  chr1_KI270714v1_random    1\n26        chrUn_GL000219v1    3",
    "crumbs": [
      "Home",
      "Bioconductor",
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>Genomic Ranges Introduction</span>"
    ]
  },
  {
    "objectID": "genomic_ranges_tutorial.html#accessing-peak-coordinates",
    "href": "genomic_ranges_tutorial.html#accessing-peak-coordinates",
    "title": "\n24  Genomic Ranges Introduction\n",
    "section": "\n24.8 Accessing Peak Coordinates",
    "text": "24.8 Accessing Peak Coordinates\n\n24.8.1 Finding Starts and Ends\n\n# Extract start and end coordinates\npeak_starts &lt;- start(ctcf_peaks)\npeak_ends &lt;- end(ctcf_peaks)\npeak_centers &lt;- start(ctcf_peaks) + (width(ctcf_peaks)/2)\n\nhead(peak_starts, 10)\n\n [1] 238305324  49982707  39921016   6708274 136645957  47669295 136784479\n [8] 139453373 122421409 102583856\n\nhead(peak_ends, 10)\n\n [1] 238305539  49982922  39921231   6708489 136646172  47669510 136784694\n [8] 139453588 122421624 102584071\n\nhead(peak_centers, 10)\n\n [1] 238305432  49982815  39921124   6708382 136646065  47669403 136784587\n [8] 139453481 122421517 102583964",
    "crumbs": [
      "Home",
      "Bioconductor",
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>Genomic Ranges Introduction</span>"
    ]
  },
  {
    "objectID": "genomic_ranges_tutorial.html#manipulating-peak-ranges",
    "href": "genomic_ranges_tutorial.html#manipulating-peak-ranges",
    "title": "\n24  Genomic Ranges Introduction\n",
    "section": "\n24.9 Manipulating Peak Ranges",
    "text": "24.9 Manipulating Peak Ranges\n\n24.9.1 Shifting Peaks\nShifting peaks is useful for various analyses, such as creating flanking regions or adjusting peak positions.\n\n# Shift peaks by different amounts\npeaks_shifted_100bp &lt;- shift(ctcf_peaks, 100)  # Shift right by 100bp\npeaks_shifted_neg50bp &lt;- shift(ctcf_peaks, -50)  # Shift left by 50bp\n\ncat(\"=== Peak Shifting Examples ===\\n\")\n\n=== Peak Shifting Examples ===\n\ncat(\"Original peak 1:\", as.character(ctcf_peaks[1]), \"\\n\")\n\nOriginal peak 1: chr2:238305324-238305539 \n\ncat(\"Shifted +100bp:\", as.character(peaks_shifted_100bp[1]), \"\\n\")\n\nShifted +100bp: chr2:238305424-238305639 \n\ncat(\"Shifted -50bp:\", as.character(peaks_shifted_neg50bp[1]), \"\\n\")\n\nShifted -50bp: chr2:238305274-238305489 \n\n# Demonstrate that width is preserved during shifting\ncat(\"\\nWidths after shifting (should be unchanged):\\n\")\n\n\nWidths after shifting (should be unchanged):\n\ncat(\"Original width:\", width(ctcf_peaks[1]), \"\\n\")\n\nOriginal width: 216 \n\ncat(\"Shifted +100bp width:\", width(peaks_shifted_100bp[1]), \"\\n\")\n\nShifted +100bp width: 216 \n\ncat(\"Shifted -50bp width:\", width(peaks_shifted_neg50bp[1]), \"\\n\")\n\nShifted -50bp width: 216 \n\n\n\n24.9.2 Setting Peak Widths\nResizing peaks is common when standardizing peak sizes or creating fixed-width windows around peak centers.\n\n# Resize peaks to fixed width (200bp) centered on original peak center\npeaks_200bp &lt;- resize(ctcf_peaks, width = 200, fix = \"center\")\n\n# Resize peaks to 500bp, keeping the start position fixed\npeaks_500bp_start &lt;- resize(ctcf_peaks, width = 500, fix = \"start\")\n\n# Resize peaks to 300bp, keeping the end position fixed\npeaks_300bp_end &lt;- resize(ctcf_peaks, width = 300, fix = \"end\")\n\ncat(\"=== Peak Resizing Examples ===\\n\")\n\n=== Peak Resizing Examples ===\n\ncat(\"Original peak 1:\", as.character(ctcf_peaks[1]), \"\\n\")\n\nOriginal peak 1: chr2:238305324-238305539 \n\ncat(\"Resized to 200bp (center):\", as.character(peaks_200bp[1]), \"\\n\")\n\nResized to 200bp (center): chr2:238305332-238305531 \n\ncat(\"Resized to 500bp (start fixed):\", as.character(peaks_500bp_start[1]), \"\\n\")\n\nResized to 500bp (start fixed): chr2:238305324-238305823 \n\ncat(\"Resized to 300bp (end fixed):\", as.character(peaks_300bp_end[1]), \"\\n\")\n\nResized to 300bp (end fixed): chr2:238305240-238305539 \n\n# Verify that all peaks now have the specified width\ncat(\"\\nWidth verification:\\n\")\n\n\nWidth verification:\n\ncat(\"200bp resize - all widths 200?\", all(width(peaks_200bp) == 200), \"\\n\")\n\n200bp resize - all widths 200? TRUE \n\ncat(\"500bp resize - all widths 500?\", all(width(peaks_500bp_start) == 500), \"\\n\")\n\n500bp resize - all widths 500? TRUE \n\ncat(\"300bp resize - all widths 300?\", all(width(peaks_300bp_end) == 300), \"\\n\")\n\n300bp resize - all widths 300? TRUE \n\n\n\n24.9.3 Creating Flanking Regions\n\n# Create flanking regions around peaks\nupstream_1kb &lt;- flank(ctcf_peaks, width = 1000, start = TRUE)\ndownstream_1kb &lt;- flank(ctcf_peaks, width = 1000, start = FALSE)\n\n# Create regions extending in both directions\nextended_peaks &lt;- resize(ctcf_peaks, width = width(ctcf_peaks) + 2000, fix = \"center\")\n\ncat(\"=== Flanking Region Examples ===\\n\")\n\n=== Flanking Region Examples ===\n\ncat(\"Original peak 1:\", as.character(ctcf_peaks[1]), \"\\n\")\n\nOriginal peak 1: chr2:238305324-238305539 \n\ncat(\"1kb upstream:\", as.character(upstream_1kb[1]), \"\\n\")\n\n1kb upstream: chr2:238304324-238305323 \n\ncat(\"1kb downstream:\", as.character(downstream_1kb[1]), \"\\n\")\n\n1kb downstream: chr2:238305540-238306539 \n\ncat(\"Extended ±1kb:\", as.character(extended_peaks[1]), \"\\n\")\n\nExtended ±1kb: chr2:238304324-238306539",
    "crumbs": [
      "Home",
      "Bioconductor",
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>Genomic Ranges Introduction</span>"
    ]
  },
  {
    "objectID": "genomic_ranges_tutorial.html#coverage",
    "href": "genomic_ranges_tutorial.html#coverage",
    "title": "\n24  Genomic Ranges Introduction\n",
    "section": "\n24.10 Coverage",
    "text": "24.10 Coverage\nCoverage refers to the number of times a genomic region is covered by ranges on a chromosome. A common use case is calculating coverage from ChIP-seq data, where we want to know how many reads overlap with each peak or for doing “peak calling” analysis.\nWe can make a toy example by simulating random reads of length 50 bp across a chromosome and then calculating coverage.\n\n# Simulate random reads on chromosome 1\nset.seed(42)  # For reproducibility\nchrom_length &lt;- 1000000  # Length of chromosome 1\nnum_reads &lt;- 100000\nread_length &lt;- 50  # Length of each read\nrandom_starts &lt;- sample(seq_len(chrom_length - read_length + 1), num_reads, replace = TRUE)\nrandom_reads &lt;- GRanges(seqnames = \"chr1\",\n                         ranges = IRanges(start = random_starts, end = random_starts + read_length-1))\nrandom_reads\n\nGRanges object with 100000 ranges and 0 metadata columns:\n           seqnames        ranges strand\n              &lt;Rle&gt;     &lt;IRanges&gt;  &lt;Rle&gt;\n       [1]     chr1   61413-61462      *\n       [2]     chr1   54425-54474      *\n       [3]     chr1 623844-623893      *\n       [4]     chr1   74362-74411      *\n       [5]     chr1   46208-46257      *\n       ...      ...           ...    ...\n   [99996]     chr1 663489-663538      *\n   [99997]     chr1 886082-886131      *\n   [99998]     chr1 933222-933271      *\n   [99999]     chr1 486340-486389      *\n  [100000]     chr1 177556-177605      *\n  -------\n  seqinfo: 1 sequence from an unspecified genome; no seqlengths\n\n\nWe are now ready to calculate the coverage of these random reads across the chromosome. The coverage() function from the GenomicRanges package computes the coverage of ranges in a GRanges object.\n\nrandom_coverage &lt;- coverage(random_reads, width = chrom_length)\nhead(random_coverage)\n\nRleList of length 1\n$chr1\ninteger-Rle of length 1000000 with 173047 runs\n  Lengths:  7  1  6  3  8  8  1  1 22  1  4 ...  3  1  2  8 12 23  5  2  8  6\n  Values :  0  1  3  4  5  6  7  8  9  8  6 ...  3  2  3  4  5  4  3  2  1  0\n\n\nWe can visualize the coverage of the random reads on chromosome 1. The coverage() function returns a Rle object, which is a run-length encoding of the coverage values.\n\nplot(random_coverage[[1]][1:10000], main=\"Coverage of Random Reads on Chromosome 1\", xlab=\"Position\", ylab=\"Coverage\", type=\"h\", col=\"blue\")\n\n\n\n\n\n\n\nThis plot shows the coverage of random reads across the first 10,000 bases of chromosome 1. The y-axis represents the number of reads covering each position, while the x-axis represents the genomic positions.\n\n# Calculate coverage density\nhist(as.numeric(random_coverage[[1]]), main=\"Coverage Density\", xlab=\"Coverage\", ylab=\"Frequency\", col=\"lightblue\")",
    "crumbs": [
      "Home",
      "Bioconductor",
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>Genomic Ranges Introduction</span>"
    ]
  },
  {
    "objectID": "genomic_ranges_tutorial.html#key-takeaways",
    "href": "genomic_ranges_tutorial.html#key-takeaways",
    "title": "\n24  Genomic Ranges Introduction\n",
    "section": "\n24.11 Key Takeaways",
    "text": "24.11 Key Takeaways\nThis tutorial demonstrated several important concepts:\n\n\nBED file format: Understanding the structure and coordinate system of BED files\n\nLoading genomic data: Using rtracklayer to import BED files into R\n\nBasic exploration: Counting features, examining distributions, and summarizing data\n\nCoordinate manipulation: Accessing starts, ends, and performing coordinate arithmetic\n\nRange operations: Shifting, resizing, and creating flanking regions\n\nAdvanced analysis: Finding overlaps and performing grouped operations\n\n\n24.11.1 Common Use Cases\n\n\nPeak calling analysis: Examining ChIP-seq peaks, ATAC-seq peaks, etc.\n\nAnnotation overlap: Finding genes or regulatory elements near peaks\n\nComparative analysis: Comparing peak sets between conditions or samples\n\nMotif analysis: Creating sequences around peak centers for motif discovery\n\nVisualization: Preparing data for genome browser tracks or custom plots\n\n24.11.2 Best Practices\n\nAlways check coordinate systems (0-based vs 1-based)\nVerify chromosome naming conventions match your reference genome\nConsider peak width distributions when setting analysis parameters\nUse appropriate genome builds for all analyses\nDocument coordinate transformations and filtering steps\n\nThis foundation in genomic ranges and BED file manipulation will serve as a basis for more advanced genomic analyses in R.",
    "crumbs": [
      "Home",
      "Bioconductor",
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>Genomic Ranges Introduction</span>"
    ]
  },
  {
    "objectID": "ranges_and_signals.html",
    "href": "ranges_and_signals.html",
    "title": "\n25  Genomic ranges and features\n",
    "section": "",
    "text": "25.1 Introduction\nGenomic ranges are essential components in the field of genomics, representing intervals on the genome that specify the start and end positions of DNA segments. These ranges can denote various genomic features, such as genes, exons, regulatory elements, and regions identified in genomic studies like ChIP-seq peaks. They play a pivotal role in the annotation, comparison, and interpretation of genomic features and experimental data, making them indispensable in biological data analysis.\nUnderstanding genomic ranges begins with the concept of coordinate systems. Different databases and tools adopt different conventions for indexing genomic coordinates. For instance, the UCSC Genome Browser uses a 0-based coordinate system, while Ensembl employs a 1-based system. Moreover, genomic ranges often include strand information, indicating whether the feature is on the positive or negative DNA strand, which is crucial for correctly interpreting gene expression and other genomic functions.\nGenomic ranges come in various forms, from single ranges defined by a simple start and end position (such as a single exon) to complex multi-range sets encompassing collections of ranges like all exons of a gene. Manipulating these ranges involves several fundamental operations. Intersection allows researchers to find overlapping regions between two sets of ranges, such as identifying ChIP-seq peaks that overlap with promoter regions. Union operations combine multiple ranges into a single contiguous range, while set difference identifies regions in one set that do not overlap with another set.\nSeveral tools and libraries have been developed to facilitate the manipulation of genomic ranges. In the R programming environment, the Bioconductor project provides the GenomicRanges package, which is specifically designed for representing and manipulating genomic ranges. This package offers a variety of functions for range arithmetic and efficient overlap queries. Another useful R package is rtracklayer, which enables the import and export of genomic data in various formats, including BED and GFF files.\nFor those who prefer a command-line interface, BEDTools offers a suite of utilities for performing a wide range of operations on genomic intervals. This toolset is highly versatile, supporting tasks like intersecting, merging, and complementing genomic intervals. In the Python ecosystem, PyRanges provides a fast and flexible library for manipulating genomic intervals, offering similar functionality to Bioconductor’s GenomicRanges.\nThe applications of genomic ranges are diverse and far-reaching. In gene annotation, for instance, RNA-seq reads are mapped to known gene models to quantify gene expression levels. Variant annotation involves mapping variants identified from sequencing data to their genomic context, predicting functional consequences based on their location within genes or intergenic regions. Comparative genomics leverages genomic ranges to compare intervals between species, identifying conserved regions that might indicate essential functional elements. Epigenomic studies utilize genomic ranges to intersect DNA methylation data or histone modification peaks with genomic features, providing insights into regulatory mechanisms.\nDespite their utility, working with genomic ranges presents several challenges. Converting coordinates between different reference genomes or different versions of the same genome can be complex and prone to errors. Integrating diverse types of genomic data, such as DNA sequences, epigenetic marks, and RNA-seq data, requires meticulous handling of genomic coordinates and ranges to ensure accurate analyses. Moreover, the sheer scale of genomic data necessitates optimized algorithms and data structures to handle large datasets efficiently.\nInterpreting genomic ranges within their biological context is crucial for drawing meaningful conclusions. For instance, a range within a gene’s promoter region might indicate potential regulatory activity. Understanding the functional implications of genomic ranges often involves overlapping these ranges with known functional elements, such as enhancers or silencers, to infer gene regulation mechanisms and their phenotypic consequences. Tools like the UCSC Genome Browser and the Integrative Genomics Viewer (IGV) are invaluable for visualizing genomic ranges alongside other genomic annotations, aiding in the interpretation and exploration of genomic data.",
    "crumbs": [
      "Home",
      "Bioconductor",
      "<span class='chapter-number'>25</span>  <span class='chapter-title'>Genomic ranges and features</span>"
    ]
  },
  {
    "objectID": "ranges_and_signals.html#bioconductor-and-genomicranges",
    "href": "ranges_and_signals.html#bioconductor-and-genomicranges",
    "title": "\n25  Genomic ranges and features\n",
    "section": "\n25.2 Bioconductor and GenomicRanges",
    "text": "25.2 Bioconductor and GenomicRanges\n\nlibrary(GenomicRanges)\n\nThe Bioconductor GenomicRanges package is a comprehensive toolkit designed to handle and manipulate genomic intervals and variables systematized on these intervals (Lawrence et al. 2013). Developed by Bioconductor, this package simplifies the complexity of managing genomic data, facilitating the efficient exploration, manipulation, and visualization of such data. GenomicRanges aids in dealing with the challenges of genomic data, including its massive size, intricate relationships, and high dimensionality.\nThe GenomicRanges package in Bioconductor covers a wide range of use cases related to the management and analysis of genomic data. Here are some key examples:\n\n\nGenomic Feature Manipulation\n\nThe GenomicRanges and GRanges classes can be used to represent and manipulate various genomic features such as genes, transcripts, exons, or single-nucleotide polymorphisms (SNPs). Users can query, subset, resize, shift, or sort these features based on their genomic coordinates.\n\n\n\nGenomic Interval Operations\n\nThe GenomicRanges package provides functions for performing operations on genomic intervals, such as finding overlaps, nearest neighbors, or disjoint intervals. These operations are fundamental to many types of genomic data analyses, such as identifying genes that overlap with ChIP-seq peaks, or finding variants that are in close proximity to each other.\n\n\n\nAlignments and Coverage\n\nThe GAlignments and GAlignmentPairs classes can be used to represent alignments of sequencing reads to a reference genome, such as those produced by a read aligner. Users can then compute coverage of these alignments over genomic ranges of interest, which is a common task in RNA-seq or ChIP-seq analysis.\n\n\n\nAnnotation and Metadata Handling\n\nThe metadata column of a GRanges object can be used to store various types of annotation data associated with genomic ranges, such as gene names, gene biotypes, or experimental scores. This makes it easy to perform analyses that integrate genomic coordinates with other types of biological information.\n\n\n\nGenome Arithmetic\n\nThe GenomicRanges package supports a version of “genome arithmetic”, which includes set operations (union, intersection, set difference) as well as other operations (like coverage, complement, or reduction) that are adapted to the specificities of genomic data.\n\n\n\nEfficient Data Handling\n\nThe CompressedGRangesList class provides a space-efficient way to represent a large list of GRanges objects, which is particularly useful when working with large genomic datasets, such as whole-genome sequencing data.\n\n\n\nThe GenomicRanges package in Bioconductor uses the S4 class system (see Table 25.1), which is a part of the methods package in R. The S4 system is a more rigorous and formal approach to object-oriented programming in R, providing enhanced capabilities for object design and function dispatch.\n\n\n\n\n\n\n\n\n\nClass Name\nDescription\nPotential Use\n\n\n\nGRanges\nRepresents a collection of genomic ranges and associated variables.\nChipSeq peaks, CpG islands, etc.\n\n\nGRangesList\nRepresents a list of GenomicRanges objects.\ntranscript models (exons, introns)\n\n\nRangesList\nRepresents a list of Ranges objects.\n\n\n\nIRanges\nRepresents a collection of integer ranges.\nused mainly to build GRanges, etc.\n\n\nGPos\nRepresents genomic positions.\nSNPs or other single nicleotide locations\n\n\nGAlignments\nRepresents alignments against a reference genome.\nSequence read locations from a BAM file\n\n\nGAlignmentPairs\nRepresents pairs of alignments, typically representing a single fragment of DNA.\nPaired-end sequence alignments\n\n\n\n\n\nTable 25.1: Classes within the GenomicRanges package. Each class has a slightly different use case.\n\n\nIn the context of the GenomicRanges package, the S4 class system allows for the creation of complex, structured data objects that can effectively encapsulate genomic intervals and associated data. This system enables the package to handle the complexity and intricacy of genomic data.\nFor example, the GenomicRanges class in the package is an S4 class that combines several basic data types into a composite object. It includes slots for sequence names (seqnames), ranges (start and end positions), strand information, and metadata. Each slot in the S4 class corresponds to a specific component of the genomic data, and methods (see Table 25.2 and Table 25.3) can be defined to interact with these slots in a structured and predictable way.\n\n\n\n\n\n\n\n\nMethod\nDescription\n\n\n\nlength\nReturns the number of ranges in the GRanges object.\n\n\nseqnames\nRetrieves the sequence names of the ranges.\n\n\nranges\nRetrieves the start and end positions of the ranges.\n\n\nstrand\nRetrieves the strand information of the ranges.\n\n\nelementMetadata\nRetrieves the metadata associated with the ranges.\n\n\nseqlevels\nReturns the levels of the factor that the seqnames slot is derived from.\n\n\nseqinfo\nRetrieves the Seqinfo (sequence information) object associated with the GRanges object.\n\n\nstart, end, width\nRetrieve or set the start or end positions, or the width of the ranges.\n\n\nresize\nResizes the ranges.\n\n\nsubset, [, [[, $\nSubset or extract elements from the GRanges object.\n\n\nsort\nSorts the GRanges object.\n\n\nshift\nShifts the ranges by a certain number of base pairs.\n\n\n\n\n\nTable 25.2: Methods for accessing, manipulating single objects\n\n\nThe S4 class system also supports inheritance, which allows for the creation of specialized subclasses that share certain characteristics with a parent class but have additional features or behaviors.\nThe S4 system’s formalism and rigor make it well-suited to the complexities of bioinformatics and genomic data analysis. It allows for the creation of robust, reliable code that can handle complex data structures and operations, making it a key part of the GenomicRanges package and other Bioconductor packages.\n\n\n\n\n\n\n\n\nMethod\nDescription\n\n\n\nfindOverlaps\nFinds overlaps between different sets of ranges.\n\n\ncountOverlaps\nCounts overlaps between different sets of ranges.\n\n\nsubsetByOverlaps\nSubsets the ranges based on overlaps.\n\n\ndistanceToNearest\nComputes the distance to the nearest range in another set of ranges.\n\n\n\n\n\nTable 25.3: Methods for comparing and combining multiple GenomicRanges-class objects\n\n\nTo get going, we can construct a GRanges object by hand as an example.\nThe GRanges class represents a collection of genomic ranges that each have a single start and end location on the genome. It can be used to store the location of genomic features such as contiguous binding sites, transcripts, and exons. These objects can be created by using the GRanges constructor function. The following code just creates a GRanges object from scratch.\n\ngr &lt;- GRanges(\n    seqnames = Rle(c(\"chr1\", \"chr2\", \"chr1\", \"chr3\"), c(1, 3, 2, 4)),\n    ranges = IRanges(101:110, end = 111:120, names = head(letters, 10)),\n    strand = Rle(strand(c(\"-\", \"+\", \"*\", \"+\", \"-\")), c(1, 2, 2, 3, 2)),\n    score = 1:10,\n    GC = seq(1, 0, length=10))\ngr\n\nGRanges object with 10 ranges and 2 metadata columns:\n    seqnames    ranges strand |     score        GC\n       &lt;Rle&gt; &lt;IRanges&gt;  &lt;Rle&gt; | &lt;integer&gt; &lt;numeric&gt;\n  a     chr1   101-111      - |         1  1.000000\n  b     chr2   102-112      + |         2  0.888889\n  c     chr2   103-113      + |         3  0.777778\n  d     chr2   104-114      * |         4  0.666667\n  e     chr1   105-115      * |         5  0.555556\n  f     chr1   106-116      + |         6  0.444444\n  g     chr3   107-117      + |         7  0.333333\n  h     chr3   108-118      + |         8  0.222222\n  i     chr3   109-119      - |         9  0.111111\n  j     chr3   110-120      - |        10  0.000000\n  -------\n  seqinfo: 3 sequences from an unspecified genome; no seqlengths\n\n\nThis creates a GRanges object with 10 genomic ranges. The output of the GRanges show() method separates the information into a left and right hand region that are separated by | symbols (see Figure 25.1). The genomic coordinates (seqnames, ranges, and strand) are located on the left-hand side and the metadata columns are located on the right. For this example, the metadata is comprised of score and GC information, but almost anything can be stored in the metadata portion of a GRanges object.\n\n\n\n\n\nFigure 25.1: The structure of a GRanges object, which behaves a bit like a vector of ranges, although the analogy is not perfect. A GRanges object is composed of the “Ranges” part the lefthand box, the “metadata” columns (the righthand box), and a “seqinfo” part that describes the names and lengths of associated sequences. Only the “Ranges” part is required. The figure also shows a few of the “accessors” and approaches to subsetting a GRanges object.\n\n\nThe components of the genomic coordinates within a GRanges object can be extracted using the seqnames, ranges, and strand accessor functions.\n\nseqnames(gr)\n\nfactor-Rle of length 10 with 4 runs\n  Lengths:    1    3    2    4\n  Values : chr1 chr2 chr1 chr3\nLevels(3): chr1 chr2 chr3\n\nranges(gr)\n\nIRanges object with 10 ranges and 0 metadata columns:\n        start       end     width\n    &lt;integer&gt; &lt;integer&gt; &lt;integer&gt;\n  a       101       111        11\n  b       102       112        11\n  c       103       113        11\n  d       104       114        11\n  e       105       115        11\n  f       106       116        11\n  g       107       117        11\n  h       108       118        11\n  i       109       119        11\n  j       110       120        11\n\nstrand(gr)\n\nfactor-Rle of length 10 with 5 runs\n  Lengths: 1 2 2 3 2\n  Values : - + * + -\nLevels(3): + - *\n\n\nNote that the GRanges object has information to the “left” side of the | that has special accessors. The information to the right side of the |, when it is present, is the metadata and is accessed using mcols(), for “metadata columns”.\n\nclass(mcols(gr))\n\n[1] \"DFrame\"\nattr(,\"package\")\n[1] \"S4Vectors\"\n\nmcols(gr)\n\nDataFrame with 10 rows and 2 columns\n      score        GC\n  &lt;integer&gt; &lt;numeric&gt;\na         1  1.000000\nb         2  0.888889\nc         3  0.777778\nd         4  0.666667\ne         5  0.555556\nf         6  0.444444\ng         7  0.333333\nh         8  0.222222\ni         9  0.111111\nj        10  0.000000\n\n\nSince the class of mcols(gr) is DFrame, we can use our DataFrame approaches to work with the data.\n\nmcols(gr)$score\n\n [1]  1  2  3  4  5  6  7  8  9 10\n\n\nWe can even assign a new column.\n\nmcols(gr)$AT = 1-mcols(gr)$GC\ngr\n\nGRanges object with 10 ranges and 3 metadata columns:\n    seqnames    ranges strand |     score        GC        AT\n       &lt;Rle&gt; &lt;IRanges&gt;  &lt;Rle&gt; | &lt;integer&gt; &lt;numeric&gt; &lt;numeric&gt;\n  a     chr1   101-111      - |         1  1.000000  0.000000\n  b     chr2   102-112      + |         2  0.888889  0.111111\n  c     chr2   103-113      + |         3  0.777778  0.222222\n  d     chr2   104-114      * |         4  0.666667  0.333333\n  e     chr1   105-115      * |         5  0.555556  0.444444\n  f     chr1   106-116      + |         6  0.444444  0.555556\n  g     chr3   107-117      + |         7  0.333333  0.666667\n  h     chr3   108-118      + |         8  0.222222  0.777778\n  i     chr3   109-119      - |         9  0.111111  0.888889\n  j     chr3   110-120      - |        10  0.000000  1.000000\n  -------\n  seqinfo: 3 sequences from an unspecified genome; no seqlengths\n\n\nAnother common way to create a GRanges object is to start with a data.frame, perhaps created by hand like below or read in using read.csv or read.table. We can convert from a data.frame, when columns are named appropriately, to a GRanges object.\n\ndf_regions = data.frame(chromosome = rep(\"chr1\",10),\n                        start=seq(1000,10000,1000),\n                        end=seq(1100, 10100, 1000))\nas(df_regions,'GRanges') # note that names have to match with GRanges slots\n\nGRanges object with 10 ranges and 0 metadata columns:\n       seqnames      ranges strand\n          &lt;Rle&gt;   &lt;IRanges&gt;  &lt;Rle&gt;\n   [1]     chr1   1000-1100      *\n   [2]     chr1   2000-2100      *\n   [3]     chr1   3000-3100      *\n   [4]     chr1   4000-4100      *\n   [5]     chr1   5000-5100      *\n   [6]     chr1   6000-6100      *\n   [7]     chr1   7000-7100      *\n   [8]     chr1   8000-8100      *\n   [9]     chr1   9000-9100      *\n  [10]     chr1 10000-10100      *\n  -------\n  seqinfo: 1 sequence from an unspecified genome; no seqlengths\n\n## fix column name\ncolnames(df_regions)[1] = 'seqnames'\ngr2 = as(df_regions,'GRanges')\ngr2\n\nGRanges object with 10 ranges and 0 metadata columns:\n       seqnames      ranges strand\n          &lt;Rle&gt;   &lt;IRanges&gt;  &lt;Rle&gt;\n   [1]     chr1   1000-1100      *\n   [2]     chr1   2000-2100      *\n   [3]     chr1   3000-3100      *\n   [4]     chr1   4000-4100      *\n   [5]     chr1   5000-5100      *\n   [6]     chr1   6000-6100      *\n   [7]     chr1   7000-7100      *\n   [8]     chr1   8000-8100      *\n   [9]     chr1   9000-9100      *\n  [10]     chr1 10000-10100      *\n  -------\n  seqinfo: 1 sequence from an unspecified genome; no seqlengths\n\n\nGRanges have one-dimensional-like behavior. For instance, we can check the length and even give GRanges names.\n\nnames(gr)\n\n [1] \"a\" \"b\" \"c\" \"d\" \"e\" \"f\" \"g\" \"h\" \"i\" \"j\"\n\nlength(gr)\n\n[1] 10\n\n\n\n25.2.1 Subsetting GRanges objects\nWhile GRanges objects look a bit like a data.frame, they can be thought of as vectors with associated ranges. Subsetting, then, works very similarly to vectors. To subset a GRanges object to include only second and third regions:\n\ngr[2:3]\n\nGRanges object with 2 ranges and 3 metadata columns:\n    seqnames    ranges strand |     score        GC        AT\n       &lt;Rle&gt; &lt;IRanges&gt;  &lt;Rle&gt; | &lt;integer&gt; &lt;numeric&gt; &lt;numeric&gt;\n  b     chr2   102-112      + |         2  0.888889  0.111111\n  c     chr2   103-113      + |         3  0.777778  0.222222\n  -------\n  seqinfo: 3 sequences from an unspecified genome; no seqlengths\n\n\nThat said, if the GRanges object has metadata columns, we can also treat it like a two-dimensional object kind of like a data.frame. Note that the information to the left of the | is not like a data.frame, so we cannot do something like gr$seqnames. Here is an example of subsetting with the subset of one metadata column.\n\ngr[2:3, \"GC\"]\n\nGRanges object with 2 ranges and 1 metadata column:\n    seqnames    ranges strand |        GC\n       &lt;Rle&gt; &lt;IRanges&gt;  &lt;Rle&gt; | &lt;numeric&gt;\n  b     chr2   102-112      + |  0.888889\n  c     chr2   103-113      + |  0.777778\n  -------\n  seqinfo: 3 sequences from an unspecified genome; no seqlengths\n\n\nThe usual head() and tail() also work just fine.\n\nhead(gr,n=2)\n\nGRanges object with 2 ranges and 3 metadata columns:\n    seqnames    ranges strand |     score        GC        AT\n       &lt;Rle&gt; &lt;IRanges&gt;  &lt;Rle&gt; | &lt;integer&gt; &lt;numeric&gt; &lt;numeric&gt;\n  a     chr1   101-111      - |         1  1.000000  0.000000\n  b     chr2   102-112      + |         2  0.888889  0.111111\n  -------\n  seqinfo: 3 sequences from an unspecified genome; no seqlengths\n\ntail(gr,n=2)\n\nGRanges object with 2 ranges and 3 metadata columns:\n    seqnames    ranges strand |     score        GC        AT\n       &lt;Rle&gt; &lt;IRanges&gt;  &lt;Rle&gt; | &lt;integer&gt; &lt;numeric&gt; &lt;numeric&gt;\n  i     chr3   109-119      - |         9  0.111111  0.888889\n  j     chr3   110-120      - |        10  0.000000  1.000000\n  -------\n  seqinfo: 3 sequences from an unspecified genome; no seqlengths\n\n\n\n25.2.2 Interval operations on one GRanges object\n\n25.2.2.1 Intra-range methods\nThe methods described in this section work one-region-at-a-time and are, therefore, called “intra-region” methods. Methods that work across all regions are described below in the Inter-range methods section.\nThe GRanges class has accessors for the “ranges” part of the data. For example:\n\n## Make a smaller GRanges subset\ng &lt;- gr[1:3]\nstart(g) # to get start locations\n\n[1] 101 102 103\n\nend(g)   # to get end locations\n\n[1] 111 112 113\n\nwidth(g) # to get the \"widths\" of each range\n\n[1] 11 11 11\n\nrange(g) # to get the \"range\" for each sequence (min(start) through max(end))\n\nGRanges object with 2 ranges and 0 metadata columns:\n      seqnames    ranges strand\n         &lt;Rle&gt; &lt;IRanges&gt;  &lt;Rle&gt;\n  [1]     chr1   101-111      -\n  [2]     chr2   102-113      +\n  -------\n  seqinfo: 3 sequences from an unspecified genome; no seqlengths\n\n\nThe GRanges class also has many methods for manipulating the ranges. The methods can be classified as intra-range methods, inter-range methods, and between-range methods. Intra-range methods operate on each element of a GRanges object independent of the other ranges in the object. For example, the flank method can be used to recover regions flanking the set of ranges represented by the GRanges object. So to get a GRanges object containing the ranges that include the 10 bases upstream of the ranges:\n\nflank(g, 10)\n\nGRanges object with 3 ranges and 3 metadata columns:\n    seqnames    ranges strand |     score        GC        AT\n       &lt;Rle&gt; &lt;IRanges&gt;  &lt;Rle&gt; | &lt;integer&gt; &lt;numeric&gt; &lt;numeric&gt;\n  a     chr1   112-121      - |         1  1.000000  0.000000\n  b     chr2    92-101      + |         2  0.888889  0.111111\n  c     chr2    93-102      + |         3  0.777778  0.222222\n  -------\n  seqinfo: 3 sequences from an unspecified genome; no seqlengths\n\n\nNote how flank pays attention to “strand”. To get the flanking regions downstream of the ranges, we can do:\n\nflank(g, 10, start=FALSE)\n\nGRanges object with 3 ranges and 3 metadata columns:\n    seqnames    ranges strand |     score        GC        AT\n       &lt;Rle&gt; &lt;IRanges&gt;  &lt;Rle&gt; | &lt;integer&gt; &lt;numeric&gt; &lt;numeric&gt;\n  a     chr1    91-100      - |         1  1.000000  0.000000\n  b     chr2   113-122      + |         2  0.888889  0.111111\n  c     chr2   114-123      + |         3  0.777778  0.222222\n  -------\n  seqinfo: 3 sequences from an unspecified genome; no seqlengths\n\n\nOther examples of intra-range methods include resize and shift. The shift method will move the ranges by a specific number of base pairs, and the resize method will extend the ranges by a specified width.\n\nshift(g, 5)\n\nGRanges object with 3 ranges and 3 metadata columns:\n    seqnames    ranges strand |     score        GC        AT\n       &lt;Rle&gt; &lt;IRanges&gt;  &lt;Rle&gt; | &lt;integer&gt; &lt;numeric&gt; &lt;numeric&gt;\n  a     chr1   106-116      - |         1  1.000000  0.000000\n  b     chr2   107-117      + |         2  0.888889  0.111111\n  c     chr2   108-118      + |         3  0.777778  0.222222\n  -------\n  seqinfo: 3 sequences from an unspecified genome; no seqlengths\n\nresize(g, 30)\n\nGRanges object with 3 ranges and 3 metadata columns:\n    seqnames    ranges strand |     score        GC        AT\n       &lt;Rle&gt; &lt;IRanges&gt;  &lt;Rle&gt; | &lt;integer&gt; &lt;numeric&gt; &lt;numeric&gt;\n  a     chr1    82-111      - |         1  1.000000  0.000000\n  b     chr2   102-131      + |         2  0.888889  0.111111\n  c     chr2   103-132      + |         3  0.777778  0.222222\n  -------\n  seqinfo: 3 sequences from an unspecified genome; no seqlengths\n\n\nThe GenomicRanges help page ?\"intra-range-methods\" summarizes these methods.\n\n25.2.2.2 Inter-range methods\nInter-range methods involve comparisons between ranges in a single GRanges object. For instance, the reduce method will align the ranges and merge overlapping ranges to produce a simplified set.\n\nreduce(g)\n\nGRanges object with 2 ranges and 0 metadata columns:\n      seqnames    ranges strand\n         &lt;Rle&gt; &lt;IRanges&gt;  &lt;Rle&gt;\n  [1]     chr1   101-111      -\n  [2]     chr2   102-113      +\n  -------\n  seqinfo: 3 sequences from an unspecified genome; no seqlengths\n\n\nThe reduce method could, for example, be used to collapse individual overlapping coding exons into a single set of coding regions.\nSometimes one is interested in the gaps or the qualities of the gaps between the ranges represented by your GRanges object. The gaps method provides this information:\n\ngaps(g)\n\nGRanges object with 2 ranges and 0 metadata columns:\n      seqnames    ranges strand\n         &lt;Rle&gt; &lt;IRanges&gt;  &lt;Rle&gt;\n  [1]     chr1     1-100      -\n  [2]     chr2     1-101      +\n  -------\n  seqinfo: 3 sequences from an unspecified genome; no seqlengths\n\n\nIn this case, we have not specified the lengths of the chromosomes, so Bioconductor is making the assumption (incorrectly) that the chromosomes end at the largest location on each chromosome. We can correct this by setting the seqlengths correctly, but we can ignore that detail for now.\nThe disjoin method represents a GRanges object as a collection of non-overlapping ranges:\n\ndisjoin(g)\n\nGRanges object with 4 ranges and 0 metadata columns:\n      seqnames    ranges strand\n         &lt;Rle&gt; &lt;IRanges&gt;  &lt;Rle&gt;\n  [1]     chr1   101-111      -\n  [2]     chr2       102      +\n  [3]     chr2   103-112      +\n  [4]     chr2       113      +\n  -------\n  seqinfo: 3 sequences from an unspecified genome; no seqlengths\n\n\nThe coverage method quantifies the degree of overlap for all the ranges in a GRanges object.\n\ncoverage(g)\n\nRleList of length 3\n$chr1\ninteger-Rle of length 111 with 2 runs\n  Lengths: 100  11\n  Values :   0   1\n\n$chr2\ninteger-Rle of length 113 with 4 runs\n  Lengths: 101   1  10   1\n  Values :   0   1   2   1\n\n$chr3\ninteger-Rle of length 0 with 0 runs\n  Lengths: \n  Values : \n\n\nThe coverage is summarized as a list of coverages, one for each chromosome. The Rle class is used to store the values. Sometimes, one must convert these values to numeric using as.numeric. In many cases, this will happen automatically, though.\n\ncovg = coverage(g)\ncovg_chr2 = covg[['chr2']]\nplot(covg_chr2, type='l')\n\n\n\n\n\n\n\nSee the GenomicRanges help page ?\"intra-range-methods\" for more details.\n\n25.2.3 Set operations for GRanges objects\nBetween-range methods calculate relationships between different GRanges objects. Of central importance are findOverlaps and related operations; these are discussed below. Additional operations treat GRanges as mathematical sets of coordinates; union(g, g2) is the union of the coordinates in g and g2. Here are examples for calculating the union, the intersect and the asymmetric difference (using setdiff).\n\ng2 &lt;- head(gr, n=2)\nGenomicRanges::union(g, g2)\n\nGRanges object with 2 ranges and 0 metadata columns:\n      seqnames    ranges strand\n         &lt;Rle&gt; &lt;IRanges&gt;  &lt;Rle&gt;\n  [1]     chr1   101-111      -\n  [2]     chr2   102-113      +\n  -------\n  seqinfo: 3 sequences from an unspecified genome; no seqlengths\n\nGenomicRanges::intersect(g, g2)\n\nGRanges object with 2 ranges and 0 metadata columns:\n      seqnames    ranges strand\n         &lt;Rle&gt; &lt;IRanges&gt;  &lt;Rle&gt;\n  [1]     chr1   101-111      -\n  [2]     chr2   102-112      +\n  -------\n  seqinfo: 3 sequences from an unspecified genome; no seqlengths\n\nGenomicRanges::setdiff(g, g2)\n\nGRanges object with 1 range and 0 metadata columns:\n      seqnames    ranges strand\n         &lt;Rle&gt; &lt;IRanges&gt;  &lt;Rle&gt;\n  [1]     chr2       113      +\n  -------\n  seqinfo: 3 sequences from an unspecified genome; no seqlengths\n\n\nThere is extensive additional help available or by looking at the vignettes in at the GenomicRanges pages.\n\n?GRanges\n\nThere are also many possible methods that work with GRanges objects. To see a complete list (long), try:\n\nmethods(class=\"GRanges\")",
    "crumbs": [
      "Home",
      "Bioconductor",
      "<span class='chapter-number'>25</span>  <span class='chapter-title'>Genomic ranges and features</span>"
    ]
  },
  {
    "objectID": "ranges_and_signals.html#grangeslist",
    "href": "ranges_and_signals.html#grangeslist",
    "title": "\n25  Genomic ranges and features\n",
    "section": "\n25.3 GRangesList",
    "text": "25.3 GRangesList\nSome important genomic features, such as spliced transcripts that are are comprised of exons, are inherently compound structures. Such a feature makes much more sense when expressed as a compound object such as a GRangesList. If we thing of each transcript as a set of exons, each transcript would be summarized as a GRanges object. However, if we have multiple transcripts, we want to somehow keep them separate, with each transcript having its own exons. The GRangesList is then a list of GRanges objects that. Continuing with the transcripts thought, a GRangesList can contain all the transcripts and their exons; each transcript is an element in the list.\n\n\n\n\n\nFigure 25.2: The structure of a GRangesList, which is a list of GRanges objects. While the analogy is not perfect, a GRangesList behaves a bit like a list. Each element in the GRangesList is a Granges object. A common use case for a GRangesList is to store a list of transcripts, each of which have exons as the regions in the GRanges.\n\n\nWhenever genomic features consist of multiple ranges that are grouped by a parent feature, they can be represented as a GRangesList object. Consider the simple example of the two transcript GRangesList below created using the GRangesList constructor.\n\ngr1 &lt;- GRanges(\n    seqnames = \"chr1\", \n    ranges = IRanges(103, 106),\n    strand = \"+\", \n    score = 5L, GC = 0.45)\ngr2 &lt;- GRanges(\n    seqnames = c(\"chr1\", \"chr1\"),\n    ranges = IRanges(c(107, 113), width = 3),\n    strand = c(\"+\", \"-\"), \n    score = 3:4, GC = c(0.3, 0.5))\n\nThe gr1 and gr2 are each GRanges objects. We can combine them into a “named” GRangesList like so:\n\ngrl &lt;- GRangesList(\"txA\" = gr1, \"txB\" = gr2)\ngrl\n\nGRangesList object of length 2:\n$txA\nGRanges object with 1 range and 2 metadata columns:\n      seqnames    ranges strand |     score        GC\n         &lt;Rle&gt; &lt;IRanges&gt;  &lt;Rle&gt; | &lt;integer&gt; &lt;numeric&gt;\n  [1]     chr1   103-106      + |         5      0.45\n  -------\n  seqinfo: 1 sequence from an unspecified genome; no seqlengths\n\n$txB\nGRanges object with 2 ranges and 2 metadata columns:\n      seqnames    ranges strand |     score        GC\n         &lt;Rle&gt; &lt;IRanges&gt;  &lt;Rle&gt; | &lt;integer&gt; &lt;numeric&gt;\n  [1]     chr1   107-109      + |         3       0.3\n  [2]     chr1   113-115      - |         4       0.5\n  -------\n  seqinfo: 1 sequence from an unspecified genome; no seqlengths\n\n\nThe show method for a GRangesList object displays it as a named list of GRanges objects, where the names of this list are considered to be the names of the grouping feature. In the example above, the groups of individual exon ranges are represented as separate GRanges objects which are further organized into a list structure where each element name is a transcript name. Many other combinations of grouped and labeled GRanges objects are possible of course, but this example is a common arrangement.\nIn some cases, GRangesLists behave quite similarly to GRanges objects.\n\n25.3.1 Basic GRangesList accessors\nJust as with GRanges object, the components of the genomic coordinates within a GRangesList object can be extracted using simple accessor methods. Not surprisingly, the GRangesList objects have many of the same accessors as GRanges objects. The difference is that many of these methods return a list since the input is now essentially a list of GRanges objects. Here are a few examples:\n\nseqnames(grl)\n\nRleList of length 2\n$txA\nfactor-Rle of length 1 with 1 run\n  Lengths:    1\n  Values : chr1\nLevels(1): chr1\n\n$txB\nfactor-Rle of length 2 with 1 run\n  Lengths:    2\n  Values : chr1\nLevels(1): chr1\n\nranges(grl)\n\nIRangesList object of length 2:\n$txA\nIRanges object with 1 range and 0 metadata columns:\n          start       end     width\n      &lt;integer&gt; &lt;integer&gt; &lt;integer&gt;\n  [1]       103       106         4\n\n$txB\nIRanges object with 2 ranges and 0 metadata columns:\n          start       end     width\n      &lt;integer&gt; &lt;integer&gt; &lt;integer&gt;\n  [1]       107       109         3\n  [2]       113       115         3\n\nstrand(grl)\n\nRleList of length 2\n$txA\nfactor-Rle of length 1 with 1 run\n  Lengths: 1\n  Values : +\nLevels(3): + - *\n\n$txB\nfactor-Rle of length 2 with 2 runs\n  Lengths: 1 1\n  Values : + -\nLevels(3): + - *\n\n\nThe length and names methods will return the length or names of the list and the seqlengths method will return the set of sequence lengths.\n\nlength(grl)\n\n[1] 2\n\nnames(grl)\n\n[1] \"txA\" \"txB\"\n\nseqlengths(grl)\n\nchr1 \n  NA",
    "crumbs": [
      "Home",
      "Bioconductor",
      "<span class='chapter-number'>25</span>  <span class='chapter-title'>Genomic ranges and features</span>"
    ]
  },
  {
    "objectID": "ranges_and_signals.html#relationships-between-region-sets",
    "href": "ranges_and_signals.html#relationships-between-region-sets",
    "title": "\n25  Genomic ranges and features\n",
    "section": "\n25.4 Relationships between region sets",
    "text": "25.4 Relationships between region sets\nOne of the more powerful approaches to genomic data integration is to ask about the relationship between sets of genomic ranges. The key features of this process are to look at overlaps and distances to the nearest feature. These functionalities, combined with the operations like flank and resize, for instance, allow pretty useful analyses with relatively little code. In general, these operations are very fast, even on thousands to millions of regions.\n\n25.4.1 Overlaps\nThe findOverlaps method in the GenomicRanges package is a very useful function that allows users to identify overlaps between two sets of genomic ranges.\nHere’s how it works:\n\n\nInputs\n\nThe function requires two GRanges objects, referred to as query and subject.\n\n\n\nProcessing\n\nThe function then compares every range in the query object with every range in the subject object, looking for overlaps. An overlap is defined as any instance where the range in the query object intersects with a range in the subject object.\n\n\n\nOutput\n\nThe function returns a Hits (see ?Hits) object, which is a compact representation of the matrix of overlaps. Each entry in the Hits object corresponds to a pair of overlapping ranges, with the query index and the subject index.\n\n\n\nHere is an example of how you might use the findOverlaps function:\n\n# Create two GRanges objects\ngr1 &lt;- gr[1:4]\ngr2 &lt;- gr[3:8]\ngr1\n\nGRanges object with 4 ranges and 3 metadata columns:\n    seqnames    ranges strand |     score        GC        AT\n       &lt;Rle&gt; &lt;IRanges&gt;  &lt;Rle&gt; | &lt;integer&gt; &lt;numeric&gt; &lt;numeric&gt;\n  a     chr1   101-111      - |         1  1.000000  0.000000\n  b     chr2   102-112      + |         2  0.888889  0.111111\n  c     chr2   103-113      + |         3  0.777778  0.222222\n  d     chr2   104-114      * |         4  0.666667  0.333333\n  -------\n  seqinfo: 3 sequences from an unspecified genome; no seqlengths\n\ngr2\n\nGRanges object with 6 ranges and 3 metadata columns:\n    seqnames    ranges strand |     score        GC        AT\n       &lt;Rle&gt; &lt;IRanges&gt;  &lt;Rle&gt; | &lt;integer&gt; &lt;numeric&gt; &lt;numeric&gt;\n  c     chr2   103-113      + |         3  0.777778  0.222222\n  d     chr2   104-114      * |         4  0.666667  0.333333\n  e     chr1   105-115      * |         5  0.555556  0.444444\n  f     chr1   106-116      + |         6  0.444444  0.555556\n  g     chr3   107-117      + |         7  0.333333  0.666667\n  h     chr3   108-118      + |         8  0.222222  0.777778\n  -------\n  seqinfo: 3 sequences from an unspecified genome; no seqlengths\n\n\n\n# Find overlaps  \noverlaps &lt;- findOverlaps(query = gr1, subject = gr2)  \noverlaps\n\nHits object with 7 hits and 0 metadata columns:\n      queryHits subjectHits\n      &lt;integer&gt;   &lt;integer&gt;\n  [1]         1           3\n  [2]         2           1\n  [3]         2           2\n  [4]         3           1\n  [5]         3           2\n  [6]         4           1\n  [7]         4           2\n  -------\n  queryLength: 4 / subjectLength: 6\n\n\nIn the resulting overlaps object, each row corresponds to an overlapping pair of ranges, with the first column giving the index of the range in gr1 and the second column giving the index of the overlapping range in gr2.\nIf you are interested in only the queryHits or the subjectHits, there are accessors for those (ie., queryHits(overlaps)). To get the actual ranges that overlap, you can use the subjectHits or queryHits as an index into the original GRanges object.\nSpend some time looking at these results. Note how the strand comes into play when determining overlaps.\n\ngr1[queryHits(overlaps)]\n\nGRanges object with 7 ranges and 3 metadata columns:\n    seqnames    ranges strand |     score        GC        AT\n       &lt;Rle&gt; &lt;IRanges&gt;  &lt;Rle&gt; | &lt;integer&gt; &lt;numeric&gt; &lt;numeric&gt;\n  a     chr1   101-111      - |         1  1.000000  0.000000\n  b     chr2   102-112      + |         2  0.888889  0.111111\n  b     chr2   102-112      + |         2  0.888889  0.111111\n  c     chr2   103-113      + |         3  0.777778  0.222222\n  c     chr2   103-113      + |         3  0.777778  0.222222\n  d     chr2   104-114      * |         4  0.666667  0.333333\n  d     chr2   104-114      * |         4  0.666667  0.333333\n  -------\n  seqinfo: 3 sequences from an unspecified genome; no seqlengths\n\ngr2[subjectHits(overlaps)]\n\nGRanges object with 7 ranges and 3 metadata columns:\n    seqnames    ranges strand |     score        GC        AT\n       &lt;Rle&gt; &lt;IRanges&gt;  &lt;Rle&gt; | &lt;integer&gt; &lt;numeric&gt; &lt;numeric&gt;\n  e     chr1   105-115      * |         5  0.555556  0.444444\n  c     chr2   103-113      + |         3  0.777778  0.222222\n  d     chr2   104-114      * |         4  0.666667  0.333333\n  c     chr2   103-113      + |         3  0.777778  0.222222\n  d     chr2   104-114      * |         4  0.666667  0.333333\n  c     chr2   103-113      + |         3  0.777778  0.222222\n  d     chr2   104-114      * |         4  0.666667  0.333333\n  -------\n  seqinfo: 3 sequences from an unspecified genome; no seqlengths\n\n\nAs you might expect, the countOverlaps method counts the regions in the second GRanges that overlap with those that overlap with each element of the first.\n\ncountOverlaps(gr1, gr2)\n\na b c d \n1 2 2 2 \n\n\nThe subsetByOverlaps method simply subsets the query GRanges object to include only those that overlap the subject.\n\nsubsetByOverlaps(gr1, gr2)\n\nGRanges object with 4 ranges and 3 metadata columns:\n    seqnames    ranges strand |     score        GC        AT\n       &lt;Rle&gt; &lt;IRanges&gt;  &lt;Rle&gt; | &lt;integer&gt; &lt;numeric&gt; &lt;numeric&gt;\n  a     chr1   101-111      - |         1  1.000000  0.000000\n  b     chr2   102-112      + |         2  0.888889  0.111111\n  c     chr2   103-113      + |         3  0.777778  0.222222\n  d     chr2   104-114      * |         4  0.666667  0.333333\n  -------\n  seqinfo: 3 sequences from an unspecified genome; no seqlengths\n\n\nIn some cases, you may be interested in only one hit when doing overlaps. Note the select parameter. See the help for findOverlaps\n\nfindOverlaps(gr1, gr2, select=\"first\")\n\n[1] 3 1 1 1\n\nfindOverlaps(gr1, gr2, select=\"first\")\n\n[1] 3 1 1 1\n\n\nThe %over% logical operator allows us to do similar things to findOverlaps and subsetByOverlaps.\n\ngr2 %over% gr1\n\n[1]  TRUE  TRUE  TRUE FALSE FALSE FALSE\n\ngr1[gr1 %over% gr2]\n\nGRanges object with 4 ranges and 3 metadata columns:\n    seqnames    ranges strand |     score        GC        AT\n       &lt;Rle&gt; &lt;IRanges&gt;  &lt;Rle&gt; | &lt;integer&gt; &lt;numeric&gt; &lt;numeric&gt;\n  a     chr1   101-111      - |         1  1.000000  0.000000\n  b     chr2   102-112      + |         2  0.888889  0.111111\n  c     chr2   103-113      + |         3  0.777778  0.222222\n  d     chr2   104-114      * |         4  0.666667  0.333333\n  -------\n  seqinfo: 3 sequences from an unspecified genome; no seqlengths\n\n\n\n25.4.2 Nearest feature\nThere are a number of useful methods that find the nearest feature (region) in a second set for each element in the first set.\nWe can review our two GRanges toy objects:\n\ng\n\nGRanges object with 3 ranges and 3 metadata columns:\n    seqnames    ranges strand |     score        GC        AT\n       &lt;Rle&gt; &lt;IRanges&gt;  &lt;Rle&gt; | &lt;integer&gt; &lt;numeric&gt; &lt;numeric&gt;\n  a     chr1   101-111      - |         1  1.000000  0.000000\n  b     chr2   102-112      + |         2  0.888889  0.111111\n  c     chr2   103-113      + |         3  0.777778  0.222222\n  -------\n  seqinfo: 3 sequences from an unspecified genome; no seqlengths\n\ngr\n\nGRanges object with 10 ranges and 3 metadata columns:\n    seqnames    ranges strand |     score        GC        AT\n       &lt;Rle&gt; &lt;IRanges&gt;  &lt;Rle&gt; | &lt;integer&gt; &lt;numeric&gt; &lt;numeric&gt;\n  a     chr1   101-111      - |         1  1.000000  0.000000\n  b     chr2   102-112      + |         2  0.888889  0.111111\n  c     chr2   103-113      + |         3  0.777778  0.222222\n  d     chr2   104-114      * |         4  0.666667  0.333333\n  e     chr1   105-115      * |         5  0.555556  0.444444\n  f     chr1   106-116      + |         6  0.444444  0.555556\n  g     chr3   107-117      + |         7  0.333333  0.666667\n  h     chr3   108-118      + |         8  0.222222  0.777778\n  i     chr3   109-119      - |         9  0.111111  0.888889\n  j     chr3   110-120      - |        10  0.000000  1.000000\n  -------\n  seqinfo: 3 sequences from an unspecified genome; no seqlengths\n\n\n\nnearest: Performs conventional nearest neighbor finding. Returns an integer vector containing the index of the nearest neighbor range in subject for each range in x. If there is no nearest neighbor NA is returned. For details of the algorithm see the man page in the IRanges package (?nearest).\nprecede: For each range in x, precede returns the index of the range in subject that is directly preceded by the range in x. Overlapping ranges are excluded. NA is returned when there are no qualifying ranges in subject.\nfollow: The opposite of precede, follow returns the index of the range in subject that is directly followed by the range in x. Overlapping ranges are excluded. NA is returned when there are no qualifying ranges in subject.\n\nOrientation and strand for precede and follow: Orientation is 5’ to 3’, consistent with the direction of translation. Because positional numbering along a chromosome is from left to right and transcription takes place from 5’ to 3’, precede and follow can appear to have ‘opposite’ behavior on the + and - strand. Using positions 5 and 6 as an example, 5 precedes 6 on the + strand but follows 6 on the - strand.\nThe table below outlines the orientation when ranges on different strands are compared. In general, a feature on * is considered to belong to both strands. The single exception is when both x and subject are * in which case both are treated as +.\n       x  |  subject  |  orientation \n     -----+-----------+----------------\na)     +  |  +        |  ---&gt; \nb)     +  |  -        |  NA\nc)     +  |  *        |  ---&gt;\nd)     -  |  +        |  NA\ne)     -  |  -        |  &lt;---\nf)     -  |  *        |  &lt;---\ng)     *  |  +        |  ---&gt;\nh)     *  |  -        |  &lt;---\ni)     *  |  *        |  ---&gt;  (the only situation where * arbitrarily means +)\n\nres = nearest(g, gr)\nres\n\n[1] 5 4 4\n\n\nWhile nearest and friends give the index of the nearest feature, the distance to the nearest is sometimes also useful to have. The distanceToNearest method calculates the nearest feature as well as reporting the distance.\n\nres = distanceToNearest(g, gr)\nres\n\nHits object with 3 hits and 1 metadata column:\n      queryHits subjectHits |  distance\n      &lt;integer&gt;   &lt;integer&gt; | &lt;integer&gt;\n  [1]         1           5 |         0\n  [2]         2           4 |         0\n  [3]         3           4 |         0\n  -------\n  queryLength: 3 / subjectLength: 10",
    "crumbs": [
      "Home",
      "Bioconductor",
      "<span class='chapter-number'>25</span>  <span class='chapter-title'>Genomic ranges and features</span>"
    ]
  },
  {
    "objectID": "ranges_and_signals.html#gene-models",
    "href": "ranges_and_signals.html#gene-models",
    "title": "\n25  Genomic ranges and features\n",
    "section": "\n25.5 Gene models",
    "text": "25.5 Gene models\nThe TxDb package provides a convenient interface to gene models from a variety of sources. The TxDb.Hsapiens.UCSC.hg38.knownGene package provides access to the UCSC knownGene gene model for the hg19 build of the human genome.\n\n\n\n\n\nFigure 25.3: A graphical representation of range operations demonstrated on a gene model.\n\n\n\nlibrary(TxDb.Hsapiens.UCSC.hg38.knownGene)\ntxdb &lt;- TxDb.Hsapiens.UCSC.hg38.knownGene\n\nThe transcripts function returns a GRanges object with the transcripts for all genes in the database.\n\ntx &lt;- transcripts(txdb)\n\nThe exons function returns a GRanges object with the exons.\n\nex &lt;- exons(txdb)\n\nThe genes function returns a GRanges object with the genes.\n\ngn &lt;- genes(txdb)\n\n\n\n\n\nLawrence, Michael, Wolfgang Huber, Hervé Pagès, Patrick Aboyoun, Marc Carlson, Robert Gentleman, Martin T. Morgan, and Vincent J. Carey. 2013. “Software for Computing and Annotating Genomic Ranges.” PLoS Computational Biology 9 (8): e1003118. https://doi.org/10.1371/journal.pcbi.1003118.",
    "crumbs": [
      "Home",
      "Bioconductor",
      "<span class='chapter-number'>25</span>  <span class='chapter-title'>Genomic ranges and features</span>"
    ]
  },
  {
    "objectID": "ranges_exercises.html",
    "href": "ranges_exercises.html",
    "title": "\n26  Ranges Exercises\n",
    "section": "",
    "text": "26.1 Exercise 1\nIn the following exercises, we will use the GenomicRanges package to explore range operations. We will use the AnnotationHub package to load DNAse hypersensitivity data from the ENCODE project. In practice, the ENCODE project published datasets like these as bed files. AnnotationHub has packaged these into GRanges objects that we can load and use directly. However, if you have a bed file of your own (peak calls, enhancer regions, etc.), you can load them into GRanges objects using rtracklayer::import.\nIn this exercise, we will use DNAse hypersensitivity data to practice working with a GRanges object.\nShow the answerlibrary(AnnotationHub)\nah = AnnotationHub()\nquery(ah, \"goldenpath/hg19/encodeDCC/wgEncodeUwDnase/wgEncodeUwDnaseK562PkRep1.narrowPeak.gz\")\n# the thing above should have only one record, so we can \n# just grab it\ndnase = query(ah, \"goldenpath/hg19/encodeDCC/wgEncodeUwDnase/wgEncodeUwDnaseK562PkRep1.narrowPeak.gz\")[[1]]\nShow the answerdnase\nclass(dnase)\nShow the answermcols(dnase)\nShow the answerlibrary(GenomicFeatures)\ntable(seqnames(dnase))\nShow the answersummary(width(dnase))\nShow the answerseqlevels(dnase)\nseqlevelsStyle(dnase)\nseqlevelsStyle(dnase) = 'ensembl'\nseqlevelsStyle(dnase)\nseqlevels(dnase)\nShow the answersum(width(dnase))\nsum(seqlengths(dnase))\nsum(width(dnase))/sum(seqlengths(dnase))",
    "crumbs": [
      "Home",
      "Bioconductor",
      "<span class='chapter-number'>26</span>  <span class='chapter-title'>Ranges Exercises</span>"
    ]
  },
  {
    "objectID": "ranges_exercises.html#exercise-1",
    "href": "ranges_exercises.html#exercise-1",
    "title": "\n26  Ranges Exercises\n",
    "section": "",
    "text": "Use the AnnotationHub package to find the goldenpath/hg19/encodeDCC/wgEncodeUwDnase/wgEncodeUwDnaseK562PkRep1.narrowPeak.gz GRanges object. Load that into R as the variable dnase.\n\n\n\nWhat type of object is dnase?\n\n\n\nWhat metadata is stored in dnase?\n\n\n\nHow many peaks are on each chromosome?\n\n\n\nWhat are the mean, min, max, and median widths of the peaks?\n\n\n\nWhat are the sequences that were used in the analysis? Do the names have “chr” or not? Experiment with changing the seqlevelsStyle to adjust the sequence names.\n\n\n\nWhat is the total amount of “landscape” covered by the peaks? Assume that the peaks do not overlap. What portion of the genome does this represent?",
    "crumbs": [
      "Home",
      "Bioconductor",
      "<span class='chapter-number'>26</span>  <span class='chapter-title'>Ranges Exercises</span>"
    ]
  },
  {
    "objectID": "ranges_exercises.html#exercise-2",
    "href": "ranges_exercises.html#exercise-2",
    "title": "\n26  Ranges Exercises\n",
    "section": "\n26.2 Exercise 2",
    "text": "26.2 Exercise 2\nIn this exercise, we are going to load the second DNAse hypersensitivity replicate to investigate overlaps with the first replicate.\n\nUse the AnnotationHub to find the second replicate, goldenpath/hg19/encodeDCC/wgEncodeUwDnase/wgEncodeUwDnaseK562PkRep2.narrowPeak.gz. Load that as dnase2.\n\n\nShow the answerquery(ah, \"goldenpath/hg19/encodeDCC/wgEncodeUwDnase/wgEncodeUwDnaseK562PkRep2.narrowPeak.gz\")\n# the thing above should have only one record, so we can \n# just grab it\ndnase2 = query(ah, \"goldenpath/hg19/encodeDCC/wgEncodeUwDnase/wgEncodeUwDnaseK562PkRep2.narrowPeak.gz\")[[1]]\n\n\n\nHow many peaks are there in dnase and dnase2? Are there are similar number?\n\n\nShow the answerlength(dnase)\nlength(dnase2)\n\n\n\nWhat are the peak sizes for dnase2?\n\n\nShow the answersummary(width(dnase2))\n\n\n\nWhat proportion of the genome does dnase2 cover?\n\n\nShow the answersum(width(dnase))/sum(seqlengths(dnase))\n\n\n\nCount the number of peaks from dnase that overlap with dnase2.\n\n\nShow the answersum(dnase %over% dnase2)\n\n\n\nAssume that your peak-caller was “too specific” and that you want to expand your peaks by 50 bp on each end (so make them 100 bp larger). Use a combination of resize (and pay attention to the fix argument) and width to do this expansion to dnase and call the new GRanges object “dnase_wide”.\n\n\nShow the answerw = width(dnase)\ndnase_wide = resize(dnase, width=w+100, fix='center') #make a copy\nwidth(dnase_wide)",
    "crumbs": [
      "Home",
      "Bioconductor",
      "<span class='chapter-number'>26</span>  <span class='chapter-title'>Ranges Exercises</span>"
    ]
  },
  {
    "objectID": "ranges_exercises.html#exercise-3",
    "href": "ranges_exercises.html#exercise-3",
    "title": "\n26  Ranges Exercises\n",
    "section": "\n26.3 Exercise 3",
    "text": "26.3 Exercise 3\nIn this exercise, we are going to look at the overlap of DNAse sites relative to genes. To get started, install and load the TxDb.Hsapiens.UCSC.hg19.knownGene txdb object.\nBiocManager::install(\"TxDb.Hsapiens.UCSC.hg19.knownGene\")\nlibrary(\"TxDb.Hsapiens.UCSC.hg19.knownGene\")\nkg = TxDb.Hsapiens.UCSC.hg19.knownGene\n\nLoad the transcripts from the knownGene txdb into a variable. What is the class of this object?\n\n\nShow the answerlibrary(\"TxDb.Hsapiens.UCSC.hg19.knownGene\")\nkg = TxDb.Hsapiens.UCSC.hg19.knownGene\ngx = genes(kg)\nclass(gx)\nlength(gx)\n\n\n\nRead about the flank method for GRanges objects. How could you use that to get the “promoter” regions of the transcripts? Let’s assume that the promoter region is 2kb upstream of the gene.\n\n\nShow the answerflank(gx,2000)\n\n\n\nInstead of using flank, could you do the same thing with the TxDb object? (See ?promoters).\n\n\nShow the answerproms = promoters(kg)\n\n\n\nDo any of the regions in the promoters overlap with each other?\n\n\nShow the answersummary(countOverlaps(proms))\n\n\n\nTo find overlap of our DNAse sites with promoters, let’s collapse overlapping “promoters” to just keep the contiguous regions by using reduce.\n\n\nShow the answer# reduce takes all overlapping regions and collapses them\n# into a single region that spans all of the overlapping regions\nprom_regions = reduce(proms)\n\n# now we can check for overlaps\nsummary(countOverlaps(prom_regions))\n\n\n\nCount the number of DNAse sites that overlap with our promoter regions.\n\n\nShow the answersum(dnase %over% prom_regions)\n# if you notice no overlap, check the seqlevels\n# and seqlevelsStyle\nseqlevelsStyle(dnase) = \"UCSC\"\nsum(dnase %over% prom_regions)\nsum(dnase2 %over% prom_regions)\n\n\n\nIs this surprising? If we were to assume that the promoter and dnase regions are “independent” of each other, what number of overlaps would we expect?\n\n\nShow the answerprop_proms = sum(width(prom_regions))/sum(seqlengths(prom_regions))\nprop_dnase = sum(width(dnase))/sum(seqlengths(prom_regions))\n# Iff the dnase and promoter regions are \n# not related, then we would expect this number\n# of DNAse overlaps with promoters.\nprop_proms * prop_dnase * length(dnase)",
    "crumbs": [
      "Home",
      "Bioconductor",
      "<span class='chapter-number'>26</span>  <span class='chapter-title'>Ranges Exercises</span>"
    ]
  },
  {
    "objectID": "ranges_exercises.html#exercise-4",
    "href": "ranges_exercises.html#exercise-4",
    "title": "\n26  Ranges Exercises\n",
    "section": "\n26.4 Exercise 4",
    "text": "26.4 Exercise 4\nWe’ll be using data from histone modification ChIP-seq experiments in human cells to illustrate the concepts of genomic ranges and features. The data consists of genomic intervals representing regions of the genome where specific histone modifications are enriched. These intervals are typically identified using ChIP-seq, a technique that maps protein-DNA interactions across the genome.\nThe ChIP-seq data is stored in a BED file format, which is a tab-delimited text file format commonly used to represent genomic intervals. Each line in the BED file corresponds to a genomic interval and contains information about the chromosome, start and end positions, and strand orientation of the interval. Additional columns may include metadata such as the signal strength or significance of the interval.\nThe AnnotationHub package in Bioconductor provides access to a wide range of genomic datasets, including ChIP-seq data. We can use this package to retrieve the ChIP-seq data for histone modifications in human cells and convert it into a GenomicRanges object for further analysis.\nhttps://www.encodeproject.org/chip-seq/histone/\nLet’s start by loading the AnnotationHub package and retrieving the ChIP-seq data for histone modifications in human cells. You can read more about the AnnotationHub package and how to use it in the Bioconductor documentation.\n\nShow the answerlibrary(AnnotationHub)\nah &lt;- AnnotationHub()\n\n\nThere are multiple ways to search the AnnotationHub database. We’ve done that for you and here are the GRanges objects for each of four histone marks, and one histone mark replicate.\n\nShow the answerh3k4me1 &lt;- ah[['AH25832']]\nh3k4me3 &lt;- ah[['AH25833']]\nh3k9ac &lt;- ah[['AH25834']]\nh3k27me3 &lt;- ah[['AH25835']]\nh3k4me3_2 &lt;- ah[['AH27284']]\n\n\nEach of these variables now represents the peak calls after a chip-seq experiment pulling down the histone mark of interest. In the encode project these records were bed files. The bed files have been converted to GRanges objects to allow computation within R.\n\nShow the answer# Grab cpg islands as well\ncpg = query(ah, c('cpg','UCSC','hg19'))[[1]]\n\n\nLet’s say that we don’t know the behavior of the histone methylation marks with respect to CpG islands. We could ask the question, “What is the overlap of the histone peaks with CpG islands?”\n\nShow the answersum(h3k4me1 %over% cpg)\n\n\nWe might want to actually count the number of bases of overlap between the methyl mark and CpG islands.\n\nShow the answer# The intersection of two peak sets results in the \n# overlapping regions as a new set of regions\n# The width of each peak is the number of overlapping bases\n# And the sum of the widths is the total bases overlapping\nsum(width(intersect(h3k4me1, cpg)))\n\n\nBut some methyl marks are known to have very broad signals, meaning that there is a higher chance of overlapping CpG islands just because there are more methylated bases. We can adjust for this by “normalizing” for all possible bases covered by either set of peaks, using union. We might think of this as a sort of “enrichment score” of one set in another set.\n\nShow the answersum(width(union(h3k4me1, cpg)))\n# and now \"normalize\" \nsum(width(intersect(h3k4me1, cpg)))/sum(width(union(h3k4me1, cpg)))\n\n\nLet’s write a small function to calculate our little enrichment score.\n\nShow the answerrange_enrichment_score &lt;- function(r1, r2) {\n  i = sum(width(intersect(r1, r2)))\n  u = sum(width(union(r1,r2)))\n  return(i/u)\n}\n\n\nAnd give it a try:\n\nShow the answerrange_enrichment_score(h3k4me1, cpg)",
    "crumbs": [
      "Home",
      "Bioconductor",
      "<span class='chapter-number'>26</span>  <span class='chapter-title'>Ranges Exercises</span>"
    ]
  },
  {
    "objectID": "atac-seq/atac-seq.html",
    "href": "atac-seq/atac-seq.html",
    "title": "\n27  Genomic ranges and ATAC-Seq\n",
    "section": "",
    "text": "R / Bioconductor packages used",
    "crumbs": [
      "Home",
      "Bioconductor",
      "<span class='chapter-number'>27</span>  <span class='chapter-title'>Genomic ranges and ATAC-Seq</span>"
    ]
  },
  {
    "objectID": "atac-seq/atac-seq.html#r-bioconductor-packages-used",
    "href": "atac-seq/atac-seq.html#r-bioconductor-packages-used",
    "title": "\n27  Genomic ranges and ATAC-Seq\n",
    "section": "",
    "text": "Rsamtools\n\n\nGenomicRanges\n\n\nGenomicFeatures\n\nGenomicAlignments\nrtracklayer\nheatmaps",
    "crumbs": [
      "Home",
      "Bioconductor",
      "<span class='chapter-number'>27</span>  <span class='chapter-title'>Genomic ranges and ATAC-Seq</span>"
    ]
  },
  {
    "objectID": "atac-seq/atac-seq.html#background",
    "href": "atac-seq/atac-seq.html#background",
    "title": "\n27  Genomic ranges and ATAC-Seq\n",
    "section": "\n27.1 Background",
    "text": "27.1 Background\nChromatin accessibility assays measure the extent to which DNA is open and accessible. Such assays now use high throughput sequencing as a quantitative readout. DNAse assays, first using microarrays(Crawford, Davis, et al. 2006) and then DNAse-Seq (Crawford, Holt, et al. 2006), requires a larger amount of DNA and is labor-indensive and has been largely supplanted by ATAC-Seq (Buenrostro et al. 2013).\nThe Assay for Transposase Accessible Chromatin with high-throughput sequencing (ATAC-seq) method maps chromatin accessibility genome-wide. This method quantifies DNA accessibility with a hyperactive Tn5 transposase that cuts and inserts sequencing adapters into regions of chromatin that are accessible. High throughput sequencing of fragments produced by the process map to regions of increased accessibility, transcription factor binding sites, and nucleosome positioning. The method is both fast and sensitive and can be used as a replacement for DNAse and MNase.\nAn early review of chromatin accessibility assays (Tsompana and Buck 2014) compares the use cases, pros and cons, and expected signals from each of the most common approaches (Figure @ref(fig:chromatinAssays)).\n\n\n\n\nChromatin accessibility methods, compared. Representative DNA fragments generated by each assay are shown, with end locations within chromatin defined by colored arrows. Bar diagrams represent data signal obtained from each assay across the entire region. The footprint created by a transcription factor (TF) is shown for ATAC-seq and DNase-seq experiments.\n\n\n\nThe first manuscript describing ATAC-Seq protocol and findings outlined how ATAC-Seq data “line up” with other datatypes such as ChIP-seq and DNAse-seq (Figure @ref(fig:greenleaf)). They also highlight how fragment length correlates with specific genomic regions and characteristics (Buenrostro et al. 2013, fig. 3).\n\n\n\n\nMultimodal chromatin comparisons. From (Buenrostro et al. 2013), Figure 4. (a) CTCF footprints observed in ATAC-seq and DNase-seq data, at a specific locus on chr1. (b) Aggregate ATAC-seq footprint for CTCF (motif shown) generated over binding sites within the genome (c) CTCF predicted binding probability inferred from ATAC-seq data, position weight matrix (PWM) scores for the CTCF motif, and evolutionary conservation (PhyloP). Right-most column is the CTCF ChIP-seq data (ENCODE) for this GM12878 cell line, demonstrating high concordance with predicted binding probability.\n\n\n\nBuenrostro et al. provide a detailed protocol for performing ATAC-Seq and quality control of results (Buenrostro et al. 2015). Updated and modified protocols that improve on signal-to-noise and reduce input DNA requirements have been described.",
    "crumbs": [
      "Home",
      "Bioconductor",
      "<span class='chapter-number'>27</span>  <span class='chapter-title'>Genomic ranges and ATAC-Seq</span>"
    ]
  },
  {
    "objectID": "atac-seq/atac-seq.html#informatics-overview",
    "href": "atac-seq/atac-seq.html#informatics-overview",
    "title": "\n27  Genomic ranges and ATAC-Seq\n",
    "section": "\n27.2 Informatics overview",
    "text": "27.2 Informatics overview\nATAC-Seq protocols typically utilize paired-end sequencing protocols. The reads are aligned to the respective genome using bowtie2, BWA, or other short-read aligner. The result, after appropriate manipulation, often using samtools, results in a BAM file. Among other details, the BAM format includes columns for:\n\nknitr::include_graphics('imgs/bam_shot.png')\n\n\n\nA BAM file in text form. The output of samtools view is the text format of the BAM file (called SAM format). Bioconductor and many other tools use BAM files for input. Note that BAM files also often include an index .bai file that enables random access into the file; one can read just a genomic region without having to read the entire file.\n\n\n\n\nsequence name (chr1)\nstart position (integer)\na CIGAR string that describes the alignment in a compact form\nthe sequence to which the pair aligns\nthe position to which the pair aligns\na bit flag field that describes multiple characteristics of the alignment\nthe sequence and quality string of the read\nadditional tags that tend to be aligner-specific\n\nDuplicate fragments (those with the same start and end position of other reads) are marked and likely discarded. Reads that fail to align “properly” are also often excluded from analysis. It is worth noting that most software packages allow simple “marking” of such reads and that there is usually no need to create a special BAM file before proceeding with downstream work.\nAfter alignment and BAM processing, the workflow can switch to Bioconductor.",
    "crumbs": [
      "Home",
      "Bioconductor",
      "<span class='chapter-number'>27</span>  <span class='chapter-title'>Genomic ranges and ATAC-Seq</span>"
    ]
  },
  {
    "objectID": "atac-seq/atac-seq.html#working-with-sequencing-data-in-bioconductor",
    "href": "atac-seq/atac-seq.html#working-with-sequencing-data-in-bioconductor",
    "title": "\n27  Genomic ranges and ATAC-Seq\n",
    "section": "\n27.3 Working with sequencing data in Bioconductor",
    "text": "27.3 Working with sequencing data in Bioconductor\nThe Bioconductor project includes several infrastructure packages for dealing with ranges (sequence name, start, end, +/- strand) on sequences (Lawrence et al. 2013) as well as capabilities with working with Fastq files directly (Morgan et al. 2016).\n\nCommonly used Bioconductor and their high-level use cases.\n\nPackage\nUse cases\n\n\n\nRsamtools\nlow level access to FASTQ, VCF, SAM, BAM, BCF formats\n\n\nGenomicRanges\nContainer and methods for handling genomic reagions\n\n\nGenomicFeatures\nWork with transcript databases, gff, gtf and BED formats\n\n\nGenomicAlignments\nReader for BAM format\n\n\nrtracklayer\nimport and export multiple UCSC file formats including BigWig and Bed\n\n\n\nAs noted in the previous section, the output of an ATAC-Seq experiment is a BAM file. As paired-end sequencing is a commonly-applied approach for ATAC-Seq, the readGAlignmentPairs function is the appropriate method to use.",
    "crumbs": [
      "Home",
      "Bioconductor",
      "<span class='chapter-number'>27</span>  <span class='chapter-title'>Genomic ranges and ATAC-Seq</span>"
    ]
  },
  {
    "objectID": "atac-seq/atac-seq.html#coverage",
    "href": "atac-seq/atac-seq.html#coverage",
    "title": "\n27  Genomic ranges and ATAC-Seq\n",
    "section": "\n28.1 Coverage",
    "text": "28.1 Coverage\nThe coverage method for genomic ranges calculates, for each base, the number of overlapping features. In the case of a BAM file from ATAC-Seq converted to a GAlignmentPairs object, the coverage gives us an idea of the extent to which reads pile up to form peaks.\n\ncvg &lt;- coverage(greenleaf)\nclass(cvg)\n\n[1] \"SimpleRleList\"\nattr(,\"package\")\n[1] \"IRanges\"\n\n\nThe coverage is returned as a SimpleRleList object. Using names can get us the names of the elements of the list.\n\nnames(cvg)\n\n [1] \"chr1\"  \"chr2\"  \"chr3\"  \"chr4\"  \"chr5\"  \"chr6\"  \"chr7\"  \"chr8\"  \"chr9\" \n[10] \"chr10\" \"chr11\" \"chr12\" \"chr13\" \"chr14\" \"chr15\" \"chr16\" \"chr17\" \"chr18\"\n[19] \"chr19\" \"chr20\" \"chr21\" \"chr22\" \"chrX\"  \"chrY\"  \"chrM\" \n\n\nThere is a name for each chromosome. Looking at the chr21 entry:\n\ncvg$chr21\n\ninteger-Rle of length 48129895 with 397462 runs\n  Lengths: 9411376      50      11      50 ...      36      14      28   10806\n  Values :       0       2       0       2 ...       1       2       1       0\n\n\nwe see that each chromosome is represented as an Rle, short for run-length-encoding. Simply put, since along the chromosome there are many repeated values, we can recode the long vector as a set of (length: value) pairs. For example, if the first 9,410,000 base pairs have 0 coverage, we encode that as (9,410,000: 0). Doing that across the chromosome can very significantly reduce the memory use for genomic coverage.\nThe following little function, plotCvgHistByChrom can plot a histogram of the coverage for a chromosome.\n\nplotCvgHistByChrom &lt;- function(cvg, chromosome) {\n    library(ggplot2)\n    cvgcounts &lt;- as.data.frame(table(cvg[[chromosome]]))\n    cvgcounts[, 1] &lt;- as.numeric(as.character(cvgcounts[, 1]))\n    colnames(cvgcounts) &lt;- c(\"Coverage\", \"Count\")\n    ggplot(cvgcounts, aes(x = Coverage, y = Count)) +\n        ggtitle(paste(\"Chromosome\", chromosome)) +\n        geom_point(alpha = 0.5) +\n        geom_smooth(span = 0.2) +\n        scale_y_log10() +\n        theme_bw()\n}\nfor (i in c(\"chr21\", \"chr22\")) {\n    print(plotCvgHistByChrom(cvg, i))\n}",
    "crumbs": [
      "Home",
      "Bioconductor",
      "<span class='chapter-number'>27</span>  <span class='chapter-title'>Genomic ranges and ATAC-Seq</span>"
    ]
  },
  {
    "objectID": "atac-seq/atac-seq.html#fragment-lengths",
    "href": "atac-seq/atac-seq.html#fragment-lengths",
    "title": "\n27  Genomic ranges and ATAC-Seq\n",
    "section": "\n28.2 Fragment Lengths",
    "text": "28.2 Fragment Lengths\nThe first ATAC-Seq manuscript (Buenrostro et al. 2013) highlighted the relationship between fragment length and nucleosomes (see Figure @ref{fig:flgreenleaf}).\n\nknitr::include_graphics(\"https://cdn.ncbi.nlm.nih.gov/pmc/blobs/0ad9/3959825/fde39a9fb288/nihms554473f2.jpg\")\n\n\n\nRelationship between fragment length and nucleosome number.\n\n\n\nRemember that we loaded the example BAM file with insert sizes (isize). We can use that “column” to examine the fragment lengths (another name for insert size). Also, note that the insert size for the first read and the second are the same (absolute value). Here, we will use first.\n\nGenomicAlignments::first(greenleaf)\nmcols(GenomicAlignments::first(greenleaf))\nclass(mcols(GenomicAlignments::first(greenleaf)))\nhead(mcols(GenomicAlignments::first(greenleaf))$isize)\nfraglengths &lt;- abs(mcols(GenomicAlignments::first(greenleaf))$isize)\n\nWe can plot the fragment length density (histogram) using the density function.\n\nplot(density(fraglengths, bw = 0.05), xlim = c(0, 1000))\n\n\n\nFragment length histogram.\n\n\n\nAnd for fun, the ggplot2 version:\n\nlibrary(dplyr)\nlibrary(ggplot2)\nfragLenPlot &lt;- table(fraglengths) %&gt;%\n    data.frame() |&gt;\n    rename(\n        InsertSize = fraglengths,\n        Count = Freq\n    ) |&gt;\n    mutate(\n        InsertSize = as.numeric(as.vector(InsertSize)),\n        Count = as.numeric(as.vector(Count))\n    ) |&gt;\n    ggplot(aes(x = InsertSize, y = Count)) +\n    geom_line()\nprint(fragLenPlot + theme_bw() + lims(x = c(-1, 500)))\n\n\n\n\n\n\n\nKnowing that the nucleosome-free regions will have insert sizes shorter than one nucleosome, we can isolate the read pairs that have that characteristic.\n\ngl_nf &lt;- greenleaf[mcols(GenomicAlignments::first(greenleaf))$isize &lt; 147]\n\nAnd the mononucleosome reads will be between 147 and 294 base pairs for insert size/fragment length.\n\ngl_mn &lt;- greenleaf[mcols(GenomicAlignments::first(greenleaf))$isize &gt; 147 &\n    mcols(GenomicAlignments::first(greenleaf))$isize &lt; 294]\n\nFinally, we expect nucleosome-free reads to be enriched near the TSS while mononucleosome reads should not be. We will use the heatmaps package to take a look at these two sets of reads with respect to the tss of the human genome.\n\nlibrary(TxDb.Hsapiens.UCSC.hg19.knownGene)\nproms &lt;- promoters(TxDb.Hsapiens.UCSC.hg19.knownGene, 250, 250)\nseqs &lt;- c(\"chr21\", \"chr22\")\nseqlevels(proms, pruning.mode = \"coarse\") &lt;- seqs # only chromosome 21 and 22\n\nTake a look at the heatmaps package vignette to learn more about the heatmaps package capabilities.\n\nlibrary(heatmaps)\ngl_nf_hm &lt;- CoverageHeatmap(proms, coverage(gl_nf), coords = c(-250, 250))\nlabel(gl_nf_hm) &lt;- \"NucFree\"\nscale(gl_nf_hm) &lt;- c(0, 10)\n\n\ngl_mn_hm &lt;- CoverageHeatmap(proms, coverage(gl_mn), coords = c(-250, 250))\nlabel(gl_mn_hm) &lt;- \"MonoNuc\"\nscale(gl_mn_hm) &lt;- c(0, 10)\nplotHeatmapMeta(list(gl_nf_hm, gl_mn_hm))\n\n\n\nEnrichment of nucleosome-free reads and depletion of mononucleosome reads around the TSS.\n\n\n\n\nplotHeatmapList(list(gl_mn_hm, gl_nf_hm))\n\n\n\nComparison of signals at TSS. Mononucleosome data on the left, nucleosome-free on the right. Both plots are scaled to the same range.",
    "crumbs": [
      "Home",
      "Bioconductor",
      "<span class='chapter-number'>27</span>  <span class='chapter-title'>Genomic ranges and ATAC-Seq</span>"
    ]
  },
  {
    "objectID": "atac-seq/atac-seq.html#viewing-data-in-igv",
    "href": "atac-seq/atac-seq.html#viewing-data-in-igv",
    "title": "\n27  Genomic ranges and ATAC-Seq\n",
    "section": "\n28.3 Viewing data in IGV",
    "text": "28.3 Viewing data in IGV\nInstall IGV from here.\nWe export the greenleaf data as a BigWig file.\n\nlibrary(rtracklayer)\nexport.bw(coverage(greenleaf), \"greenleaf.bw\")\n\n\n\nExercise: In IGV, choose hg19. Then, load the greenleaf.bw file and explore chromosomes 21 and 22.\n\nExercise: Export the nucleosome-free portion of the data as a BigWig file and examine that in IGV. Where do you expect to see the strongest signals?",
    "crumbs": [
      "Home",
      "Bioconductor",
      "<span class='chapter-number'>27</span>  <span class='chapter-title'>Genomic ranges and ATAC-Seq</span>"
    ]
  },
  {
    "objectID": "atac-seq/atac-seq.html#additional-work",
    "href": "atac-seq/atac-seq.html#additional-work",
    "title": "\n27  Genomic ranges and ATAC-Seq\n",
    "section": "\n28.4 Additional work",
    "text": "28.4 Additional work\nFor those working extensively on ATAC-Seq, there is a great workflow/tutorial available from Thomas Carrol:\n\nhttps://rockefelleruniversity.github.io/RU_ATAC_Workshop.html\n\nFeel free to work through it. In addition to the work above, there is also the ATACseqQC package vignette that offers more than just QC. At least a couple more packages are available in Bioconductor.",
    "crumbs": [
      "Home",
      "Bioconductor",
      "<span class='chapter-number'>27</span>  <span class='chapter-title'>Genomic ranges and ATAC-Seq</span>"
    ]
  },
  {
    "objectID": "atac-seq/atac-seq.html#macs2",
    "href": "atac-seq/atac-seq.html#macs2",
    "title": "\n27  Genomic ranges and ATAC-Seq\n",
    "section": "MACS2",
    "text": "MACS2\nThe MACS2 package is a commonly-used package for calling peaks. Installation and other details are available1.\n1 https://github.com/taoliu/MACSpip install macs2\n\n\n\n\nBuenrostro, Jason D, Paul G Giresi, Lisa C Zaba, Howard Y Chang, and William J Greenleaf. 2013. “Transposition of Native Chromatin for Fast and Sensitive Epigenomic Profiling of Open Chromatin, DNA-binding Proteins and Nucleosome Position.” Nature Methods 10 (12): 1213–18. https://doi.org/10.1038/nmeth.2688.\n\n\nBuenrostro, Jason D, Beijing Wu, Howard Y Chang, and William J Greenleaf. 2015. “ATAC-seq: A Method for Assaying Chromatin Accessibility Genome-Wide.” Current Protocols in Molecular Biology / Edited by Frederick M. Ausubel ... [Et Al.] 109 (January): 21.29.1–9. https://doi.org/10.1002/0471142727.mb2129s109.\n\n\nCrawford, Gregory E, Sean Davis, Peter C Scacheri, Gabriel Renaud, Mohamad J Halawi, Michael R Erdos, Roland Green, Paul S Meltzer, Tyra G Wolfsberg, and Francis S Collins. 2006. “DNase-chip: A High-Resolution Method to Identify DNase I Hypersensitive Sites Using Tiled Microarrays.” Nature Methods 3 (7): 503–9. http://www.ncbi.nlm.nih.gov/pubmed/16791207?dopt=AbstractPlus.\n\n\nCrawford, Gregory E, Ingeborg E Holt, James Whittle, Bryn D Webb, Denise Tai, Sean Davis, Elliott H Margulies, et al. 2006. “Genome-Wide Mapping of DNase Hypersensitive Sites Using Massively Parallel Signature Sequencing (MPSS).” Genome Research 16 (1): 123–31. http://www.ncbi.nlm.nih.gov/pubmed/16344561?dopt=AbstractPlus.\n\n\nLawrence, Michael, Wolfgang Huber, Hervé Pagès, Patrick Aboyoun, Marc Carlson, Robert Gentleman, Martin T Morgan, and Vincent J Carey. 2013. “Software for Computing and Annotating Genomic Ranges.” PLoS Computational Biology 9 (8): e1003118. https://doi.org/10.1371/journal.pcbi.1003118.\n\n\nMorgan, Martin, Herve Pages, V Obenchain, and N Hayden. 2016. “Rsamtools: Binary Alignment (BAM), FASTA, Variant Call (BCF), and Tabix File Import.” R Package Version 1 (0): 677–89.\n\n\nTsompana, Maria, and Michael J Buck. 2014. “Chromatin Accessibility: A Window into the Genome.” Epigenetics & Chromatin 7 (1): 33. https://doi.org/10.1186/1756-8935-7-33.",
    "crumbs": [
      "Home",
      "Bioconductor",
      "<span class='chapter-number'>27</span>  <span class='chapter-title'>Genomic ranges and ATAC-Seq</span>"
    ]
  },
  {
    "objectID": "single_cell/setup.html",
    "href": "single_cell/setup.html",
    "title": "\n28  Single Cell RNA-seq Analysis\n",
    "section": "",
    "text": "28.1 Introduction\nSingle-cell RNA sequencing (scRNA-seq) is a powerful technique that allows us to measure gene expression in individual cells rather than bulk tissue samples. This capability has revolutionized our understanding of cellular heterogeneity, developmental processes, and disease mechanisms. Unlike traditional bulk RNA-seq, which provides an average expression profile across all cells in a sample, scRNA-seq reveals the unique molecular signatures of individual cells, enabling identification of distinct cell types, states, and trajectories.\nThis tutorial will walk through a typical scRNA-seq analysis workflow using R and the Bioconductor ecosystem. We’ll cover data loading, quality control, normalization, dimensionality reduction, clustering, and visualization - the fundamental steps that transform raw sequencing data into biological insights.\nThe runTSNE() function applies t-SNE to the PCA results, creating a 2D representation of the data. The dimred=\"PCA\" argument specifies that we want to use the PCA coordinates as input for t-SNE.\nThe clusterCells() function uses graph-based clustering to identify groups of similar cells. Graph-based clustering was popularized by the Seurat package. It builds a nearest-neighbor graph and then finds communities within this graph. The resulting clusters often correspond to distinct cell types or states. The use.dimred = \"TSNE\" argument specifies that we want to use the t-SNE coordinates for clustering. This is an important choide because t-SNE captures local structure and can reveal distinct cell populations that may not be apparent in the original high-dimensional space. There are other clustering methods available, such as k-means or hierarchical clustering, but graph-based clustering is particularly effective for single-cell data due to its ability to handle complex, non-linear relationships.",
    "crumbs": [
      "Home",
      "Bioconductor",
      "<span class='chapter-number'>28</span>  <span class='chapter-title'>Single Cell RNA-seq Analysis</span>"
    ]
  },
  {
    "objectID": "single_cell/setup.html#setup-and-data-loading",
    "href": "single_cell/setup.html#setup-and-data-loading",
    "title": "\n28  Single Cell RNA-seq Analysis\n",
    "section": "\n28.2 Setup and Data Loading",
    "text": "28.2 Setup and Data Loading\n\n28.2.1 Loading Essential Libraries\n\n# Load essential libraries for single-cell RNA-seq analysis\nlibrary(scRNAseq)\nlibrary(scran)\nlibrary(scater)\n\nWe start by loading three essential R packages for single-cell analysis:\n\n\nscRNAseq: Provides access to curated single-cell RNA-seq datasets, making it easy to practice analysis techniques on real data. At the time of writing, it includes more than 50 datasets covering various tissues, organisms, and experimental conditions. See Risso and Cole (2024)\n\n\nscran: Contains methods for normalization, feature selection, and clustering specifically designed for single-cell data. See Lun, McCarthy, and Marioni (2016).\n\nscater: Offers tools for quality control, visualization, and exploratory data analysis. See McCarthy et al. (2017).\n\nSingle-cell data has unique characteristics (sparsity, high dimensionality, technical noise) that require specialized tools. These packages form the backbone of the Bioconductor single-cell analysis ecosystem and provide methods that account for the specific challenges of single-cell data.\n\n28.2.2 Exploring Available Datasets\nWe are going to use the scRNAseq package to access a curated collection of single-cell datasets. This package provides a convenient way to load and explore publicly available datasets that have been processed and annotated.\n\ndatasets &lt;- scRNAseq::surveyDatasets()\n\nThe surveyDatasets() function queries the scRNAseq package database to show all available datasets. This creates a simple table of publicly available single-cell datasets that have been processed and made easily accessible.\nHaving access to well-curated datasets is useful for learning and benchmarking. These datasets come from published studies and represent different tissues, conditions, and experimental designs, allowing us to practice on real biological data without having to process raw sequencing files.\n\n\nDataFrame with 10 rows and 15 columns\n                     name     version        path                 object\n              &lt;character&gt; &lt;character&gt; &lt;character&gt;            &lt;character&gt;\n1       aztekin-tail-2019  2023-12-14          NA single_cell_experiment\n2  splicing-demonstrati..  2023-12-20          NA single_cell_experiment\n3      marques-brain-2016  2023-12-19          NA single_cell_experiment\n4   grun-bone_marrow-2016  2023-12-14          NA single_cell_experiment\n5         giladi-hsc-2018  2023-12-21      crispr single_cell_experiment\n6         giladi-hsc-2018  2023-12-21         rna single_cell_experiment\n7     macosko-retina-2015  2023-12-19          NA single_cell_experiment\n8        messmer-esc-2019  2023-12-19          NA single_cell_experiment\n9  ernst-spermatogenesi..  2023-12-21  cellranger single_cell_experiment\n10 ernst-spermatogenesi..  2023-12-21  emptydrops single_cell_experiment\n                    title            description taxonomy_id   genome      rows\n              &lt;character&gt;            &lt;character&gt;      &lt;List&gt;   &lt;List&gt; &lt;integer&gt;\n1  Identification of a .. Identification of a ..        8355 Xenla9.1     31535\n2  [reprocessed, subset.. [reprocessed, subset..       10090   GRCm38     54448\n3  Oligodendrocyte hete.. Oligodendrocyte hete..       10090   GRCm38     23556\n4  De Novo Prediction o.. De Novo Prediction o..       10090   GRCm38     23536\n5  Single-cell characte.. Single-cell characte..       10090  MGSCv37        30\n6  Single-cell characte.. Single-cell characte..       10090  MGSCv37     27389\n7  Highly Parallel Geno.. Highly Parallel Geno..       10090   GRCm38     24658\n8  Transcriptional Hete.. Transcriptional Hete..        9606   GRCh38     58302\n9  Staged developmental.. Staged developmental..       10090   GRCm38     33226\n10 Staged developmental.. Staged developmental..       10090   GRCm38     32105\n     columns            assays                               column_annotations\n   &lt;integer&gt;            &lt;List&gt;                                           &lt;List&gt;\n1      13199            counts sample,DevelopmentalStage,DaysPostAmputation,...\n2       2325 spliced,unspliced                                         celltype\n3       5069            counts           source_name,age,inferred cell type,...\n4       1915            counts                                  sample,protocol\n5      24070            counts                              amplification.batch\n6      81408            counts               Amp_batch_ID,Seq_batch_ID,tier,...\n7      49300            counts                                          cluster\n8       1344            counts    Source Name,phenotype,single cell quality,...\n9      53510            counts                       Sample,Barcode,Library,...\n10     68937            counts                       Sample,Barcode,Library,...\n   reduced_dimensions alternative_experiments\n               &lt;List&gt;                  &lt;List&gt;\n1                UMAP                        \n2                                            \n3                                            \n4                                            \n5                                            \n6                                            \n7                                            \n8                                        ERCC\n9                                            \n10                                           \n                                                                    sources\n                                                       &lt;SplitDataFrameList&gt;\n1                            ArrayExpress:E-MTAB-7716:NA,PubMed:31097661:NA\n2                      GEO:GSE109033:NA,GEO:GSM2928341:NA,SRA:SRR6459157:NA\n3                                        GEO:GSE75330:NA,PubMed:27284195:NA\n4                                        GEO:GSE76983:NA,PubMed:27345837:NA\n5                                        PubMed:29915358:NA,GEO:GSE11349:NA\n6                                        PubMed:29915358:NA,GEO:GSE92575:NA\n7  GEO:GSE63472:NA,PubMed:26000488:NA,URL:http://mccarrolllab...:2024-02-23\n8                            PubMed:30673604:NA,ArrayExpress:E-MTAB-6819:NA\n9                            ArrayExpress:E-MTAB-6946:NA,PubMed:30890697:NA\n10                           ArrayExpress:E-MTAB-6946:NA,PubMed:30890697:NA\n\n\nEach row represents a different study. The ‘columns’ field shows the number of cells, while ‘rows’ shows the number of genes. This helps you choose datasets appropriate for your computational resources and analysis goals. This little lightweight database is not the richest way to explore datasets, but it is a good starting point. In this tutorial, we will use the ZeiselBrainData dataset, which contains single-cell RNA-seq data from mouse brain tissue (Zeisel et al. (2015)).\n\n\n\n\n\n\nAbstract: Zeisel Brain Dataset\n\n\n\nThe mammalian cerebral cortex supports cognitive functions such as sensorimotor integration, memory, and social behaviors. Normal brain function relies on a diverse set of differentiated cell types, including neurons, glia, and vasculature. Here, we have used large-scale single-cell RNA sequencing (RNA-seq) to classify cells in the mouse somatosensory cortex and hippocampal CA1 region. We found 47 molecularly distinct subclasses, comprising all known major cell types in the cortex. We identified numerous marker genes, which allowed alignment with known cell types, morphology, and location. We found a layer I interneuron expressing Pax6 and a distinct postmitotic oligodendrocyte subclass marked by Itpr2. Across the diversity of cortical cell types, transcription factors formed a complex, layered regulatory code, suggesting a mechanism for the maintenance of adult cell type identity.\n\n\n\nsce &lt;- scRNAseq::ZeiselBrainData()\n\nWhat is the sce object? It is a SingleCellExperiment object, which is a data structure specifically designed for single-cell RNA-seq data. As with other Bioconductor data structures, it is built on top of the SummarizedExperiment class (see Figure 28.1), which allows us to store not only the count matrix but also metadata about cells and features (genes).\n\n\n\n\n\nFigure 28.1: The SingleCellExperiment object.\n\n\nPrinting the sce object gives us a summary of the dataset:\n\nsce\n\nclass: SingleCellExperiment \ndim: 20006 3005 \nmetadata(0):\nassays(1): counts\nrownames(20006): Tspan12 Tshz1 ... mt-Rnr1 mt-Nd4l\nrowData names(1): featureType\ncolnames(3005): 1772071015_C02 1772071017_G12 ... 1772066098_A12\n  1772058148_F03\ncolData names(9): tissue group # ... level1class level2class\nreducedDimNames(0):\nmainExpName: gene\naltExpNames(2): repeat ERCC\n\n\nThe SingleCellExperiment object contains:\n\n\nAssays: The main data matrix (counts) and any additional matrices (e.g., normalized counts).\n\nRow metadata: Information about genes (e.g., gene names, IDs).\n\nColumn metadata: Information about cells (e.g., cell barcodes, tissue type, experimental conditions).\n\nIt can also contain additional information such as dimensionality reductions (e.g., PCA, t-SNE), clustering results, and quality control metrics.\nIn fact, we are going to add some of these additional information in the next steps. First, we add a new assay with the log-normalized counts, which is a common preprocessing step in single-cell RNA-seq analysis.\n\nsce &lt;- logNormCounts(sce)\n\nThe logNormCounts() function performs log-normalization of the count data. Raw count data from scRNA-seq experiments contains technical variation due to differences in sequencing depth between cells. Some cells might have been sequenced more deeply than others, leading to higher total counts that don’t reflect true biological differences. Log-normalization addresses this by:\n\n\nSize factor normalization: Scaling each cell’s counts by a size factor that accounts for differences in sequencing dept. There are several methods to calculate size factors, but the most common is the median of ratios method, which divides each cell’s counts by the median count for each gene across all cells. This helps to adjust for differences in library size.\n\nLog transformation: Taking log(count + 1) to stabilize variance and make the data more normally distributed\n\nFor each gene g in cell i, the log-normalized expression becomes:\n\\[log_2(count_{g_i} × {size factor}_{i} + 1)\\]\nThis log transformation is useful because the next steps in the downstream analyses (like clustering and dimensionality reduction) assume that the data is approximately normally distributed (or at least bell-shaped). Log-normalization helps achieve this by reducing the impact of outliers and making the data more suitable for PCA and visualization.",
    "crumbs": [
      "Home",
      "Bioconductor",
      "<span class='chapter-number'>28</span>  <span class='chapter-title'>Single Cell RNA-seq Analysis</span>"
    ]
  },
  {
    "objectID": "single_cell/setup.html#dimensionality-reduction",
    "href": "single_cell/setup.html#dimensionality-reduction",
    "title": "\n28  Single Cell RNA-seq Analysis\n",
    "section": "\n28.3 Dimensionality reduction",
    "text": "28.3 Dimensionality reduction\nSingle-cell datasets are extremely high-dimensional - typically containing 15,000-30,000 genes per cell. This creates several challenges: computational complexity, noise dominance, and visualization difficulties. Dimensionality reduction techniques help us identify the most informative patterns in the data while reducing noise and computational burden.\n\n28.3.1 Feature Selection: Identifying Highly Variable Genes\nBefore applying dimensionality reduction, we need to identify the most informative genes. Highly variable genes (HVGs) are those that show significant variation across cells, indicating they may be biologically relevant. These genes are often more informative for downstream analyses like clustering and visualization.\nNot all genes are equally informative for distinguishing cell types. Genes that show little variation across cells (like housekeeping genes) don’t help us identify distinct cell populations. Highly variable genes are more likely to:\n\nReflect biological differences between cell types\nCapture developmental or activation states\nRepresent responses to environmental conditions\n\n\nlibrary(scran)\ntop_var_genes &lt;- getTopHVGs(sce, n=2000)\n\nThe getTopHVGs() function identifies the top 2000 highly variable genes based on their variance across cells. Choosing a subset of highly variable genes reduces the dimensionality of the dataset while retaining the most informative features for downstream analyses.\nThe algorithm models the relationship between gene expression mean and variance, then identifies genes that show more variation than expected based on their expression level. This accounts for the fact that highly expressed genes naturally show more variance.\n\n# Visualizing the highly variable genes\nlibrary(scran)\ndec &lt;- modelGeneVar(sce)\n\n# Visualizing the fit:\nfit.mv &lt;- metadata(dec)\nplot(fit.mv$mean, fit.mv$var, xlab=\"Mean of log-expression\",\n    ylab=\"Variance of log-expression\", ylim=c(0, 4))\ncurve(fit.mv$trend(x), col=\"dodgerblue\", add=TRUE, lwd=2)\npoints(\n    fit.mv$mean[top_var_genes], fit.mv$var[top_var_genes], \n    col=\"red\", pch=16, cex=0.5\n)\n\n\n\n\n\n\nFigure 28.2: A plot of the mean vs variance of log-expression for all the genes in the dataset, with the highly variable genes highlighted in red. Here, the x-axis represents the mean log-expression of each gene, and the y-axis represents the variance of log-expression. The blue curve represents the trend of variance as a function of mean expression, while the red points represent the highly variable genes.\n\n\n\n\nFigure 28.2 shows the mean vs. variance of log-expression for all genes in the dataset, with highly variable genes highlighted in red. The blue curve represents the trend of variance as a function of mean expression, while the red points represent the highly variable genes.\nWe next proceed to calculating PCA using the subset of highly variable genes. PCA is a linear dimensionality reduction technique that finds the directions of maximum variance in the data. It’s computationally efficient and provides a good foundation for further analysis. The first few principal components capture the most important patterns of variation in the dataset.\n\n\n\n\n\n\nWhy PCA first?\n\n\n\nPCA is a widely linear dimensionality reduction technique that:\n\nReduces dimensionality while preserving variance\nHelps visualize high-dimensional data in 2D or 3D\nServes as a preprocessing step for other methods like t-SNE or UMAP\nIs computationally efficient and easy to interpret\n\n\n\n\n# Perform PCA on the highly variable genes\nset.seed(100) # See below.\nsce &lt;- fixedPCA(sce, subset.row=top_var_genes)\n\nreducedDimNames(sce)\n\n[1] \"PCA\"\n\n\nNote that we assign the output of fixedPCA() to the sce object. This function performs PCA on the highly variable genes we identified earlier, reducing the dimensionality of the dataset while retaining the most informative features. Because dimensionality reduction is a common step in single-cell RNA-seq analysis, we can store the PCA results in the reducedDims slot of the SingleCellExperiment object. This allows us to easily access and visualize the PCA coordinates later.\n\n28.3.2 Non-linear Dimensionality Reduction with t-SNE\nAfter PCA, we can apply non-linear dimensionality reduction techniques like t-SNE (t-distributed Stochastic Neighbor Embedding) to visualize the data in 2D or 3D. t-SNE is particularly useful for revealing local structure and separating distinct cell populations.",
    "crumbs": [
      "Home",
      "Bioconductor",
      "<span class='chapter-number'>28</span>  <span class='chapter-title'>Single Cell RNA-seq Analysis</span>"
    ]
  },
  {
    "objectID": "single_cell/setup.html#why-t-sne-after-pca",
    "href": "single_cell/setup.html#why-t-sne-after-pca",
    "title": "\n28  Single Cell RNA-seq Analysis\n",
    "section": "\n28.4 Why t-SNE after PCA",
    "text": "28.4 Why t-SNE after PCA\nThis is a two-step approach: 1. PCA first: Reduces noise and computational burden while preserving global structure 2. t-SNE second: Reveals local neighborhood structure and separates distinct cell populations",
    "crumbs": [
      "Home",
      "Bioconductor",
      "<span class='chapter-number'>28</span>  <span class='chapter-title'>Single Cell RNA-seq Analysis</span>"
    ]
  },
  {
    "objectID": "single_cell/setup.html#finding-marker-genes",
    "href": "single_cell/setup.html#finding-marker-genes",
    "title": "\n28  Single Cell RNA-seq Analysis\n",
    "section": "\n28.5 Finding Marker Genes",
    "text": "28.5 Finding Marker Genes\nMarker genes are genes that are differentially expressed in a specific cell type or cluster compared to others. Identifying marker genes helps us understand the molecular characteristics of each cell type and can provide insights into their functions.\nTo find marker genes, we can use the findMarkers() function from the scran package. This function performs pairwise differential expression analysis between clusters or cell types. We can specify the clusters we want to compare and the method for calculating differential expression.\n\nmarkers &lt;- findMarkers(sce, \n    groups=as.numeric(as.factor(colData(sce)$level1class)), pval.type=\"all\",\n    direction=\"up\", subset.row=top_var_genes, test=\"t\")\n\nThe findMarkers() function identifies genes that are significantly upregulated in each cluster compared to all other clusters. The pval.type=\"all\" argument specifies that we want to calculate p-values for all comparisons, and the direction=\"up\" argument indicates that we are interested in genes that are upregulated in the specified clusters.\nThe subset.row=top_var_genes argument restricts the analysis to the highly variable genes we identified earlier, which are more likely to be informative for distinguishing cell types.\nThe resulting markers object is a list where each element corresponds to a cluster and contains the differential expression results for that cluster. We can visualize the top marker genes for each cluster using a heatmap.\nTake my word for it that we can visualize the top marker genes for each cluster using a heatmap. This allows us to see which genes are most strongly associated with each cluster and how they differ across clusters.\n\n# visualize the top marker gene for cluster 1 in a violin plot(\nlibrary(scater)\ntop_marker_genes_for_all_clusters = lapply(markers, function(x) {\n    return(head(rownames(x), 10))\n})\nplotHeatmap(sce, features=unique(unlist(top_marker_genes_for_all_clusters)),\n    cluster_rows=TRUE, cluster_cols=TRUE, show_colnames=FALSE,\n    main=\"Top marker genes for cluster 1\",\n    color_columns_by=\"level1class\")",
    "crumbs": [
      "Home",
      "Bioconductor",
      "<span class='chapter-number'>28</span>  <span class='chapter-title'>Single Cell RNA-seq Analysis</span>"
    ]
  },
  {
    "objectID": "single_cell/setup.html#interactive-visualization-with-isee",
    "href": "single_cell/setup.html#interactive-visualization-with-isee",
    "title": "\n28  Single Cell RNA-seq Analysis\n",
    "section": "\n28.6 Interactive Visualization with iSEE",
    "text": "28.6 Interactive Visualization with iSEE\nThe Interactive SummarizedExperiment Explorer (iSEE) is a powerful tool for exploring single-cell data interactively. It allows us to visualize and interact with the data in real-time, making it easier to explore complex datasets and identify patterns.\nA key component of iSEE is the SingleCellExperiment object, which serves as the data container for single-cell RNA-seq data. iSEE provides a user-friendly interface for exploring the data.\nTo use iSEE, we first need to install the iSEE package and then launch the iSEE app.\n\n# Install iSEE package if not already installed\niSEE::iSEE(sce)\n\nThe iSEE() function launches the iSEE app, which provides an interactive interface for exploring the SingleCellExperiment object.\nYou may want to play with a few genes to see how they are expressed across the different clusters. You can also explore the metadata associated with the cells, such as tissue type, cell type annotations, and quality control metrics. The set of “marker genes” we identified earlier can also be visualized in iSEE, allowing us to see how these genes are expressed across different clusters and cell types.\nFor cluster 1, the marker genes are:\n\ncat(paste(top_marker_genes_for_all_clusters[[1]],'\\n'))\n\nAqp4 \n Cldn10 \n Clu \n Aldoc \n Ntsr2 \n Mt2 \n Fabp7 \n Gja1 \n Gpr37l1 \n Prdx6 \n\n\nSee the",
    "crumbs": [
      "Home",
      "Bioconductor",
      "<span class='chapter-number'>28</span>  <span class='chapter-title'>Single Cell RNA-seq Analysis</span>"
    ]
  },
  {
    "objectID": "single_cell/setup.html#bonus-working-with-10x-genomics-data",
    "href": "single_cell/setup.html#bonus-working-with-10x-genomics-data",
    "title": "\n28  Single Cell RNA-seq Analysis\n",
    "section": "\n28.7 Bonus: Working with 10x Genomics Data",
    "text": "28.7 Bonus: Working with 10x Genomics Data\nIn the previous sections, we used a curated dataset from the scRNAseq package. However, many real-world single-cell RNA-seq datasets come from 10x Genomics platforms, which produce data in specific formats. In this section, we’ll cover a hypothetical workflow for loading and processing raw 10x Genomics data using R.\n\n28.7.1 Understanding 10x Genomics Output Formats\n10x Genomics Cell Ranger produces several output formats:\n\n\nMatrix Market format (.mtx files): Sparse matrix format with separate files for the matrix, gene names, and cell barcodes\n\nHDF5 format (.h5 files): A single compressed file containing all the data\n\nFiltered vs. unfiltered: Cell Ranger provides both versions - filtered contains only cells that pass basic quality filters\n\n28.7.2 Loading 10x Data from Matrix Market Files\n\n# Loading from standard 10x output directory\n# This assumes you have a directory with:\n# - matrix.mtx.gz (or matrix.mtx)\n# - features.tsv.gz (or genes.tsv.gz for older versions)\n# - barcodes.tsv.gz\n\nlibrary(DropletUtils)\nlibrary(SingleCellExperiment)\n\n# Method 1: Using DropletUtils (recommended)\nsce_10x &lt;- read10xCounts(\"path/to/10x/output/\", col.names = TRUE)\n\n# Method 2: Using Seurat (if you prefer the Seurat ecosystem)\n# library(Seurat)\n# seurat_obj &lt;- Read10X(\"path/to/10x/output/\")\n# sce_10x &lt;- as.SingleCellExperiment(CreateSeuratObject(seurat_obj))\n\nWhat we’re doing: The read10xCounts() function from DropletUtils reads the three files that comprise 10x output and creates a SingleCellExperiment object. The col.names = TRUE parameter ensures that cell barcodes are used as column names.\nWhy DropletUtils: This package is specifically designed for handling droplet-based single-cell data (like 10x Genomics). It includes specialized functions for:\n\nReading 10x formats efficiently\nDistinguishing empty droplets from cells\nHandling ambient RNA contamination\nQuality control specific to droplet-based methods\n\n28.7.3 Loading from HDF5 Files\n\n# For HDF5 format files\nlibrary(rhdf5)\n\n# Method 1: Using DropletUtils\nsce_h5 &lt;- read10xCounts(\"path/to/sample.h5\", type = \"HDF5\")\n\n# Method 2: Direct HDF5 reading (more control)\nh5_file &lt;- \"path/to/sample.h5\"\ngene_info &lt;- h5read(h5_file, \"matrix/features\")\nbarcodes &lt;- h5read(h5_file, \"matrix/barcodes\")\nmatrix_data &lt;- h5read(h5_file, \"matrix/data\")\n# Additional processing needed to reconstruct sparse matrix\n\nWhat we’re doing: HDF5 files are more convenient as they contain all information in a single file. The type = \"HDF5\" parameter tells the function to expect this format instead of separate matrix files.\nWhen to use H5 files:\n\n\nConvenience: Single file is easier to manage and transfer\n\nSpeed: Often faster to read than multiple compressed files\n\nStorage: More efficient storage, especially for large datasets\n\n28.7.4 Essential Quality Control After Loading Raw Data\n\n# Add mitochondrial gene information\nlibrary(scuttle)\nis.mito &lt;- grepl(\"^MT-\", rownames(sce_10x)) # Human mitochondrial genes\n# For mouse data, use: is.mito &lt;- grepl(\"^mt-\", rownames(sce_10x))\n\n# Calculate per-cell QC metrics\nsce_10x &lt;- addPerCellQC(sce_10x, subsets = list(Mito = is.mito))\n\n# Calculate per-gene QC metrics\nsce_10x &lt;- addPerFeatureQC(sce_10x)\n\n# View the QC metrics\ncolnames(colData(sce_10x))\n\nWhat we’re doing: We’re calculating quality control metrics that are essential for raw 10x data:\n\n\nMitochondrial genes: High mitochondrial gene expression often indicates dying cells\n\nPer-cell metrics: Total counts, number of detected genes, mitochondrial percentage\n\nPer-gene metrics: How many cells express each gene\n\nWhy QC is crucial for raw data: Unlike curated datasets, raw 10x data contains:\n\n\nEmpty droplets: Droplets that captured ambient RNA but no cells\n\nDying cells: Cells with compromised membranes\n\nDoublets: Droplets containing multiple cells\n\nLow-quality cells: Cells with very few detected genes\n\n28.7.5 Filtering Low-Quality Cells and Genes\n\n# Visualize QC metrics\nlibrary(scater)\nplotColData(sce_10x, x = \"sum\", y = \"detected\", colour_by = \"subsets_Mito_percent\")\n\n# Set filtering thresholds (these are examples - adjust based on your data)\n# Filter cells\nhigh_mito &lt;- sce_10x$subsets_Mito_percent &gt; 20  # Remove cells with &gt;20% mitochondrial reads\nlow_lib_size &lt;- sce_10x$sum &lt; 1000              # Remove cells with &lt;1000 total counts\nlow_n_features &lt;- sce_10x$detected &lt; 500        # Remove cells with &lt;500 detected genes\n\n# Filter genes (remove genes expressed in very few cells)\nlow_expression &lt;- rowSums(counts(sce_10x) &gt; 0) &lt; 10  # Expressed in &lt;10 cells\n\n# Apply filters\nsce_filtered &lt;- sce_10x[!low_expression, !(high_mito | low_lib_size | low_n_features)]\n\n# Check how many cells and genes remain\ndim(sce_10x)      # Before filtering\ndim(sce_filtered) # After filtering\n\nWhat we’re doing: We’re applying filters to remove low-quality cells and rarely expressed genes. The specific thresholds should be adjusted based on your dataset characteristics.\nUnderstanding the filters:\n\n\nMitochondrial percentage: Cells with high mitochondrial gene expression are likely dying\n\nLibrary size: Total number of UMIs per cell - very low values suggest poor capture\n\nNumber of features: How many different genes are detected - very low values suggest poor quality\n\nGene expression frequency: Genes expressed in very few cells are often noise\n\nCritical considerations:\n\n\nThresholds are dataset-dependent: What works for one experiment may not work for another\n\nBiological vs. technical variation: Some cell types naturally have different RNA content\n\nVisualization first: Always plot your QC metrics before setting thresholds",
    "crumbs": [
      "Home",
      "Bioconductor",
      "<span class='chapter-number'>28</span>  <span class='chapter-title'>Single Cell RNA-seq Analysis</span>"
    ]
  },
  {
    "objectID": "single_cell/setup.html#further-reading-and-resources",
    "href": "single_cell/setup.html#further-reading-and-resources",
    "title": "\n28  Single Cell RNA-seq Analysis\n",
    "section": "\n28.8 Further Reading and Resources",
    "text": "28.8 Further Reading and Resources\nFor more in-depth information on single-cell RNA-seq analysis, consider the following resources:\n\nThe Orchestrating Single-Cell RNA-seq Analysis book provides a comprehensive guide to single-cell RNA-seq analysis using Bioconductor.\nThe Seurat package documentation offers extensive tutorials and vignettes for single-cell analysis in the Seurat ecosystem.\n\n\n\n\n\nLun, Aaron T. L., Davis J. McCarthy, and John C. Marioni. 2016. “A Step-by-Step Workflow for Low-Level Analysis of Single-Cell RNA-Seq Data with Bioconductor.” F1000Res. 5: 2122. https://doi.org/10.12688/f1000research.9501.2.\n\n\nMcCarthy, Davis J., Kieran R. Campbell, Aaron T. L. Lun, and Quin F. Willis. 2017. “Scater: Pre-Processing, Quality Control, Normalisation and Visualisation of Single-Cell RNA-Seq Data in R.” Bioinformatics 33: 1179–86. https://doi.org/10.1093/bioinformatics/btw777.\n\n\nRisso, Davide, and Michael Cole. 2024. scRNAseq: Collection of Public Single-Cell RNA-Seq Datasets. https://doi.org/10.18129/B9.bioc.scRNAseq.\n\n\nZeisel, Amit, Ana B. Muñoz-Manchado, Simone Codeluppi, Peter Lönnerberg, Gioele La Manno, Anna Juréus, Sueli Marques, et al. 2015. “Cell Types in the Mouse Cortex and Hippocampus Revealed by Single-Cell RNA-seq.” Science 347 (6226): 1138–42. https://doi.org/10.1126/science.aaa1934.",
    "crumbs": [
      "Home",
      "Bioconductor",
      "<span class='chapter-number'>28</span>  <span class='chapter-title'>Single Cell RNA-seq Analysis</span>"
    ]
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Bourgon, Richard, Robert Gentleman, and Wolfgang Huber. 2010.\n“Independent Filtering Increases Detection Power for\nHigh-Throughput Experiments.” Proceedings of the National\nAcademy of Sciences 107 (21): 9546–51. https://doi.org/10.1073/pnas.0914005107.\n\n\nBrouwer-Visser, Jurriaan, Wei-Yi Cheng, Anna Bauer-Mehren, Daniela\nMaisel, Katharina Lechner, Emilia Andersson, Joel T. Dudley, and\nFrancesca Milletti. 2018. “Regulatory T-Cell\nGenes Drive Altered\nImmune Microenvironment in Adult\nSolid Cancers and Allow for\nImmune Contextual Patient\nSubtyping.” Cancer Epidemiology, Biomarkers\n& Prevention 27 (1): 103–12. https://doi.org/10.1158/1055-9965.EPI-17-0461.\n\n\nBuenrostro, Jason D, Paul G Giresi, Lisa C Zaba, Howard Y Chang, and\nWilliam J Greenleaf. 2013. “Transposition of Native Chromatin for\nFast and Sensitive Epigenomic Profiling of Open Chromatin, DNA-binding Proteins and Nucleosome\nPosition.” Nature Methods 10 (12): 1213–18. https://doi.org/10.1038/nmeth.2688.\n\n\nBuenrostro, Jason D, Beijing Wu, Howard Y Chang, and William J\nGreenleaf. 2015. “ATAC-seq: A Method\nfor Assaying Chromatin Accessibility Genome-Wide.”\nCurrent Protocols in Molecular Biology / Edited by Frederick M.\nAusubel ... [Et Al.] 109 (January): 21.29.1–9. https://doi.org/10.1002/0471142727.mb2129s109.\n\n\nCenter, Pew Research. 2016. “Lifelong Learning and\nTechnology.” Pew Research Center: Internet,\nScience & Tech. https://www.pewresearch.org/internet/2016/03/22/lifelong-learning-and-technology/.\n\n\nCrawford, Gregory E, Sean Davis, Peter C Scacheri, Gabriel Renaud,\nMohamad J Halawi, Michael R Erdos, Roland Green, Paul S Meltzer, Tyra G\nWolfsberg, and Francis S Collins. 2006. “DNase-chip: A High-Resolution Method to Identify\nDNase I Hypersensitive Sites Using Tiled\nMicroarrays.” Nature Methods 3 (7): 503–9. http://www.ncbi.nlm.nih.gov/pubmed/16791207?dopt=AbstractPlus.\n\n\nCrawford, Gregory E, Ingeborg E Holt, James Whittle, Bryn D Webb, Denise\nTai, Sean Davis, Elliott H Margulies, et al. 2006. “Genome-Wide\nMapping of DNase Hypersensitive Sites Using Massively\nParallel Signature Sequencing (MPSS).” Genome\nResearch 16 (1): 123–31. http://www.ncbi.nlm.nih.gov/pubmed/16344561?dopt=AbstractPlus.\n\n\nDeRisi, J. L., V. R. Iyer, and P. O. Brown. 1997. “Exploring the\nMetabolic and Genetic Control of Gene Expression on a Genomic\nScale.” Science (New York, N.Y.) 278 (5338): 680–86. https://doi.org/10.1126/science.278.5338.680.\n\n\nGreener, Joe G., Shaun M. Kandathil, Lewis Moffat, and David T. Jones.\n2022. “A Guide to Machine Learning for Biologists.”\nNature Reviews Molecular Cell Biology 23 (1): 40–55. https://doi.org/10.1038/s41580-021-00407-0.\n\n\nKnowles, Malcolm S., Elwood F. Holton, and Richard A. Swanson. 2005.\nThe Adult Learner: The Definitive Classic in Adult Education and\nHuman Resource Development. 6th ed. Amsterdam ; Boston: Elsevier.\n\n\nLawrence, Michael, Wolfgang Huber, Hervé Pagès, Patrick Aboyoun, Marc\nCarlson, Robert Gentleman, Martin T. Morgan, and Vincent J. Carey.\n2013a. “Software for Computing and Annotating Genomic\nRanges.” PLoS Computational Biology 9 (8): e1003118. https://doi.org/10.1371/journal.pcbi.1003118.\n\n\nLawrence, Michael, Wolfgang Huber, Hervé Pagès, Patrick Aboyoun, Marc\nCarlson, Robert Gentleman, Martin T Morgan, and Vincent J Carey. 2013b.\n“Software for Computing and Annotating Genomic Ranges.”\nPLoS Computational Biology 9 (8): e1003118. https://doi.org/10.1371/journal.pcbi.1003118.\n\n\nLibbrecht, Maxwell W., and William Stafford Noble. 2015. “Machine\nLearning Applications in Genetics and Genomics.” Nature\nReviews Genetics 16 (6): 321–32. https://doi.org/10.1038/nrg3920.\n\n\nLun, Aaron T. L., Davis J. McCarthy, and John C. Marioni. 2016. “A\nStep-by-Step Workflow for Low-Level Analysis of Single-Cell RNA-Seq Data\nwith Bioconductor.” F1000Res. 5: 2122. https://doi.org/10.12688/f1000research.9501.2.\n\n\nMcCarthy, Davis J., Kieran R. Campbell, Aaron T. L. Lun, and Quin F.\nWillis. 2017. “Scater: Pre-Processing, Quality Control,\nNormalisation and Visualisation of Single-Cell\nRNA-Seq Data in\nR.” Bioinformatics 33: 1179–86. https://doi.org/10.1093/bioinformatics/btw777.\n\n\nMorgan, Martin, Herve Pages, V Obenchain, and N Hayden. 2016.\n“Rsamtools: Binary Alignment (BAM), FASTA, Variant Call (BCF), and\nTabix File Import.” R Package Version 1 (0): 677–89.\n\n\nRisso, Davide, and Michael Cole. 2024. scRNAseq: Collection of\nPublic Single-Cell RNA-Seq Datasets. https://doi.org/10.18129/B9.bioc.scRNAseq.\n\n\nStudent. 1908. “The Probable Error of a\nMean.” Biometrika 6 (1): 1–25. https://doi.org/10.2307/2331554.\n\n\nTsompana, Maria, and Michael J Buck. 2014. “Chromatin\nAccessibility: A Window into the Genome.” Epigenetics &\nChromatin 7 (1): 33. https://doi.org/10.1186/1756-8935-7-33.\n\n\nZeisel, Amit, Ana B. Muñoz-Manchado, Simone Codeluppi, Peter Lönnerberg,\nGioele La Manno, Anna Juréus, Sueli Marques, et al. 2015. “Cell\nTypes in the Mouse Cortex and Hippocampus Revealed by Single-Cell RNA-seq.” Science 347 (6226):\n1138–42. https://doi.org/10.1126/science.aaa1934.",
    "crumbs": [
      "Home",
      "References"
    ]
  },
  {
    "objectID": "appendix.html",
    "href": "appendix.html",
    "title": "Appendix A — Appendix",
    "section": "",
    "text": "A.1 Data Sets",
    "crumbs": [
      "Home",
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>Appendix</span>"
    ]
  },
  {
    "objectID": "appendix.html#data-sets",
    "href": "appendix.html#data-sets",
    "title": "Appendix A — Appendix",
    "section": "",
    "text": "BRFSS subset\nALL clinical data\nALL expression data",
    "crumbs": [
      "Home",
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>Appendix</span>"
    ]
  },
  {
    "objectID": "appendix.html#swirl",
    "href": "appendix.html#swirl",
    "title": "Appendix A — Appendix",
    "section": "\nA.2 Swirl",
    "text": "A.2 Swirl\nThe following is from the swirl website.\n\nThe swirl R package makes it fun and easy to learn R programming and data science. If you are new to R, have no fear.\n\nTo get started, we need to install a new package into R.\n\ninstall.packages('swirl')\n\nOnce installed, we want to load it into the R workspace so we can use it.\n\nlibrary('swirl')\n\nFinally, to get going, start swirl and follow the instructions.\n\nswirl()",
    "crumbs": [
      "Home",
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>Appendix</span>"
    ]
  },
  {
    "objectID": "git_and_github.html",
    "href": "git_and_github.html",
    "title": "Appendix B — Git and GitHub",
    "section": "",
    "text": "B.1 install Git and GitHub CLI\nGit is a version control system that allows you to track changes in your code and collaborate with others. GitHub is a web-based platform that hosts Git repositories, making it easy to share and collaborate on projects. Github is NOT the only place to host Git repositories, but it is the most popular and has a large community of users.\nYou can use git by itself locally for version control. However, if you want to collaborate with others, you will need to use a remote repository, such as GitHub. This allows you to share your code with others, track changes, and collaborate on projects.\nTo use Git and GitHub, you need to have Git installed on your computer. You can download it from git-scm.com. After installation, you can check if Git is installed correctly by running the following command in your terminal:\nWe also need the gh command line tool to interact with GitHub. You can install it from cli.github.com. To install, go to the releases page and download the appropriate version for your operating system. For the Mac, it is the file named something like “Macos Universal” and the file will have a .pkg extension. You can install it by double-clicking the file after downloading it.",
    "crumbs": [
      "Home",
      "Appendices",
      "<span class='chapter-number'>B</span>  <span class='chapter-title'>Git and GitHub</span>"
    ]
  },
  {
    "objectID": "git_and_github.html#install-git-and-github-cli",
    "href": "git_and_github.html#install-git-and-github-cli",
    "title": "Appendix B — Git and GitHub",
    "section": "",
    "text": "git --version\n\n\n\n\n\n\n\nUsing the RStudio Terminal\n\n\n\nIf you are using RStudio, you can use the built-in terminal to run Git commands. To open the terminal, go to the “Terminal” tab in the bottom pane of RStudio. This allows you to run Git commands directly from RStudio without needing to switch to a separate terminal application.\n\nFor more details, see the RStudio terminal documentation.",
    "crumbs": [
      "Home",
      "Appendices",
      "<span class='chapter-number'>B</span>  <span class='chapter-title'>Git and GitHub</span>"
    ]
  },
  {
    "objectID": "git_and_github.html#configure-git",
    "href": "git_and_github.html#configure-git",
    "title": "Appendix B — Git and GitHub",
    "section": "B.2 Configure Git",
    "text": "B.2 Configure Git\nAfter installing Git, you need to configure it with your name and email address. This information will be used to identify you as the author of the commits you make. Run the following commands in your terminal, replacing “Your Name” and “you@example.com” with your actual name and email address:\ngit config --global user.name \"Your Name\"\ngit config --global user.email \"you@example.com\"",
    "crumbs": [
      "Home",
      "Appendices",
      "<span class='chapter-number'>B</span>  <span class='chapter-title'>Git and GitHub</span>"
    ]
  },
  {
    "objectID": "git_and_github.html#create-a-github-account",
    "href": "git_and_github.html#create-a-github-account",
    "title": "Appendix B — Git and GitHub",
    "section": "B.3 Create a GitHub account",
    "text": "B.3 Create a GitHub account\nIf you don’t already have a GitHub account, you can create one for free at github.com.",
    "crumbs": [
      "Home",
      "Appendices",
      "<span class='chapter-number'>B</span>  <span class='chapter-title'>Git and GitHub</span>"
    ]
  },
  {
    "objectID": "git_and_github.html#login-to-github-cli",
    "href": "git_and_github.html#login-to-github-cli",
    "title": "Appendix B — Git and GitHub",
    "section": "B.4 Login to GitHub CLI",
    "text": "B.4 Login to GitHub CLI\nAfter installing the GitHub CLI, you need to log in to your GitHub account. Run the following command in your terminal:\ngh auth login",
    "crumbs": [
      "Home",
      "Appendices",
      "<span class='chapter-number'>B</span>  <span class='chapter-title'>Git and GitHub</span>"
    ]
  },
  {
    "objectID": "git_and_github.html#introduction-to-version-control-with-git",
    "href": "git_and_github.html#introduction-to-version-control-with-git",
    "title": "Appendix B — Git and GitHub",
    "section": "B.5 Introduction to Version Control with Git",
    "text": "B.5 Introduction to Version Control with Git\nWelcome to the world of version control! Think of Git as a “save” button for your entire project, but with the ability to go back to previous saves, see exactly what you changed, and even work on different versions of your project at the same time. It’s an essential tool for reproducible and collaborative research.\nIn this tutorial, we’ll learn the absolute basics of Git using the command line directly within RStudio.\n\nB.5.1 Key Git Commands We’ll Learn Today:\n\ngit init: Initializes a new Git repository in your project folder. This is the first step to start tracking your files.\ngit add: Tells Git which files you want to track changes for. You can think of this as putting your changes into a “staging area.”\ngit commit: Takes a snapshot of your staged changes. This is like creating a permanent save point with a descriptive message.\ngit restore: Discards changes in your working directory. It’s a way to undo modifications you haven’t committed yet.\ngit branch: Allows you to create separate timelines of your project. This is useful for developing new features without affecting your main work.\ngit merge: Combines the changes from one branch into another.",
    "crumbs": [
      "Home",
      "Appendices",
      "<span class='chapter-number'>B</span>  <span class='chapter-title'>Git and GitHub</span>"
    ]
  },
  {
    "objectID": "git_and_github.html#the-toy-example-an-r-script",
    "href": "git_and_github.html#the-toy-example-an-r-script",
    "title": "Appendix B — Git and GitHub",
    "section": "B.6 The Toy Example: An R Script",
    "text": "B.6 The Toy Example: An R Script\nFirst, let’s create a simple R script that we can use for our Git exercise. In RStudio, create a new R Script and save it as data_analysis.R.\n# data_analysis.R\n\n# Load necessary libraries\nlibrary(ggplot2)\nlibrary(dplyr)\n\n# Create some sample data\ndata &lt;- data.frame(\n  x = 1:10,\n  y = (1:10) ^ 2\n)\n\n# Initial data summary\nsummary(data)",
    "crumbs": [
      "Home",
      "Appendices",
      "<span class='chapter-number'>B</span>  <span class='chapter-title'>Git and GitHub</span>"
    ]
  },
  {
    "objectID": "git_and_github.html#lets-get-started-with-git",
    "href": "git_and_github.html#lets-get-started-with-git",
    "title": "Appendix B — Git and GitHub",
    "section": "B.7 Let’s Get Started with Git!",
    "text": "B.7 Let’s Get Started with Git!\nOpen the Terminal in RStudio (you can usually find it as a tab next to the Console). We’ll be typing all our Git commands here.\n\n\n\n\n\n\nFigure B.1: This is an overview of how git works along with the commands that make it tick. See this video\n\n\n\n\nB.7.1 Step 1: Initialize Your Git Repository\nFirst, we need to tell Git to start tracking our project folder.\ngit init\nYou’ll see a message like Initialized empty Git repository in.... You might also notice a new .git folder in your project directory (it might be hidden). This is where Git stores all its tracking information. Your default branch is automatically named main.\n\n\nB.7.2 Step 2: Your First Commit\nNow, let’s add our data_analysis.R script to Git’s tracking and make our first “commit.”\n\nAdd the file to the staging area:\ngit add data_analysis.R\nCommit the staged file with a message:\ngit commit -m \"Initial commit: Add basic data script\"\nThe -m flag lets you write your commit message directly in the command. Good commit messages are short but descriptive!\n\n\n\nB.7.3 Step 3: Making and Undoing a Change\nLet’s modify our R script. Add a plotting section to the end of data_analysis.R.\n# ... (keep the previous code)\n\n# Create a plot\nggplot(data, aes(x = x, y = y)) +\n  geom_point() +\n  ggtitle(\"A Simple Scatter Plot\")\nNow, what if we decided we didn’t want this change after all? We can use git restore to go back to our last committed version.\ngit restore data_analysis.R\nIf you look at your data_analysis.R file now, the plotting code will be gone!\n\n\nB.7.4 Step 4: Branching Out\nBranches are a powerful feature. Let’s create a new branch to add our plot without messing up our main branch.\n\nCreate a new branch and switch to it:\ngit checkout -b add-plot\nThis is a shortcut for git branch add-plot and git checkout add-plot.\n\nNow, re-add the plotting code to data_analysis.R.\n# ... (keep the previous code)\n\n# Create a plot\nggplot(data, aes(x = x, y = y)) +\n  geom_point() +\n  ggtitle(\"A Simple Scatter Plot\")\nLet’s commit this change on our new add-plot branch.\ngit add data_analysis.R\ngit commit -m \"feat: Add scatter plot\"\n\n\nB.7.5 Step 5: Seeing Branches in Action\nNow for the magic of branches. Let’s switch back to our main branch.\ngit checkout main\nNow, open your data_analysis.R script in the RStudio editor. The plotting code is gone! That’s because the change only exists on the add-plot branch. The main branch is exactly as we last left it.\nLet’s switch back to our feature branch.\ngit checkout add-plot\nCheck the data_analysis.R script again. The plotting code is back! This demonstrates how branches allow you to work on different versions of your project in isolation.\n\n\nB.7.6 Step 6: Merging Your Work\nOur plot is complete and we’re happy with it. It’s time to merge it back into our main branch to incorporate the new feature.\n\nSwitch back to the main branch, which is our target for the merge:\ngit checkout main\nMerge the add-plot branch into main:\ngit merge add-plot\n\nYou’ll see a message indicating that the merge happened. Now, your main branch has the updated data_analysis.R script with the plotting code!",
    "crumbs": [
      "Home",
      "Appendices",
      "<span class='chapter-number'>B</span>  <span class='chapter-title'>Git and GitHub</span>"
    ]
  },
  {
    "objectID": "additional_resources.html",
    "href": "additional_resources.html",
    "title": "Appendix C — Additional resources",
    "section": "",
    "text": "C.1 AI",
    "crumbs": [
      "Home",
      "Appendices",
      "<span class='chapter-number'>C</span>  <span class='chapter-title'>Additional resources</span>"
    ]
  },
  {
    "objectID": "additional_resources.html#ai",
    "href": "additional_resources.html#ai",
    "title": "Appendix C — Additional resources",
    "section": "",
    "text": "chatGPT\nGemini\nClaude\nDeepSeek\nPerplexity",
    "crumbs": [
      "Home",
      "Appendices",
      "<span class='chapter-number'>C</span>  <span class='chapter-title'>Additional resources</span>"
    ]
  },
  {
    "objectID": "dataviz.html",
    "href": "dataviz.html",
    "title": "Appendix D — Data Visualization with ggplot2",
    "section": "",
    "text": "Start with this worked example to get a feel for the ggplot2 package.\n\nhttps://rkabacoff.github.io/datavis/IntroGGPLOT.html\n\nThen, for more detail, I refer you to this excellent ggplot2 tutorial.\nFinally, for more R graphics inspiration, see the R Graph Gallery.",
    "crumbs": [
      "Home",
      "Appendices",
      "<span class='chapter-number'>D</span>  <span class='chapter-title'>Data Visualization with ggplot2</span>"
    ]
  },
  {
    "objectID": "matrix_exercises.html",
    "href": "matrix_exercises.html",
    "title": "Appendix E — Matrix Exercises",
    "section": "",
    "text": "E.1 Data preparation\nFor this set of exercises, we are going to rely on a dataset that comes with R. It gives the number of sunspots per month from 1749-1983. The dataset comes as a ts or time series data type which I convert to a matrix using the following code.\nJust run the code as is and focus on the rest of the exercises.\ndata(sunspots)\nsunspot_mat &lt;- matrix(as.vector(sunspots),ncol=12,byrow = TRUE)\ncolnames(sunspot_mat) &lt;- as.character(1:12)\nrownames(sunspot_mat) &lt;- as.character(1749:1983)",
    "crumbs": [
      "Home",
      "Appendices",
      "<span class='chapter-number'>E</span>  <span class='chapter-title'>Matrix Exercises</span>"
    ]
  },
  {
    "objectID": "matrix_exercises.html#exercises",
    "href": "matrix_exercises.html#exercises",
    "title": "Appendix E — Matrix Exercises",
    "section": "\nE.2 Exercises",
    "text": "E.2 Exercises\n\nAfter the conversion above, what does sunspot_mat look like? Use functions to find the number of rows, the number of columns, the class, and some basic summary statistics.\n\n\nncol(sunspot_mat)\nnrow(sunspot_mat)\ndim(sunspot_mat)\nsummary(sunspot_mat)\nhead(sunspot_mat)\ntail(sunspot_mat)\n\n\nPractice subsetting the matrix a bit by selecting:\n\nThe first 10 years (rows)\nThe month of July (7th column)\nThe value for July, 1979 using the rowname to do the selection.\n\n\n\n\nsunspot_mat[1:10,]\nsunspot_mat[,7]\nsunspot_mat['1979',7]\n\nThese next few exercises take advantage of the fact that calling a univariate statistical function (one that expects a vector) works for matrices by just making a vector of all the values in the matrix.\n\nWhat is the highest (max) number of sunspots recorded in these data?\n\n\nmax(sunspot_mat)\n\n\nAnd the minimum?\n\n\nmin(sunspot_mat)\n\n\nAnd the overall mean and median?\n\n\nmean(sunspot_mat)\nmedian(sunspot_mat)\n\n\nUse the hist() function to look at the distribution of all the monthly sunspot data.\n\n\nhist(sunspot_mat)\n\n\nRead about the breaks argument to hist() to try to increase the number of breaks in the histogram to increase the resolution slightly. Adjust your hist() and breaks to your liking.\n\n\nhist(sunspot_mat, breaks=40)\n\nNow, let’s move on to summarizing the data a bit to learn about the pattern of sunspots varies by month or by year.\n\nExamine the dataset again. What do the columns represent? And the rows?\n\n\n# just a quick glimpse of the data will give us a sense\nhead(sunspot_mat)\n\n\nWe’d like to look at the distribution of sunspots by month. How can we do that?\n\n\n# the mean of the columns is the mean number of sunspots per month.\ncolMeans(sunspot_mat)\n\n# Another way to write the same thing:\napply(sunspot_mat, 2, mean)\n\n\nAssign the month summary above to a variable and summarize it to get a sense of the spread over months.\n\n\nmonthmeans = colMeans(sunspot_mat)\nsummary(monthmeans)\n\n\nPlay the same game for years to get the per-year mean?\n\n\nymeans = rowMeans(sunspot_mat)\nsummary(ymeans)\n\n\nMake a plot of the yearly means. Do you see a pattern?\n\n\nplot(ymeans)\n# or make it clearer\nplot(ymeans, type='l')",
    "crumbs": [
      "Home",
      "Appendices",
      "<span class='chapter-number'>E</span>  <span class='chapter-title'>Matrix Exercises</span>"
    ]
  },
  {
    "objectID": "license.html",
    "href": "license.html",
    "title": "",
    "section": "",
    "text": "Code\n\n\n\n\n\n    To the extent possible under law,  Sean Davis has waived all copyright and related or neighboring rights to Statistical analysis of functional genomics dataa. This work is published from:  United States."
  }
]