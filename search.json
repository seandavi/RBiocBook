[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "The RBioc Book",
    "section": "",
    "text": "Preface",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#who-is-this-book-for",
    "href": "index.html#who-is-this-book-for",
    "title": "The RBioc Book",
    "section": "Who is this book for?",
    "text": "Who is this book for?\n\nPeople who want to learn data science\nPeople who want to teach data science\nPeople who want to learn how to teach data science\nPeople who want to learn how to learn data science",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#why-this-book",
    "href": "index.html#why-this-book",
    "title": "The RBioc Book",
    "section": "Why this book?",
    "text": "Why this book?\nThis book is a collection of resources for learning R and Bioconductor. It is meant to be largely self-directed, but for those looking to teach data science, it can also be used as a guide for structuring a course. Material is a bit variable in terms of difficulty, prerequisites, and format which is a reflection of the organic creation of the material.\nStudents are encouraged to work with others to learn the material. Instructors are encouraged to use the material to create a course that is tailored to the needs of their students and to spend lots of time in 1:1 and small groups to support students in their learning. See below for additional thoughts on adult learning and how it relates to this material.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#adult-learners",
    "href": "index.html#adult-learners",
    "title": "The RBioc Book",
    "section": "Adult learners",
    "text": "Adult learners\nAdult Learning Theory, also known as Andragogy, is the concept and practice of designing, developing, and delivering instructional experiences for adult learners. It is based on the belief that adults learn differently than children, and thus, require distinct approaches to engage, motivate, and retain information (Center 2016). The term was first introduced by Malcolm Knowles, an American educator who is known for his work in adult education (Knowles, Holton, and Swanson 2005).\nOne of the fundamental principles of Adult Learning Theory is that adults are self-directed learners. This means that we prefer to take control of our own learning process and set personal goals for themselves. We are motivated by our desire to solve problems or gain knowledge to improve our lives (see Figure 1). As a result, educational content for adults should be relevant and applicable to real-life situations. Furthermore, adult learners should be given opportunities to actively engage in the learning process by making choices, setting goals, and evaluating their progress.\n\n\n\n\n\nFigure 1: Why do adults choose to learn something?\n\n\nAnother key aspect of Adult Learning Theory is the role of experience. We bring a wealth of experience to the learning process, which serves as a resource for new learning. We often have well-established beliefs, values, and mental models that can influence our willingness to accept new ideas and concepts. Therefore, it is essential to acknowledge and respect our shared and unique past experiences and create an environment where we all feel comfortable sharing our perspectives.\nTo effectively learn as a group of adult learners, it is crucial to establish a collaborative learning environment that promotes open communication and fosters trust among participants. We all appreciate and strive for a respectful and supportive atmosphere where we can express our opinions without fear of judgment. Instructors should help facilitate discussions, encourage peer-to-peer interactions, and incorporate group activities and collaboration to capitalize on the collective knowledge of participants.\nAdditionally, adult learners often have multiple responsibilities outside of the learning environment, such as work and family commitments. As a result, we require flexible learning opportunities that accommodate busy schedules. Offering a variety of instructional formats, such as online modules, self-paced learning, or evening classes, can help ensure that adult learners have access to education despite any time constraints.\nAdult learners benefit from a learner-centered approach that focuses on the individual needs, preferences, and interests of each participant can greatly enhance the overall learning experience. In addition, we tend to be more intrinsically motivated to learn when we have a sense of autonomy and can practice and experiment (see Figure 2) with new concepts in a safe environment.\n\n\n\n\n\n\n\nFigure 2: How to stay stuck in data science (or anything). The “Read-Do” loop tends to deliver the best results. Too much reading between doing can be somewhat effective. Reading and simply copy-paste is probably the least effective. When working through material, experiment. Try to break things. Incorporate your own experience or applications whenever possible.\n\n\n\n\nUnderstanding Adult Learning Theory and its principles can significantly enhance the effectiveness of teaching and learning as adults. By respecting our autonomy, acknowledging our experiences, creating a supportive learning environment, offering flexible learning opportunities, and utilizing diverse teaching methods, we can better cater to the unique needs and preferences of adult learners.\nIn practice, that means that we will will not be prescriptive in our approach to teaching data science. We will not tell you what to do, but rather we will provide you with a variety of options and you can choose what works best for you. We will also provide you with a variety of resources and you can choose where to focus your time. Given that we cannot possibly cover everything, we will provide you with a framework for learning and you can fill in the gaps as you see fit. A key component of our success as adult learners is to gain the confidence to ask questions and problem-solve on our own.\n\n\n\n\nCenter, Pew Research. 2016. “Lifelong Learning and Technology.” Pew Research Center: Internet, Science & Tech. https://www.pewresearch.org/internet/2016/03/22/lifelong-learning-and-technology/.\n\n\nKnowles, Malcolm S., Elwood F. Holton, and Richard A. Swanson. 2005. The Adult Learner: The Definitive Classic in Adult Education and Human Resource Development. 6th ed. Amsterdam ; Boston: Elsevier.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "intro.html",
    "href": "intro.html",
    "title": "\n1  Introducing R and RStudio\n",
    "section": "",
    "text": "1.1 Introduction\nIn this chapter, we will discuss the basics of R and RStudio, two essential tools in genomics data analysis. We will cover the advantages of using R and RStudio, how to set up RStudio, and the different panels of the RStudio interface.",
    "crumbs": [
      "Introduction",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introducing R and RStudio</span>"
    ]
  },
  {
    "objectID": "intro.html#questions",
    "href": "intro.html#questions",
    "title": "\n1  Introducing R and RStudio\n",
    "section": "Questions",
    "text": "Questions\n\nWhat is R?\nWhy use R?\nWhy not use R?\nWhy use RStudio and how does it differ from R?",
    "crumbs": [
      "Introduction",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introducing R and RStudio</span>"
    ]
  },
  {
    "objectID": "intro.html#learning-objectives",
    "href": "intro.html#learning-objectives",
    "title": "\n1  Introducing R and RStudio\n",
    "section": "Learning Objectives",
    "text": "Learning Objectives\n\nKnow advantages of analyzing data in R\nKnow advantages of using RStudio\nBe able to start RStudio on your computer\nIdentify the panels of the RStudio interface\nBe able to customize the RStudio layout",
    "crumbs": [
      "Introduction",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introducing R and RStudio</span>"
    ]
  },
  {
    "objectID": "intro.html#what-is-r",
    "href": "intro.html#what-is-r",
    "title": "\n1  Introducing R and RStudio\n",
    "section": "\n1.2 What is R?",
    "text": "1.2 What is R?\n[R](https://en.wikipedia.org/wiki/R_(programming_language) is a programming language and software environment designed for statistical computing and graphics. It is widely used by statisticians, data scientists, and researchers for data analysis and visualization. R is an open-source language, which means it is free to use, modify, and distribute. Over the years, R has become particularly popular in the fields of genomics and bioinformatics, owing to its extensive libraries and powerful data manipulation capabilities.\nThe R language is a dialect of the S language, which was developed in the 1970s at Bell Laboratories. The first version of R was written by Robert Gentleman and Ross Ihaka and released in 1995 (see this slide deck for Ross Ihaka’s take on R’s history). Since then, R has been continuously developed by the R Core Team, a group of statisticians and computer scientists. The R Core Team releases a new version of R every year.\n\n\n\n\n\n\n\nFigure 1.1: Google trends showing the popularity of R over time based on Google searches",
    "crumbs": [
      "Introduction",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introducing R and RStudio</span>"
    ]
  },
  {
    "objectID": "intro.html#why-use-r",
    "href": "intro.html#why-use-r",
    "title": "\n1  Introducing R and RStudio\n",
    "section": "\n1.3 Why use R?",
    "text": "1.3 Why use R?\nThere are several reasons why R is a popular choice for data analysis, particularly in genomics and bioinformatics. These include:\n\n\nOpen-source: R is free to use and has a large community of developers who contribute to its growth and development. What is “open-source”?\n\n\nExtensive libraries: There are thousands of R packages available for a wide range of tasks, including specialized packages for genomics and bioinformatics. These libraries have been extensively tested and ara available for free.\n\nData manipulation: R has powerful data manipulation capabilities, making it easy (or at least possible) to clean, process, and analyze large datasets.\n\nGraphics and visualization: R has excellent tools for creating high-quality graphics and visualizations that can be customized to meet the specific needs of your analysis. In most cases, graphics produced by R are publication-quality.\n\nReproducible research: R enables you to create reproducible research by recording your analysis in a script, which can be easily shared and executed by others. In addition, R does not have a meaningful graphical user interface (GUI), which renders analysis in R much more reproducible than tools that rely on GUI interactions.\n\nCross-platform: R runs on Windows, Mac, and Linux (as well as more obscure systems).\n\nInteroperability with other languages: R can interfact with FORTRAN, C, and many other languages.\n\nScalability: R is useful for small and large projects.\n\nI can develop code for analysis on my Mac laptop. I can then install the same code on our 20k core cluster and run it in parallel on 100 samples, monitor the process, and then update a database (for example) with R when complete.",
    "crumbs": [
      "Introduction",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introducing R and RStudio</span>"
    ]
  },
  {
    "objectID": "intro.html#why-not-use-r",
    "href": "intro.html#why-not-use-r",
    "title": "\n1  Introducing R and RStudio\n",
    "section": "\n1.4 Why not use R?",
    "text": "1.4 Why not use R?\n\nR cannot do everything.\nR is not always the “best” tool for the job.\nR will not hold your hand. Often, it will slap your hand instead.\nThe documentation can be opaque (but there is documentation).\nR can drive you crazy (on a good day) or age you prematurely (on a bad one).\nFinding the right package to do the job you want to do can be challenging; worse, some contributed packages are unreliable.]{}\nR does not have a meaningfully useful graphical user interface (GUI).",
    "crumbs": [
      "Introduction",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introducing R and RStudio</span>"
    ]
  },
  {
    "objectID": "intro.html#r-license-and-the-open-source-ideal",
    "href": "intro.html#r-license-and-the-open-source-ideal",
    "title": "\n1  Introducing R and RStudio\n",
    "section": "\n1.5 R License and the Open Source Ideal",
    "text": "1.5 R License and the Open Source Ideal\nR is free (yes, totally free!) and distributed under GNU license. In particular, this license allows one to:\n\nDownload the source code\nModify the source code to your heart’s content\nDistribute the modified source code and even charge money for it, but you must distribute the modified source code under the original GNU license]{}\n\nThis license means that R will always be available, will always be open source, and can grow organically without constraint.",
    "crumbs": [
      "Introduction",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introducing R and RStudio</span>"
    ]
  },
  {
    "objectID": "intro.html#rstudio",
    "href": "intro.html#rstudio",
    "title": "\n1  Introducing R and RStudio\n",
    "section": "\n1.6 RStudio",
    "text": "1.6 RStudio\nRStudio is an integrated development environment (IDE) for R. It provides a graphical user interface (GUI) for R, making it easier to write and execute R code. RStudio also provides several other useful features, including a built-in console, syntax-highlighting editor, and tools for plotting, history, debugging, workspace management, and workspace viewing. RStudio is available in both free and commercial editions; the commercial edition provides some additional features, including support for multiple sessions and enhanced debugging\n\n1.6.1 Getting started with RStudio\nTo get started with RStudio, you first need to install both R and RStudio on your computer. Follow these steps:\n\nDownload and install R from the official R website.\nDownload and install RStudio from the official RStudio website.\nLaunch RStudio. You should see the RStudio interface with four panels.\n\n1.6.2 The RStudio Interface\nRStudio’s interface consists of four panels (see Figure 1.2):\n\n\nConsole\n\nThis panel displays the R console, where you can enter and execute R commands directly. The console also shows the output of your code, error messages, and other information.\n\n\n\nSource\n\nThis panel is where you write and edit your R scripts. You can create new scripts, open existing ones, and run your code from this panel.\n\n\n\nEnvironment\n\nThis panel displays your current workspace, including all variables, data objects, and functions that you have created or loaded in your R session.\n\n\n\nPlots, Packages, Help, and Viewer\n\nThese panels display plots, installed packages, help files, and web content, respectively.\n\n\n\n\n\n\n\n\nFigure 1.2: The RStudio interface. In this layout, the source pane is in the upper left, the console is in the lower left, the environment panel is in the top right and the viewer/help/files panel is in the bottom right.\n\n\n\n\n\n\n\n\nDo I need to use RStudio?\n\n\n\nNo. You can use R without RStudio. However, RStudio makes it easier to write and execute R code, and it provides several useful features that are not available in the basic R console. Note that the only part of RStudio that is actually interacting with R directly is the console. The other panels are simply providing a GUI that enhances the user experience.\n\n\n\n\n\n\n\n\nCustomizing the RStudio Interface\n\n\n\nYou can customize the layout of RStudio to suit your preferences. To do so, go to Tools &gt; Global Options &gt; Appearance. Here, you can change the theme, font size, and panel layout. You can also resize the panels as needed to gain screen real estate (see Figure 1.3).\n\n\n\n\n\n\n\nFigure 1.3: Dealing with limited screen real estate can be a challenge, particularly when you want to open another window to, for example, view a web page. You can resize the panes by sliding the center divider (red arrows) or by clicking on the minimize/maximize buttons (see blue arrow).\n\n\nIn summary, R and RStudio are powerful tools for genomics data analysis. By understanding the advantages of using R and RStudio and familiarizing yourself with the RStudio interface, you can efficiently analyze and visualize your data. In the following chapters, we will delve deeper into the functionality of R, Bioconductor, and various statistical methods to help you gain a comprehensive understanding of genomics data analysis.",
    "crumbs": [
      "Introduction",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introducing R and RStudio</span>"
    ]
  },
  {
    "objectID": "r_intro_mechanics.html",
    "href": "r_intro_mechanics.html",
    "title": "\n2  R mechanics\n",
    "section": "",
    "text": "2.1 Learning objectives",
    "crumbs": [
      "Introduction",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>R mechanics</span>"
    ]
  },
  {
    "objectID": "r_intro_mechanics.html#learning-objectives",
    "href": "r_intro_mechanics.html#learning-objectives",
    "title": "\n2  R mechanics\n",
    "section": "",
    "text": "Be able to start R and RStudio\nLearn to interact with the R console\nKnow the difference between expressions and assignment\nRecognize valid and invalid R names\nKnow how to access the R help system\nKnow how to assign values to variables, find what is in R memory, and remove values from R memory",
    "crumbs": [
      "Introduction",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>R mechanics</span>"
    ]
  },
  {
    "objectID": "r_intro_mechanics.html#installing-r",
    "href": "r_intro_mechanics.html#installing-r",
    "title": "\n2  R mechanics\n",
    "section": "\n2.2 Installing R",
    "text": "2.2 Installing R\nR is available for Windows, Mac, and Linux. To install R, go to the Comprehensive R Archive Network (CRAN). Click on the download link for your operating system and follow the instructions.",
    "crumbs": [
      "Introduction",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>R mechanics</span>"
    ]
  },
  {
    "objectID": "r_intro_mechanics.html#installing-rstudio",
    "href": "r_intro_mechanics.html#installing-rstudio",
    "title": "\n2  R mechanics\n",
    "section": "\n2.3 Installing RStudio",
    "text": "2.3 Installing RStudio\nRStudio is an Integrated Development Environment (IDE) for R. It is available for Windows, Mac, and Linux. To install RStudio, go to the RStudio download page. Click on the download link for your operating system and follow the instructions.",
    "crumbs": [
      "Introduction",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>R mechanics</span>"
    ]
  },
  {
    "objectID": "r_intro_mechanics.html#starting-r",
    "href": "r_intro_mechanics.html#starting-r",
    "title": "\n2  R mechanics\n",
    "section": "\n2.4 Starting R",
    "text": "2.4 Starting R\nHow to start R depends a bit on the operating system (Mac, Windows, Linux) and interface. In this course, we will largely be using an Integrated Development Environment (IDE) called RStudio, but there is nothing to prohibit using R at the command line or in some other interface (and there are a few).",
    "crumbs": [
      "Introduction",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>R mechanics</span>"
    ]
  },
  {
    "objectID": "r_intro_mechanics.html#rstudio-a-quick-tour",
    "href": "r_intro_mechanics.html#rstudio-a-quick-tour",
    "title": "\n2  R mechanics\n",
    "section": "\n2.5 RStudio: A Quick Tour",
    "text": "2.5 RStudio: A Quick Tour\nThe RStudio interface has multiple panes. All of these panes are simply for convenience except the “Console” panel, typically in the lower left corner (by default). The console pane contains the running R interface. If you choose to run R outside RStudio, the interaction will be identical to working in the console pane. This is useful to keep in mind as some environments, such as a computer cluster, encourage using R without RStudio.\n\nPanes\nOptions\nHelp\nEnvironment, History, and Files",
    "crumbs": [
      "Introduction",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>R mechanics</span>"
    ]
  },
  {
    "objectID": "r_intro_mechanics.html#interacting-with-r",
    "href": "r_intro_mechanics.html#interacting-with-r",
    "title": "\n2  R mechanics\n",
    "section": "\n2.6 Interacting with R",
    "text": "2.6 Interacting with R\nThe only meaningful way of interacting with R is by typing into the R console. At the most basic level, anything that we type at the command line will fall into one of two categories:\n\n\nAssignments\n\nx = 1\ny &lt;- 2\n\n\n\nExpressions\n\n1 + pi + sin(42)\n\n[1] 3.225071\n\n\n\n\nThe assignment type is obvious because either the The &lt;- or = are used. Note that when we type expressions, R will return a result. In this case, the result of R evaluating 1 + pi + sin(42) is 3.2250711.\nThe standard R prompt is a “&gt;” sign. When present, R is waiting for the next expression or assignment. If a line is not a complete R command, R will continue the next line with a “+”. For example, typing the fillowing with a “Return” after the second “+” will result in R giving back a “+” on the next line, a prompt to keep typing.\n\n1 + pi +\nsin(3.7)\n\n[1] 3.611757\n\n\nR can be used as a glorified calculator by using R expressions. Mathematical operations include:\n\nAddition: +\n\nSubtraction: -\n\nMultiplication: *\n\nDivision: /\n\nExponentiation: ^\n\nModulo: %%\n\n\nThe ^ operator raises the number to its left to the power of the number to its right: for example 3^2 is 9. The modulo returns the remainder of the division of the number to the left by the number on its right, for example 5 modulo 3 or 5 %% 3 is 2.\n\n2.6.1 Expressions\n\n5 + 2\n28 %% 3\n3^2\n5 + 4 * 4 + 4 ^ 4 / 10\n\nNote that R follows order-of-operations and groupings based on parentheses.\n\n5 + 4 / 9\n(5 + 4) / 9\n\n\n2.6.2 Assignment\nWhile using R as a calculator is interesting, to do useful and interesting things, we need to assign values to objects. To create objects, we need to give it a name followed by the assignment operator &lt;- (or, entirely equivalently, =) and the value we want to give it:\n\nweight_kg &lt;- 55 \n\n&lt;- is the assignment operator. Assigns values on the right to objects on the left, it is like an arrow that points from the value to the object. Using an = is equivalent (in nearly all cases). Learn to use &lt;- as it is good programming practice.\nObjects can be given any name such as x, current_temperature, or subject_id (see below). You want your object names to be explicit and not too long. They cannot start with a number (2x is not valid but x2 is). R is case sensitive (e.g., weight_kg is different from Weight_kg). There are some names that cannot be used because they represent the names of fundamental functions in R (e.g., if, else, for, see here for a complete list). In general, even if it’s allowed, it’s best to not use other function names, which we’ll get into shortly (e.g., c, T, mean, data, df, weights). When in doubt, check the help to see if the name is already in use. It’s also best to avoid dots (.) within a variable name as in my.dataset. It is also recommended to use nouns for variable names, and verbs for function names.\nWhen assigning a value to an object, R does not print anything. You can force to print the value by typing the name:\n\nweight_kg\n\n[1] 55\n\n\nNow that R has weight_kg in memory, which R refers to as the “global environment”, we can do arithmetic with it. For instance, we may want to convert this weight in pounds (weight in pounds is 2.2 times the weight in kg).\n\n2.2 * weight_kg\n\n[1] 121\n\n\nWe can also change a variable’s value by assigning it a new one:\n\nweight_kg &lt;- 57.5\n2.2 * weight_kg\n\n[1] 126.5\n\n\nThis means that assigning a value to one variable does not change the values of other variables. For example, let’s store the animal’s weight in pounds in a variable.\n\nweight_lb &lt;- 2.2 * weight_kg\n\nand then change weight_kg to 100.\n\nweight_kg &lt;- 100\n\nWhat do you think is the current content of the object weight_lb, 126.5 or 220?\nYou can see what objects (variables) are stored by viewing the Environment tab in Rstudio. You can also use the ls() function. You can remove objects (variables) with the rm() function. You can do this one at a time or remove several objects at once. You can also use the little broom button in your environment pane to remove everything from your environment.\n\nls()\nrm(weight_lb, weight_kg)\nls()\n\nWhat happens when you type the following, now?\n\nweight_lb # oops! you should get an error because weight_lb no longer exists!",
    "crumbs": [
      "Introduction",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>R mechanics</span>"
    ]
  },
  {
    "objectID": "r_intro_mechanics.html#rules-for-names-in-r",
    "href": "r_intro_mechanics.html#rules-for-names-in-r",
    "title": "\n2  R mechanics\n",
    "section": "\n2.7 Rules for Names in R",
    "text": "2.7 Rules for Names in R\nR allows users to assign names to objects such as variables, functions, and even dimensions of data. However, these names must follow a few rules.\n\nNames may contain any combination of letters, numbers, underscore, and “.”\nNames may not start with numbers, underscore.\nR names are case-sensitive.\n\nExamples of valid R names include:\npi\nx\ncamelCaps\nmy_stuff\nMY_Stuff\nthis.is.the.name.of.the.man\nABC123\nabc1234asdf\n.hi",
    "crumbs": [
      "Introduction",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>R mechanics</span>"
    ]
  },
  {
    "objectID": "r_intro_mechanics.html#resources-for-getting-help",
    "href": "r_intro_mechanics.html#resources-for-getting-help",
    "title": "\n2  R mechanics\n",
    "section": "\n2.8 Resources for Getting Help",
    "text": "2.8 Resources for Getting Help\nThere is extensive built-in help and documentation within R. A separate page contains a collection of additional resources.\nIf the name of the function or object on which help is sought is known, the following approaches with the name of the function or object will be helpful. For a concrete example, examine the help for the print method.\n\nhelp(print)\nhelp('print')\n?print\n\nIf the name of the function or object on which help is sought is not known, the following from within R will be helpful.\n\nhelp.search('microarray')\nRSiteSearch('microarray')\napropos('histogram')\n\nThere are also tons of online resources that Google will include in searches if online searching feels more appropriate.\nI strongly recommend using help(\"newfunction\"\") for all functions that are new or unfamiliar to you.\nThere are also many open and free resources and reference guides for R.\n\n\nQuick-R: a quick online reference for data input, basic statistics and plots\nR reference card PDF by Tom Short\nRstudio cheatsheets",
    "crumbs": [
      "Introduction",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>R mechanics</span>"
    ]
  },
  {
    "objectID": "r_basics.html",
    "href": "r_basics.html",
    "title": "\n3  Up and Running with R\n",
    "section": "",
    "text": "3.1 The R User Interface\nThe RStudio interface is simple. You type R code into the bottom line of the RStudio console pane and then click Enter to run it. The code you type is called a command, because it will command your computer to do something for you. The line you type it into is called the command line.\nWhen you type a command at the prompt and hit Enter, your computer executes the command and shows you the results. Then RStudio displays a fresh prompt for your next command. For example, if you type 1 + 1 and hit Enter, RStudio will display:\nYou’ll notice that a [1] appears next to your result. R is just letting you know that this line begins with the first value in your result. Some commands return more than one value, and their results may fill up multiple lines. For example, the command 100:130 returns 31 values; it creates a sequence of integers from 100 to 130. Notice that new bracketed numbers appear at the start of the second and third lines of output. These numbers just mean that the second line begins with the 14th value in the result, and the third line begins with the 25th value. You can mostly ignore the numbers that appear in brackets:\nIf you type an incomplete command and press Enter, R will display a + prompt, which means R is waiting for you to type the rest of your command. Either finish the command or hit Escape to start over:\nIf you type a command that R doesn’t recognize, R will return an error message. If you ever see an error message, don’t panic. R is just telling you that your computer couldn’t understand or do what you asked it to do. You can then try a different command at the next prompt:\nOnce you get the hang of the command line, you can easily do anything in R that you would do with a calculator. For example, you could do some basic arithmetic:\n2 * 3   \n\n[1] 6\n\n4 - 1   \n\n[1] 3\n\n# this obeys order-of-operations\n6 / (4 - 1)   \n\n[1] 2",
    "crumbs": [
      "Introduction",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Up and Running with R</span>"
    ]
  },
  {
    "objectID": "r_basics.html#the-r-user-interface",
    "href": "r_basics.html#the-r-user-interface",
    "title": "\n3  Up and Running with R\n",
    "section": "",
    "text": "Figure 3.1: Your computer does your bidding when you type R commands at the prompt in the bottom line of the console pane. Don’t forget to hit the Enter key. When you first open RStudio, the console appears in the pane on your left, but you can change this with File &gt; Tools &gt; Global Options in the menu bar.\n\n\n\n&gt; 1 + 1\n[1] 2\n&gt;\n\n&gt; 100:130\n [1] 100 101 102 103 104 105 106 107 108 109 110 111 112\n[14] 113 114 115 116 117 118 119 120 121 122 123 124 125\n[25] 126 127 128 129 130\n\n\n\n\n\n\nTip\n\n\n\nThe colon operator (:) returns every integer between two integers. It is an easy way to create a sequence of numbers.\n\n\n\n\n\n\n\n\nWhen do we compile?\n\n\n\nIn some languages, like C, Java, and FORTRAN, you have to compile your human-readable code into machine-readable code (often 1s and 0s) before you can run it. If you’ve programmed in such a language before, you may wonder whether you have to compile your R code before you can use it. The answer is no. R is a dynamic programming language, which means R automatically interprets your code as you run it.\n\n\n\n&gt; 5 -\n+\n+ 1\n[1] 4\n\n&gt; 3 % 5\nError: unexpected input in \"3 % 5\"\n&gt;\n\n\n\n\n\n\nTip\n\n\n\nWhenever you get an error message in R, consider googling the error message. You’ll often find that someone else has had the same problem and has posted a solution online. Simply cutting-and-pasting the error message into a search engine will often work\n\n\n\n\n\n\n\n\n\n\nTip\n\n\n\nR treats the hashtag character, #, in a special way; R will not run anything that follows a hashtag on a line. This makes hashtags very useful for adding comments and annotations to your code. Humans will be able to read the comments, but your computer will pass over them. The hashtag is known as the commenting symbol in R.\n\n\n\n\n\n\n\n\nCancelling commands\n\n\n\nSome R commands may take a long time to run. You can cancel a command once it has begun by pressing ctrl + c or by clicking the “stop sign” if it is available in Rstudio. Note that it may also take R a long time to cancel the command.\n\n\n\n3.1.1 An exercise\nThat’s the basic interface for executing R code in RStudio. Think you have it? If so, try doing these simple tasks. If you execute everything correctly, you should end up with the same number that you started with:\n\nChoose any number and add 2 to it.\nMultiply the result by 3.\nSubtract 6 from the answer.\nDivide what you get by 3.\n\n\n10 + 2\n\n[1] 12\n\n12 * 3\n\n[1] 36\n\n36 - 6\n\n[1] 30\n\n30 / 3\n\n[1] 10",
    "crumbs": [
      "Introduction",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Up and Running with R</span>"
    ]
  },
  {
    "objectID": "r_basics.html#objects",
    "href": "r_basics.html#objects",
    "title": "\n3  Up and Running with R\n",
    "section": "\n3.2 Objects",
    "text": "3.2 Objects\nNow that you know how to use R, let’s use it to make a virtual die. The : operator from a couple of pages ago gives you a nice way to create a group of numbers from one to six. The : operator returns its results as a vector (we are going to work with vectors in more detail), a one-dimensional set of numbers:\n1:6\n## 1 2 3 4 5 6\nThat’s all there is to how a virtual die looks! But you are not done yet. Running 1:6 generated a vector of numbers for you to see, but it didn’t save that vector anywhere for later use. If we want to use those numbers again, we’ll have to ask your computer to save them somewhere. You can do that by creating an R object.\nR lets you save data by storing it inside an R object. What is an object? Just a name that you can use to call up stored data. For example, you can save data into an object like a or b. Wherever R encounters the object, it will replace it with the data saved inside, like so:\n\na &lt;- 1\na\n\n[1] 1\n\n\n\na + 2\n\n[1] 3\n\n\n\n\n\n\n\n\nWhat just happened?\n\n\n\n\nTo create an R object, choose a name and then use the less-than symbol, &lt;, followed by a minus sign, -, to save data into it. This combination looks like an arrow, &lt;-. R will make an object, give it your name, and store in it whatever follows the arrow. So a &lt;- 1 stores 1 in an object named a.\nWhen you ask R what’s in a, R tells you on the next line.\nYou can use your object in new R commands, too. Since a previously stored the value of 1, you’re now adding 1 to 2.\n\n\n\n\n\n\n\n\n\nAssignment vs expressions\n\n\n\nEverything that you type into the R console can be assigned to one of two categories:\n\nAssignments\nExpressions\n\nAn expression is a command that tells R to do something. For example, 1 + 2 is an expression that tells R to add 1 and 2. When you type an expression into the R console, R will evaluate the expression and return the result. For example, if you type 1 + 2 into the R console, R will return 3. Expressions can have “side effects” but they don’t explicitly result in anything being added to R memory.\n\n5 + 2\n\n[1] 7\n\n28 %% 3\n\n[1] 1\n\n3^2\n\n[1] 9\n\n5 + 4 * 4 + 4 ^ 4 / 10\n\n[1] 46.6\n\n\nWhile using R as a calculator is interesting, to do useful and interesting things, we need to assign values to objects. To create objects, we need to give it a name followed by the assignment operator &lt;- (or, entirely equivalently, =) and the value we want to give it:\n\nweight_kg &lt;- 55\n\n\n\nSo, for another example, the following code would create an object named die that contains the numbers one through six. To see what is stored in an object, just type the object’s name by itself:\n\ndie &lt;- 1:6\ndie\n\n[1] 1 2 3 4 5 6\n\n\nWhen you create an object, the object will appear in the environment pane of RStudio, as shown in Figure 3.2. This pane will show you all of the objects you’ve created since opening RStudio.\n\n\n\n\n\nFigure 3.2: Assignment creates an object in the environment pane.\n\n\nYou can name an object in R almost anything you want, but there are a few rules. First, a name cannot start with a number. Second, a name cannot use some special symbols, like ^, !, $, @, +, -, /, or *:\n\n\nGood names\nNames that cause errors\n\n\n\na\n1trial\n\n\nb\n$\n\n\nFOO\n^mean\n\n\nmy_var\n2nd\n\n\n.day\n!bad\n\n\n\n\n\n\n\n\n\nCapitalization matters\n\n\n\nR is case-sensitive, so name and Name will refer to different objects:\n&gt; Name = 0\n&gt; Name + 1\n[1] 1\n&gt; name + 1\nError: object 'name' not found\nThe error above is a common one!\n\n\nFinally, R will overwrite any previous information stored in an object without asking you for permission. So, it is a good idea to not use names that are already taken:\n\nmy_number &lt;- 1\nmy_number \n\n[1] 1\n\n\n\nmy_number &lt;- 999\nmy_number\n\n[1] 999\n\n\nYou can see which object names you have already used with the function ls:\nls()\nYour environment will contain different names than mine, because you have probably created different objects.\nYou can also see which names you have used by examining RStudio’s environment pane.\nWe now have a virtual die that is stored in the computer’s memory and which has a name that we can use to refer to it. You can access it whenever you like by typing the word die.\nSo what can you do with this die? Quite a lot. R will replace an object with its contents whenever the object’s name appears in a command. So, for example, you can do all sorts of math with the die. Math isn’t so helpful for rolling dice, but manipulating sets of numbers will be your stock and trade as a data scientist. So let’s take a look at how to do that:\n\ndie - 1\n\n[1] 0 1 2 3 4 5\n\ndie / 2\n\n[1] 0.5 1.0 1.5 2.0 2.5 3.0\n\ndie * die\n\n[1]  1  4  9 16 25 36\n\n\nR uses element-wise execution when working with a vector like die. When you manipulate a set of numbers, R will apply the same operation to each element in the set. So for example, when you run die - 1, R subtracts one from each element of die.\nWhen you use two or more vectors in an operation, R will line up the vectors and perform a sequence of individual operations. For example, when you run die * die, R lines up the two die vectors and then multiplies the first element of vector 1 by the first element of vector 2. R then multiplies the second element of vector 1 by the second element of vector 2, and so on, until every element has been multiplied. The result will be a new vector the same length as the first two {Figure 3.3}.\n\n\n\n\n\nFigure 3.3: “When R performs element-wise execution, it matches up vectors and then manipulates each pair of elements independently.”\n\n\nIf you give R two vectors of unequal lengths, R will repeat the shorter vector until it is as long as the longer vector, and then do the math, as shown in Figure 3.4. This isn’t a permanent change–the shorter vector will be its original size after R does the math. If the length of the short vector does not divide evenly into the length of the long vector, R will return a warning message. This behavior is known as vector recycling, and it helps R do element-wise operations:\n\n1:2\n\n[1] 1 2\n\n1:4\n\n[1] 1 2 3 4\n\ndie\n\n[1] 1 2 3 4 5 6\n\ndie + 1:2\n\n[1] 2 4 4 6 6 8\n\ndie + 1:4\n\nWarning in die + 1:4: longer object length is not a multiple of shorter object\nlength\n\n\n[1] 2 4 6 8 6 8\n\n\n\n\n\n\n\nFigure 3.4: “R will repeat a short vector to do element-wise operations with two vectors of uneven lengths.”\n\n\nElement-wise operations are a very useful feature in R because they manipulate groups of values in an orderly way. When you start working with data sets, element-wise operations will ensure that values from one observation or case are only paired with values from the same observation or case. Element-wise operations also make it easier to write your own programs and functions in R.\n\n\n\n\n\n\nElement-wise operations are not matrix operations\n\n\n\nIt is important to know that operations with vectors are not the same that you might expect if you are expecting R to perform “matrix” operations. R can do inner multiplication with the %*% operator and outer multiplication with the %o% operator:\n# Inner product (1*1 + 2*2 + 3*3 + 4*4 + 5*5 + 6*6)\ndie %*% die\n# Outer product\ndie %o% die\n\n\nNow that you can do math with your die object, let’s look at how you could “roll” it. Rolling your die will require something more sophisticated than basic arithmetic; you’ll need to randomly select one of the die’s values. And for that, you will need a function.",
    "crumbs": [
      "Introduction",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Up and Running with R</span>"
    ]
  },
  {
    "objectID": "r_basics.html#functions",
    "href": "r_basics.html#functions",
    "title": "\n3  Up and Running with R\n",
    "section": "\n3.3 Functions",
    "text": "3.3 Functions\nR has many functions and puts them all at our disposal. We can use functions to do simple and sophisticated tasks. For example, we can round a number with the round function, or calculate its factorial with the factorial function. Using a function is pretty simple. Just write the name of the function and then the data you want the function to operate on in parentheses:\n\nround(3.1415)\n\n[1] 3\n\nfactorial(3)\n\n[1] 6\n\n\nThe data that you pass into the function is called the function’s argument. The argument can be raw data, an R object, or even the results of another R function. In this last case, R will work from the innermost function to the outermost Figure 3.5.\n\nmean(1:6)\n\n[1] 3.5\n\nmean(die)\n\n[1] 3.5\n\nround(mean(die))\n\n[1] 4\n\n\n\n\n\n\n\nFigure 3.5: “When you link functions together, R will resolve them from the innermost operation to the outermost. Here R first looks up die, then calculates the mean of one through six, then rounds the mean.”\n\n\nReturning to our die, we can use the sample function to randomly select one of the die’s values; in other words, the sample function can simulate rolling the die.\nThe sample function takes two arguments: a vector named x and a number named size. sample will return size elements from the vector:\n\nsample(x = 1:4, size = 2)\n\n[1] 3 2\n\n\nTo roll your die and get a number back, set x to die and sample one element from it. You’ll get a new (maybe different) number each time you roll it:\n\nsample(x = die, size = 1)\n\n[1] 2\n\nsample(x = die, size = 1)\n\n[1] 6\n\nsample(x = die, size = 1)\n\n[1] 6\n\n\nMany R functions take multiple arguments that help them do their job. You can give a function as many arguments as you like as long as you separate each argument with a comma.\nYou may have noticed that I set die and 1 equal to the names of the arguments in sample, x and size. Every argument in every R function has a name. You can specify which data should be assigned to which argument by setting a name equal to data, as in the preceding code. This becomes important as you begin to pass multiple arguments to the same function; names help you avoid passing the wrong data to the wrong argument. However, using names is optional. You will notice that R users do not often use the name of the first argument in a function. So you might see the previous code written as:\n\nsample(die, size = 1)\n\n[1] 4\n\n\nOften, the name of the first argument is not very descriptive, and it is usually obvious what the first piece of data refers to anyways.\nBut how do you know which argument names to use? If you try to use a name that a function does not expect, you will likely get an error:\nround(3.1415, corners = 2)\n## Error in round(3.1415, corners = 2) : unused argument(s) (corners = 2)\nIf you’re not sure which names to use with a function, you can look up the function’s arguments with args. To do this, place the name of the function in the parentheses behind args. For example, you can see that the round function takes two arguments, one named x and one named digits:\n\nargs(round)\n\nfunction (x, digits = 0, ...) \nNULL\n\n\nDid you notice that args shows that the digits argument of round is already set to 0? Frequently, an R function will take optional arguments like digits. These arguments are considered optional because they come with a default value. You can pass a new value to an optional argument if you want, and R will use the default value if you do not. For example, round will round your number to 0 digits past the decimal point by default. To override the default, supply your own value for digits:\n\nround(3.1415)\n\n[1] 3\n\nround(3.1415, digits = 2)\n\n[1] 3.14\n\n# pi happens to be a built-in value in R\npi\n\n[1] 3.141593\n\nround(pi)\n\n[1] 3\n\n\nYou should write out the names of each argument after the first one or two when you call a function with multiple arguments. Why? First, this will help you and others understand your code. It is usually obvious which argument your first input refers to (and sometimes the second input as well). However, you’d need a large memory to remember the third and fourth arguments of every R function. Second, and more importantly, writing out argument names prevents errors.\nIf you do not write out the names of your arguments, R will match your values to the arguments in your function by order. For example, in the following code, the first value, die, will be matched to the first argument of sample, which is named x. The next value, 1, will be matched to the next argument, size:\n\nsample(die, 1)\n\n[1] 1\n\n\nAs you provide more arguments, it becomes more likely that your order and R’s order may not align. As a result, values may get passed to the wrong argument. Argument names prevent this. R will always match a value to its argument name, no matter where it appears in the order of arguments:\n\nsample(size = 1, x = die)\n\n[1] 3\n\n\n\n3.3.1 Sample with Replacement\nIf you set size = 2, you can almost simulate a pair of dice. Before we run that code, think for a minute why that might be the case. sample will return two numbers, one for each die:\n\nsample(die, size = 2)\n\n[1] 3 1\n\n\nI said this “almost” works because this method does something funny. If you use it many times, you’ll notice that the second die never has the same value as the first die, which means you’ll never roll something like a pair of threes or snake eyes. What is going on?\nBy default, sample builds a sample without replacement. To see what this means, imagine that sample places all of the values of die in a jar or urn. Then imagine that sample reaches into the jar and pulls out values one by one to build its sample. Once a value has been drawn from the jar, sample sets it aside. The value doesn’t go back into the jar, so it cannot be drawn again. So if sample selects a six on its first draw, it will not be able to select a six on the second draw; six is no longer in the jar to be selected. Although sample creates its sample electronically, it follows this seemingly physical behavior.\nOne side effect of this behavior is that each draw depends on the draws that come before it. In the real world, however, when you roll a pair of dice, each die is independent of the other. If the first die comes up six, it does not prevent the second die from coming up six. In fact, it doesn’t influence the second die in any way whatsoever. You can recreate this behavior in sample by adding the argument replace = TRUE:\n\nsample(die, size = 2, replace = TRUE)\n\n[1] 1 3\n\n\nThe argument replace = TRUE causes sample to sample with replacement. Our jar example provides a good way to understand the difference between sampling with replacement and without. When sample uses replacement, it draws a value from the jar and records the value. Then it puts the value back into the jar. In other words, sample replaces each value after each draw. As a result, sample may select the same value on the second draw. Each value has a chance of being selected each time. It is as if every draw were the first draw.\nSampling with replacement is an easy way to create independent random samples. Each value in your sample will be a sample of size one that is independent of the other values. This is the correct way to simulate a pair of dice:\n\nsample(die, size = 2, replace = TRUE)\n\n[1] 5 4\n\n\nCongratulate yourself; you’ve just run your first simulation in R! You now have a method for simulating the result of rolling a pair of dice. If you want to add up the dice, you can feed your result straight into the sum function:\n\ndice &lt;- sample(die, size = 2, replace = TRUE)\ndice\n\n[1] 4 4\n\nsum(dice)\n\n[1] 8\n\n\nWhat would happen if you call dice multiple times? Would R generate a new pair of dice values each time? Let’s give it a try:\n\ndice\n\n[1] 4 4\n\ndice\n\n[1] 4 4\n\ndice\n\n[1] 4 4\n\n\nThe name dice refers to a vector of two numbers. Calling more than once does not change the favlue. Each time you call dice, R will show you the result of that one time you called sample and saved the output to dice. R won’t rerun sample(die, 2, replace = TRUE) to create a new roll of the dice. Once you save a set of results to an R object, those results do not change.\nHowever, it would be convenient to have an object that can re-roll the dice whenever you call it. You can make such an object by writing your own R function.",
    "crumbs": [
      "Introduction",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Up and Running with R</span>"
    ]
  },
  {
    "objectID": "r_basics.html#write-functions",
    "href": "r_basics.html#write-functions",
    "title": "\n3  Up and Running with R\n",
    "section": "\n3.4 Writing Your Own Functions",
    "text": "3.4 Writing Your Own Functions\nTo recap, you already have working R code that simulates rolling a pair of dice:\n\ndie &lt;- 1:6\ndice &lt;- sample(die, size = 2, replace = TRUE)\nsum(dice)\n\n[1] 7\n\n\nYou can retype this code into the console anytime you want to re-roll your dice. However, this is an awkward way to work with the code. It would be easier to use your code if you wrapped it into its own function, which is exactly what we’ll do now. We’re going to write a function named roll that you can use to roll your virtual dice. When you’re finished, the function will work like this: each time you call roll(), R will return the sum of rolling two dice:\nroll()\n## 8 \n\nroll()\n## 3\n\nroll()\n## 7\nFunctions may seem mysterious or fancy, but they are just another type of R object. Instead of containing data, they contain code. This code is stored in a special format that makes it easy to reuse the code in new situations. You can write your own functions by recreating this format.\n\n3.4.1 The Function Constructor\nEvery function in R has three basic parts: a name, a body of code, and a set of arguments. To make your own function, you need to replicate these parts and store them in an R object, which you can do with the function function. To do this, call function() and follow it with a pair of braces, {}:\n\nmy_function &lt;- function() {}\n\nThis function, as written, doesn’t do anything (yet). However, it is a valid function. You can call it by typing its name followed by an open and closed parenthesis:\n\nmy_function()\n\nNULL\n\n\nfunction will build a function out of whatever R code you place between the braces. For example, you can turn your dice code into a function by calling:\n\nroll &lt;- function() {\n  die &lt;- 1:6\n  dice &lt;- sample(die, size = 2, replace = TRUE)\n  sum(dice)\n}\n\n\n\n\n\n\n\nIndentation and readability\n\n\n\nNotice each line of code between the braces is indented. This makes the code easier to read but has no impact on how the code runs. R ignores spaces and line breaks and executes one complete expression at a time. Note that in other languages like python, spacing is extremely important and part of the language.\n\n\nJust hit the Enter key between each line after the first brace, {. R will wait for you to type the last brace, }, before it responds.\nDon’t forget to save the output of function to an R object. This object will become your new function. To use it, write the object’s name followed by an open and closed parenthesis:\n\nroll()\n\n[1] 9\n\n\nYou can think of the parentheses as the “trigger” that causes R to run the function. If you type in a function’s name without the parentheses, R will show you the code that is stored inside the function. If you type in the name with the parentheses, R will run that code:\n\nroll\n\nfunction() {\n  die &lt;- 1:6\n  dice &lt;- sample(die, size = 2, replace = TRUE)\n  sum(dice)\n}\n\nroll()\n\n[1] 2\n\n\nThe code that you place inside your function is known as the body of the function. When you run a function in R, R will execute all of the code in the body and then return the result of the last line of code. If the last line of code doesn’t return a value, neither will your function, so you want to ensure that your final line of code returns a value. One way to check this is to think about what would happen if you ran the body of code line by line in the command line. Would R display a result after the last line, or would it not?\nHere’s some code that would display a result:\ndice\n1 + 1\nsqrt(2)\nAnd here’s some code that would not:\ndice &lt;- sample(die, size = 2, replace = TRUE)\ntwo &lt;- 1 + 1\na &lt;- sqrt(2)\nAgain, this is just showing the distinction between expressions and assignments.",
    "crumbs": [
      "Introduction",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Up and Running with R</span>"
    ]
  },
  {
    "objectID": "r_basics.html#arguments",
    "href": "r_basics.html#arguments",
    "title": "\n3  Up and Running with R\n",
    "section": "\n3.5 Arguments",
    "text": "3.5 Arguments\nWhat if we removed one line of code from our function and changed the name die to bones (just a name–don’t think of it as important), like this?\n\nroll2 &lt;- function() {\n  dice &lt;- sample(bones, size = 2, replace = TRUE)\n  sum(dice)\n}\n\nNow I’ll get an error when I run the function. The function needs the object bones to do its job, but there is no object named bones to be found (you can check by typing ls() which will show you the names in the environment, or memory).\nroll2()\n## Error in sample(bones, size = 2, replace = TRUE) : \n##   object 'bones' not found\nYou can supply bones when you call roll2 if you make bones an argument of the function. To do this, put the name bones in the parentheses that follow function when you define roll2:\n\nroll2 &lt;- function(bones) {\n  dice &lt;- sample(bones, size = 2, replace = TRUE)\n  sum(dice)\n}\n\nNow roll2 will work as long as you supply bones when you call the function. You can take advantage of this to roll different types of dice each time you call roll2.\nRemember, we’re rolling pairs of dice:\n\nroll2(bones = 1:4)\n\n[1] 7\n\nroll2(bones = 1:6)\n\n[1] 2\n\nroll2(1:20)\n\n[1] 37\n\n\nNotice that roll2 will still give an error if you do not supply a value for the bones argument when you call roll2:\nroll2()\n## Error in sample(bones, size = 2, replace = TRUE) : \n##   argument \"bones\" is missing, with no default\nYou can prevent this error by giving the bones argument a default value. To do this, set bones equal to a value when you define roll2:\n\nroll2 &lt;- function(bones = 1:6) {\n  dice &lt;- sample(bones, size = 2, replace = TRUE)\n  sum(dice)\n}\n\nNow you can supply a new value for bones if you like, and roll2 will use the default if you do not:\n\nroll2()\n\n[1] 4\n\n\nYou can give your functions as many arguments as you like. Just list their names, separated by commas, in the parentheses that follow function. When the function is run, R will replace each argument name in the function body with the value that the user supplies for the argument. If the user does not supply a value, R will replace the argument name with the argument’s default value (if you defined one).\nTo summarize, function helps you construct your own R functions. You create a body of code for your function to run by writing code between the braces that follow function. You create arguments for your function to use by supplying their names in the parentheses that follow function. Finally, you give your function a name by saving its output to an R object, as shown in Figure 3.6.\nOnce you’ve created your function, R will treat it like every other function in R. Think about how useful this is. Have you ever tried to create a new Excel option and add it to Microsoft’s menu bar? Or a new slide animation and add it to Powerpoint’s options? When you work with a programming language, you can do these types of things. As you learn to program in R, you will be able to create new, customized, reproducible tools for yourself whenever you like.\n\n\n\n\n\nFigure 3.6: “Every function in R has the same parts, and you can use function to create these parts. Assign the result to a name, so you can call the function later.”",
    "crumbs": [
      "Introduction",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Up and Running with R</span>"
    ]
  },
  {
    "objectID": "r_basics.html#scripts",
    "href": "r_basics.html#scripts",
    "title": "\n3  Up and Running with R\n",
    "section": "\n3.6 Scripts",
    "text": "3.6 Scripts\nScripts are code that are saved for later reuse or editing. An R script is just a plain text file that you save R code in. You can open an R script in RStudio by going to File &gt; New File &gt; R script in the menu bar. RStudio will then open a fresh script above your console pane, as shown in Figure 3.7.\nI strongly encourage you to write and edit all of your R code in a script before you run it in the console. Why? This habit creates a reproducible record of your work. When you’re finished for the day, you can save your script and then use it to rerun your entire analysis the next day. Scripts are also very handy for editing and proofreading your code, and they make a nice copy of your work to share with others. To save a script, click the scripts pane, and then go to File &gt; Save As in the menu bar.\n\n\n\n\n\nFigure 3.7: “When you open an R Script (File &gt; New File &gt; R Script in the menu bar), RStudio creates a fourth pane (or puts a new tab in the existing pane) above the console where you can write and edit your code.”\n\n\nRStudio comes with many built-in features that make it easy to work with scripts. First, you can automatically execute a line of code in a script by clicking the Run button at the top of the editor panel.\nR will run whichever line of code your cursor is on. If you have a whole section highlighted, R will run the highlighted code. Alternatively, you can run the entire script by clicking the Source button. Don’t like clicking buttons? You can use Control + Return as a shortcut for the Run button. On Macs, that would be Command + Return.\nIf you’re not convinced about scripts, you soon will be. It becomes a pain to write multi-line code in the console’s single-line command line. Let’s avoid that headache and open your first script now before we move to the next chapter.\n\n\n\n\n\n\nTip\n\n\n\nExtract function\nRStudio comes with a tool that can help you build functions. To use it, highlight the lines of code in your R script that you want to turn into a function. Then click Code &gt; Extract Function in the menu bar. RStudio will ask you for a function name to use and then wrap your code in a function call. It will scan the code for undefined variables and use these as arguments.\nYou may want to double-check RStudio’s work. It assumes that your code is correct, so if it does something surprising, you may have a problem in your code.",
    "crumbs": [
      "Introduction",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Up and Running with R</span>"
    ]
  },
  {
    "objectID": "r_basics.html#summary",
    "href": "r_basics.html#summary",
    "title": "\n3  Up and Running with R\n",
    "section": "\n3.7 Summary",
    "text": "3.7 Summary\nWe’ve covered a lot of ground already. You now have a virtual die stored in your computer’s memory, as well as your own R function that rolls a pair of dice. You’ve also begun speaking the R language.\nThe two most important components of the R language are objects, which store data, and functions, which manipulate data. R also uses a host of operators like +, -, *, /, and &lt;- to do basic tasks. As a data scientist, you will use R objects to store data in your computer’s memory, and you will use functions to automate tasks and do complicated calculations.",
    "crumbs": [
      "Introduction",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Up and Running with R</span>"
    ]
  },
  {
    "objectID": "packages_and_dice.html",
    "href": "packages_and_dice.html",
    "title": "\n4  Packages and more dice\n",
    "section": "",
    "text": "4.1 Packages\nR is a powerful language for data science and programming, allowing beginners and experts alike to manipulate, analyze, and visualize data effectively. One of the most appealing features of R is its extensive library of packages, which are essential tools for expanding its capabilities and streamlining the coding process.\nAn R package is a collection of reusable functions, datasets, and compiled code created by other users and developers to extend the functionality of the base R language. These packages cover a wide range of applications, such as data manipulation, statistical analysis, machine learning, and data visualization. By utilizing existing R packages, you can leverage the expertise of others and save time by avoiding the need to create custom functions from scratch.\nUsing others’ R packages is incredibly beneficial as it allows you to take advantage of the collective knowledge of the R community. Developers often create packages to address specific challenges, optimize performance, or implement popular algorithms or methodologies. By incorporating these packages into your projects, you can enhance your productivity, reduce development time, and ensure that you are using well-tested and reliable code.",
    "crumbs": [
      "Introduction",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Packages and more dice</span>"
    ]
  },
  {
    "objectID": "packages_and_dice.html#packages",
    "href": "packages_and_dice.html#packages",
    "title": "\n4  Packages and more dice\n",
    "section": "",
    "text": "4.1.1 install.packages\nTo install an R package, you can use the install.packages() function in the R console or script. For example, to install the popular data manipulation package “dplyr,” simply type install.packages(“dplyr”). This command will download the package from the Comprehensive R Archive Network (CRAN) and install it on your local machine. Keep in mind that you only need to install a package once, unless you want to update it to a newer version.\nIn our case, we want to install the ggplot2 package.\n\ninstall.packages('ggplot2')\n\n\n4.1.2 library\nAfter installing an R package, you will need to load it into your R session before using its functions. To load a package, use the library() function followed by the package name, such as library(dplyr). Loading a package makes its functions and datasets available for use in your current R session. Note that you need to load a package every time you start a new R session.\n\nlibrary(ggplot2)\n\nNow, the functionality of the ggplot2 package is available in our R session.\n\n\n\n\n\n\nInstalling vs loading packages\n\n\n\nThe main thing to remember is that you only need to install a package once, but you need to load it with library each time you wish to use it in a new R session. R will unload all of its packages each time you close RStudio.\n\n\n\n4.1.3 Finding R packages\nFinding useful R packages can be done in several ways. First, browsing CRAN (https://cran.r-project.org/) and Bioconductor (more later, https://bioconductor.org) are an excellent starting points, as they host thousands of packages categorized by topic. Additionally, online forums like Stack Overflow and R-bloggers can provide valuable recommendations based on user experiences. Social media platforms such as Twitter, where developers and data scientists often share new packages and updates, can also be a helpful resource. Finally, don’t forget to ask your colleagues or fellow R users for their favorite packages, as they may have insights on which ones best suit your specific needs.",
    "crumbs": [
      "Introduction",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Packages and more dice</span>"
    ]
  },
  {
    "objectID": "packages_and_dice.html#are-our-dice-fair",
    "href": "packages_and_dice.html#are-our-dice-fair",
    "title": "\n4  Packages and more dice\n",
    "section": "\n4.2 Are our dice fair?",
    "text": "4.2 Are our dice fair?\nWell, let’s review our code.\n\nroll2 &lt;- function(bones = 1:6) {\n  dice = sample(bones, size = 2, replace = TRUE)\n  sum(dice)\n}\n\nIf our dice are fair, then each number should show up equally. What does the sum look like with our two dice?\n\n\n\n\n\nFigure 4.1: In an ideal world, a histogram of the results would look like this\n\n\nRead the help page for replicate (i.e., help(\"replicate\")). In short, it suggests that we can repeat our dice rolling as many times as we like and replicate will return a vector of the sums for each roll.\n\nrolls = replicate(n = 100, roll2())\n\nWhat does rolls look like?\n\nhead(rolls)\n\n[1] 8 3 5 7 4 6\n\nlength(rolls)\n\n[1] 100\n\nmean(rolls)\n\n[1] 6.92\n\nsummary(rolls)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n   2.00    6.00    7.00    6.92    9.00   12.00 \n\n\nThis looks like it roughly agrees with our sketched out ideal histogram in Figure 4.1. However, now that we’ve loaded the qplot function from the ggplot2 package, we can make a histogram of the data themselves.\n\nqplot(rolls, binwidth=1)\n\nWarning: `qplot()` was deprecated in ggplot2 3.4.0.\n\n\n\n\n\n\n\nFigure 4.2: Histogram of the sums from 100 rolls of our fair dice\n\n\n\n\nHow does your histogram look (and yours will be different from mine since we are sampling random values)? Is it what you expect?\nWhat happens to our histogram as we increase the number of replicates?\n\nrolls = replicate(n = 100000, roll2())\nqplot(rolls, binwidth=1)\n\n\n\n\n\n\nFigure 4.3: Histogram with 100000 rolls much more closely approximates the pyramidal shape we anticipated",
    "crumbs": [
      "Introduction",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Packages and more dice</span>"
    ]
  },
  {
    "objectID": "packages_and_dice.html#bonus-exercise",
    "href": "packages_and_dice.html#bonus-exercise",
    "title": "\n4  Packages and more dice\n",
    "section": "\n4.3 Bonus exercise",
    "text": "4.3 Bonus exercise\nHow would you change the roll2 function to weight the dice?",
    "crumbs": [
      "Introduction",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Packages and more dice</span>"
    ]
  },
  {
    "objectID": "reading_and_writing.html",
    "href": "reading_and_writing.html",
    "title": "\n5  Reading and writing data files\n",
    "section": "",
    "text": "5.1 Introduction\nIn this chapter, we will discuss how to read and write data files in R. Data files are essential for storing and sharing data across different platforms and applications. R provides a variety of functions and packages to read and write data files in different formats, such as text files, CSV files, Excel files. By mastering these functions, you can efficiently import and export data in R, enabling you to perform data analysis and visualization tasks effectively.",
    "crumbs": [
      "Introduction",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Reading and writing data files</span>"
    ]
  },
  {
    "objectID": "reading_and_writing.html#csv-files",
    "href": "reading_and_writing.html#csv-files",
    "title": "\n5  Reading and writing data files\n",
    "section": "\n5.2 CSV files",
    "text": "5.2 CSV files\nComma-Separated Values (CSV) files are a common file format for storing tabular data. They consist of rows and columns, with each row representing a record and each column representing a variable or attribute. CSV files are widely used for data storage and exchange due to their simplicity and compatibility with various software applications. In R, you can read and write CSV files using the read.csv() and write.csv() functions, respectively. A commonly used alternative is to use the readr package, which provides faster and more user-friendly functions for reading and writing CSV files.\n\n5.2.1 Writing a CSV file\nSince we are going to use the readr package, we need to install it first. You can install the readr package using the following command:\n\ninstall.packages(\"readr\")\n\nOnce the package is installed, you can load it into your R session using the library() function:\n\nlibrary(readr)\n\nSince we don’t have a CSV file sitting around, let’s create a simple data frame to write to a CSV file. Here’s an example data frame:\n\ndf &lt;- data.frame(\n  id = c(1, 2, 3, 4, 5),\n  name = c(\"Alice\", \"Bob\", \"Charlie\", \"David\", \"Eve\"),\n  age = c(25, 30, 35, 40, 45)\n)\n\nNow, you can write this data frame to a CSV file using the write_csv() function from the readr package. Here’s how you can do it:\n\nwrite_csv(df, \"data.csv\")\n\nYou can check the current working directory to see if the CSV file was created successfully. If you want to specify a different directory or file path, you can provide the full path in the write_csv() function.\n\n# see what the current working directory is\ngetwd()\n\n[1] \"/Users/seandavis/Documents/git/RBiocBook\"\n\n# and check to see that the file was created\ndir(pattern = \"data.csv\")\n\n[1] \"data.csv\"\n\n\n\n5.2.2 Reading a CSV file\nNow that we have a CSV file, let’s read it back into R using the read_csv() function from the readr package. Here’s how you can do it:\n\ndf2 &lt;- read_csv(\"data.csv\")\n\nRows: 5 Columns: 3\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (1): name\ndbl (2): id, age\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nYou can check the structure of the data frame df2 to verify that the data was read correctly:\n\ndf2\n\n# A tibble: 5 × 3\n     id name      age\n  &lt;dbl&gt; &lt;chr&gt;   &lt;dbl&gt;\n1     1 Alice      25\n2     2 Bob        30\n3     3 Charlie    35\n4     4 David      40\n5     5 Eve        45\n\n\nThe readr package can read CSV files with various delimiters, headers, and data types, making it a versatile tool for handling tabular data in R. It can also read CSV files directly from web locations like so:\n\ndf3 &lt;- read_csv(\"https://data.cdc.gov/resource/pwn4-m3yp.csv\")\n\nRows: 1000 Columns: 10\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr  (1): state\ndbl  (6): tot_cases, new_cases, tot_deaths, new_deaths, new_historic_cases, ...\ndttm (3): date_updated, start_date, end_date\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nThe dataset that you just downloaded is described here: Covid-19 data from CDC",
    "crumbs": [
      "Introduction",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Reading and writing data files</span>"
    ]
  },
  {
    "objectID": "reading_and_writing.html#excel-files",
    "href": "reading_and_writing.html#excel-files",
    "title": "\n5  Reading and writing data files\n",
    "section": "\n5.3 Excel files",
    "text": "5.3 Excel files\nMicrosoft Excel files are another common file format for storing tabular data. Excel files can contain multiple sheets, formulas, and formatting options, making them a popular choice for data storage and analysis. In R, you can read and write Excel files using the readxl package. This package provides functions to import and export data from Excel files, enabling you to work with Excel data in R.\n\n5.3.1 Reading an Excel file\nTo read an Excel file in R, you need to install and load the readxl package. You can install the readxl package using the following command:\n\ninstall.packages(\"readxl\")\n\nOnce the package is installed, you can load it into your R session using the library() function:\n\nlibrary(readxl)\n\nNow, you can read an Excel file using the read_excel() function from the readxl package. We don’t have an excel file available, so let’s download one from the internet. Here’s an example:\n\ndownload.file('https://www.w3resource.com/python-exercises/pandas/excel/SaleData.xlsx', 'SaleData.xlsx')\n\nNow, you can read the Excel file into R using the read_excel() function:\n\ndf_excel &lt;- read_excel(\"SaleData.xlsx\")\n\nYou can check the structure of the data frame df_excel to verify that the data was read correctly:\n\ndf_excel\n\n# A tibble: 45 × 8\n   OrderDate           Region  Manager SalesMan  Item  Units Unit_price Sale_amt\n   &lt;dttm&gt;              &lt;chr&gt;   &lt;chr&gt;   &lt;chr&gt;     &lt;chr&gt; &lt;dbl&gt;      &lt;dbl&gt;    &lt;dbl&gt;\n 1 2018-01-06 00:00:00 East    Martha  Alexander Tele…    95       1198   113810\n 2 2018-01-23 00:00:00 Central Hermann Shelli    Home…    50        500    25000\n 3 2018-02-09 00:00:00 Central Hermann Luis      Tele…    36       1198    43128\n 4 2018-02-26 00:00:00 Central Timothy David     Cell…    27        225     6075\n 5 2018-03-15 00:00:00 West    Timothy Stephen   Tele…    56       1198    67088\n 6 2018-04-01 00:00:00 East    Martha  Alexander Home…    60        500    30000\n 7 2018-04-18 00:00:00 Central Martha  Steven    Tele…    75       1198    89850\n 8 2018-05-05 00:00:00 Central Hermann Luis      Tele…    90       1198   107820\n 9 2018-05-22 00:00:00 West    Douglas Michael   Tele…    32       1198    38336\n10 2018-06-08 00:00:00 East    Martha  Alexander Home…    60        500    30000\n# ℹ 35 more rows\n\n\nThe readxl package provides various options to read Excel files with multiple sheets, specific ranges, and data types, making it a versatile tool for handling Excel data in R.\n\n5.3.2 Writing an Excel file\nTo write an Excel file in R, you can use the write_xlsx() function from the writexl package. You can install the writexl package using the following command:\n\ninstall.packages(\"writexl\")\n\nOnce the package is installed, you can load it into your R session using the library() function:\n\nlibrary(writexl)\n\nThe write_xlsx() function allows you to write a data frame to an Excel file. Here’s an example:\n\nwrite_xlsx(df, \"data.xlsx\")\n\nYou can check the current working directory to see if the Excel file was created successfully. If you want to specify a different directory or file path, you can provide the full path in the write_xlsx() function.\n\n# see what the current working directory is\ngetwd()\n\n[1] \"/Users/seandavis/Documents/git/RBiocBook\"\n\n# and check to see that the file was created\ndir(pattern = \"data.xlsx\")\n\n[1] \"data.xlsx\"",
    "crumbs": [
      "Introduction",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Reading and writing data files</span>"
    ]
  },
  {
    "objectID": "reading_and_writing.html#additional-options",
    "href": "reading_and_writing.html#additional-options",
    "title": "\n5  Reading and writing data files\n",
    "section": "\n5.4 Additional options",
    "text": "5.4 Additional options\n\nGoogle Sheets: You can read and write data from Google Sheets using the googlesheets4 package. This package provides functions to interact with Google Sheets, enabling you to import and export data from Google Sheets to R.\nJSON files: You can read and write JSON files using the jsonlite package. This package provides functions to convert R objects to JSON format and vice versa, enabling you to work with JSON data in R.\nDatabase files: You can read and write data from database files using the DBI and RSQLite packages. These packages provide functions to interact with various database systems, enabling you to import and export data from databases to R.",
    "crumbs": [
      "Introduction",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Reading and writing data files</span>"
    ]
  },
  {
    "objectID": "dataviz.html",
    "href": "dataviz.html",
    "title": "6  Data Visualization with ggplot2",
    "section": "",
    "text": "Start with this worked example to get a feel for the ggplot2 package.\n\nhttps://rkabacoff.github.io/datavis/IntroGGPLOT.html\n\nThen, for more detail, I refer you to this excellent ggplot2 tutorial.\nFinally, for more R graphics inspiration, see the R Graph Gallery.",
    "crumbs": [
      "Introduction",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Data Visualization with ggplot2</span>"
    ]
  },
  {
    "objectID": "vectors.html",
    "href": "vectors.html",
    "title": "\n7  Vectors\n",
    "section": "",
    "text": "7.1 What is a Vector?\nA vector is the simplest and most basic data structure in R. It is a one-dimensional, ordered collection of elements, where all the elements are of the same data type. Vectors can store various types of data, such as numeric, character, or logical values. Figure 7.1 shows a pictorial representation of three vector examples.\nIn this chapter, we will provide a comprehensive overview of vectors, including how to create, access, and manipulate them. We will also discuss some unique properties and rules associated with vectors, and explore their applications in data analysis tasks.\nIn R, even a single value is a vector with length=1.\nz = 1\nz\n\n[1] 1\n\nlength(z)\n\n[1] 1\nIn the code above, we “assigned” the value 1 to the variable named z. Typing z by itself is an “expression” that returns a result which is, in this case, the value that we just assigned. The length method takes an R object and returns the R length. There are numerous ways of asking R about what an object represents, and length is one of them.\nVectors can contain numbers, strings (character data), or logical values (TRUE and FALSE) or other “atomic” data types Table 7.1. Vectors cannot contain a mix of types! We will introduce another data structure, the R list for situations when we need to store a mix of base R data types.",
    "crumbs": [
      "R Data Structures",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Vectors</span>"
    ]
  },
  {
    "objectID": "vectors.html#what-is-a-vector",
    "href": "vectors.html#what-is-a-vector",
    "title": "\n7  Vectors\n",
    "section": "",
    "text": "Figure 7.1: “Pictorial representation of three vector examples. The first vector is a numeric vector. The second is a ‘logical’ vector. The third is a character vector. Vectors also have indices and, optionally, names.”\n\n\n\n\n\n\n\n\n\n\n\nData type\nStores\n\n\n\nnumeric\nfloating point numbers\n\n\ninteger\nintegers\n\n\ncomplex\ncomplex numbers\n\n\nfactor\ncategorical data\n\n\ncharacter\nstrings\n\n\nlogical\nTRUE or FALSE\n\n\nNA\nmissing\n\n\nNULL\nempty\n\n\nfunction\nfunction type\n\n\n\n\n\nTable 7.1: Atomic (simplest) data types in R.",
    "crumbs": [
      "R Data Structures",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Vectors</span>"
    ]
  },
  {
    "objectID": "vectors.html#creating-vectors",
    "href": "vectors.html#creating-vectors",
    "title": "\n7  Vectors\n",
    "section": "\n7.2 Creating vectors",
    "text": "7.2 Creating vectors\nCharacter vectors (also sometimes called “string” vectors) are entered with each value surrounded by single or double quotes; either is acceptable, but they must match. They are always displayed by R with double quotes. Here are some examples of creating vectors:\n\n# examples of vectors\nc('hello','world')\n\n[1] \"hello\" \"world\"\n\nc(1,3,4,5,1,2)\n\n[1] 1 3 4 5 1 2\n\nc(1.12341e7,78234.126)\n\n[1] 11234100.00    78234.13\n\nc(TRUE,FALSE,TRUE,TRUE)\n\n[1]  TRUE FALSE  TRUE  TRUE\n\n# note how in the next case the TRUE is converted to \"TRUE\"\n# with quotes around it.\nc(TRUE,'hello')\n\n[1] \"TRUE\"  \"hello\"\n\n\nWe can also create vectors as “regular sequences” of numbers. For example:\n\n# create a vector of integers from 1 to 10\nx = 1:10\n# and backwards\nx = 10:1\n\nThe seq function can create more flexible regular sequences.\n\n# create a vector of numbers from 1 to 4 skipping by 0.3\ny = seq(1,4,0.3)\n\nAnd creating a new vector by concatenating existing vectors is possible, as well.\n\n# create a sequence by concatenating two other sequences\nz = c(y,x)\nz\n\n [1]  1.0  1.3  1.6  1.9  2.2  2.5  2.8  3.1  3.4  3.7  4.0 10.0  9.0  8.0  7.0\n[16]  6.0  5.0  4.0  3.0  2.0  1.0",
    "crumbs": [
      "R Data Structures",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Vectors</span>"
    ]
  },
  {
    "objectID": "vectors.html#vector-operations",
    "href": "vectors.html#vector-operations",
    "title": "\n7  Vectors\n",
    "section": "\n7.3 Vector Operations",
    "text": "7.3 Vector Operations\nOperations on a single vector are typically done element-by-element. For example, we can add 2 to a vector, 2 is added to each element of the vector and a new vector of the same length is returned.\n\nx = 1:10\nx + 2\n\n [1]  3  4  5  6  7  8  9 10 11 12\n\n\nIf the operation involves two vectors, the following rules apply. If the vectors are the same length: R simply applies the operation to each pair of elements.\n\nx + x\n\n [1]  2  4  6  8 10 12 14 16 18 20\n\n\nIf the vectors are different lengths, but one length a multiple of the other, R reuses the shorter vector as needed.\n\nx = 1:10\ny = c(1,2)\nx * y\n\n [1]  1  4  3  8  5 12  7 16  9 20\n\n\nIf the vectors are different lengths, but one length not a multiple of the other, R reuses the shorter vector as needed and delivers a warning.\n\nx = 1:10\ny = c(2,3,4)\nx * y\n\nWarning in x * y: longer object length is not a multiple of shorter object\nlength\n\n\n [1]  2  6 12  8 15 24 14 24 36 20\n\n\nTypical operations include multiplication (“*”), addition, subtraction, division, exponentiation (“^”), but many operations in R operate on vectors and are then called “vectorized”.\nBe aware of the recycling rule when working with vectors of different lengths, as it may lead to unexpected results if you’re not careful.",
    "crumbs": [
      "R Data Structures",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Vectors</span>"
    ]
  },
  {
    "objectID": "vectors.html#logical-vectors",
    "href": "vectors.html#logical-vectors",
    "title": "\n7  Vectors\n",
    "section": "\n7.4 Logical Vectors",
    "text": "7.4 Logical Vectors\nLogical vectors are vectors composed on only the values TRUE and FALSE. Note the all-upper-case and no quotation marks.\n\na = c(TRUE,FALSE,TRUE)\n\n# we can also create a logical vector from a numeric vector\n# 0 = false, everything else is 1\nb = c(1,0,217)\nd = as.logical(b)\nd\n\n[1]  TRUE FALSE  TRUE\n\n# test if a and d are the same at every element\nall.equal(a,d)\n\n[1] TRUE\n\n# We can also convert from logical to numeric\nas.numeric(a)\n\n[1] 1 0 1\n\n\n\n7.4.1 Logical Operators\nSome operators like &lt;, &gt;, ==, &gt;=, &lt;=, != can be used to create logical vectors.\n\n# create a numeric vector\nx = 1:10\n# testing whether x &gt; 5 creates a logical vector\nx &gt; 5\n\n [1] FALSE FALSE FALSE FALSE FALSE  TRUE  TRUE  TRUE  TRUE  TRUE\n\nx &lt;= 5\n\n [1]  TRUE  TRUE  TRUE  TRUE  TRUE FALSE FALSE FALSE FALSE FALSE\n\nx != 5\n\n [1]  TRUE  TRUE  TRUE  TRUE FALSE  TRUE  TRUE  TRUE  TRUE  TRUE\n\nx == 5\n\n [1] FALSE FALSE FALSE FALSE  TRUE FALSE FALSE FALSE FALSE FALSE\n\n\nWe can also assign the results to a variable:\n\ny = (x == 5)\ny\n\n [1] FALSE FALSE FALSE FALSE  TRUE FALSE FALSE FALSE FALSE FALSE",
    "crumbs": [
      "R Data Structures",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Vectors</span>"
    ]
  },
  {
    "objectID": "vectors.html#indexing-vectors",
    "href": "vectors.html#indexing-vectors",
    "title": "\n7  Vectors\n",
    "section": "\n7.5 Indexing Vectors",
    "text": "7.5 Indexing Vectors\nIn R, an index is used to refer to a specific element or set of elements in an vector (or other data structure). [R uses [ and ] to perform indexing, although other approaches to getting subsets of larger data structures are common in R.\n\nx = seq(0,1,0.1)\n# create a new vector from the 4th element of x\nx[4]\n\n[1] 0.3\n\n\nWe can even use other vectors to perform the “indexing”.\n\nx[c(3,5,6)]\n\n[1] 0.2 0.4 0.5\n\ny = 3:6\nx[y]\n\n[1] 0.2 0.3 0.4 0.5\n\n\nCombining the concept of indexing with the concept of logical vectors results in a very power combination.\n\n# use help('rnorm') to figure out what is happening next\nmyvec = rnorm(10)\n\n# create logical vector that is TRUE where myvec is &gt;0.25\ngt1 = (myvec &gt; 0.25)\nsum(gt1)\n\n[1] 5\n\n# and use our logical vector to create a vector of myvec values that are &gt;0.25\nmyvec[gt1]\n\n[1] 0.9920401 0.4123509 0.4837873 0.9089888 0.2869754\n\n# or &lt;=0.25 using the logical \"not\" operator, \"!\"\nmyvec[!gt1]\n\n[1]  0.2332562 -0.7828833 -1.2242465 -1.2165037 -0.2254581\n\n# shorter, one line approach\nmyvec[myvec &gt; 0.25]\n\n[1] 0.9920401 0.4123509 0.4837873 0.9089888 0.2869754",
    "crumbs": [
      "R Data Structures",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Vectors</span>"
    ]
  },
  {
    "objectID": "vectors.html#named-vectors",
    "href": "vectors.html#named-vectors",
    "title": "\n7  Vectors\n",
    "section": "\n7.6 Named Vectors",
    "text": "7.6 Named Vectors\nNamed vectors are vectors with labels or names assigned to their elements. These names can be used to access and manipulate the elements in a more meaningful way.\nTo create a named vector, use the names() function:\n\nfruit_prices &lt;- c(0.5, 0.75, 1.25)\nnames(fruit_prices) &lt;- c(\"apple\", \"banana\", \"cherry\")\nprint(fruit_prices)\n\n apple banana cherry \n  0.50   0.75   1.25 \n\n\nYou can also access and modify elements using their names:\n\nbanana_price &lt;- fruit_prices[\"banana\"]\nprint(banana_price)\n\nbanana \n  0.75 \n\nfruit_prices[\"apple\"] &lt;- 0.6\nprint(fruit_prices)\n\n apple banana cherry \n  0.60   0.75   1.25",
    "crumbs": [
      "R Data Structures",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Vectors</span>"
    ]
  },
  {
    "objectID": "vectors.html#character-vectors-a.k.a.-strings",
    "href": "vectors.html#character-vectors-a.k.a.-strings",
    "title": "\n7  Vectors\n",
    "section": "\n7.7 Character Vectors, A.K.A. Strings",
    "text": "7.7 Character Vectors, A.K.A. Strings\nR uses the paste function to concatenate strings.\n\npaste(\"abc\",\"def\")\n\n[1] \"abc def\"\n\npaste(\"abc\",\"def\",sep=\"THISSEP\")\n\n[1] \"abcTHISSEPdef\"\n\npaste0(\"abc\",\"def\")\n\n[1] \"abcdef\"\n\n## [1] \"abcdef\"\npaste(c(\"X\",\"Y\"),1:10)\n\n [1] \"X 1\"  \"Y 2\"  \"X 3\"  \"Y 4\"  \"X 5\"  \"Y 6\"  \"X 7\"  \"Y 8\"  \"X 9\"  \"Y 10\"\n\npaste(c(\"X\",\"Y\"),1:10,sep=\"_\")\n\n [1] \"X_1\"  \"Y_2\"  \"X_3\"  \"Y_4\"  \"X_5\"  \"Y_6\"  \"X_7\"  \"Y_8\"  \"X_9\"  \"Y_10\"\n\n\nWe can count the number of characters in a string.\n\nnchar('abc')\n\n[1] 3\n\nnchar(c('abc','d',123456))\n\n[1] 3 1 6\n\n\nPulling out parts of strings is also sometimes useful.\n\nsubstr('This is a good sentence.',start=10,stop=15)\n\n[1] \" good \"\n\n\nAnother common operation is to replace something in a string with something (a find-and-replace).\n\nsub('This','That','This is a good sentence.')\n\n[1] \"That is a good sentence.\"\n\n\nWhen we want to find all strings that match some other string, we can use grep, or “grab regular expression”.\n\ngrep('bcd',c('abcdef','abcd','bcde','cdef','defg'))\n\n[1] 1 2 3\n\ngrep('bcd',c('abcdef','abcd','bcde','cdef','defg'),value=TRUE)\n\n[1] \"abcdef\" \"abcd\"   \"bcde\"  \n\n\nRead about the grepl function (?grepl). Use that function to return a logical vector (TRUE/FALSE) for each entry above with an a in it.",
    "crumbs": [
      "R Data Structures",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Vectors</span>"
    ]
  },
  {
    "objectID": "vectors.html#missing-values-aka-na",
    "href": "vectors.html#missing-values-aka-na",
    "title": "\n7  Vectors\n",
    "section": "\n7.8 Missing Values, AKA “NA”",
    "text": "7.8 Missing Values, AKA “NA”\nR has a special value, “NA”, that represents a “missing” value, or Not Available, in a vector or other data structure. Here, we just create a vector to experiment.\n\nx = 1:5\nx\n\n[1] 1 2 3 4 5\n\nlength(x)\n\n[1] 5\n\n\n\nis.na(x)\n\n[1] FALSE FALSE FALSE FALSE FALSE\n\nx[2] = NA\nx\n\n[1]  1 NA  3  4  5\n\n\nThe length of x is unchanged, but there is one value that is marked as “missing” by virtue of being NA.\n\nlength(x)\n\n[1] 5\n\nis.na(x)\n\n[1] FALSE  TRUE FALSE FALSE FALSE\n\n\nWe can remove NA values by using indexing. In the following, is.na(x) returns a logical vector the length of x. The ! is the logical NOT operator and converts TRUE to FALSE and vice-versa.\n\nx[!is.na(x)]\n\n[1] 1 3 4 5",
    "crumbs": [
      "R Data Structures",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Vectors</span>"
    ]
  },
  {
    "objectID": "vectors.html#exercises",
    "href": "vectors.html#exercises",
    "title": "\n7  Vectors\n",
    "section": "\n7.9 Exercises",
    "text": "7.9 Exercises\n\n\nCreate a numeric vector called temperatures containing the following values: 72, 75, 78, 81, 76, 73.\n\nShow answertemperatures &lt;- c(72, 75, 78, 81, 76, 73, 93)\n\n\n\n\nCreate a character vector called days containing the following values: “Monday”, “Tuesday”, “Wednesday”, “Thursday”, “Friday”, “Saturday”, “Sunday”.\n\nShow answerdays &lt;- c(\"Monday\", \"Tuesday\", \"Wednesday\", \"Thursday\", \"Friday\", \"Saturday\", \"Sunday\")\n\n\n\n\nCalculate the average temperature for the week and store it in a variable called average_temperature.\n\nShow answeraverage_temperature &lt;- mean(temperatures)\n\n\n\n\nCreate a named vector called weekly_temperatures, where the names are the days of the week and the values are the temperatures from the temperatures vector.\n\nShow answerweekly_temperatures &lt;- temperatures\nnames(weekly_temperatures) &lt;- days\n\n\n\n\nCreate a numeric vector called ages containing the following values: 25, 30, 35, 40, 45, 50, 55, 60.\n\nShow answerages &lt;- c(25, 30, 35, 40, 45, 50, 55, 60)\n\n\n\n\nCreate a logical vector called is_adult by checking if the elements in the ages vector are greater than or equal to 18.\n\nShow answeris_adult &lt;- ages &gt;= 18\n\n\n\n\nCalculate the sum and product of the ages vector.\n\nShow answersum_ages &lt;- sum(ages)\nproduct_ages &lt;- prod(ages)\n\n\n\n\nExtract the ages greater than or equal to 40 from the ages vector and store them in a variable called older_ages.\n\nShow answerolder_ages &lt;- ages[ages &gt;= 40]",
    "crumbs": [
      "R Data Structures",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Vectors</span>"
    ]
  },
  {
    "objectID": "matrices.html",
    "href": "matrices.html",
    "title": "\n8  Matrices\n",
    "section": "",
    "text": "8.1 Creating a matrix\nThere are many ways to create a matrix in R. One of the simplest is to use the matrix() function. In the code below, we’ll create a matrix from a vector from 1:16.\nmat1 &lt;- matrix(1:16,nrow=4)\nmat1\n\n     [,1] [,2] [,3] [,4]\n[1,]    1    5    9   13\n[2,]    2    6   10   14\n[3,]    3    7   11   15\n[4,]    4    8   12   16\nThe same is possible, but specifying that the matrix be “filled” by row.\nmat1 &lt;- matrix(1:16,nrow=4,byrow = TRUE)\nmat1\n\n     [,1] [,2] [,3] [,4]\n[1,]    1    2    3    4\n[2,]    5    6    7    8\n[3,]    9   10   11   12\n[4,]   13   14   15   16\nNotice the subtle difference in the order that the numbers go into the matrix.\nWe can also build a matrix from parts by “binding” vectors together:\nx &lt;- 1:10 \ny &lt;- rnorm(10)\nEach of the vectors above is of length 10 and both are “numeric”, so we can make them into a matrix. Using rbind binds rows (r) into a matrix.\nmat &lt;- rbind(x,y)\nmat\n\n        [,1]      [,2]      [,3]       [,4]       [,5]       [,6]       [,7]\nx  1.0000000  2.000000 3.0000000  4.0000000  5.0000000  6.0000000  7.0000000\ny -0.2260241 -1.094141 0.1714483 -0.2555133 -0.1925184 -0.7936755 -0.8252991\n        [,8]      [,9]     [,10]\nx  8.0000000 9.0000000 10.000000\ny -0.8556162 0.7268338 -1.243649\nThe alternative to rbind is cbind that binds columns (c) together.\nmat &lt;- cbind(x,y)\nmat\n\n       x          y\n [1,]  1 -0.2260241\n [2,]  2 -1.0941408\n [3,]  3  0.1714483\n [4,]  4 -0.2555133\n [5,]  5 -0.1925184\n [6,]  6 -0.7936755\n [7,]  7 -0.8252991\n [8,]  8 -0.8556162\n [9,]  9  0.7268338\n[10,] 10 -1.2436490\nInspecting the names associated with rows and columns is often useful, particularly if the names have human meaning.\nrownames(mat)\n\nNULL\n\ncolnames(mat)\n\n[1] \"x\" \"y\"\nWe can also change the names of the matrix by assigning valid names to the columns or rows.\ncolnames(mat) = c('apples','oranges')\ncolnames(mat)\n\n[1] \"apples\"  \"oranges\"\n\nmat\n\n      apples    oranges\n [1,]      1 -0.2260241\n [2,]      2 -1.0941408\n [3,]      3  0.1714483\n [4,]      4 -0.2555133\n [5,]      5 -0.1925184\n [6,]      6 -0.7936755\n [7,]      7 -0.8252991\n [8,]      8 -0.8556162\n [9,]      9  0.7268338\n[10,]     10 -1.2436490\nMatrices have dimensions.\ndim(mat)\n\n[1] 10  2\n\nnrow(mat)\n\n[1] 10\n\nncol(mat)\n\n[1] 2",
    "crumbs": [
      "R Data Structures",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Matrices</span>"
    ]
  },
  {
    "objectID": "matrices.html#accessing-elements-of-a-matrix",
    "href": "matrices.html#accessing-elements-of-a-matrix",
    "title": "\n8  Matrices\n",
    "section": "\n8.2 Accessing elements of a matrix",
    "text": "8.2 Accessing elements of a matrix\nIndexing for matrices works as for vectors except that we now need to include both the row and column (in that order). We can access elements of a matrix using the square bracket [ indexing method. Elements can be accessed as var[r, c]. Here, r and c are vectors describing the elements of the matrix to select.\n\n\n\n\n\n\nImportant\n\n\n\nThe indices in R start with one, meaning that the first element of a vector or the first row/column of a matrix is indexed as one.\nThis is different from some other programming languages, such as Python, which use zero-based indexing, meaning that the first element of a vector or the first row/column of a matrix is indexed as zero.\nIt is important to be aware of this difference when working with data in R, especially if you are coming from a programming background that uses zero-based indexing. Using the wrong index can lead to unexpected results or errors in your code.\n\n\n\n# The 2nd element of the 1st row of mat\nmat[1,2]\n\n   oranges \n-0.2260241 \n\n# The first ROW of mat\nmat[1,]\n\n    apples    oranges \n 1.0000000 -0.2260241 \n\n# The first COLUMN of mat\nmat[,1]\n\n [1]  1  2  3  4  5  6  7  8  9 10\n\n# and all elements of mat that are &gt; 4; note no comma\nmat[mat&gt;4]\n\n[1]  5  6  7  8  9 10\n\n## [1]  5  6  7  8  9 10\n\n\n\n\n\n\n\nCaution\n\n\n\nNote that in the last case, there is no “,”, so R treats the matrix as a long vector (length=20). This is convenient, sometimes, but it can also be a source of error, as some code may “work” but be doing something unexpected.\n\n\nWe can also use indexing to exclude a row or column by prefixing the selection with a - sign.\n\nmat[,-1]       # remove first column\n\n [1] -0.2260241 -1.0941408  0.1714483 -0.2555133 -0.1925184 -0.7936755\n [7] -0.8252991 -0.8556162  0.7268338 -1.2436490\n\nmat[-c(1:5),]  # remove first five rows\n\n     apples    oranges\n[1,]      6 -0.7936755\n[2,]      7 -0.8252991\n[3,]      8 -0.8556162\n[4,]      9  0.7268338\n[5,]     10 -1.2436490",
    "crumbs": [
      "R Data Structures",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Matrices</span>"
    ]
  },
  {
    "objectID": "matrices.html#changing-values-in-a-matrix",
    "href": "matrices.html#changing-values-in-a-matrix",
    "title": "\n8  Matrices\n",
    "section": "\n8.3 Changing values in a matrix",
    "text": "8.3 Changing values in a matrix\nWe can create a matrix filled with random values drawn from a normal distribution for our work below.\n\nm = matrix(rnorm(20),nrow=10)\nsummary(m)\n\n       V1                V2         \n Min.   :-1.2254   Min.   :-1.4417  \n 1st Qu.:-0.6030   1st Qu.:-1.0352  \n Median : 0.3881   Median :-0.1091  \n Mean   : 0.2752   Mean   : 0.1471  \n 3rd Qu.: 1.1030   3rd Qu.: 0.9386  \n Max.   : 1.6500   Max.   : 2.8153  \n\n\nMultiplication and division works similarly to vectors. When multiplying by a vector, for example, the values of the vector are reused. In the simplest case, let’s multiply the matrix by a constant (vector of length 1).\n\n# multiply all values in the matrix by 20\nm2 = m*20\nsummary(m2)\n\n       V1                V2         \n Min.   :-24.508   Min.   :-28.834  \n 1st Qu.:-12.059   1st Qu.:-20.703  \n Median :  7.761   Median : -2.181  \n Mean   :  5.505   Mean   :  2.943  \n 3rd Qu.: 22.059   3rd Qu.: 18.772  \n Max.   : 33.000   Max.   : 56.306  \n\n\nBy combining subsetting with assignment, we can make changes to just part of a matrix.\n\n# and add 100 to the first column of m\nm2[,1] = m2[,1] + 100\n# summarize m\nsummary(m2)\n\n       V1               V2         \n Min.   : 75.49   Min.   :-28.834  \n 1st Qu.: 87.94   1st Qu.:-20.703  \n Median :107.76   Median : -2.181  \n Mean   :105.50   Mean   :  2.943  \n 3rd Qu.:122.06   3rd Qu.: 18.772  \n Max.   :133.00   Max.   : 56.306  \n\n\nA somewhat common transformation for a matrix is to transpose which changes rows to columns. One might need to do this if an assay output from a lab machine puts samples in rows and genes in columns, for example, while in Bioconductor/R, we often want the samples in columns and the genes in rows.\n\nt(m2)\n\n          [,1]      [,2]     [,3]      [,4]      [,5]      [,6]       [,7]\n[1,]  86.90116 113.48913 75.49234 133.00012 108.15383 129.18570 107.368657\n[2,] -27.62233  19.16009 56.30615 -14.23669  17.60645 -28.83382  -3.364782\n           [,8]      [,9]     [,10]\n[1,] 85.4795888  91.05885 124.91609\n[2,] -0.9980966 -22.85878  34.26772",
    "crumbs": [
      "R Data Structures",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Matrices</span>"
    ]
  },
  {
    "objectID": "matrices.html#calculations-on-matrix-rows-and-columns",
    "href": "matrices.html#calculations-on-matrix-rows-and-columns",
    "title": "\n8  Matrices\n",
    "section": "\n8.4 Calculations on matrix rows and columns",
    "text": "8.4 Calculations on matrix rows and columns\nAgain, we just need a matrix to play with. We’ll use rnorm again, but with a slight twist.\n\nm3 = matrix(rnorm(100,5,2),ncol=10) # what does the 5 mean here? And the 2?\n\nSince these data are from a normal distribution, we can look at a row (or column) to see what the mean and standard deviation are.\n\nmean(m3[,1])\n\n[1] 5.80262\n\nsd(m3[,1])\n\n[1] 2.140594\n\n# or a row\nmean(m3[1,])\n\n[1] 4.987707\n\nsd(m3[1,])\n\n[1] 1.03461\n\n\nThere are some useful convenience functions for computing means and sums of data in all of the columns and rows of matrices.\n\ncolMeans(m3)\n\n [1] 5.802620 5.460025 4.821321 4.992476 3.679565 5.079196 4.623018 4.522687\n [9] 4.943004 4.878935\n\nrowMeans(m3)\n\n [1] 4.987707 5.146057 5.552158 5.725254 4.228795 5.374928 4.062281 4.421042\n [9] 5.316340 3.988285\n\nrowSums(m3)\n\n [1] 49.87707 51.46057 55.52158 57.25254 42.28795 53.74928 40.62281 44.21042\n [9] 53.16340 39.88285\n\ncolSums(m3)\n\n [1] 58.02620 54.60025 48.21321 49.92476 36.79565 50.79196 46.23018 45.22687\n [9] 49.43004 48.78935\n\n\nWe can look at the distribution of column means:\n\n# save as a variable\ncmeans = colMeans(m3)\nsummary(cmeans)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  3.680   4.673   4.911   4.880   5.058   5.803 \n\n\nNote that this is centered pretty closely around the selected mean of 5 above.\nHow about the standard deviation? There is not a colSd function, but it turns out that we can easily apply functions that take vectors as input, like sd and “apply” them across either the rows (the first dimension) or columns (the second) dimension.\n\ncsds = apply(m3, 2, sd)\nsummary(csds)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  1.249   1.723   1.985   1.909   2.215   2.327 \n\n\nAgain, take a look at the distribution which is centered quite close to the selected standard deviation when we created our matrix.",
    "crumbs": [
      "R Data Structures",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Matrices</span>"
    ]
  },
  {
    "objectID": "matrices.html#exercises",
    "href": "matrices.html#exercises",
    "title": "\n8  Matrices\n",
    "section": "\n8.5 Exercises",
    "text": "8.5 Exercises\n\n8.5.1 Data preparation\nFor this set of exercises, we are going to rely on a dataset that comes with R. It gives the number of sunspots per month from 1749-1983. The dataset comes as a ts or time series data type which I convert to a matrix using the following code.\nJust run the code as is and focus on the rest of the exercises.\n\ndata(sunspots)\nsunspot_mat &lt;- matrix(as.vector(sunspots),ncol=12,byrow = TRUE)\ncolnames(sunspot_mat) &lt;- as.character(1:12)\nrownames(sunspot_mat) &lt;- as.character(1749:1983)\n\n\n8.5.2 Questions\n\n\nAfter the conversion above, what does sunspot_mat look like? Use functions to find the number of rows, the number of columns, the class, and some basic summary statistics.\n\nShow answerncol(sunspot_mat)\nnrow(sunspot_mat)\ndim(sunspot_mat)\nsummary(sunspot_mat)\nhead(sunspot_mat)\ntail(sunspot_mat)\n\n\n\n\nPractice subsetting the matrix a bit by selecting:\n\nThe first 10 years (rows)\nThe month of July (7th column)\nThe value for July, 1979 using the rowname to do the selection.\n\n\nShow answersunspot_mat[1:10,]\nsunspot_mat[,7]\nsunspot_mat['1979',7]\n\n\n\n\n\n\nThese next few exercises take advantage of the fact that calling a univariate statistical function (one that expects a vector) works for matrices by just making a vector of all the values in the matrix. What is the highest (max) number of sunspots recorded in these data?\n\nShow answermax(sunspot_mat)\n\n\n\n\nAnd the minimum?\n\nShow answermin(sunspot_mat)\n\n\n\n\nAnd the overall mean and median?\n\nShow answermean(sunspot_mat)\nmedian(sunspot_mat)\n\n\n\n\nUse the hist() function to look at the distribution of all the monthly sunspot data.\n\nShow answerhist(sunspot_mat)\n\n\n\n\nRead about the breaks argument to hist() to try to increase the number of breaks in the histogram to increase the resolution slightly. Adjust your hist() and breaks to your liking.\n\nShow answerhist(sunspot_mat, breaks=40)\n\n\n\n\nNow, let’s move on to summarizing the data a bit to learn about the pattern of sunspots varies by month or by year. Examine the dataset again. What do the columns represent? And the rows?\n\nShow answer# just a quick glimpse of the data will give us a sense\nhead(sunspot_mat)\n\n\n\n\nWe’d like to look at the distribution of sunspots by month. How can we do that?\n\nShow answer# the mean of the columns is the mean number of sunspots per month.\ncolMeans(sunspot_mat)\n\n# Another way to write the same thing:\napply(sunspot_mat, 2, mean)\n\n\n\n\nAssign the month summary above to a variable and summarize it to get a sense of the spread over months.\n\nShow answermonthmeans = colMeans(sunspot_mat)\nsummary(monthmeans)\n\n\n\n\nPlay the same game for years to get the per-year mean?\n\nShow answerymeans = rowMeans(sunspot_mat)\nsummary(ymeans)\n\n\n\n\nMake a plot of the yearly means. Do you see a pattern?\n\nShow answerplot(ymeans)\n# or make it clearer\nplot(ymeans, type='l')",
    "crumbs": [
      "R Data Structures",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Matrices</span>"
    ]
  },
  {
    "objectID": "dataframes_intro.html",
    "href": "dataframes_intro.html",
    "title": "\n9  Data Frames\n",
    "section": "",
    "text": "9.1 Learning goals",
    "crumbs": [
      "R Data Structures",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Data Frames</span>"
    ]
  },
  {
    "objectID": "dataframes_intro.html#learning-goals",
    "href": "dataframes_intro.html#learning-goals",
    "title": "\n9  Data Frames\n",
    "section": "",
    "text": "Understand how data.frames are different from matrices.\nKnow a few functions for examing the contents of a data.frame.\nList approaches for subsetting data.frames.\nBe able to load and save tabular data from and to disk.\nShow how to create a data.frames from scratch.",
    "crumbs": [
      "R Data Structures",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Data Frames</span>"
    ]
  },
  {
    "objectID": "dataframes_intro.html#learning-objectives",
    "href": "dataframes_intro.html#learning-objectives",
    "title": "\n9  Data Frames\n",
    "section": "\n9.2 Learning objectives",
    "text": "9.2 Learning objectives\n\nLoad the yeast growth dataset into R using read.csv.\nExamine the contents of the dataset.\nUse subsetting to find genes that may be involved with nutrient metabolism and transport.\nSummarize data measurements by categories.",
    "crumbs": [
      "R Data Structures",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Data Frames</span>"
    ]
  },
  {
    "objectID": "dataframes_intro.html#dataset",
    "href": "dataframes_intro.html#dataset",
    "title": "\n9  Data Frames\n",
    "section": "\n9.3 Dataset",
    "text": "9.3 Dataset\nThe data used here are borrowed directly from the fantastic Bioconnector tutorials and are a cleaned up version of the data from Brauer et al. Coordination of Growth Rate, Cell Cycle, Stress Response, and Metabolic Activity in Yeast (2008) Mol Biol Cell 19:352-367. These data are from a gene expression microarray, and in this paper the authors examine the relationship between growth rate and gene expression in yeast cultures limited by one of six different nutrients (glucose, leucine, ammonium, sulfate, phosphate, uracil). If you give yeast a rich media loaded with nutrients except restrict the supply of a single nutrient, you can control the growth rate to any rate you choose. By starving yeast of specific nutrients you can find genes that:\n\nRaise or lower their expression in response to growth rate. Growth-rate dependent expression patterns can tell us a lot about cell cycle control, and how the cell responds to stress. The authors found that expression of &gt;25% of all yeast genes is linearly correlated with growth rate, independent of the limiting nutrient. They also found that the subset of negatively growth-correlated genes is enriched for peroxisomal functions, and positively correlated genes mainly encode ribosomal functions.\nRespond differently when different nutrients are being limited. If you see particular genes that respond very differently when a nutrient is sharply restricted, these genes might be involved in the transport or metabolism of that specific nutrient.\n\nThe dataset can be downloaded directly from:\n\nbrauer2007_tidy.csv\n\nWe are going to read this dataset into R and then use it as a playground for learning about data.frames.",
    "crumbs": [
      "R Data Structures",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Data Frames</span>"
    ]
  },
  {
    "objectID": "dataframes_intro.html#reading-in-data",
    "href": "dataframes_intro.html#reading-in-data",
    "title": "\n9  Data Frames\n",
    "section": "\n9.4 Reading in data",
    "text": "9.4 Reading in data\nR has many capabilities for reading in data. Many of the functions have names that help us to understand what data format is to be expected. In this case, the filename that we want to read ends in .csv, meaning comma-separated-values. The read.csv() function reads in .csv files. As usual, it is worth reading help('read.csv') to get a better sense of the possible bells-and-whistles.\nThe read.csv() function can read directly from a URL, so we do not need to download the file directly. This dataset is relatively large (about 16MB), so this may take a bit depending on your network connection speed.\n\noptions(width=60)\n\n\nurl = paste0(\n    'https://raw.githubusercontent.com',\n    '/bioconnector/workshops/master/data/brauer2007_tidy.csv'\n)\nydat &lt;- read.csv(url)\n\nOur variable, ydat, now “contains” the downloaded and read data. We can check to see what data type read.csv gave us:\n\nclass(ydat)\n\n[1] \"data.frame\"",
    "crumbs": [
      "R Data Structures",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Data Frames</span>"
    ]
  },
  {
    "objectID": "dataframes_intro.html#inspecting-data.frames",
    "href": "dataframes_intro.html#inspecting-data.frames",
    "title": "\n9  Data Frames\n",
    "section": "\n9.5 Inspecting data.frames",
    "text": "9.5 Inspecting data.frames\nOur ydat variable is a data.frame. As I mentioned, the dataset is fairly large, so we will not be able to look at it all at once on the screen. However, R gives us many tools to inspect a data.frame.\n\nOverviews of content\n\n\nhead() to show first few rows\n\ntail() to show last few rows\n\n\nSize\n\n\ndim() for dimensions (rows, columns)\nnrow()\nncol()\n\nobject.size() for power users interested in the memory used to store an object\n\n\nData and attribute summaries\n\n\ncolnames() to get the names of the columns\n\nrownames() to get the “names” of the rows–may not be present\n\nsummary() to get per-column summaries of the data in the data.frame.\n\n\n\n\nhead(ydat)\n\n  symbol systematic_name nutrient rate expression\n1   SFB2         YNL049C  Glucose 0.05      -0.24\n2   &lt;NA&gt;         YNL095C  Glucose 0.05       0.28\n3   QRI7         YDL104C  Glucose 0.05      -0.02\n4   CFT2         YLR115W  Glucose 0.05      -0.33\n5   SSO2         YMR183C  Glucose 0.05       0.05\n6   PSP2         YML017W  Glucose 0.05      -0.69\n                            bp\n1        ER to Golgi transport\n2   biological process unknown\n3 proteolysis and peptidolysis\n4      mRNA polyadenylylation*\n5              vesicle fusion*\n6   biological process unknown\n                             mf\n1    molecular function unknown\n2    molecular function unknown\n3 metalloendopeptidase activity\n4                   RNA binding\n5              t-SNARE activity\n6    molecular function unknown\n\ntail(ydat)\n\n       symbol systematic_name nutrient rate expression\n198425   DOA1         YKL213C   Uracil  0.3       0.14\n198426   KRE1         YNL322C   Uracil  0.3       0.28\n198427   MTL1         YGR023W   Uracil  0.3       0.27\n198428   KRE9         YJL174W   Uracil  0.3       0.43\n198429   UTH1         YKR042W   Uracil  0.3       0.19\n198430   &lt;NA&gt;         YOL111C   Uracil  0.3       0.04\n                                               bp\n198425    ubiquitin-dependent protein catabolism*\n198426      cell wall organization and biogenesis\n198427      cell wall organization and biogenesis\n198428     cell wall organization and biogenesis*\n198429 mitochondrion organization and biogenesis*\n198430                 biological process unknown\n                                        mf\n198425          molecular function unknown\n198426 structural constituent of cell wall\n198427          molecular function unknown\n198428          molecular function unknown\n198429          molecular function unknown\n198430          molecular function unknown\n\ndim(ydat)\n\n[1] 198430      7\n\nnrow(ydat)\n\n[1] 198430\n\nncol(ydat)\n\n[1] 7\n\ncolnames(ydat)\n\n[1] \"symbol\"          \"systematic_name\" \"nutrient\"       \n[4] \"rate\"            \"expression\"      \"bp\"             \n[7] \"mf\"             \n\nsummary(ydat)\n\n    symbol          systematic_name      nutrient        \n Length:198430      Length:198430      Length:198430     \n Class :character   Class :character   Class :character  \n Mode  :character   Mode  :character   Mode  :character  \n                                                         \n                                                         \n                                                         \n      rate          expression             bp           \n Min.   :0.0500   Min.   :-6.500000   Length:198430     \n 1st Qu.:0.1000   1st Qu.:-0.290000   Class :character  \n Median :0.2000   Median : 0.000000   Mode  :character  \n Mean   :0.1752   Mean   : 0.003367                     \n 3rd Qu.:0.2500   3rd Qu.: 0.290000                     \n Max.   :0.3000   Max.   : 6.640000                     \n      mf           \n Length:198430     \n Class :character  \n Mode  :character  \n                   \n                   \n                   \n\n\nIn RStudio, there is an additional function, View() (note the capital “V”) that opens the first 1000 rows (default) in the RStudio window, akin to a spreadsheet view.\n\nView(ydat)",
    "crumbs": [
      "R Data Structures",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Data Frames</span>"
    ]
  },
  {
    "objectID": "dataframes_intro.html#accessing-variables-columns-and-subsetting",
    "href": "dataframes_intro.html#accessing-variables-columns-and-subsetting",
    "title": "\n9  Data Frames\n",
    "section": "\n9.6 Accessing variables (columns) and subsetting",
    "text": "9.6 Accessing variables (columns) and subsetting\nIn R, data.frames can be subset similarly to other two-dimensional data structures. The [ in R is used to denote subsetting of any kind. When working with two-dimensional data, we need two values inside the [ ] to specify the details. The specification is [rows, columns]. For example, to get the first three rows of ydat, use:\n\nydat[1:3, ]\n\n  symbol systematic_name nutrient rate expression\n1   SFB2         YNL049C  Glucose 0.05      -0.24\n2   &lt;NA&gt;         YNL095C  Glucose 0.05       0.28\n3   QRI7         YDL104C  Glucose 0.05      -0.02\n                            bp\n1        ER to Golgi transport\n2   biological process unknown\n3 proteolysis and peptidolysis\n                             mf\n1    molecular function unknown\n2    molecular function unknown\n3 metalloendopeptidase activity\n\n\nNote how the second number, the columns, is blank. R takes that to mean “all the columns”. Similarly, we can combine rows and columns specification arbitrarily.\n\nydat[1:3, 1:3]\n\n  symbol systematic_name nutrient\n1   SFB2         YNL049C  Glucose\n2   &lt;NA&gt;         YNL095C  Glucose\n3   QRI7         YDL104C  Glucose\n\n\nBecause selecting a single variable, or column, is such a common operation, there are two shortcuts for doing so with data.frames. The first, the $ operator works like so:\n\n# Look at the column names, just to refresh memory\ncolnames(ydat)\n\n[1] \"symbol\"          \"systematic_name\" \"nutrient\"       \n[4] \"rate\"            \"expression\"      \"bp\"             \n[7] \"mf\"             \n\n# Note that I am using \"head\" here to limit the output\nhead(ydat$symbol)\n\n[1] \"SFB2\" NA     \"QRI7\" \"CFT2\" \"SSO2\" \"PSP2\"\n\n# What is the actual length of \"symbol\"?\nlength(ydat$symbol)\n\n[1] 198430\n\n\nThe second is related to the fact that, in R, data.frames are also lists. We subset a list by using [[]] notation. To get the second column of ydat, we can use:\n\nhead(ydat[[2]])\n\n[1] \"YNL049C\" \"YNL095C\" \"YDL104C\" \"YLR115W\" \"YMR183C\"\n[6] \"YML017W\"\n\n\nAlternatively, we can use the column name:\n\nhead(ydat[[\"systematic_name\"]])\n\n[1] \"YNL049C\" \"YNL095C\" \"YDL104C\" \"YLR115W\" \"YMR183C\"\n[6] \"YML017W\"\n\n\n\n9.6.1 Some data exploration\nThere are a couple of columns that include numeric values. Which columns are numeric?\n\nclass(ydat$symbol)\n\n[1] \"character\"\n\nclass(ydat$rate)\n\n[1] \"numeric\"\n\nclass(ydat$expression)\n\n[1] \"numeric\"\n\n\nMake histograms of: - the expression values - the rate values\nWhat does the table() function do? Could you use that to look a the rate column given that that column appears to have repeated values?\nWhat rate corresponds to the most nutrient-starved condition?\n\n9.6.2 More advanced indexing and subsetting\nWe can use, for example, logical values (TRUE/FALSE) to subset data.frames.\n\nhead(ydat[ydat$symbol == 'LEU1', ])\n\n     symbol systematic_name nutrient rate expression   bp\nNA     &lt;NA&gt;            &lt;NA&gt;     &lt;NA&gt;   NA         NA &lt;NA&gt;\nNA.1   &lt;NA&gt;            &lt;NA&gt;     &lt;NA&gt;   NA         NA &lt;NA&gt;\nNA.2   &lt;NA&gt;            &lt;NA&gt;     &lt;NA&gt;   NA         NA &lt;NA&gt;\nNA.3   &lt;NA&gt;            &lt;NA&gt;     &lt;NA&gt;   NA         NA &lt;NA&gt;\nNA.4   &lt;NA&gt;            &lt;NA&gt;     &lt;NA&gt;   NA         NA &lt;NA&gt;\nNA.5   &lt;NA&gt;            &lt;NA&gt;     &lt;NA&gt;   NA         NA &lt;NA&gt;\n       mf\nNA   &lt;NA&gt;\nNA.1 &lt;NA&gt;\nNA.2 &lt;NA&gt;\nNA.3 &lt;NA&gt;\nNA.4 &lt;NA&gt;\nNA.5 &lt;NA&gt;\n\ntail(ydat[ydat$symbol == 'LEU1', ])\n\n         symbol systematic_name nutrient rate expression\nNA.47244   &lt;NA&gt;            &lt;NA&gt;     &lt;NA&gt;   NA         NA\nNA.47245   &lt;NA&gt;            &lt;NA&gt;     &lt;NA&gt;   NA         NA\nNA.47246   &lt;NA&gt;            &lt;NA&gt;     &lt;NA&gt;   NA         NA\nNA.47247   &lt;NA&gt;            &lt;NA&gt;     &lt;NA&gt;   NA         NA\nNA.47248   &lt;NA&gt;            &lt;NA&gt;     &lt;NA&gt;   NA         NA\nNA.47249   &lt;NA&gt;            &lt;NA&gt;     &lt;NA&gt;   NA         NA\n           bp   mf\nNA.47244 &lt;NA&gt; &lt;NA&gt;\nNA.47245 &lt;NA&gt; &lt;NA&gt;\nNA.47246 &lt;NA&gt; &lt;NA&gt;\nNA.47247 &lt;NA&gt; &lt;NA&gt;\nNA.47248 &lt;NA&gt; &lt;NA&gt;\nNA.47249 &lt;NA&gt; &lt;NA&gt;\n\n\nWhat is the problem with this approach? It appears that there are a bunch of NA values. Taking a quick look at the symbol column, we see what the problem.\n\nsummary(ydat$symbol)\n\n   Length     Class      Mode \n   198430 character character \n\n\nUsing the is.na() function, we can make filter further to get down to values of interest.\n\nhead(ydat[ydat$symbol == 'LEU1' & !is.na(ydat$symbol), ])\n\n      symbol systematic_name nutrient rate expression\n1526    LEU1         YGL009C  Glucose 0.05      -1.12\n7043    LEU1         YGL009C  Glucose 0.10      -0.77\n12555   LEU1         YGL009C  Glucose 0.15      -0.67\n18071   LEU1         YGL009C  Glucose 0.20      -0.59\n23603   LEU1         YGL009C  Glucose 0.25      -0.20\n29136   LEU1         YGL009C  Glucose 0.30       0.03\n                        bp\n1526  leucine biosynthesis\n7043  leucine biosynthesis\n12555 leucine biosynthesis\n18071 leucine biosynthesis\n23603 leucine biosynthesis\n29136 leucine biosynthesis\n                                          mf\n1526  3-isopropylmalate dehydratase activity\n7043  3-isopropylmalate dehydratase activity\n12555 3-isopropylmalate dehydratase activity\n18071 3-isopropylmalate dehydratase activity\n23603 3-isopropylmalate dehydratase activity\n29136 3-isopropylmalate dehydratase activity\n\n\nSometimes, looking at the data themselves is not that important. Using dim() is one possibility to look at the number of rows and columns after subsetting.\n\ndim(ydat[ydat$expression &gt; 3, ])\n\n[1] 714   7\n\n\nFind the high expressed genes when leucine-starved. For this task we can also use subset which allows us to treat column names as R variables (no $ needed).\n\nsubset(ydat, nutrient == 'Leucine' & rate == 0.05 & expression &gt; 3)\n\n       symbol systematic_name nutrient rate expression\n133768   QDR2         YIL121W  Leucine 0.05       4.61\n133772   LEU1         YGL009C  Leucine 0.05       3.84\n133858   BAP3         YDR046C  Leucine 0.05       4.29\n135186   &lt;NA&gt;         YPL033C  Leucine 0.05       3.43\n135187   &lt;NA&gt;         YLR267W  Leucine 0.05       3.23\n135288   HXT3         YDR345C  Leucine 0.05       5.16\n135963   TPO2         YGR138C  Leucine 0.05       3.75\n135965   YRO2         YBR054W  Leucine 0.05       4.40\n136102   GPG1         YGL121C  Leucine 0.05       3.08\n136109  HSP42         YDR171W  Leucine 0.05       3.07\n136119   HXT5         YHR096C  Leucine 0.05       4.90\n136151   &lt;NA&gt;         YJL144W  Leucine 0.05       3.06\n136152   MOH1         YBL049W  Leucine 0.05       3.43\n136153   &lt;NA&gt;         YBL048W  Leucine 0.05       3.95\n136189  HSP26         YBR072W  Leucine 0.05       4.86\n136231   NCA3         YJL116C  Leucine 0.05       4.03\n136233   &lt;NA&gt;         YBR116C  Leucine 0.05       3.28\n136486   &lt;NA&gt;         YGR043C  Leucine 0.05       3.07\n137443   ADH2         YMR303C  Leucine 0.05       4.15\n137448   ICL1         YER065C  Leucine 0.05       3.54\n137451   SFC1         YJR095W  Leucine 0.05       3.72\n137569   MLS1         YNL117W  Leucine 0.05       3.76\n                                              bp\n133768                       multidrug transport\n133772                      leucine biosynthesis\n133858                      amino acid transport\n135186                                  meiosis*\n135187                biological process unknown\n135288                          hexose transport\n135963                       polyamine transport\n135965                biological process unknown\n136102                       signal transduction\n136109                       response to stress*\n136119                          hexose transport\n136151                   response to dessication\n136152                biological process unknown\n136153                                      &lt;NA&gt;\n136189                       response to stress*\n136231 mitochondrion organization and biogenesis\n136233                                      &lt;NA&gt;\n136486                biological process unknown\n137443                             fermentation*\n137448                          glyoxylate cycle\n137451                       fumarate transport*\n137569                          glyoxylate cycle\n                                           mf\n133768         multidrug efflux pump activity\n133772 3-isopropylmalate dehydratase activity\n133858        amino acid transporter activity\n135186             molecular function unknown\n135187             molecular function unknown\n135288          glucose transporter activity*\n135963          spermine transporter activity\n135965             molecular function unknown\n136102             signal transducer activity\n136109               unfolded protein binding\n136119          glucose transporter activity*\n136151             molecular function unknown\n136152             molecular function unknown\n136153                                   &lt;NA&gt;\n136189               unfolded protein binding\n136231             molecular function unknown\n136233                                   &lt;NA&gt;\n136486                 transaldolase activity\n137443         alcohol dehydrogenase activity\n137448              isocitrate lyase activity\n137451 succinate:fumarate antiporter activity\n137569               malate synthase activity",
    "crumbs": [
      "R Data Structures",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Data Frames</span>"
    ]
  },
  {
    "objectID": "dataframes_intro.html#aggregating-data",
    "href": "dataframes_intro.html#aggregating-data",
    "title": "\n9  Data Frames\n",
    "section": "\n9.7 Aggregating data",
    "text": "9.7 Aggregating data\nAggregating data, or summarizing by category, is a common way to look for trends or differences in measurements between categories. Use aggregate to find the mean expression by gene symbol.\n\nhead(aggregate(ydat$expression, by=list( ydat$symbol), mean))\n\n  Group.1           x\n1    AAC1  0.52888889\n2    AAC3 -0.21628571\n3   AAD10  0.43833333\n4   AAD14 -0.07166667\n5   AAD16  0.24194444\n6    AAD4 -0.79166667\n\n# or \nhead(aggregate(expression ~ symbol, mean, data=ydat))\n\n  symbol  expression\n1   AAC1  0.52888889\n2   AAC3 -0.21628571\n3  AAD10  0.43833333\n4  AAD14 -0.07166667\n5  AAD16  0.24194444\n6   AAD4 -0.79166667",
    "crumbs": [
      "R Data Structures",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Data Frames</span>"
    ]
  },
  {
    "objectID": "dataframes_intro.html#creating-a-data.frame-from-scratch",
    "href": "dataframes_intro.html#creating-a-data.frame-from-scratch",
    "title": "\n9  Data Frames\n",
    "section": "\n9.8 Creating a data.frame from scratch",
    "text": "9.8 Creating a data.frame from scratch\nSometimes it is useful to combine related data into one object. For example, let’s simulate some data.\n\nsmoker = factor(rep(c(\"smoker\", \"non-smoker\"), each=50))\nsmoker_numeric = as.numeric(smoker)\nx = rnorm(100)\nrisk = x + 2*smoker_numeric\n\nWe have two varibles, risk and smoker that are related. We can make a data.frame out of them:\n\nsmoker_risk = data.frame(smoker = smoker, risk = risk)\nhead(smoker_risk)\n\n  smoker     risk\n1 smoker 2.268207\n2 smoker 5.170728\n3 smoker 4.066137\n4 smoker 3.470311\n5 smoker 3.708060\n6 smoker 4.309701\n\n\nR also has plotting shortcuts that work with data.frames to simplify plotting\n\nplot( risk ~ smoker, data=smoker_risk)",
    "crumbs": [
      "R Data Structures",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Data Frames</span>"
    ]
  },
  {
    "objectID": "dataframes_intro.html#saving-a-data.frame",
    "href": "dataframes_intro.html#saving-a-data.frame",
    "title": "\n9  Data Frames\n",
    "section": "\n9.9 Saving a data.frame",
    "text": "9.9 Saving a data.frame\nOnce we have a data.frame of interest, we may want to save it. The most portable way to save a data.frame is to use one of the write functions. In this case, let’s save the data as a .csv file.\n\nwrite.csv(smoker_risk, \"smoker_risk.csv\")",
    "crumbs": [
      "R Data Structures",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Data Frames</span>"
    ]
  },
  {
    "objectID": "factors.html",
    "href": "factors.html",
    "title": "\n10  Factors\n",
    "section": "",
    "text": "10.1 Factors\nA factor is a special type of vector, normally used to hold a categorical variable–such as smoker/nonsmoker, state of residency, zipcode–in many statistical functions. Such vectors have class “factor”. Factors are primarily used in Analysis of Variance (ANOVA) or other situations when “categories” are needed. When a factor is used as a predictor variable, the corresponding indicator variables are created (more later).\nNote of caution that factors in R often appear to be character vectors when printed, but you will notice that they do not have double quotes around them. They are stored in R as numbers with a key name, so sometimes you will note that the factor behaves like a numeric vector.\n# create the character vector\ncitizen&lt;-c(\"uk\",\"us\",\"no\",\"au\",\"uk\",\"us\",\"us\",\"no\",\"au\") \n\n# convert to factor\ncitizenf&lt;-factor(citizen)                                \ncitizen             \n\n[1] \"uk\" \"us\" \"no\" \"au\" \"uk\" \"us\" \"us\" \"no\" \"au\"\n\ncitizenf\n\n[1] uk us no au uk us us no au\nLevels: au no uk us\n\n# convert factor back to character vector\nas.character(citizenf)\n\n[1] \"uk\" \"us\" \"no\" \"au\" \"uk\" \"us\" \"us\" \"no\" \"au\"\n\n# convert to numeric vector\nas.numeric(citizenf)\n\n[1] 3 4 2 1 3 4 4 2 1\nR stores many data structures as vectors with “attributes” and “class” (just so you have seen this).\nattributes(citizenf)\n\n$levels\n[1] \"au\" \"no\" \"uk\" \"us\"\n\n$class\n[1] \"factor\"\n\nclass(citizenf)\n\n[1] \"factor\"\n\n# note that after unclassing, we can see the \n# underlying numeric structure again\nunclass(citizenf)\n\n[1] 3 4 2 1 3 4 4 2 1\nattr(,\"levels\")\n[1] \"au\" \"no\" \"uk\" \"us\"\nTabulating factors is a useful way to get a sense of the “sample” set available.\ntable(citizenf)\n\ncitizenf\nau no uk us \n 2  2  2  3",
    "crumbs": [
      "R Data Structures",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Factors</span>"
    ]
  },
  {
    "objectID": "dplyr_intro_msleep.html",
    "href": "dplyr_intro_msleep.html",
    "title": "\n11  Introduction to dplyr: mammal sleep dataset\n",
    "section": "",
    "text": "11.1 Learning goals",
    "crumbs": [
      "Exploratory data analysis",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Introduction to dplyr: mammal sleep dataset</span>"
    ]
  },
  {
    "objectID": "dplyr_intro_msleep.html#learning-goals",
    "href": "dplyr_intro_msleep.html#learning-goals",
    "title": "\n11  Introduction to dplyr: mammal sleep dataset\n",
    "section": "",
    "text": "Know that dplyr is just a different approach to manipulating data in data.frames.\nList the commonly used dplyr verbs and how they can be used to manipulate data.frames.\nShow how to aggregate and summarized data using dplyr\n\nKnow what the piping operator, |&gt;, is and how it can be used.",
    "crumbs": [
      "Exploratory data analysis",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Introduction to dplyr: mammal sleep dataset</span>"
    ]
  },
  {
    "objectID": "dplyr_intro_msleep.html#learning-objectives",
    "href": "dplyr_intro_msleep.html#learning-objectives",
    "title": "\n11  Introduction to dplyr: mammal sleep dataset\n",
    "section": "\n11.2 Learning objectives",
    "text": "11.2 Learning objectives\n\nSelect subsets of the mammal sleep dataset.\nReorder the dataset.\nAdd columns to the dataset based on existing columns.\nSummarize the amount of sleep by categorical variables using group_by and summarize.",
    "crumbs": [
      "Exploratory data analysis",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Introduction to dplyr: mammal sleep dataset</span>"
    ]
  },
  {
    "objectID": "dplyr_intro_msleep.html#what-is-dplyr",
    "href": "dplyr_intro_msleep.html#what-is-dplyr",
    "title": "\n11  Introduction to dplyr: mammal sleep dataset\n",
    "section": "\n11.3 What is dplyr?",
    "text": "11.3 What is dplyr?\nThe dplyr package is a specialized package for working with data.frames (and the related tibble) to transform and summarize tabular data with rows and columns. For another explanation of dplyr see the dplyr package vignette: Introduction to dplyr",
    "crumbs": [
      "Exploratory data analysis",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Introduction to dplyr: mammal sleep dataset</span>"
    ]
  },
  {
    "objectID": "dplyr_intro_msleep.html#why-is-dplyr-userful",
    "href": "dplyr_intro_msleep.html#why-is-dplyr-userful",
    "title": "\n11  Introduction to dplyr: mammal sleep dataset\n",
    "section": "\n11.4 Why Is dplyr userful?",
    "text": "11.4 Why Is dplyr userful?\ndplyr contains a set of functions–commonly called the dplyr “verbs”–that perform common data manipulations such as filtering for rows, selecting specific columns, re-ordering rows, adding new columns and summarizing data. In addition, dplyr contains a useful function to perform another common task which is the “split-apply-combine” concept.\nCompared to base functions in R, the functions in dplyr are often easier to work with, are more consistent in the syntax and are targeted for data analysis around data frames, instead of just vectors.",
    "crumbs": [
      "Exploratory data analysis",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Introduction to dplyr: mammal sleep dataset</span>"
    ]
  },
  {
    "objectID": "dplyr_intro_msleep.html#data-mammals-sleep",
    "href": "dplyr_intro_msleep.html#data-mammals-sleep",
    "title": "\n11  Introduction to dplyr: mammal sleep dataset\n",
    "section": "\n11.5 Data: Mammals Sleep",
    "text": "11.5 Data: Mammals Sleep\nThe msleep (mammals sleep) data set contains the sleep times and weights for a set of mammals and is available in the dagdata repository on github. This data set contains 83 rows and 11 variables. The data happen to be available as a dataset in the ggplot2 package. To get access to the msleep dataset, we need to first install the ggplot2 package.\n\ninstall.packages('ggplot2')\n\nThen, we can load the library.\n\nlibrary(ggplot2)\ndata(msleep)\n\nAs with many datasets in R, “help” is available to describe the dataset itself.\n\n?msleep\n\nThe columns are described in the help page, but are included here, also.\n\n\ncolumn name\nDescription\n\n\n\nname\ncommon name\n\n\ngenus\ntaxonomic rank\n\n\nvore\ncarnivore, omnivore or herbivore?\n\n\norder\ntaxonomic rank\n\n\nconservation\nthe conservation status of the mammal\n\n\nsleep_total\ntotal amount of sleep, in hours\n\n\nsleep_rem\nrem sleep, in hours\n\n\nsleep_cycle\nlength of sleep cycle, in hours\n\n\nawake\namount of time spent awake, in hours\n\n\nbrainwt\nbrain weight in kilograms\n\n\nbodywt\nbody weight in kilograms",
    "crumbs": [
      "Exploratory data analysis",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Introduction to dplyr: mammal sleep dataset</span>"
    ]
  },
  {
    "objectID": "dplyr_intro_msleep.html#dplyr-verbs",
    "href": "dplyr_intro_msleep.html#dplyr-verbs",
    "title": "\n11  Introduction to dplyr: mammal sleep dataset\n",
    "section": "\n11.6 dplyr verbs",
    "text": "11.6 dplyr verbs\nThe dplyr verbs are listed here. There are many other functions available in dplyr, but we will focus on just these.\n\n\n\n\n\n\ndplyr verbs\nDescription\n\n\n\nselect()\nselect columns\n\n\nfilter()\nfilter rows\n\n\narrange()\nre-order or arrange rows\n\n\nmutate()\ncreate new columns\n\n\nsummarise()\nsummarise values\n\n\ngroup_by()\nallows for group operations in the “split-apply-combine” concept",
    "crumbs": [
      "Exploratory data analysis",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Introduction to dplyr: mammal sleep dataset</span>"
    ]
  },
  {
    "objectID": "dplyr_intro_msleep.html#using-the-dplyr-verbs",
    "href": "dplyr_intro_msleep.html#using-the-dplyr-verbs",
    "title": "\n11  Introduction to dplyr: mammal sleep dataset\n",
    "section": "\n11.7 Using the dplyr verbs",
    "text": "11.7 Using the dplyr verbs\nThe two most basic functions are select() and filter(), which selects columns and filters rows respectively. What are the equivalent ways to select columns without dplyr? And filtering to include only specific rows?\nBefore proceeding, we need to install the dplyr package:\n\ninstall.packages('dplyr')\n\nAnd then load the library:\n\nlibrary(dplyr)\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\n\n\n11.7.1 Selecting columns: select()\n\nSelect a set of columns such as the name and the sleep_total columns.\n\nsleepData &lt;- select(msleep, name, sleep_total)\nhead(sleepData)\n\n# A tibble: 6 × 2\n  name                       sleep_total\n  &lt;chr&gt;                            &lt;dbl&gt;\n1 Cheetah                           12.1\n2 Owl monkey                        17  \n3 Mountain beaver                   14.4\n4 Greater short-tailed shrew        14.9\n5 Cow                                4  \n6 Three-toed sloth                  14.4\n\n\nTo select all the columns except a specific column, use the “-” (subtraction) operator (also known as negative indexing). For example, to select all columns except name:\n\nhead(select(msleep, -name))\n\n# A tibble: 6 × 10\n  genus      vore  order    conservation sleep_total sleep_rem sleep_cycle awake\n  &lt;chr&gt;      &lt;chr&gt; &lt;chr&gt;    &lt;chr&gt;              &lt;dbl&gt;     &lt;dbl&gt;       &lt;dbl&gt; &lt;dbl&gt;\n1 Acinonyx   carni Carnivo… lc                  12.1      NA        NA      11.9\n2 Aotus      omni  Primates &lt;NA&gt;                17         1.8      NA       7  \n3 Aplodontia herbi Rodentia nt                  14.4       2.4      NA       9.6\n4 Blarina    omni  Soricom… lc                  14.9       2.3       0.133   9.1\n5 Bos        herbi Artioda… domesticated         4         0.7       0.667  20  \n6 Bradypus   herbi Pilosa   &lt;NA&gt;                14.4       2.2       0.767   9.6\n# ℹ 2 more variables: brainwt &lt;dbl&gt;, bodywt &lt;dbl&gt;\n\n\nTo select a range of columns by name, use the “:” operator. Note that dplyr allows us to use the column names without quotes and as “indices” of the columns.\n\nhead(select(msleep, name:order))\n\n# A tibble: 6 × 4\n  name                       genus      vore  order       \n  &lt;chr&gt;                      &lt;chr&gt;      &lt;chr&gt; &lt;chr&gt;       \n1 Cheetah                    Acinonyx   carni Carnivora   \n2 Owl monkey                 Aotus      omni  Primates    \n3 Mountain beaver            Aplodontia herbi Rodentia    \n4 Greater short-tailed shrew Blarina    omni  Soricomorpha\n5 Cow                        Bos        herbi Artiodactyla\n6 Three-toed sloth           Bradypus   herbi Pilosa      \n\n\nTo select all columns that start with the character string “sl”, use the function starts_with().\n\nhead(select(msleep, starts_with(\"sl\")))\n\n# A tibble: 6 × 3\n  sleep_total sleep_rem sleep_cycle\n        &lt;dbl&gt;     &lt;dbl&gt;       &lt;dbl&gt;\n1        12.1      NA        NA    \n2        17         1.8      NA    \n3        14.4       2.4      NA    \n4        14.9       2.3       0.133\n5         4         0.7       0.667\n6        14.4       2.2       0.767\n\n\nSome additional options to select columns based on a specific criteria include:\n\n\nends_with() = Select columns that end with a character string\n\ncontains() = Select columns that contain a character string\n\nmatches() = Select columns that match a regular expression\n\none_of() = Select column names that are from a group of names\n\n11.7.2 Selecting rows: filter()\n\nThe filter() function allows us to filter rows to include only those rows that match the filter. For example, we can filter the rows for mammals that sleep a total of more than 16 hours.\n\nfilter(msleep, sleep_total &gt;= 16)\n\n# A tibble: 8 × 11\n  name    genus vore  order conservation sleep_total sleep_rem sleep_cycle awake\n  &lt;chr&gt;   &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt;              &lt;dbl&gt;     &lt;dbl&gt;       &lt;dbl&gt; &lt;dbl&gt;\n1 Owl mo… Aotus omni  Prim… &lt;NA&gt;                17         1.8      NA       7  \n2 Long-n… Dasy… carni Cing… lc                  17.4       3.1       0.383   6.6\n3 North … Dide… omni  Dide… lc                  18         4.9       0.333   6  \n4 Big br… Epte… inse… Chir… lc                  19.7       3.9       0.117   4.3\n5 Thick-… Lutr… carni Dide… lc                  19.4       6.6      NA       4.6\n6 Little… Myot… inse… Chir… &lt;NA&gt;                19.9       2         0.2     4.1\n7 Giant … Prio… inse… Cing… en                  18.1       6.1      NA       5.9\n8 Arctic… Sper… herbi Rode… lc                  16.6      NA        NA       7.4\n# ℹ 2 more variables: brainwt &lt;dbl&gt;, bodywt &lt;dbl&gt;\n\n\nFilter the rows for mammals that sleep a total of more than 16 hours and have a body weight of greater than 1 kilogram.\n\nfilter(msleep, sleep_total &gt;= 16, bodywt &gt;= 1)\n\n# A tibble: 3 × 11\n  name    genus vore  order conservation sleep_total sleep_rem sleep_cycle awake\n  &lt;chr&gt;   &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt;              &lt;dbl&gt;     &lt;dbl&gt;       &lt;dbl&gt; &lt;dbl&gt;\n1 Long-n… Dasy… carni Cing… lc                  17.4       3.1       0.383   6.6\n2 North … Dide… omni  Dide… lc                  18         4.9       0.333   6  \n3 Giant … Prio… inse… Cing… en                  18.1       6.1      NA       5.9\n# ℹ 2 more variables: brainwt &lt;dbl&gt;, bodywt &lt;dbl&gt;\n\n\nFilter the rows for mammals in the Perissodactyla and Primates taxonomic order. The %in% operator is a logical operator that returns TRUE for values of a vector that are present in a second vector.\n\nfilter(msleep, order %in% c(\"Perissodactyla\", \"Primates\"))\n\n# A tibble: 15 × 11\n   name   genus vore  order conservation sleep_total sleep_rem sleep_cycle awake\n   &lt;chr&gt;  &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt;              &lt;dbl&gt;     &lt;dbl&gt;       &lt;dbl&gt; &lt;dbl&gt;\n 1 Owl m… Aotus omni  Prim… &lt;NA&gt;                17         1.8      NA       7  \n 2 Grivet Cerc… omni  Prim… lc                  10         0.7      NA      14  \n 3 Horse  Equus herbi Peri… domesticated         2.9       0.6       1      21.1\n 4 Donkey Equus herbi Peri… domesticated         3.1       0.4      NA      20.9\n 5 Patas… Eryt… omni  Prim… lc                  10.9       1.1      NA      13.1\n 6 Galago Gala… omni  Prim… &lt;NA&gt;                 9.8       1.1       0.55   14.2\n 7 Human  Homo  omni  Prim… &lt;NA&gt;                 8         1.9       1.5    16  \n 8 Mongo… Lemur herbi Prim… vu                   9.5       0.9      NA      14.5\n 9 Macaq… Maca… omni  Prim… &lt;NA&gt;                10.1       1.2       0.75   13.9\n10 Slow … Nyct… carni Prim… &lt;NA&gt;                11        NA        NA      13  \n11 Chimp… Pan   omni  Prim… &lt;NA&gt;                 9.7       1.4       1.42   14.3\n12 Baboon Papio omni  Prim… &lt;NA&gt;                 9.4       1         0.667  14.6\n13 Potto  Pero… omni  Prim… lc                  11        NA        NA      13  \n14 Squir… Saim… omni  Prim… &lt;NA&gt;                 9.6       1.4      NA      14.4\n15 Brazi… Tapi… herbi Peri… vu                   4.4       1         0.9    19.6\n# ℹ 2 more variables: brainwt &lt;dbl&gt;, bodywt &lt;dbl&gt;\n\n\nYou can use the boolean operators (e.g. &gt;, &lt;, &gt;=, &lt;=, !=, %in%) to create the logical tests.",
    "crumbs": [
      "Exploratory data analysis",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Introduction to dplyr: mammal sleep dataset</span>"
    ]
  },
  {
    "objectID": "dplyr_intro_msleep.html#piping-with",
    "href": "dplyr_intro_msleep.html#piping-with",
    "title": "\n11  Introduction to dplyr: mammal sleep dataset\n",
    "section": "\n11.8 “Piping”” with |>\n",
    "text": "11.8 “Piping”” with |&gt;\n\nIt is not unusual to want to perform a set of operations using dplyr. The pipe operator |&gt; allows us to “pipe” the output from one function into the input of the next. While there is nothing special about how R treats operations that are written in a pipe, the idea of piping is to allow us to read multiple functions operating one after another from left-to-right. Without piping, one would either 1) save each step in set of functions as a temporary variable and then pass that variable along the chain or 2) have to “nest” functions, which can be hard to read.\nHere’s an example we have already used:\n\nhead(select(msleep, name, sleep_total))\n\n# A tibble: 6 × 2\n  name                       sleep_total\n  &lt;chr&gt;                            &lt;dbl&gt;\n1 Cheetah                           12.1\n2 Owl monkey                        17  \n3 Mountain beaver                   14.4\n4 Greater short-tailed shrew        14.9\n5 Cow                                4  \n6 Three-toed sloth                  14.4\n\n\nNow in this case, we will pipe the msleep data frame to the function that will select two columns (name and sleep\\_total) and then pipe the new data frame to the function head(), which will return the head of the new data frame.\n\nmsleep |&gt; \n    select(name, sleep_total) |&gt; \n    head()\n\n# A tibble: 6 × 2\n  name                       sleep_total\n  &lt;chr&gt;                            &lt;dbl&gt;\n1 Cheetah                           12.1\n2 Owl monkey                        17  \n3 Mountain beaver                   14.4\n4 Greater short-tailed shrew        14.9\n5 Cow                                4  \n6 Three-toed sloth                  14.4\n\n\nYou will soon see how useful the pipe operator is when we start to combine many functions.\nNow that you know about the pipe operator (|&gt;), we will use it throughout the rest of this tutorial.\n\n11.8.1 Arrange Or Re-order Rows Using arrange()\n\nTo arrange (or re-order) rows by a particular column, such as the taxonomic order, list the name of the column you want to arrange the rows by:\n\nmsleep |&gt; arrange(order) |&gt; head()\n\n# A tibble: 6 × 11\n  name    genus vore  order conservation sleep_total sleep_rem sleep_cycle awake\n  &lt;chr&gt;   &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt;              &lt;dbl&gt;     &lt;dbl&gt;       &lt;dbl&gt; &lt;dbl&gt;\n1 Tenrec  Tenr… omni  Afro… &lt;NA&gt;                15.6       2.3      NA       8.4\n2 Cow     Bos   herbi Arti… domesticated         4         0.7       0.667  20  \n3 Roe de… Capr… herbi Arti… lc                   3        NA        NA      21  \n4 Goat    Capri herbi Arti… lc                   5.3       0.6      NA      18.7\n5 Giraffe Gira… herbi Arti… cd                   1.9       0.4      NA      22.1\n6 Sheep   Ovis  herbi Arti… domesticated         3.8       0.6      NA      20.2\n# ℹ 2 more variables: brainwt &lt;dbl&gt;, bodywt &lt;dbl&gt;\n\n\nNow we will select three columns from msleep, arrange the rows by the taxonomic order and then arrange the rows by sleep_total. Finally, show the head of the final data frame:\n\nmsleep |&gt; \n    select(name, order, sleep_total) |&gt;\n    arrange(order, sleep_total) |&gt; \n    head()\n\n# A tibble: 6 × 3\n  name     order        sleep_total\n  &lt;chr&gt;    &lt;chr&gt;              &lt;dbl&gt;\n1 Tenrec   Afrosoricida        15.6\n2 Giraffe  Artiodactyla         1.9\n3 Roe deer Artiodactyla         3  \n4 Sheep    Artiodactyla         3.8\n5 Cow      Artiodactyla         4  \n6 Goat     Artiodactyla         5.3\n\n\nSame as above, except here we filter the rows for mammals that sleep for 16 or more hours, instead of showing the head of the final data frame:\n\nmsleep |&gt; \n    select(name, order, sleep_total) |&gt;\n    arrange(order, sleep_total) |&gt; \n    filter(sleep_total &gt;= 16)\n\n# A tibble: 8 × 3\n  name                   order           sleep_total\n  &lt;chr&gt;                  &lt;chr&gt;                 &lt;dbl&gt;\n1 Big brown bat          Chiroptera             19.7\n2 Little brown bat       Chiroptera             19.9\n3 Long-nosed armadillo   Cingulata              17.4\n4 Giant armadillo        Cingulata              18.1\n5 North American Opossum Didelphimorphia        18  \n6 Thick-tailed opposum   Didelphimorphia        19.4\n7 Owl monkey             Primates               17  \n8 Arctic ground squirrel Rodentia               16.6\n\n\nFor something slightly more complicated do the same as above, except arrange the rows in the sleep_total column in a descending order. For this, use the function desc()\n\nmsleep |&gt; \n    select(name, order, sleep_total) |&gt;\n    arrange(order, desc(sleep_total)) |&gt; \n    filter(sleep_total &gt;= 16)\n\n# A tibble: 8 × 3\n  name                   order           sleep_total\n  &lt;chr&gt;                  &lt;chr&gt;                 &lt;dbl&gt;\n1 Little brown bat       Chiroptera             19.9\n2 Big brown bat          Chiroptera             19.7\n3 Giant armadillo        Cingulata              18.1\n4 Long-nosed armadillo   Cingulata              17.4\n5 Thick-tailed opposum   Didelphimorphia        19.4\n6 North American Opossum Didelphimorphia        18  \n7 Owl monkey             Primates               17  \n8 Arctic ground squirrel Rodentia               16.6",
    "crumbs": [
      "Exploratory data analysis",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Introduction to dplyr: mammal sleep dataset</span>"
    ]
  },
  {
    "objectID": "dplyr_intro_msleep.html#create-new-columns-using-mutate",
    "href": "dplyr_intro_msleep.html#create-new-columns-using-mutate",
    "title": "\n11  Introduction to dplyr: mammal sleep dataset\n",
    "section": "\n11.9 Create New Columns Using mutate()\n",
    "text": "11.9 Create New Columns Using mutate()\n\nThe mutate() function will add new columns to the data frame. Create a new column called rem_proportion, which is the ratio of rem sleep to total amount of sleep.\n\nmsleep |&gt; \n    mutate(rem_proportion = sleep_rem / sleep_total) |&gt;\n    head()\n\n# A tibble: 6 × 12\n  name    genus vore  order conservation sleep_total sleep_rem sleep_cycle awake\n  &lt;chr&gt;   &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt;              &lt;dbl&gt;     &lt;dbl&gt;       &lt;dbl&gt; &lt;dbl&gt;\n1 Cheetah Acin… carni Carn… lc                  12.1      NA        NA      11.9\n2 Owl mo… Aotus omni  Prim… &lt;NA&gt;                17         1.8      NA       7  \n3 Mounta… Aplo… herbi Rode… nt                  14.4       2.4      NA       9.6\n4 Greate… Blar… omni  Sori… lc                  14.9       2.3       0.133   9.1\n5 Cow     Bos   herbi Arti… domesticated         4         0.7       0.667  20  \n6 Three-… Brad… herbi Pilo… &lt;NA&gt;                14.4       2.2       0.767   9.6\n# ℹ 3 more variables: brainwt &lt;dbl&gt;, bodywt &lt;dbl&gt;, rem_proportion &lt;dbl&gt;\n\n\nYou can add many new columns using mutate (separated by commas). Here we add a second column called bodywt_grams which is the bodywt column in grams.\n\nmsleep |&gt; \n    mutate(rem_proportion = sleep_rem / sleep_total, \n           bodywt_grams = bodywt * 1000) |&gt;\n    head()\n\n# A tibble: 6 × 13\n  name    genus vore  order conservation sleep_total sleep_rem sleep_cycle awake\n  &lt;chr&gt;   &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt;              &lt;dbl&gt;     &lt;dbl&gt;       &lt;dbl&gt; &lt;dbl&gt;\n1 Cheetah Acin… carni Carn… lc                  12.1      NA        NA      11.9\n2 Owl mo… Aotus omni  Prim… &lt;NA&gt;                17         1.8      NA       7  \n3 Mounta… Aplo… herbi Rode… nt                  14.4       2.4      NA       9.6\n4 Greate… Blar… omni  Sori… lc                  14.9       2.3       0.133   9.1\n5 Cow     Bos   herbi Arti… domesticated         4         0.7       0.667  20  \n6 Three-… Brad… herbi Pilo… &lt;NA&gt;                14.4       2.2       0.767   9.6\n# ℹ 4 more variables: brainwt &lt;dbl&gt;, bodywt &lt;dbl&gt;, rem_proportion &lt;dbl&gt;,\n#   bodywt_grams &lt;dbl&gt;\n\n\nIs there a relationship between rem_proportion and bodywt? How about sleep_total?\n\n11.9.1 Create summaries: summarise()\n\nThe summarise() function will create summary statistics for a given column in the data frame such as finding the mean. For example, to compute the average number of hours of sleep, apply the mean() function to the column sleep_total and call the summary value avg_sleep.\n\nmsleep |&gt; \n    summarise(avg_sleep = mean(sleep_total))\n\n# A tibble: 1 × 1\n  avg_sleep\n      &lt;dbl&gt;\n1      10.4\n\n\nThere are many other summary statistics you could consider such sd(), min(), max(), median(), sum(), n() (returns the length of vector), first() (returns first value in vector), last() (returns last value in vector) and n_distinct() (number of distinct values in vector).\n\nmsleep |&gt; \n    summarise(avg_sleep = mean(sleep_total), \n              min_sleep = min(sleep_total),\n              max_sleep = max(sleep_total),\n              total = n())\n\n# A tibble: 1 × 4\n  avg_sleep min_sleep max_sleep total\n      &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt; &lt;int&gt;\n1      10.4       1.9      19.9    83",
    "crumbs": [
      "Exploratory data analysis",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Introduction to dplyr: mammal sleep dataset</span>"
    ]
  },
  {
    "objectID": "dplyr_intro_msleep.html#grouping-data-group_by",
    "href": "dplyr_intro_msleep.html#grouping-data-group_by",
    "title": "\n11  Introduction to dplyr: mammal sleep dataset\n",
    "section": "\n11.10 Grouping data: group_by()\n",
    "text": "11.10 Grouping data: group_by()\n\nThe group_by() verb is an important function in dplyr. The group_by allows us to use the concept of “split-apply-combine”. We literally want to split the data frame by some variable (e.g. taxonomic order), apply a function to the individual data frames and then combine the output. This approach is similar to the aggregate function from R, but group_by integrates with dplyr.\nLet’s do that: split the msleep data frame by the taxonomic order, then ask for the same summary statistics as above. We expect a set of summary statistics for each taxonomic order.\n\nmsleep |&gt; \n    group_by(order) |&gt;\n    summarise(avg_sleep = mean(sleep_total), \n              min_sleep = min(sleep_total), \n              max_sleep = max(sleep_total),\n              total = n())\n\n# A tibble: 19 × 5\n   order           avg_sleep min_sleep max_sleep total\n   &lt;chr&gt;               &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt; &lt;int&gt;\n 1 Afrosoricida        15.6       15.6      15.6     1\n 2 Artiodactyla         4.52       1.9       9.1     6\n 3 Carnivora           10.1        3.5      15.8    12\n 4 Cetacea              4.5        2.7       5.6     3\n 5 Chiroptera          19.8       19.7      19.9     2\n 6 Cingulata           17.8       17.4      18.1     2\n 7 Didelphimorphia     18.7       18        19.4     2\n 8 Diprotodontia       12.4       11.1      13.7     2\n 9 Erinaceomorpha      10.2       10.1      10.3     2\n10 Hyracoidea           5.67       5.3       6.3     3\n11 Lagomorpha           8.4        8.4       8.4     1\n12 Monotremata          8.6        8.6       8.6     1\n13 Perissodactyla       3.47       2.9       4.4     3\n14 Pilosa              14.4       14.4      14.4     1\n15 Primates            10.5        8        17      12\n16 Proboscidea          3.6        3.3       3.9     2\n17 Rodentia            12.5        7        16.6    22\n18 Scandentia           8.9        8.9       8.9     1\n19 Soricomorpha        11.1        8.4      14.9     5",
    "crumbs": [
      "Exploratory data analysis",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Introduction to dplyr: mammal sleep dataset</span>"
    ]
  },
  {
    "objectID": "eda_and_univariate_brfss.html",
    "href": "eda_and_univariate_brfss.html",
    "title": "\n12  Case Study: Behavioral Risk Factor Surveillance System\n",
    "section": "",
    "text": "12.1 A Case Study on the Behavioral Risk Factor Surveillance System\nThe Behavioral Risk Factor Surveillance System (BRFSS) is a large-scale health survey conducted annually by the Centers for Disease Control and Prevention (CDC) in the United States. The BRFSS collects information on various health-related behaviors, chronic health conditions, and the use of preventive services among the adult population (18 years and older) through telephone interviews. The main goal of the BRFSS is to identify and monitor the prevalence of risk factors associated with chronic diseases, inform public health policies, and evaluate the effectiveness of health promotion and disease prevention programs. The data collected through BRFSS is crucial for understanding the health status and needs of the population, and it serves as a valuable resource for researchers, policy makers, and healthcare professionals in making informed decisions and designing targeted interventions.\nIn this chapter, we will walk through an exploratory data analysis (EDA) of the Behavioral Risk Factor Surveillance System dataset using R. EDA is an important step in the data analysis process, as it helps you to understand your data, identify trends, and detect any anomalies before performing more advanced analyses. We will use various R functions and packages to explore the dataset, with a focus on active learning and hands-on experience.",
    "crumbs": [
      "Exploratory data analysis",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Case Study: Behavioral Risk Factor Surveillance System</span>"
    ]
  },
  {
    "objectID": "eda_and_univariate_brfss.html#loading-the-dataset",
    "href": "eda_and_univariate_brfss.html#loading-the-dataset",
    "title": "\n12  Case Study: Behavioral Risk Factor Surveillance System\n",
    "section": "\n12.2 Loading the Dataset",
    "text": "12.2 Loading the Dataset\nFirst, let’s load the dataset into R. We will use the read.csv() function from the base R package to read the data and store it in a data frame called brfss. Make sure the CSV file is in your working directory, or provide the full path to the file.\nFirst, we need to get the data. Either download the data from THIS LINK or have R do it directly from the command-line (preferred):\n\ndownload.file('https://raw.githubusercontent.com/seandavi/ITR/master/BRFSS-subset.csv',\n              destfile = 'BRFSS-subset.csv')\n\n\n\npath &lt;- file.choose()    # look for BRFSS-subset.csv\n\n\nstopifnot(file.exists(path))\nbrfss &lt;- read.csv(path)",
    "crumbs": [
      "Exploratory data analysis",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Case Study: Behavioral Risk Factor Surveillance System</span>"
    ]
  },
  {
    "objectID": "eda_and_univariate_brfss.html#inspecting-the-data",
    "href": "eda_and_univariate_brfss.html#inspecting-the-data",
    "title": "\n12  Case Study: Behavioral Risk Factor Surveillance System\n",
    "section": "\n12.3 Inspecting the Data",
    "text": "12.3 Inspecting the Data\nOnce the data is loaded, let’s take a look at the first few rows of the dataset using the head() function:\n\nhead(brfss)\n\n  Age   Weight    Sex Height Year\n1  31 48.98798 Female 157.48 1990\n2  57 81.64663 Female 157.48 1990\n3  43 80.28585   Male 177.80 1990\n4  72 70.30682   Male 170.18 1990\n5  31 49.89516 Female 154.94 1990\n6  58 54.43108 Female 154.94 1990\n\n\nThis will display the first six rows of the dataset, allowing you to get a feel for the data structure and variable types.\nNext, let’s check the dimensions of the dataset using the dim() function:\n\ndim(brfss)\n\n[1] 20000     5\n\n\nThis will return the number of rows and columns in the dataset, which is important to know for subsequent analyses.",
    "crumbs": [
      "Exploratory data analysis",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Case Study: Behavioral Risk Factor Surveillance System</span>"
    ]
  },
  {
    "objectID": "eda_and_univariate_brfss.html#summary-statistics",
    "href": "eda_and_univariate_brfss.html#summary-statistics",
    "title": "\n12  Case Study: Behavioral Risk Factor Surveillance System\n",
    "section": "\n12.4 Summary Statistics",
    "text": "12.4 Summary Statistics\nNow that we have a basic understanding of the data structure, let’s calculate some summary statistics. The summary() function in R provides a quick overview of the main statistics for each variable in the dataset:\n\nsummary(brfss)\n\n      Age            Weight           Sex                Height     \n Min.   :18.00   Min.   : 34.93   Length:20000       Min.   :105.0  \n 1st Qu.:36.00   1st Qu.: 61.69   Class :character   1st Qu.:162.6  \n Median :51.00   Median : 72.57   Mode  :character   Median :168.0  \n Mean   :50.99   Mean   : 75.42                      Mean   :169.2  \n 3rd Qu.:65.00   3rd Qu.: 86.18                      3rd Qu.:177.8  \n Max.   :99.00   Max.   :278.96                      Max.   :218.0  \n NA's   :139     NA's   :649                         NA's   :184    \n      Year     \n Min.   :1990  \n 1st Qu.:1990  \n Median :2000  \n Mean   :2000  \n 3rd Qu.:2010  \n Max.   :2010  \n               \n\n\nThis will display the minimum, first quartile, median, mean, third quartile, and maximum for each numeric variable, and the frequency counts for each factor level for categorical variables.",
    "crumbs": [
      "Exploratory data analysis",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Case Study: Behavioral Risk Factor Surveillance System</span>"
    ]
  },
  {
    "objectID": "eda_and_univariate_brfss.html#data-visualization",
    "href": "eda_and_univariate_brfss.html#data-visualization",
    "title": "\n12  Case Study: Behavioral Risk Factor Surveillance System\n",
    "section": "\n12.5 Data Visualization",
    "text": "12.5 Data Visualization\nVisualizing the data can help you identify patterns and trends in the dataset. Let’s start by creating a histogram of the Age variable using the hist() function.\nThis will create a histogram showing the frequency distribution of ages in the dataset. You can customize the appearance of the histogram by adjusting the parameters within the hist() function.\n\nhist(brfss$Age, main = \"Age Distribution\", \n     xlab = \"Age\", col = \"lightblue\")\n\n\n\n\n\n\n\n\n\n\n\n\n\nWhat are the options for a histogram?\n\n\n\nThe hist() function has many options. For example, you can change the number of bins, the color of the bars, the title, and the x-axis label. You can also add a vertical line at the mean or median, or add a normal curve to the histogram. For more information, type ?hist in the R console.\nMore generally, it is important to understand the options available for each function you use. You can do this by reading the documentation for the function, which can be accessed by typing ?function_name or help(\"function_name\")in the R console.\n\n\nNext, let’s create a boxplot to compare the distribution of Weight between males and females. We will use the boxplot() function for this. This will create a boxplot comparing the weight distribution between males and females. You can customize the appearance of the boxplot by adjusting the parameters within the boxplot() function.\n\nboxplot(brfss$Weight ~ brfss$Sex, main = \"Weight Distribution by Sex\", \n        xlab = \"Sex\", ylab = \"Weight\", col = c(\"pink\", \"lightblue\"))",
    "crumbs": [
      "Exploratory data analysis",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Case Study: Behavioral Risk Factor Surveillance System</span>"
    ]
  },
  {
    "objectID": "eda_and_univariate_brfss.html#analyzing-relationships-between-variables",
    "href": "eda_and_univariate_brfss.html#analyzing-relationships-between-variables",
    "title": "\n12  Case Study: Behavioral Risk Factor Surveillance System\n",
    "section": "\n12.6 Analyzing Relationships Between Variables",
    "text": "12.6 Analyzing Relationships Between Variables\nTo further explore the data, let’s investigate the relationship between age and weight using a scatterplot. We will use the plot() function for this:\nThis will create a scatterplot of age and weight, allowing you to visually assess the relationship between these two variables.\n\nplot(brfss$Age, brfss$Weight, main = \"Scatterplot of Age and Weight\", \n     xlab = \"Age\", ylab = \"Weight\", col = \"darkblue\")  \n\n\n\n\n\n\n\nTo quantify the strength of the relationship between age and weight, we can calculate the correlation coefficient using the cor() function:\nThis will return the correlation coefficient between age and weight, which can help you determine whether there is a linear relationship between these variables.\n\ncor(brfss$Age, brfss$Weight)\n\n[1] NA\n\n\nWhy does cor() give a value of NA? What can we do about it? A quick glance at help(\"cor\") will give you the answer.\n\ncor(brfss$Age, brfss$Weight, use = \"complete.obs\")\n\n[1] 0.02699989",
    "crumbs": [
      "Exploratory data analysis",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Case Study: Behavioral Risk Factor Surveillance System</span>"
    ]
  },
  {
    "objectID": "eda_and_univariate_brfss.html#exercises",
    "href": "eda_and_univariate_brfss.html#exercises",
    "title": "\n12  Case Study: Behavioral Risk Factor Surveillance System\n",
    "section": "\n12.7 Exercises",
    "text": "12.7 Exercises\n\n\nWhat is the mean weight in this dataset? How about the median? What is the difference between the two? What does this tell you about the distribution of weights in the dataset?\n\nShow answermean(brfss$Weight, na.rm = TRUE)\n\n[1] 75.42455\n\nShow answermedian(brfss$Weight, na.rm = TRUE)\n\n[1] 72.57478\n\nShow answermean(brfss$Weight, na.rm=TRUE) - median(brfss$Weight, na.rm = TRUE)\n\n[1] 2.849774\n\n\n\n\nGiven the findings about the mean and median in the previous exercise, use the hist() function to create a histogram of the weight distribution in this dataset. How would you describe the shape of this distribution?\n\nShow answerhist(brfss$Weight, xlab=\"Weight (kg)\", breaks = 30)\n\n\n\n\n\n\n\n\n\nUse plot() to examine the relationship between height and weight in this dataset.\n\nShow answerplot(brfss$Height, brfss$Weight)\n\n\n\n\n\n\n\n\n\nWhat is the correlation between height and weight? What does this tell you about the relationship between these two variables?\n\nShow answercor(brfss$Height, brfss$Weight, use = \"complete.obs\")\n\n[1] 0.5140928\n\n\n\n\nCreate a histogram of the height distribution in this dataset. How would you describe the shape of this distribution?\n\nShow answerhist(brfss$Height, xlab=\"Height (cm)\", breaks = 30)",
    "crumbs": [
      "Exploratory data analysis",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Case Study: Behavioral Risk Factor Surveillance System</span>"
    ]
  },
  {
    "objectID": "eda_and_univariate_brfss.html#conclusion",
    "href": "eda_and_univariate_brfss.html#conclusion",
    "title": "\n12  Case Study: Behavioral Risk Factor Surveillance System\n",
    "section": "\n12.8 Conclusion",
    "text": "12.8 Conclusion\nIn this chapter, we have demonstrated how to perform an exploratory data analysis on the Behavioral Risk Factor Surveillance System dataset using R. We covered data loading, inspection, summary statistics, visualization, and the analysis of relationships between variables. By actively engaging with the R code and data, you have gained valuable experience in using R for EDA and are well-equipped to tackle more complex analyses in your future work.\nRemember that EDA is just the beginning of the data analysis process, and further statistical modeling and hypothesis testing will likely be necessary to draw meaningful conclusions from your data. However, EDA is a crucial step in understanding your data and informing your subsequent analyses.",
    "crumbs": [
      "Exploratory data analysis",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Case Study: Behavioral Risk Factor Surveillance System</span>"
    ]
  },
  {
    "objectID": "eda_and_univariate_brfss.html#learn-about-the-data",
    "href": "eda_and_univariate_brfss.html#learn-about-the-data",
    "title": "\n12  Case Study: Behavioral Risk Factor Surveillance System\n",
    "section": "\n12.9 Learn about the data",
    "text": "12.9 Learn about the data\nUsing the data exploration techniques you have seen to explore the brfss dataset.\n\nsummary()\ndim()\ncolnames()\nhead()\ntail()\nclass()\nView()\n\nYou may want to investigate individual columns visually using plotting like hist(). For categorical data, consider using something like table().",
    "crumbs": [
      "Exploratory data analysis",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Case Study: Behavioral Risk Factor Surveillance System</span>"
    ]
  },
  {
    "objectID": "eda_and_univariate_brfss.html#clean-data",
    "href": "eda_and_univariate_brfss.html#clean-data",
    "title": "\n12  Case Study: Behavioral Risk Factor Surveillance System\n",
    "section": "\n12.10 Clean data",
    "text": "12.10 Clean data\nR read Year as an integer value, but it’s really a factor\n\nbrfss$Year &lt;- factor(brfss$Year)",
    "crumbs": [
      "Exploratory data analysis",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Case Study: Behavioral Risk Factor Surveillance System</span>"
    ]
  },
  {
    "objectID": "eda_and_univariate_brfss.html#weight-in-1990-vs.-2010-females",
    "href": "eda_and_univariate_brfss.html#weight-in-1990-vs.-2010-females",
    "title": "\n12  Case Study: Behavioral Risk Factor Surveillance System\n",
    "section": "\n12.11 Weight in 1990 vs. 2010 Females",
    "text": "12.11 Weight in 1990 vs. 2010 Females\n\nCreate a subset of the data\n\n\nbrfssFemale &lt;- brfss[brfss$Sex == \"Female\",]\nsummary(brfssFemale)\n\n      Age            Weight           Sex                Height     \n Min.   :18.00   Min.   : 36.29   Length:12039       Min.   :105.0  \n 1st Qu.:37.00   1st Qu.: 57.61   Class :character   1st Qu.:157.5  \n Median :52.00   Median : 65.77   Mode  :character   Median :163.0  \n Mean   :51.92   Mean   : 69.05                      Mean   :163.3  \n 3rd Qu.:67.00   3rd Qu.: 77.11                      3rd Qu.:168.0  \n Max.   :99.00   Max.   :272.16                      Max.   :200.7  \n NA's   :103     NA's   :560                         NA's   :140    \n   Year     \n 1990:5718  \n 2010:6321  \n            \n            \n            \n            \n            \n\n\n\nVisualize\n\n\nplot(Weight ~ Year, brfssFemale)\n\n\n\n\n\n\n\n\nStatistical test\n\n\nt.test(Weight ~ Year, brfssFemale)\n\n\n    Welch Two Sample t-test\n\ndata:  Weight by Year\nt = -27.133, df = 11079, p-value &lt; 2.2e-16\nalternative hypothesis: true difference in means between group 1990 and group 2010 is not equal to 0\n95 percent confidence interval:\n -8.723607 -7.548102\nsample estimates:\nmean in group 1990 mean in group 2010 \n          64.81838           72.95424",
    "crumbs": [
      "Exploratory data analysis",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Case Study: Behavioral Risk Factor Surveillance System</span>"
    ]
  },
  {
    "objectID": "eda_and_univariate_brfss.html#weight-and-height-in-2010-males",
    "href": "eda_and_univariate_brfss.html#weight-and-height-in-2010-males",
    "title": "\n12  Case Study: Behavioral Risk Factor Surveillance System\n",
    "section": "\n12.12 Weight and height in 2010 Males",
    "text": "12.12 Weight and height in 2010 Males\n\nCreate a subset of the data\n\n\nbrfss2010Male &lt;- subset(brfss,  Year == 2010 & Sex == \"Male\")\nsummary(brfss2010Male)\n\n      Age            Weight           Sex                Height      Year     \n Min.   :18.00   Min.   : 36.29   Length:3679        Min.   :135   1990:   0  \n 1st Qu.:45.00   1st Qu.: 77.11   Class :character   1st Qu.:173   2010:3679  \n Median :57.00   Median : 86.18   Mode  :character   Median :178              \n Mean   :56.25   Mean   : 88.85                      Mean   :178              \n 3rd Qu.:68.00   3rd Qu.: 99.79                      3rd Qu.:183              \n Max.   :99.00   Max.   :278.96                      Max.   :218              \n NA's   :30      NA's   :49                          NA's   :31               \n\n\n\nVisualize the relationship\n\n\nhist(brfss2010Male$Weight)\n\n\n\n\n\n\nhist(brfss2010Male$Height)\n\n\n\n\n\n\nplot(Weight ~ Height, brfss2010Male)\n\n\n\n\n\n\n\n\nFit a linear model (regression)\n\n\nfit &lt;- lm(Weight ~ Height, brfss2010Male)\nfit\n\n\nCall:\nlm(formula = Weight ~ Height, data = brfss2010Male)\n\nCoefficients:\n(Intercept)       Height  \n   -86.8747       0.9873  \n\n\nSummarize as ANOVA table\n\nanova(fit)\n\nAnalysis of Variance Table\n\nResponse: Weight\n            Df  Sum Sq Mean Sq F value    Pr(&gt;F)    \nHeight       1  197664  197664   693.8 &lt; 2.2e-16 ***\nResiduals 3617 1030484     285                      \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\nPlot points, superpose fitted regression line; where am I?\n\n\nplot(Weight ~ Height, brfss2010Male)\nabline(fit, col=\"blue\", lwd=2)\n# Substitute your own weight and height...\npoints(73 * 2.54, 178 / 2.2, col=\"red\", cex=4, pch=20)\n\n\n\n\n\n\n\n\nClass and available ‘methods’\n\n\nclass(fit)                 # 'noun'\nmethods(class=class(fit))  # 'verb'\n\n\nDiagnostics\n\n\nplot(fit)\n# Note that the \"plot\" above does not have a \".lm\"\n# However, R will use \"plot.lm\". Why?\n?plot.lm",
    "crumbs": [
      "Exploratory data analysis",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Case Study: Behavioral Risk Factor Surveillance System</span>"
    ]
  },
  {
    "objectID": "norm.html",
    "href": "norm.html",
    "title": "\n13  Working with distribution functions\n",
    "section": "",
    "text": "13.1 pnorm\nThis function gives the probability function for a normal distribution. If you do not specify the mean and standard deviation, R defaults to standard normal. Figure 13.1\npnorm(q, mean = 0, sd = 1, lower.tail = TRUE, log.p = FALSE)\nThe R help file for pnorm provides the template above. The value you input for q is a value on the x-axis, and the returned value is the area under the distribution curve to the left of that point.\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\nℹ Please use `linewidth` instead.\nThis function gives the probability function for a normal distribution. If you do not specify the mean and standard deviation, R defaults to standard normal.\npnorm(q, mean = 0, sd = 1, lower.tail = TRUE, log.p = FALSE) The R help file for pnorm provides the template above. The value you input for q is a value on the x-axis, and the returned value is the area under the distribution curve to the left of that point.\nThe option lower.tail = TRUE tells R to use the area to the left of the given point. This is the default, so will remain true even without entering it. In order to compute the area to the right of the given point, you can either switch to lower.tail = FALSE, or simply calculate 1-pnorm() instead. This is demonstrated below.\nFigure 13.1: The pnorm function takes a quantile (value on the x-axis) and returns the area under the curve to the left of that value.\n\n\n\n\n\n\n\n\n\nFigure 13.2: The pnorm function takes a quantile (value on the x-axis) and returns the area under the curve to the left of that value.\n\n\n\n\n\n\n\n\n\nFigure 13.3: The pnorm function takes a quantile (value on the x-axis) and returns the area under the curve to the left of that value.\n\n\n\n\n\n\n\n\n\nFigure 13.4: The pnorm function takes a quantile (value on the x-axis) and returns the area under the curve to the left of that value.\nThe option lower.tail = TRUE tells R to use the area to the left of the given point. This is the default, so will remain true even without entering it. In order to compute the area to the right of the given point, you can either switch to lower.tail = FALSE, or simply calculate 1-pnorm() instead.",
    "crumbs": [
      "statististics",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Working with distribution functions</span>"
    ]
  },
  {
    "objectID": "norm.html#dnorm",
    "href": "norm.html#dnorm",
    "title": "\n13  Working with distribution functions\n",
    "section": "\n13.2 dnorm",
    "text": "13.2 dnorm\nThis function calculates the probability density function (PDF) for the normal distribution. It gives the probability density (height of the curve) at a specified value (x).\n\n\n\n\n\n\n\nFigure 13.5: The dnorm function returns the height of the normal distribution at a given point.\n\n\n\n\n\n\n\n\n\nFigure 13.6: The dnorm function returns the height of the normal distribution at a given point.\n\n\n\n\n\n\n\n\n\nFigure 13.7: The dnorm function returns the height of the normal distribution at a given point.",
    "crumbs": [
      "statististics",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Working with distribution functions</span>"
    ]
  },
  {
    "objectID": "norm.html#qnorm",
    "href": "norm.html#qnorm",
    "title": "\n13  Working with distribution functions\n",
    "section": "\n13.3 qnorm",
    "text": "13.3 qnorm\nThis function calculates the quantiles of the normal distribution. It returns the value (x) corresponding to a specified probability (p). It is the inverse of thepnorm function.\n\n\n\n\n\n\n\n\n\n\n\nFigure 13.8: The qnorm function is the inverse of the pnorm function in that it takes a probability and gives the quantile.\n\n\n\n\n\n\n\n\n\nFigure 13.9: The qnorm function is the inverse of the pnorm function in that it takes a probability and gives the quantile.\n\n\n\n\n\n\n\n\n\n\n\nFigure 13.10: The qnorm function is the inverse of the pnorm function in that it takes a probability and gives the quantile.\n\n\n\n\n\n\n\n\n\nFigure 13.11: The qnorm function is the inverse of the pnorm function in that it takes a probability and gives the quantile.\n\n\n\n\n\n\n\n\n\n\n\nFigure 13.12: The qnorm function is the inverse of the pnorm function in that it takes a probability and gives the quantile.",
    "crumbs": [
      "statististics",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Working with distribution functions</span>"
    ]
  },
  {
    "objectID": "norm.html#rnorm",
    "href": "norm.html#rnorm",
    "title": "\n13  Working with distribution functions\n",
    "section": "\n13.4 rnorm",
    "text": "13.4 rnorm\n\nprint(r1)\n\n\n\n\n\n\nFigure 13.13: The rnorm function takes a number of samples and returns a vector of random numbers from the normal distribution (with mean=0, sd=1 as defaults)",
    "crumbs": [
      "statististics",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Working with distribution functions</span>"
    ]
  },
  {
    "objectID": "norm.html#iq-scores",
    "href": "norm.html#iq-scores",
    "title": "\n13  Working with distribution functions\n",
    "section": "\n13.5 IQ scores",
    "text": "13.5 IQ scores\nNormal Distribution and its Application with IQ\nThe normal distribution, also known as the Gaussian distribution, is a continuous probability distribution characterized by its bell-shaped curve. It is defined by two parameters: the mean (µ) and the standard deviation (σ). The mean represents the central tendency of the distribution, while the standard deviation represents the dispersion or spread of the data.\nThe IQ scores are an excellent example of the normal distribution, as they are designed to follow this distribution pattern. The mean IQ score is set at 100, and the standard deviation is set at 15. This means that the majority of the population (about 68%) have an IQ score between 85 and 115, while 95% of the population have an IQ score between 70 and 130.\n\n\nWhat is the probability of having an IQ score between 85 and 115?\n\nShow answerpnorm(115, mean = 100, sd = 15) - pnorm(85, mean = 100, sd = 15)\n\n\n\n\nWhat is the 90th percentile of the IQ scores?\n\nShow answerqnorm(0.9, mean = 100, sd = 15)\n\n\n\n\nWhat is the probability of having an IQ score above 130?\n\nShow answer1 - pnorm(130, mean = 100, sd = 15)\n\n\n\n\nWhat is the probability of having an IQ score below 70?\n\nShow answerpnorm(70, mean = 100, sd = 15)",
    "crumbs": [
      "statististics",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Working with distribution functions</span>"
    ]
  },
  {
    "objectID": "t-stats-and-tests.html",
    "href": "t-stats-and-tests.html",
    "title": "\n14  The t-statistic and t-distribution\n",
    "section": "",
    "text": "14.1 Background\nThe t-test is a statistical hypothesis test that is commonly used when the data are normally distributed (follow a normal distribution) if the value of the population standard deviation were known. When the population standard deviation is not known and is replaced by an estimate based no the data, the test statistic follows a Student’s t distribution.\nT-tests are handy hypothesis tests in statistics when you want to compare means. You can compare a sample mean to a hypothesized or target value using a one-sample t-test. You can compare the means of two groups with a two-sample t-test. If you have two groups with paired observations (e.g., before and after measurements), use the paired t-test.\nA t-test looks at the t-statistic, the t-distribution values, and the degrees of freedom to determine the statistical significance. To conduct a test with three or more means, we would use an analysis of variance.\nThe distriubution that the t-statistic follows was described in a famous paper (Student 1908) by “Student”, a pseudonym for William Sealy Gosset.",
    "crumbs": [
      "statististics",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>The t-statistic and t-distribution</span>"
    ]
  },
  {
    "objectID": "t-stats-and-tests.html#the-z-score-and-probability",
    "href": "t-stats-and-tests.html#the-z-score-and-probability",
    "title": "\n14  The t-statistic and t-distribution\n",
    "section": "\n14.2 The Z-score and probability",
    "text": "14.2 The Z-score and probability\nBefore talking about the t-distribution and t-scores, lets review the Z-score, its relation to the normal distribution, and probability.\nThe Z-score is defined as:\n\\[Z = \\frac{x - \\mu}{\\sigma} \\tag{14.1}\\]\nwhere \\(\\mu\\) is a the population mean from which \\(x\\) is drawn and \\(\\sigma\\) is the population standard deviation (taken as known, not estimated from the data).\nThe probability of observing a \\(Z\\) score of \\(z\\) or greater can be calculated by \\(pnorm(z,\\mu,\\sigma)\\).\nFor example, let’s assume that our “population” is known and it truly has a mean 0 and standard deviation 1. If we have observations drawn from that population, we can assign a probability of seeing that observation by random chance under the assumption that the null hypothesis is TRUE.\n\nzscore = seq(-5,5,1)\n\nFor each value of zscore, let’s calculate the p-value and put the results in a data.frame.\n\ndf = data.frame(\n    zscore = zscore,\n    pval   = pnorm(zscore, 0, 1)\n)\ndf\n\n   zscore         pval\n1      -5 2.866516e-07\n2      -4 3.167124e-05\n3      -3 1.349898e-03\n4      -2 2.275013e-02\n5      -1 1.586553e-01\n6       0 5.000000e-01\n7       1 8.413447e-01\n8       2 9.772499e-01\n9       3 9.986501e-01\n10      4 9.999683e-01\n11      5 9.999997e-01\n\n\nWhy is the p-value of something 5 population standard deviations away from the mean (zscore=5) nearly 1 in this calculation? What is the default for pnorm with respect to being one-sided or two-sided?\nLet’s plot the values of probability vs z-score:\n\nplot(df$zscore, df$pval, type='b')\n\n\n\n\n\n\n\nThis plot is the empirical cumulative density function (cdf) for our data. How can we use it? If we know the z-score, we can look up the probability of observing that value. Since we have constructed our experiment to follow the standard normal distribution, this cdf also represents the cdf of the standard normal distribution.\n\n14.2.1 Small diversion: two-sided pnorm function\nThe pnorm function returns the “one-sided” probability of having a value at least as extreme as the observed \\(x\\) and uses the “lower” tail by default. Let’s create a function that computes two-sided p-values.\n\nTake the absolute value of x\nCompute pnorm with lower.tail=FALSE so we get lower p-values with larger values of \\(x\\).\nSince we want to include both tails, we need to multiply the area (probability) returned by pnorm by 2.\n\n\ntwosidedpnorm = function(x,mu=0,sd=1) {\n    2*pnorm(abs(x),mu,sd,lower.tail=FALSE)\n}\n\nAnd we can test this to see how likely it is to be 2 or 3 standard deviations from the mean:\n\ntwosidedpnorm(2)\n\n[1] 0.04550026\n\ntwosidedpnorm(3)\n\n[1] 0.002699796",
    "crumbs": [
      "statististics",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>The t-statistic and t-distribution</span>"
    ]
  },
  {
    "objectID": "t-stats-and-tests.html#the-t-distribution",
    "href": "t-stats-and-tests.html#the-t-distribution",
    "title": "\n14  The t-statistic and t-distribution\n",
    "section": "\n14.3 The t-distribution",
    "text": "14.3 The t-distribution\nWe spent time above working with z-scores and probability. An important aspect of working with the normal distribution is that we MUST assume that we know the standard deviation. Remember that the Z-score is defined as:\n\\[Z = \\frac{x - \\mu}{\\sigma}\\]\nThe formula for the population standard deviation is:\n\\[\\sigma = \\sqrt{\\frac{1}{N}\\sum_{i=1}^{N}({xi - \\mu)^2}} \\tag{14.2}\\]\nIn general, the population standard deviation is taken as “known” as we did above.\nIf we do not but only have a sample from the population, instead of using the Z-score, we use the t-score defined as:\n\\[t = \\frac{x - \\bar{x}}{s} \\tag{14.3}\\]\nThis looks quite similar to the formula for Z-score, but here we have to estimate the standard deviation, \\(s\\) from the data. The formula for \\(s\\) is:\n\\[s = \\sqrt{\\frac{1}{N-1}\\sum_{i=1}^{N}({x_{i} - \\bar{x})^2}} \\tag{14.4}\\]\nSince we are estimating the standard deviation from the data, this leads to extra variability that shows up as “fatter tails” for smaller sample sizes than for larger sample sizes. We can see this by comparing the t-distribution for various numbers of degrees of freedom (sample sizes).\nWe can look at the effect of sample size on the distributions graphically by looking at the densities for 3, 5, 10, 20 degrees of freedom and the normal distribution:\n\nlibrary(dplyr)\nlibrary(ggplot2)\nt_values = seq(-6,6,0.01)\ndf = data.frame(\n    value = t_values,\n    t_3   = dt(t_values,3),\n    t_6   = dt(t_values,6),\n    t_10  = dt(t_values,10),\n    t_20  = dt(t_values,20),\n    Normal= dnorm(t_values)\n) |&gt;\n    tidyr::gather(\"Distribution\", \"density\", -value)\nggplot(df, aes(x=value, y=density, color=Distribution)) + \n    geom_line()\n\n\n\n\n\n\nFigure 14.1: t-distributions for various degrees of freedom. Note that the tails are fatter for smaller degrees of freedom, which is a result of estimating the standard deviation from the data.\n\n\n\n\nThe dt and dnorm functions give the density of the distributions for each point.\n\ndf2 = df |&gt; \n    group_by(Distribution) |&gt;\n    arrange(value) |&gt; \n    mutate(cdf=cumsum(density))\nggplot(df2, aes(x=value, y=cdf, color=Distribution)) + \n    geom_line()\n\n\n\n\n\n\n\n\n14.3.1 p-values based on Z vs t\nWhen we have a “sample” of data and want to compute the statistical significance of the difference of the mean from the population mean, we calculate the standard deviation of the sample means (standard error).\n\\[z = \\frac{x - \\mu}{\\sigma/\\sqrt{n}}\\]\nLet’s look at the relationship between the p-values of Z (from the normal distribution) vs t for a sample of data.\n\nset.seed(5432)\nsamp = rnorm(5,mean = 0.5)\nz = sqrt(length(samp)) * mean(samp) #simplifying assumption (sigma=1, mu=0)\n\nAnd the p-value if we assume we know the standard deviation:\n\npnorm(z, lower.tail = FALSE)\n\n[1] 0.02428316\n\n\nIn reality, we don’t know the standard deviation, so we have to estimate it from the data. We can do this by calculating the sample standard deviation:\n\nts = sqrt(length(samp)) * mean(samp) / sd(samp)\npnorm(ts, lower.tail = FALSE)\n\n[1] 0.0167297\n\npt(ts,df = length(samp)-1, lower.tail = FALSE)\n\n[1] 0.0503001\n\n\n\n14.3.2 Experiment\nWhen sampling from a normal distribution, we often calculate p-values to test hypotheses or determine the statistical significance of our results. The p-value represents the probability of obtaining a test statistic as extreme or more extreme than the one observed, under the null hypothesis.\nIn a typical scenario, we assume that the population mean and standard deviation are known. However, in many real-life situations, we don’t know the true population standard deviation, and we have to estimate it using the sample standard deviation (Equation 14.4). This estimation introduces some uncertainty into our calculations, which affects the p-values. When we include an estimate of the standard deviation, we switch from using the standard normal (z) distribution to the t-distribution for calculating p-values.\nWhat would happen if we used the normal distribution to calculate p-values when we use the sample standard deviation? Let’s find out!\n\nSimulate a bunch of samples of size n from the standard normal distribution\nCalculate the p-value distribution for those samples based on the normal.\nCalculate the p-value distribution for those samples based on the normal, but with the estimated standard deviation.\nCalculate the p-value distribution for those samples based on the t-distribution.\n\nCreate a function that draws a sample of size n from the standard normal distribution.\n\nzf = function(n) {\n    samp = rnorm(n)\n    z = sqrt(length(samp)) * mean(samp) / 1 #simplifying assumption (sigma=1, mu=0)\n    z\n}\n\nAnd give it a try:\n\nzf(5)\n\n[1] 0.7406094\n\n\nPerform 10000 replicates of our sampling and z-scoring. We are using the assumption that we know the population standard deviation; in this case, we do know since we are sampling from the standard normal distribution.\n\nz10k = replicate(10000,zf(5))\nhist(pnorm(z10k))\n\n\n\n\n\n\n\nAnd do the same, but now creating a t-score function. We are using the assumption that we don’t know the population standard deviation; in this case, we must estimate it from the data. Note the difference in the calculation of the t-score (ts) as compared to the z-score (z).\n\ntf = function(n) {\n    samp = rnorm(n)\n    # now, using the sample standard deviation since we \n    # \"don't know\" the population standard deviation\n    ts = sqrt(length(samp)) * mean(samp) / sd(samp)\n    ts\n}\n\nIf we use those t-scores and calculate the p-values based on the normal distribution, the histogram of those p-values looks like:\n\nt10k = replicate(10000,tf(5))\nhist(pnorm(t10k))\n\n\n\n\n\n\n\nSince we are using the normal distribution to calculate the p-values, we are, in effect, assuming that we know the population standard deviation. This assumption is incorrect, and we can see that the p-values are not uniformly distributed between 0 and 1.\nIf we use those t-scores and calculate the p-values based on the t-distribution, the histogram of those p-values looks like:\n\nhist(pt(t10k,5))\n\n\n\n\n\n\n\nNow, the p-values are uniformly distributed between 0 and 1, as expected.\nWhat is a qqplot and how do we use it? A qqplot is a plot of the quantiles of two distributions against each other. If the two distributions are identical, the points will fall on a straight line. If the two distributions are different, the points will deviate from the straight line. We can use a qqplot to compare the t-distribution to the normal distribution. If the t-distribution is identical to the normal distribution, the points will fall on a straight line. If the t-distribution is different from the normal distribution, the points will deviate from the straight line. In this case, we can see that the t-distribution is different from the normal distribution, as the points deviate from the straight line. What would happen if we increased the sample size? The t-distribution would approach the normal distribution, and the points would fall closer and closer to the straight line.\n\nqqplot(z10k,t10k)\nabline(0,1)",
    "crumbs": [
      "statististics",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>The t-statistic and t-distribution</span>"
    ]
  },
  {
    "objectID": "t-stats-and-tests.html#summary-of-t-distribution-vs-normal-distribution",
    "href": "t-stats-and-tests.html#summary-of-t-distribution-vs-normal-distribution",
    "title": "\n14  The t-statistic and t-distribution\n",
    "section": "\n14.4 Summary of t-distribution vs normal distribution",
    "text": "14.4 Summary of t-distribution vs normal distribution\nThe t-distribution is a family of probability distributions that depends on a parameter called degrees of freedom, which is related to the sample size. The t-distribution approaches the standard normal distribution as the sample size increases but has heavier tails for smaller sample sizes. This means that the t-distribution is more conservative in calculating p-values for small samples, making it harder to reject the null hypothesis. Including an estimate of the standard deviation changes the way we calculate p-values by switching from the standard normal distribution to the t-distribution, which accounts for the uncertainty introduced by estimating the population standard deviation from the sample. This adjustment is particularly important for small sample sizes, as it provides a more accurate assessment of the statistical significance of our results.",
    "crumbs": [
      "statististics",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>The t-statistic and t-distribution</span>"
    ]
  },
  {
    "objectID": "t-stats-and-tests.html#t.test",
    "href": "t-stats-and-tests.html#t.test",
    "title": "\n14  The t-statistic and t-distribution\n",
    "section": "\n14.5 t.test",
    "text": "14.5 t.test\n\n14.5.1 One-sample\nWe are going to use the t.test function to perform a one-sample t-test. The t.test function takes a vector of values as input that represents the sample values. In this case, we’ll simulate our sample using the rnorm function and presume that our “effect-size” is 1.\n\nx = rnorm(20,1)\n# small sample\n# Just use the first 5 values of the sample\nt.test(x[1:5])\n\n\n    One Sample t-test\n\ndata:  x[1:5]\nt = 0.97599, df = 4, p-value = 0.3843\nalternative hypothesis: true mean is not equal to 0\n95 percent confidence interval:\n -1.029600  2.145843\nsample estimates:\nmean of x \n0.5581214 \n\n\nIn this case, we set up the experiment so that the null hypothesis is true (the true mean is not zero, but actually 1). However, we only have a small sample size that leads to a modest p-value.\nIncreasing the sample size allows us to see the effect more clearly.\n\nt.test(x[1:20])\n\n\n    One Sample t-test\n\ndata:  x[1:20]\nt = 3.8245, df = 19, p-value = 0.001144\nalternative hypothesis: true mean is not equal to 0\n95 percent confidence interval:\n 0.3541055 1.2101894\nsample estimates:\nmean of x \n0.7821474 \n\n\n\n14.5.2 two-sample\n\nx = rnorm(10,0.5)\ny = rnorm(10,-0.5)\nt.test(x,y)\n\n\n    Welch Two Sample t-test\n\ndata:  x and y\nt = 3.4296, df = 17.926, p-value = 0.003003\nalternative hypothesis: true difference in means is not equal to 0\n95 percent confidence interval:\n 0.5811367 2.4204048\nsample estimates:\n mean of x  mean of y \n 0.7039205 -0.7968502 \n\n\n\n14.5.3 from a data.frame\nIn some situations, you may have data and groups as columns in a data.frame. See the following data.frame, for example\n\ndf = data.frame(value=c(x,y),group=as.factor(rep(c('g1','g2'),each=10)))\ndf\n\n         value group\n1   1.12896674    g1\n2  -1.26838101    g1\n3   1.04577597    g1\n4   1.69075585    g1\n5   0.18672204    g1\n6   1.99715092    g1\n7   1.15424947    g1\n8   0.37671442    g1\n9  -0.09565723    g1\n10  0.82290783    g1\n11 -1.48530261    g2\n12 -1.29200440    g2\n13 -0.18778362    g2\n14  0.59205742    g2\n15 -2.10065248    g2\n16 -0.29961560    g2\n17 -0.38985115    g2\n18 -2.47126235    g2\n19 -0.63654380    g2\n20  0.30245611    g2\n\n\nR allows us to perform a t-test using the formula notation.\n\nt.test(value ~ group, data=df)\n\n\n    Welch Two Sample t-test\n\ndata:  value by group\nt = 3.4296, df = 17.926, p-value = 0.003003\nalternative hypothesis: true difference in means between group g1 and group g2 is not equal to 0\n95 percent confidence interval:\n 0.5811367 2.4204048\nsample estimates:\nmean in group g1 mean in group g2 \n       0.7039205       -0.7968502 \n\n\nYou read that as value is a function of group. In practice, this will do a t-test between the values in g1 vs g2.\n\n14.5.4 Equivalence to linear model\n\nt.test(value ~ group, data=df, var.equal=TRUE)\n\n\n    Two Sample t-test\n\ndata:  value by group\nt = 3.4296, df = 18, p-value = 0.002989\nalternative hypothesis: true difference in means between group g1 and group g2 is not equal to 0\n95 percent confidence interval:\n 0.5814078 2.4201337\nsample estimates:\nmean in group g1 mean in group g2 \n       0.7039205       -0.7968502 \n\n\nThis is equivalent to:\n\nres = lm(value ~ group, data=df)\nsummary(res)\n\n\nCall:\nlm(formula = value ~ group, data = df)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-1.9723 -0.5600  0.2511  0.5252  1.3889 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)   \n(Intercept)   0.7039     0.3094   2.275  0.03538 * \ngroupg2      -1.5008     0.4376  -3.430  0.00299 **\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.9785 on 18 degrees of freedom\nMultiple R-squared:  0.3952,    Adjusted R-squared:  0.3616 \nF-statistic: 11.76 on 1 and 18 DF,  p-value: 0.002989",
    "crumbs": [
      "statististics",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>The t-statistic and t-distribution</span>"
    ]
  },
  {
    "objectID": "t-stats-and-tests.html#power-calculations",
    "href": "t-stats-and-tests.html#power-calculations",
    "title": "\n14  The t-statistic and t-distribution\n",
    "section": "\n14.6 Power calculations",
    "text": "14.6 Power calculations\nThe power of a statistical test is the probability that the test will reject the null hypothesis when the alternative hypothesis is true. In other words, the power of a statistical test is the probability of not making a Type II error. The power of a statistical test depends on the significance level (alpha), the sample size, and the effect size.\nThe power.t.test function can be used to calculate the power of a one-sample t-test.\nLooking at help(\"power.t.test\"), we see that the function takes the following arguments:\n\n\nn - sample size\n\ndelta - effect size\n\nsd - standard deviation of the sample\n\nsig.level - significance level\n\npower - power\n\nWe need to supply four of these arguments to calculate the fifth. For example, if we want to calculate the power of a one-sample t-test with a sample size of 5, a standard deviation of 1, and an effect size of 1, we can use the following command:\n\npower.t.test(n = 5, delta = 1, sd = 1, sig.level = 0.05)\n\n\n     Two-sample t test power calculation \n\n              n = 5\n          delta = 1\n             sd = 1\n      sig.level = 0.05\n          power = 0.2859276\n    alternative = two.sided\n\nNOTE: n is number in *each* group\n\n\nThis gives a nice summary of the power calculation. We can also extract the power value from the result:\n\npower.t.test(n = 5, delta = 1, sd = 1, \n             sig.level = 0.05, type='one.sample')$power\n\n[1] 0.4013203\n\n\n\n\n\n\n\n\nTip\n\n\n\nWhen getting results from a function that don’t look “computable” such as those from power.t.test, you can use the $ operator to extract the value you want. In this case, we want the power value from the result of power.t.test.\nHow would you know what to extract? You can use the names function or the str function to see the structure of the result. For example:\n\nnames(power.t.test(n = 5, delta = 1, sd = 1, \n             sig.level = 0.05, type='one.sample'))\n\n[1] \"n\"           \"delta\"       \"sd\"          \"sig.level\"   \"power\"      \n[6] \"alternative\" \"note\"        \"method\"     \n\n# or \nstr(power.t.test(n = 5, delta = 1, sd = 1, \n             sig.level = 0.05, type='one.sample'))\n\nList of 8\n $ n          : num 5\n $ delta      : num 1\n $ sd         : num 1\n $ sig.level  : num 0.05\n $ power      : num 0.401\n $ alternative: chr \"two.sided\"\n $ note       : NULL\n $ method     : chr \"One-sample t test power calculation\"\n - attr(*, \"class\")= chr \"power.htest\"\n\n\n\n\nAlternatively, we may know a lot about our experimental system and want to calculate the sample size needed to achieve a certain power. For example, if we want to achieve a power of 0.8 with a standard deviation of 1 and an effect size of 1, we can use the following command:\n\npower.t.test(delta = 1, sd = 1, sig.level = 0.05, power = 0.8, type = \"one.sample\")\n\n\n     One-sample t test power calculation \n\n              n = 9.937864\n          delta = 1\n             sd = 1\n      sig.level = 0.05\n          power = 0.8\n    alternative = two.sided\n\n\nThe power.t.test function is convenient and quite fast. As we’ve seen before, though, sometimes the distribution of the test statistics is now easily calculated. In those cases, we can use simulation to calculate the power of a statistical test. For example, if we want to calculate the power of a one-sample t-test with a sample size of 5, a standard deviation of 1, and an effect size of 1, we can use the following command:\n\nsim_t_test_pval &lt;- function(n = 5, delta = 1, sd = 1, sig.level = 0.05) {\n    x = rnorm(n, delta, sd)\n    t.test(x)$p.value &lt;= sig.level\n}\npow = mean(replicate(1000, sim_t_test_pval()))\npow\n\n[1] 0.405\n\n\nLet’s break this down. First, we define a function called sim_t_test_pval that takes the same arguments as the power.t.test function. Inside the function, we simulate a sample of size n from a normal distribution with mean delta and standard deviation sd. Then, we perform a one-sample t-test on the sample and return a logical value indicating whether the p-value is less than the significance level. Next, we use the replicate function to repeat the simulation 1000 times. Finally, we calculate the proportion of simulations in which the p-value was less than the significance level. This proportion is an estimate of the power of the one-sample t-test.\nLet’s compare the results of the power.t.test function and our simulation-based approach:\n\npower.t.test(n = 5, delta = 1, sd = 1, sig.level = 0.05, type='one.sample')$power\n\n[1] 0.4013203\n\nmean(replicate(1000, sim_t_test_pval(n = 5, delta = 1, sd = 1, sig.level = 0.05)))\n\n[1] 0.414",
    "crumbs": [
      "statististics",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>The t-statistic and t-distribution</span>"
    ]
  },
  {
    "objectID": "t-stats-and-tests.html#resources",
    "href": "t-stats-and-tests.html#resources",
    "title": "\n14  The t-statistic and t-distribution\n",
    "section": "\n14.7 Resources",
    "text": "14.7 Resources\nSee the pwr package for more information on power calculations.\n\n\n\n\nStudent. 1908. “The Probable Error of a Mean.” Biometrika 6 (1): 1–25. https://doi.org/10.2307/2331554.",
    "crumbs": [
      "statististics",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>The t-statistic and t-distribution</span>"
    ]
  },
  {
    "objectID": "kmeans.html",
    "href": "kmeans.html",
    "title": "\n15  K-means clustering\n",
    "section": "",
    "text": "15.1 History of the k-means algorithm\nThe k-means clustering algorithm was first proposed by Stuart Lloyd in 1957 as a technique for pulse-code modulation. However, it was not published until 1982. In 1965, Edward W. Forgy published an essentially identical method, which became widely known as the k-means algorithm. Since then, k-means clustering has become one of the most popular unsupervised learning techniques in data analysis and machine learning.\nK-means clustering is a method for finding patterns or groups in a dataset. It is an unsupervised learning technique, meaning that it doesn’t rely on previously labeled data for training. Instead, it identifies structures or patterns directly from the data based on the similarity between data points (see Figure 15.1).\nIn simple terms, k-means clustering aims to divide a dataset into k distinct groups or clusters, where each data point belongs to the cluster with the nearest mean (average). The goal is to minimize the variability within each cluster while maximizing the differences between clusters. This helps to reveal hidden patterns or relationships in the data that might not be apparent otherwise.",
    "crumbs": [
      "statististics",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>K-means clustering</span>"
    ]
  },
  {
    "objectID": "kmeans.html#history-of-the-k-means-algorithm",
    "href": "kmeans.html#history-of-the-k-means-algorithm",
    "title": "\n15  K-means clustering\n",
    "section": "",
    "text": "Figure 15.1: K-means clustering takes a dataset and divides it into k clusters.",
    "crumbs": [
      "statististics",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>K-means clustering</span>"
    ]
  },
  {
    "objectID": "kmeans.html#the-k-means-algorithm",
    "href": "kmeans.html#the-k-means-algorithm",
    "title": "\n15  K-means clustering\n",
    "section": "\n15.2 The k-means algorithm",
    "text": "15.2 The k-means algorithm\nThe k-means algorithm follows these general steps:\n\nChoose the number of clusters k.\nInitialize the cluster centroids randomly by selecting k data points from the dataset.\nAssign each data point to the nearest centroid.\nUpdate the centroids by computing the mean of all the data points assigned to each centroid.\nRepeat steps 3 and 4 until the centroids no longer change or a certain stopping criterion is met (e.g., a maximum number of iterations).\n\nThe algorithm converges when the centroids stabilize or no longer change significantly. The final clusters represent the underlying patterns or structures in the data. Advantages and disadvantages of k-means clustering",
    "crumbs": [
      "statististics",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>K-means clustering</span>"
    ]
  },
  {
    "objectID": "kmeans.html#pros-and-cons-of-k-means-clustering",
    "href": "kmeans.html#pros-and-cons-of-k-means-clustering",
    "title": "\n15  K-means clustering\n",
    "section": "\n15.3 Pros and cons of k-means clustering",
    "text": "15.3 Pros and cons of k-means clustering\nCompared to other clustering algorithms, k-means has several advantages:\n\n\nSimplicity and ease of implementation\n\nThe k-means algorithm is relatively straightforward and can be easily implemented, even for large datasets.\n\n\n\nScalability\n\nThe algorithm can be adapted for large datasets using various optimization techniques or parallel processing.\n\n\n\nSpeed\n\nK-means is generally faster than other clustering algorithms, especially when the number of clusters k is small.\n\n\n\nInterpretability\n\nThe results of k-means clustering are easy to understand, as the algorithm assigns each data point to a specific cluster based on its similarity to the cluster’s centroid.\n\n\n\nHowever, k-means clustering has several disadvantages as well:\n\n\nChoice of k\n\nSelecting the appropriate number of clusters can be challenging and often requires domain knowledge or experimentation. A poor choice of k may yield poor results.\n\n\n\nSensitivity to initial conditions\n\nThe algorithm’s results can vary depending on the initial placement of centroids. To overcome this issue, the algorithm can be run multiple times with different initializations and the best solution can be chosen based on a criterion (e.g., minimizing within-cluster variation).\n\n\n\nAssumes spherical clusters\n\nK-means assumes that clusters are spherical and evenly sized, which may not always be the case in real-world datasets. This can lead to poor performance if the underlying clusters have different shapes or densities.\n\n\n\nSensitivity to outliers\n\nThe algorithm is sensitive to outliers, which can heavily influence the position of centroids and the final clustering result. Preprocessing the data to remove or mitigate the impact of outliers can help improve the performance of k-means clustering.\n\n\n\nDespite limitations, k-means clustering remains a popular and widely used method for exploring and analyzing data, particularly in biological data analysis, where identifying patterns and relationships can provide valuable insights into complex systems and processes.",
    "crumbs": [
      "statististics",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>K-means clustering</span>"
    ]
  },
  {
    "objectID": "kmeans.html#an-example-of-k-means-clustering",
    "href": "kmeans.html#an-example-of-k-means-clustering",
    "title": "\n15  K-means clustering\n",
    "section": "\n15.4 An example of k-means clustering",
    "text": "15.4 An example of k-means clustering\n\n15.4.1 The data and experimental background\nThe data we are going to use are from DeRisi, Iyer, and Brown (1997). From their abstract:\n\nDNA microarrays containing virtually every gene of Saccharomyces cerevisiae were used to carry out a comprehensive investigation of the temporal program of gene expression accompanying the metabolic shift from fermentation to respiration. The expression profiles observed for genes with known metabolic functions pointed to features of the metabolic reprogramming that occur during the diauxic shift, and the expression patterns of many previously uncharacterized genes provided clues to their possible functions.\n\nThese data are available from NCBI GEO as GSE28.\nIn the case of the baker’s or brewer’s yeast Saccharomyces cerevisiae growing on glucose with plenty of aeration, the diauxic growth pattern is commonly observed in batch culture. During the first growth phase, when there is plenty of glucose and oxygen available, the yeast cells prefer glucose fermentation to aerobic respiration even though aerobic respiration is the more efficient pathway to grow on glucose. This experiment profiles gene expression for 6400 genes over a time course during which the cells are undergoing a diauxic shift.\nThe data in deRisi et al. have no replicates and are time course data. Sometimes, seeing how groups of genes behave can give biological insight into the experimental system or the function of individual genes. We can use clustering to group genes that have a similar expression pattern over time and then potentially look at the genes that do so.\nOur goal, then, is to use kmeans clustering to divide highly variable (informative) genes into groups and then to visualize those groups.",
    "crumbs": [
      "statististics",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>K-means clustering</span>"
    ]
  },
  {
    "objectID": "kmeans.html#getting-data",
    "href": "kmeans.html#getting-data",
    "title": "\n15  K-means clustering\n",
    "section": "\n15.5 Getting data",
    "text": "15.5 Getting data\nThese data were deposited at NCBI GEO back in 2002. GEOquery can pull them out easily.\n\nlibrary(GEOquery)\ngse = getGEO(\"GSE28\")[[1]]\nclass(gse)\n\n[1] \"ExpressionSet\"\nattr(,\"package\")\n[1] \"Biobase\"\n\n\nGEOquery is a little dated and was written before the SummarizedExperiment existed. However, Bioconductor makes a conversion from the old ExpressionSet that GEOquery uses to the SummarizedExperiment that we see so commonly used now.\n\nlibrary(SummarizedExperiment)\ngse = as(gse, \"SummarizedExperiment\")\ngse\n\nclass: SummarizedExperiment \ndim: 6400 7 \nmetadata(3): experimentData annotation protocolData\nassays(1): exprs\nrownames(6400): 1 2 ... 6399 6400\nrowData names(20): ID ORF ... FAILED IS_CONTAMINATED\ncolnames(7): GSM887 GSM888 ... GSM892 GSM893\ncolData names(33): title geo_accession ... supplementary_file\n  data_row_count\n\n\nTaking a quick look at the colData(), it might be that we want to reorder the columns a bit.\n\ncolData(gse)$title\n\n[1] \"diauxic shift timecourse: 15.5 hr\" \"diauxic shift timecourse: 0 hr\"   \n[3] \"diauxic shift timecourse: 18.5 hr\" \"diauxic shift timecourse: 9.5 hr\" \n[5] \"diauxic shift timecourse: 11.5 hr\" \"diauxic shift timecourse: 13.5 hr\"\n[7] \"diauxic shift timecourse: 20.5 hr\"\n\n\nSo, we can reorder by hand to get the time course correct:\n\ngse = gse[, c(2,4,5,6,1,3,7)]",
    "crumbs": [
      "statististics",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>K-means clustering</span>"
    ]
  },
  {
    "objectID": "kmeans.html#preprocessing",
    "href": "kmeans.html#preprocessing",
    "title": "\n15  K-means clustering\n",
    "section": "\n15.6 Preprocessing",
    "text": "15.6 Preprocessing\nIn gene expression data analysis, the primary objective is often to identify genes that exhibit significant differences in expression levels across various conditions, such as diseased vs. healthy samples or different time points in a time-course experiment. However, gene expression datasets are typically large, noisy, and contain numerous genes that do not exhibit substantial changes in expression levels. Analyzing all genes in the dataset can be computationally intensive and may introduce noise or false positives in the results.\nOne common approach to reduce the complexity of the dataset and focus on the most informative genes is to subset the genes based on their standard deviation in expression levels across the samples. The standard deviation is a measure of dispersion or variability in the data, and genes with high standard deviations have more variation in their expression levels across the samples.\nBy selecting genes with high standard deviations, we focus on genes that show relatively large changes in expression levels across different conditions. These genes are more likely to be biologically relevant and involved in the underlying processes or pathways of interest. In contrast, genes with low standard deviations exhibit little or no change in expression levels and are less likely to be informative for the analysis. It turns out that applying filtering based on criteria such as standard deviation can also increase power and reduce false positives in the analysis (Bourgon, Gentleman, and Huber 2010).\nTo subset the genes for analysis based on their standard deviation, the following steps can be followed: Calculate the standard deviation of each gene’s expression levels across all samples. Set a threshold for the standard deviation, which can be determined based on domain knowledge, data distribution, or a specific percentile of the standard deviation values (e.g., selecting the top 10% or 25% of genes with the highest standard deviations). Retain only the genes with a standard deviation above the chosen threshold for further analysis.\nBy subsetting the genes based on their standard deviation, we can reduce the complexity of the dataset, speed up the subsequent analysis, and increase the likelihood of detecting biologically meaningful patterns and relationships in the gene expression data. The threshold for the standard deviation cutoff is rather arbitrary, so it may be beneficial to try a few to check for sensitivity of findings.\n\nsds = apply(assays(gse)[[1]], 1, sd)\nhist(sds)\n\n\n\n\n\n\nFigure 15.2: Histogram of standard deviations for all genes in the deRisi dataset.\n\n\n\n\nExamining the plot, we can see that the most highly variable genes have an sd &gt; 0.8 or so (arbitrary). We can, for convenience, create a new SummarizedExperiment that contains only our most highly variable genes.\n\nidx = sds&gt;0.8 & !is.na(sds)\ngse_sub = gse[idx,]",
    "crumbs": [
      "statististics",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>K-means clustering</span>"
    ]
  },
  {
    "objectID": "kmeans.html#clustering",
    "href": "kmeans.html#clustering",
    "title": "\n15  K-means clustering\n",
    "section": "\n15.7 Clustering",
    "text": "15.7 Clustering\nNow, gse_sub contains a subset of our data.\nThe kmeans function takes a matrix and the number of clusters as arguments.\n\nk = 4\nkm = kmeans(assays(gse_sub)[[1]], 4)\n\nThe km kmeans result contains a vector, km$cluster, which gives the cluster associated with each gene. We can plot the genes for each cluster to see how these different genes behave.\n\nexpression_values = assays(gse_sub)[[1]]\npar(mfrow=c(2,2), mar=c(3,4,1,2)) # this allows multiple plots per page\nfor(i in 1:k) {\n    matplot(t(expression_values[km$cluster==i, ]), type='l', ylim=c(-3,3),\n            ylab = paste(\"cluster\", i))\n}\n\n\n\n\n\n\nFigure 15.3: Gene expression profiles for the four clusters identified by k-means clustering. Each line represents a gene in the cluster, and each column represents a time point in the experiment. Each cluster shows a distinct trend where the genes in the cluster are potentially co-regulated.\n\n\n\n\nTry this with different size k. Perhaps go back to choose more genes (using a smaller cutoff for sd).",
    "crumbs": [
      "statististics",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>K-means clustering</span>"
    ]
  },
  {
    "objectID": "kmeans.html#summary",
    "href": "kmeans.html#summary",
    "title": "\n15  K-means clustering\n",
    "section": "\n15.8 Summary",
    "text": "15.8 Summary\nIn this lesson, we have learned how to use k-means clustering to identify groups of genes that behave similarly over time. We have also learned how to subset our data to focus on the most informative genes.\n\n\n\n\nBourgon, Richard, Robert Gentleman, and Wolfgang Huber. 2010. “Independent Filtering Increases Detection Power for High-Throughput Experiments.” Proceedings of the National Academy of Sciences 107 (21): 9546–51. https://doi.org/10.1073/pnas.0914005107.\n\n\nDeRisi, J. L., V. R. Iyer, and P. O. Brown. 1997. “Exploring the Metabolic and Genetic Control of Gene Expression on a Genomic Scale.” Science (New York, N.Y.) 278 (5338): 680–86. https://doi.org/10.1126/science.278.5338.680.",
    "crumbs": [
      "statististics",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>K-means clustering</span>"
    ]
  },
  {
    "objectID": "ml_practical.html",
    "href": "ml_practical.html",
    "title": "\n16  Machine Learning\n",
    "section": "",
    "text": "16.1 What is Machine Learning?\nMachine learning is a subfield of artificial intelligence that focuses on the development of algorithms and models that enable computers to learn and make decisions or predictions without explicit programming. It has emerged as a powerful tool for solving complex problems across various industries, including healthcare, finance, marketing, and natural language processing. This chapter provides an overview of machine learning, its types, key concepts, applications, and challenges.\nMachine learning in biology is a really broad topic. Greener et al. (2022) present a nice overview of the different types of machine learning methods that are used in biology. Libbrecht and Noble (2015) also present an early review of machine learning in genetics and genomics.",
    "crumbs": [
      "statististics",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Machine Learning</span>"
    ]
  },
  {
    "objectID": "ml_practical.html#classes-of-machine-learning",
    "href": "ml_practical.html#classes-of-machine-learning",
    "title": "\n16  Machine Learning\n",
    "section": "\n16.2 Classes of Machine Learning",
    "text": "16.2 Classes of Machine Learning\n\n16.2.1 Supervised learning\nSupervised learning is a type of machine learning where the model learns from labeled data, i.e., input-output pairs, to make predictions. It includes tasks like regression (predicting continuous values) and classification (predicting discrete classes or categories).\n\n16.2.2 Unsupervised learning\nUnsupervised learning involves learning from unlabeled data, where the model discovers patterns or structures within the data. Common unsupervised learning tasks include clustering (grouping similar data points), dimensionality reduction (reducing the number of features or variables), and anomaly detection (identifying unusual data points).\n\n\n\n\n\n\nTerminology and Concepts\n\n\n\n\n\nData\n\nData is the foundation of machine learning and can be structured (tabular) or unstructured (text, images, audio). It is usually divided into training, validation, and testing sets for model development and evaluation.\n\n\n\nFeatures\n\nFeatures are the variables or attributes used to describe the data points. Feature engineering and selection are crucial steps in machine learning to improve model performance and interpretability.\n\n\n\nModels and Algorithms\n\nModels are mathematical representations of the relationship between features and the target variable(s). Algorithms are the methods used to train models, such as linear regression, decision trees, and neaural networks.\n\n\n\nHyperparameters and Tuning\n\nHyperparameters are adjustable parameters that control the learning process of an algorithm. Tuning involves finding the optimal set of hyperparameters to improve model performance.\n\n\n\nEvaluation Metrics\n\nEvaluation metrics quantify the performance of a model, such as accuracy, precision, recall, F1-score (for classification), and mean squared error, R-squared (for regression).\n\n\n\n\n\n\nCodeset.seed(123)\nsinsim &lt;- function(n,sd=0.1) {\n  x &lt;- seq(0,1,length.out=n)\n  y &lt;- sin(2*pi*x) + rnorm(n,0,sd)\n  return(data.frame(x=x,y=y))\n}\ndat &lt;- sinsim(100,0.25)\nlibrary(ggplot2)\nlibrary(patchwork)\np_base &lt;- ggplot(dat,aes(x=x,y=y)) +\n geom_point(alpha=0.7) +\n theme_bw()\np_lm &lt;- p_base + \n geom_smooth(method=\"lm\", se=FALSE, alpha=0.6, formula = y ~ x) \np_lmsin &lt;- p_base +\n geom_smooth(method=\"lm\",formula=y~sin(2*pi*x), se=FALSE, alpha=0.6) \np_loess_wide &lt;- p_base +\n  geom_smooth(method=\"loess\",span=0.5, se=FALSE, alpha=0.6, formula = y ~ x) \np_loess_narrow &lt;- p_base + \n geom_smooth(method=\"loess\",span=0.1, se=FALSE, alpha=0.6, formula = y ~ x) \np_lm + p_lmsin + p_loess_wide + p_loess_narrow + plot_layout(ncol=2) +\n  plot_annotation(tag_levels = 'A') & \n  theme(plot.tag = element_text(size = 8))\n\n\n\n\n\n\nFigure 16.1: Data simulated according to the function \\(f(x) = sin(2 \\pi x) + N(0,0.25)\\) fitted with four different models. A) A simple linear model demonstrates underfitting. B) A linear model with a sin function (\\(y = sin(2 \\pi x)\\)) and C) a loess model with a wide span (0.5) demonstrate good fits. D) A loess model with a narrow span (0.1) is a good example of overfitting.\n\n\n\n\nIn Figure 16.1, we simulate data according to the function \\(f(x) = sin(2 \\pi x) + N(0,0.25)\\) and fit four different models. Choosing a model that is too simple (A) will result in underfitting the data, while choosing a model that is too complex (D) will result in overfitting the data.\nWhen thinking about machine learning, it can help to have a simple framework in mind. In Figure 16.2, we present a simple view of machine learning according to the scikit-learn package.\n\n\n\n\n\nFigure 16.2: A simple view of machine learning according the sklearn.\n\n\nWe’re going to focus on supervised learning here. Here is a rough schematic (see Figure 16.3) of the supervised learning process from the mlr3 book.\n\n\n\n\n\nFigure 16.3: A schematic of the supervised learning process.\n\n\nIn nearly all cases, we will have a training set and a test set. The training set is used to train the model, and the test set is used to evaluate the model (see Figure 16.4). Even when we don’t have a separate test set, we will usually create one by splitting the data.\n\n\n\n\n\nFigure 16.4: Training and testing sets.",
    "crumbs": [
      "statististics",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Machine Learning</span>"
    ]
  },
  {
    "objectID": "ml_practical.html#supervised-learning-1",
    "href": "ml_practical.html#supervised-learning-1",
    "title": "\n16  Machine Learning\n",
    "section": "\n16.3 Supervised Learning",
    "text": "16.3 Supervised Learning\n\n16.3.1 Linear regression\nIn statistics, linear regression is a linear approach for modelling the relationship between a scalar response and one or more explanatory variables (also known as dependent and independent variables). The case of one explanatory variable is called simple linear regression; for more than one, the process is called multiple linear regression. This term is distinct from multivariate linear regression, where multiple correlated dependent variables are predicted, rather than a single scalar variable.\nIn linear regression, the relationships are modeled using linear predictor functions whose unknown model parameters are estimated from the data. Such models are called linear models. Most commonly, the conditional mean of the response given the values of the explanatory variables (or predictors) is assumed to be an affine function of those values; less commonly, the conditional median or some other quantile is used. Like all forms of regression analysis, linear regression focuses on the conditional probability distribution of the response given the values of the predictors, rather than on the joint probability distribution of all of these variables, which is the domain of multivariate analysis.\nLinear regression was the first type of regression analysis to be studied rigorously, and to be used extensively in practical applications. This is because models which depend linearly on their unknown parameters are easier to fit than models which are non-linearly related to their parameters and because the statistical properties of the resulting estimators are easier to determine.\n\n16.3.2 K-nearest Neighbor\n\n\n\n\nFigure. The k-nearest neighbor algorithm can be used for regression or classification.\n\n\n\nThe k-nearest neighbors algorithm (k-NN) is a non-parametric supervised learning method first developed by Evelyn Fix and Joseph Hodges in 1951, and later expanded by Thomas Cover. It is used for classification and regression. In both cases, the input consists of the k closest training examples in a data set.\nThe k-nearest neighbor (k-NN) algorithm is a simple, yet powerful, supervised machine learning method used for classification and regression tasks. It is an instance-based, non-parametric learning method that stores the entire training dataset and makes predictions based on the similarity between data points. The underlying principle of the k-NN algorithm is that similar data points (those that are close to each other in multidimensional space) are likely to have similar outcomes or belong to the same class.\nHere’s a description of how the k-NN algorithm works:\n\nDetermine the value of k: The first step is to choose the number of nearest neighbors (k) to consider when making predictions. The value of k is a user-defined hyperparameter and can significantly impact the algorithm’s performance. A small value of k can lead to overfitting, while a large value may result in underfitting.\nCompute distance: Calculate the distance between the new data point (query point) and each data point in the training dataset. The most common distance metrics used are Euclidean, Manhattan, and Minkowski distance. The choice of distance metric depends on the problem and the nature of the data.\nFind k-nearest neighbors: Identify the k data points in the training dataset that are closest to the query point, based on the chosen distance metric.\nMake predictions: Once the k-nearest neighbors are identified, the final step is to make predictions. The prediction for the query point can be made in two ways:\n\nFor classification, determine the class labels of the k-nearest neighbors and assign the class label with the highest frequency (majority vote) to the query point. In case of a tie, one can choose the class with the smallest average distance to the query point or randomly select one among the tied classes.\nFor regression tasks, the k-NN algorithm follows a similar process, but instead of majority voting, it calculates the mean (or median) of the target values of the k-nearest neighbors and assigns it as the prediction for the query point.\n\n\n\nThe k-NN algorithm is known for its simplicity, ease of implementation, and ability to handle multi-class problems. However, it has some drawbacks, such as high computational cost (especially for large datasets), sensitivity to the choice of k and distance metric, and poor performance with high-dimensional or noisy data. Scaling and preprocessing the data, as well as using dimensionality reduction techniques, can help mitigate some of these issues.\n\nIn k-NN classification, the output is a class membership. An object is classified by a plurality vote of its neighbors, with the object being assigned to the class most common among its k nearest neighbors (k is a positive integer, typically small). If k = 1, then the object is simply assigned to the class of that single nearest neighbor.\nIn k-NN regression, the output is the property value for the object. This value is the average of the values of k nearest neighbors.\n\nk-NN is a type of classification where the function is only approximated locally and all computation is deferred until function evaluation. Since this algorithm relies on distance for classification, if the features represent different physical units or come in vastly different scales then normalizing the training data can improve its accuracy dramatically.\nBoth for classification and regression, a useful technique can be to assign weights to the contributions of the neighbors, so that the nearer neighbors contribute more to the average than the more distant ones. For example, a common weighting scheme consists in giving each neighbor a weight of 1/d, where d is the distance to the neighbor.\nThe neighbors are taken from a set of objects for which the class (for k-NN classification) or the object property value (for k-NN regression) is known. This can be thought of as the training set for the algorithm, though no explicit training step is required.",
    "crumbs": [
      "statististics",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Machine Learning</span>"
    ]
  },
  {
    "objectID": "ml_practical.html#penalized-regression",
    "href": "ml_practical.html#penalized-regression",
    "title": "\n16  Machine Learning\n",
    "section": "\n16.4 Penalized regression",
    "text": "16.4 Penalized regression\nAdapted from http://www.sthda.com/english/articles/37-model-selection-essentials-in-r/153-penalized-regression-essentials-ridge-lasso-elastic-net/.\nPenalized regression is a type of regression analysis that introduces a penalty term to the loss function in order to prevent overfitting and improve the model’s ability to generalize. Remember that in regression, the loss function is the sum of squares Equation 16.1.\n\\[L = \\sum_{i=0}^{n}{(\\hat{y}_i - y_i)}^2 \\tag{16.1}\\]\nIn Equation 16.1, \\(\\hat{y}_i\\) is the predicted output, \\(y_i\\) is the actual output, and n is the number of observations. The goal of regression is to minimize the loss function by finding the optimal values of the model parameters or coefficients. The model parameters are estimated using the training data. The model is then evaluated using the test data. If the model performs well on the training data but poorly on the test data, it is said to be overfit. Overfitting occurs when the model learns the training data too well, including the noise, and is not able to generalize well to new data. This is a common problem in machine learning, particularly when there are a large number of predictors compared to the number of observations, and can be addressed by penalized regression.\nThe two most common types of penalized regression are Ridge Regression (L2 penalty) and LASSO Regression (L1 penalty). Both Ridge and LASSO help to reduce model complexity and prevent over-fitting which may result from simple linear regression. However, the choice between Ridge and LASSO depends on the situation and the dataset at hand. If feature selection is important for the interpretation of the model, LASSO might be preferred. If the goal is prediction accuracy and the model needs to retain all features, Ridge might be the better choice.\n\n16.4.1 Ridge regression\nRidge regression shrinks the regression coefficients, so that variables, with minor contribution to the outcome, have their coefficients close to zero. The shrinkage of the coefficients is achieved by penalizing the regression model with a penalty term called L2-norm, which is the sum of the squared coefficients. The amount of the penalty can be fine-tuned using a constant called lambda (λ). Selecting a good value for λ is critical. When λ=0, the penalty term has no effect, and ridge regression will produce the classical least square coefficients. However, as λ increases to infinite, the impact of the shrinkage penalty grows, and the ridge regression coefficients will get close zero. The loss function for Ridge Regression is:\n\\[L = \\sum_{i=0}^{n}{(\\hat{y}_i - y_i)}^2 + \\lambda\\sum_{j=0}^{k} \\beta_j^2 \\tag{16.2}\\]\nHere, \\(\\hat{y}_i\\) is the predicted output, \\(y_i\\) is the actual output, \\({\\beta_j}\\) represents the model parameters or coefficients, and λ is the regularization parameter. The second term, λ∑βj^2, is the penalty term where all parameters are squared and summed. Ridge regression tends to shrink the coefficients but doesn’t necessarily zero them.\nNote that, in contrast to the ordinary least square regression, ridge regression is highly affected by the scale of the predictors. Therefore, it is better to standardize (i.e., scale) the predictors before applying the ridge regression (James et al. 2014), so that all the predictors are on the same scale. The standardization of a predictor x, can be achieved using the formula \\(x' = \\frac{x}{sd(x)}\\), where \\(sd(x)\\) is the standard deviation of \\(x\\). The consequence of this is that, all standardized predictors will have a standard deviation of one allowing the final fit to not depend on the scale on which the predictors are measured.\nOne important advantage of the ridge regression, is that it still performs well, compared to the ordinary least square method (see Equation 16.1), in a situation where you have a large multivariate data with the number of predictors (p) larger than the number of observations (n). One disadvantage of the ridge regression is that, it will include all the predictors in the final model, unlike the stepwise regression methods, which will generally select models that involve a reduced set of variables. Ridge regression shrinks the coefficients towards zero, but it will not set any of them exactly to zero. The LASSO regression is an alternative that overcomes this drawback.\n\n16.4.2 LASSO regression\nLASSO stands for Least Absolute Shrinkage and Selection Operator. It shrinks the regression coefficients toward zero by penalizing the regression model with a penalty term called L1-norm, which is the sum of the absolute coefficients. In the case of LASSO regression, the penalty has the effect of forcing some of the coefficient estimates, with a minor contribution to the model, to be exactly equal to zero. This means that, LASSO can be also seen as an alternative to the subset selection methods for performing variable selection in order to reduce the complexity of the model. As in ridge regression, selecting a good value of \\(\\lambda\\) for the LASSO is critical. The loss function for LASSO Regression is:\n\\[L = \\sum_{i=0}^{n}{(\\hat{y}_i - y_i)}^2 + \\lambda\\sum_{j=0}^{k} |\\beta_j| \\tag{16.3}\\]\nSimilar to Ridge, ŷi is the predicted output, yi is the actual output, βj represents the model parameters or coefficients, and λ is the regularization parameter. The second term, λ∑|βj|, is the penalty term where the absolute values of all parameters are summed. LASSO regression tends to shrink the coefficients and can zero out some of them, effectively performing variable selection.\nOne obvious advantage of LASSO regression over ridge regression, is that it produces simpler and more interpretable models that incorporate only a reduced set of the predictors. However, neither ridge regression nor the LASSO will universally dominate the other. Generally, LASSO might perform better in a situation where some of the predictors have large coefficients, and the remaining predictors have very small coefficients. Ridge regression will perform better when the outcome is a function of many predictors, all with coefficients of roughly equal size (James et al. 2014).\nCross-validation methods can be used for identifying which of these two techniques is better on a particular data set.\n\n16.4.3 Elastic Net\nElastic Net produces a regression model that is penalized with both the L1-norm and L2-norm. The consequence of this is to effectively shrink coefficients (like in ridge regression) and to set some coefficients to zero (as in LASSO).\n\n16.4.4 Classification and Regression Trees (CART)\nDecision Tree Learning is supervised learning approach used in statistics, data mining and machine learning. In this formalism, a classification or regression decision tree is used as a predictive model to draw conclusions about a set of observations. Decision trees are a popular machine learning method used for both classification and regression tasks. They are hierarchical, tree-like structures that model the relationship between features and the target variable by recursively splitting the data into subsets based on the feature values. Each internal node in the tree represents a decision or test on a feature, and each branch represents the outcome of that test. The leaf nodes contain the final prediction, which is the majority class for classification tasks or the mean/median of the target values for regression tasks.\n\n\n\n\n\n\n\nFigure 16.5: An example of a decision tree that performs classification, also sometimes called a classification tree.\n\n\n\n\nHere’s an overview of the decision tree learning process:\n\nSelect the best feature and split value: Start at the root node and choose the feature and split value that results in the maximum reduction of impurity (or increase in information gain) in the child nodes. For classification tasks, impurity measures like Gini index or entropy are commonly used, while for regression tasks, mean squared error (MSE) or mean absolute error (MAE) can be used.\nSplit the data: Partition the dataset into subsets based on the chosen feature and split value.\nRecursion: Repeat steps 1 and 2 for each subset until a stopping criterion is met. Stopping criteria can include reaching a maximum tree depth, a minimum number of samples per leaf, or no further improvement in impurity.\nPrune the tree (optional): To reduce overfitting, decision trees can be pruned by removing branches that do not significantly improve the model’s performance on the validation dataset. This can be done using techniques like reduced error pruning or cost-complexity pruning.\n\nDecision trees have several advantages, such as:\n\n\nInterpretability\n\nThey are easy to understand, visualize, and explain, even for non-experts.\n\n\n\nMinimal data preprocessing\n\nDecision trees can handle both numerical and categorical data, and they are robust to outliers and missing values.\n\n\n\nNon-linear relationships\n\nThey can capture complex non-linear relationships between features and the target variable.\n\n\n\nHowever, decision trees also have some drawbacks:\n\n\nOverfitting\n\nThey are prone to overfitting, especially when the tree is deep or has few samples per leaf. Pruning and setting stopping criteria can help mitigate this issue.\n\n\n\nInstability\n\nSmall changes in the data can lead to different tree structures. This can be addressed by using ensemble methods like random forests or gradient boosting machines (GBMs).\n\n\n\nGreedy learning\n\nDecision tree algorithms use a greedy approach, meaning they make locally optimal choices at each node. This may not always result in a globally optimal tree.\n\n\n\nDespite these limitations, decision trees are widely used in various applications due to their simplicity, interpretability, and ability to handle diverse data types.\n\n16.4.5 RandomForest\nRandom forests or random decision forests is an ensemble learning method for classification, regression and other tasks that operates by constructing a multitude of decision trees at training time. For classification tasks, the output of the random forest is the class selected by most trees. For regression tasks, the mean or average prediction of the individual trees is returned. Random decision forests correct for decision trees’ habit of overfitting to their training set. Random forests generally outperform decision trees, but their accuracy is lower than gradient boosted trees[citation needed]. However, data characteristics can affect their performance.\nThe first algorithm for random decision forests was created in 1995 by Tin Kam Ho using the random subspace method, which, in Ho’s formulation, is a way to implement the “stochastic discrimination” approach to classification proposed by Eugene Kleinberg.\nAn extension of the algorithm was developed by Leo Breiman and Adele Cutler, who registered “Random Forests” as a trademark in 2006 (as of 2019[update], owned by Minitab, Inc.). The extension combines Breiman’s “bagging” idea and random selection of features, introduced first by Ho and later independently by Amit and Geman in order to construct a collection of decision trees with controlled variance.\nRandom forests are frequently used as “blackbox” models in businesses, as they generate reasonable predictions across a wide range of data while requiring little configuration.\n\n\n\n\n\n\n\nFigure 16.6: Random forests or random decision forests is an ensemble learning method for classification, regression and other tasks that operates by constructing a multitude of decision trees at training time.\n\n\n\n\n\n\n\n\nGreener, Joe G., Shaun M. Kandathil, Lewis Moffat, and David T. Jones. 2022. “A Guide to Machine Learning for Biologists.” Nature Reviews Molecular Cell Biology 23 (1): 40–55. https://doi.org/10.1038/s41580-021-00407-0.\n\n\nLibbrecht, Maxwell W., and William Stafford Noble. 2015. “Machine Learning Applications in Genetics and Genomics.” Nature Reviews Genetics 16 (6): 321–32. https://doi.org/10.1038/nrg3920.",
    "crumbs": [
      "statististics",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Machine Learning</span>"
    ]
  },
  {
    "objectID": "machine_learning_mlr3.html",
    "href": "machine_learning_mlr3.html",
    "title": "\n17  Machine Learning 2\n",
    "section": "",
    "text": "17.1 Overview\nIn this chapter, we focus on practical aspects of machine learning. The goal is to provide a hands-on introduction to the application of machine learning techniques to real-world data. While the theoretical foundations of machine learning are important, the ability to apply these techniques to solve practical problems is equally crucial. In this chapter, we will use the mlr3 package in R to build and evaluate machine learning models for classification and regression tasks.\nWe will use three examples to illustrate the machine learning workflow:\nWe’ll be applying knn, decision trees, and random forests, linear regression, and penalized regression models to these datasets.\nThe mlr3 R package is a modern, object-oriented machine learning framework in R that builds on the success of its predecessor, the mlr package. It provides a flexible and extensible platform for handling common machine learning tasks such as data preprocessing, model training, hyperparameter tuning, and model evaluation Figure 17.1. The package is designed to guide and standardize the process of using complex machine learning pipelines.\nFigure 17.1: The mlr3 ecosystem.",
    "crumbs": [
      "statististics",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Machine Learning 2</span>"
    ]
  },
  {
    "objectID": "machine_learning_mlr3.html#overview",
    "href": "machine_learning_mlr3.html#overview",
    "title": "\n17  Machine Learning 2\n",
    "section": "",
    "text": "Cancer types classification: We will classify different types of cancer based on gene expression data.\n\nAge prediction from DNA methylation: We will predict the chronological age of individuals based on DNA methylation patterns.\n\nGene expression prediction: We will predict gene expression levels based on histone modification data.\n\n\n\n\n\n17.1.1 Key features of mlr3\n\n\nTask abstraction\n\nmlr3 encapsulates different types of learning problems like classification, regression, and survival analysis into “Task” objects, making it easier to handle various learning scenarios. Examples of tasks include classification tasks, regression tasks, and survival tasks.\n\n\n\nModular design\n\nThe package follows a modular design, allowing users to quickly swap out different components such as learners (algorithms), measures (performance metrics), and resampling strategies. Examples of learners include linear regression, logistic regression, and random forests. Examples of measures include accuracy, precision, recall, and F1 score. Examples of resampling strategies include cross-validation, bootstrapping, and holdout validation.\n\n\n\nExtensibility\n\nUsers can extend the functionality of mlr3 by adding custom components like learners, measures, and preprocessing steps via the R6 object-oriented system.\n\n\n\nPreprocessing\n\nmlr3 provides a flexible way to preprocess data using “PipeOps” (pipeline operations), allowing users to create reusable preprocessing pipelines.\n\n\n\nTuning and model selection\n\nmlr3 supports hyperparameter tuning and model selection using various search strategies like grid search, random search, and Bayesian optimization.\n\n\n\nParallelization\n\nThe package allows for parallelization of model training and evaluation, making it suitable for large-scale machine learning tasks.\n\n\n\nBenchmarking\n\nmlr3 facilitates benchmarking of multiple algorithms on multiple tasks, simplifying the process of comparing and selecting the best models.\n\n\n\nYou can find more information, including tutorials and examples, on the official mlr3 GitHub repository1 and the mlr3 book2.\n1 https://github.com/mlr-org/mlr32 https://mlr3book.mlr-org.com/",
    "crumbs": [
      "statististics",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Machine Learning 2</span>"
    ]
  },
  {
    "objectID": "machine_learning_mlr3.html#the-mlr3-workflow",
    "href": "machine_learning_mlr3.html#the-mlr3-workflow",
    "title": "\n17  Machine Learning 2\n",
    "section": "\n17.2 The mlr3 workflow",
    "text": "17.2 The mlr3 workflow\nThe mlr3 package is designed to simplify the process of creating and deploying complex machine learning pipelines. The package follows a modular design, which means that users can quickly swap out different components such as learners (algorithms), measures (performance metrics), and resampling strategies. The package also supports parallelization of model training and evaluation, making it suitable for large-scale machine learning tasks.\n\n\n\n\n\ngraph TD\n  A[Load data]\n  B[Create a task\\nregression, classification, etc.]\n  B --&gt; learner[Choose learner]\n  A --&gt; C[Split data]\n  C --&gt; trainset(Training set)\n  C --&gt; testset[Test set]\n  trainset --&gt; train[Train model]\n  learner --&gt; train\n  testset --&gt; testing[Test model]\n  train --&gt; testing\n  testing --&gt; eval[Evaluate &\\nInterpret]\n\n\n\n\nFigure 17.2: The simplified workflow of a machine learning pipeline using mlr3.\n\n\n\n\nThe following sections describe each of these steps in detail.\n\n17.2.1 The machine learning Task\nImagine you want to teach a computer how to make predictions or decisions, similar to how you might teach a student. To do this, you need to clearly define what you want the computer to learn and work on. This is called defining a “task.” Let’s break down what this involves and why it’s important.\n\n17.2.1.1 Step 1: Understand the Problem\nFirst, think about what problem you want to solve or what question you want the computer to answer. For example: - Do you want to predict the weather for tomorrow? - Are you trying to figure out if an email is spam or not? - Do you want to know how much a house might sell for?\nThese questions define your task type. In machine learning, there are several common task types:\n\n\nClassification: Deciding which category something belongs to (e.g., spam or not spam).\n\nRegression: Predicting a number (e.g., the price of a house).\n\nClustering: Grouping similar items together (e.g., customer segmentation).\n\n17.2.1.2 Step 2: Choose Your Data\nNext, you need data that is related to your problem. Think of data as the information or examples you’ll use to teach the computer. For instance, if your task is to predict house prices, your data might include:\n\nThe size of the house\nThe number of bedrooms\nThe location of the house\nThe age of the house\n\nThese pieces of information are called features. Features are the input that the computer uses to make predictions.\n\n17.2.1.3 Step 3: Define the Target\nAlong with features, you need to define the target. The target is what you want to predict or decide. In the house price example, the target would be the actual price of the house.\n\n17.2.1.4 Step 4: Create the Task\nNow that you have your problem, data, and target, you can create the task. In mlr3, a task brings together the type of problem (task type), the features (input data), and the target (what you want to predict).\nHere’s a simple summary:\n\n\nTask Type: What kind of problem are you solving? (e.g., classification, regression)\n\nFeatures: What information do you have to make the prediction? (e.g., size, location)\n\nTarget: What are you trying to predict? (e.g., house price)\n\nBy clearly defining these elements, you set a solid foundation for the machine learning process. This helps ensure that the computer can learn effectively and make accurate predictions.\n\n17.2.1.5 mlr3 and Tasks\nThe mlr3 package uses the concept of “Tasks” to encapsulate different types of learning problems like classification, regression, and survival analysis. A Task contains the data (features and target variable) and additional metadata to define the machine learning problem. For example, in a classification task, the target variable is a label (stored as a character or factor), while in a regression task, the target variable is a numeric quantity (stored as an integer or numeric).\nThere are a number of Task Types that are supported by mlr3. To create a task from a data.frame(), data.table() or Matrix(), you first need to select the right task type:\n\nClassification Task: The target is a label (stored as character or factor) with only relatively few distinct values → TaskClassif.\nRegression Task: The target is a numeric quantity (stored as integer or numeric) → TaskRegr.\nSurvival Task: The target is the (right-censored) time to an event. More censoring types are currently in development → mlr3proba::TaskSurv in add-on package mlr3proba.\nDensity Task: An unsupervised task to estimate the density → mlr3proba::TaskDens in add-on package mlr3proba.\nCluster Task: An unsupervised task type; there is no target and the aim is to identify similar groups within the feature space → mlr3cluster::TaskClust in add-on package mlr3cluster.\nSpatial Task: Observations in the task have spatio-temporal information (e.g. coordinates) → mlr3spatiotempcv::TaskRegrST or mlr3spatiotempcv::TaskClassifST in add-on package mlr3spatiotempcv.\nOrdinal Regression Task: The target is ordinal → TaskOrdinal in add-on package mlr3ordinal (still in development).\n\n17.2.2 The “Learner” in Machine Learning\nAfter you’ve defined your task, the next step in teaching a computer to make predictions or decisions is to choose a “learner.” Let’s explore what a learner is and how it fits into the mlr3 package.\n\n17.2.2.1 What is a “Learner”?\nThink of a learner as the method or tool that the computer uses to learn from the data. Another common name for a “learner” is a “model.” It’s similar to choosing a tutor or a teacher for a student. Different learners have different ways of understanding and processing information. For example:\n\nSome learners might be great at recognizing patterns in data, like a tutor who is excellent at spotting trends.\nOthers might be good at making decisions based on rules, like a tutor who uses step-by-step logic.\n\nIn machine learning, there are many types of learners, each with its own strengths and weaknesses. Here are a few examples:\n\n\nDecision Trees: These learners make decisions by asking a series of questions, like “Is the house larger than 1000 square feet?” and “Does it have more than 3 bedrooms?”\n\nk-Nearest Neighbors: These learners make predictions based on the similarity of new data points to existing data points.\n\nLinear Regression: This learner tries to fit a straight line through the data points to make predictions about numbers.\n\nRandom Forests: These are like a group of decision trees working together to make more accurate predictions.\n\nSupport Vector Machines: These learners find the best boundary that separates different categories in the data.\n\n17.2.2.2 Choosing the Right Learner\nSelecting the right learner is crucial because different learners work better for different types of tasks and data. For example:\n\nIf your task is to classify emails as spam or not spam, a decision tree or a support vector machine might work well.\nIf you’re predicting house prices, linear regression or random forests could be good choices.\n\nThe goal is to find a learner that can understand the patterns in your data and make accurate predictions. This is where the mlr3 package comes in handy. It provides a wide range of learners that you can choose from, making it easier to experiment and find the best learner for your task.\n\n17.2.2.3 Learners in mlr3\nIn the mlr3 package, learners are pre-built tools that you can easily use for your tasks. Here’s how it works:\n\n\nSelect a Learner: mlr3 provides a variety of learners to choose from, like decision trees, linear regression, and more.\n\nTrain the Learner: Once you’ve selected a learner, you provide it with your task (the problem, data, and target). The learner uses this information to learn and make predictions.\n\nEvaluate and Improve: After training, you can test how well the learner performs and make adjustments if needed, such as trying a different learner or fine-tuning the current one.\n\n17.2.2.4 mlr3 and Learners\nObjects of class Learner provide a unified interface to many popular machine learning algorithms in R. They consist of methods to train and predict a model for a Task and provide meta-information about the learners, such as the hyperparameters (which control the behavior of the learner) you can set.\nThe base class of each learner is Learner, specialized for regression as LearnerRegr and for classification as LearnerClassif. Other types of learners, provided by extension packages, also inherit from the Learner base class, e.g. mlr3proba::LearnerSurv or mlr3cluster::LearnerClust.\nAll Learners work in a two-stage procedure:\n\n\n\n\n\nFigure 17.3: Two stages of a learner. Top: data (features and a target) are passed to an (untrained) learner. Bottom: new data are passed to the trained model which makes predictions for the ‘missing’ target column.\n\n\n\n\nTraining stage: The training data (features and target) is passed to the Learner’s $train() function which trains and stores a model, i.e. the relationship of the target and features.\n\nPredict stage: The new data, usually a different slice of the original data than used for training, is passed to the $predict() method of the Learner. The model trained in the first step is used to predict the missing target, e.g. labels for classification problems or the numerical value for regression problems.\n\nThere are a number of predefined learners. The mlr3 package ships with the following set of classification and regression learners. We deliberately keep this small to avoid unnecessary dependencies:\n\n\nclassif.featureless: Simple baseline classification learner. The default is to always predict the label that is most frequent in the training set. While this is not very useful by itself, it can be used as a “fallback learner” to make predictions in case another, more sophisticated, learner failed for some reason.\n\nregr.featureless: Simple baseline regression learner. The default is to always predict the mean of the target in training set. Similar to mlr_learners_classif.featureless, it makes for a good “fallback learner”\n\nclassif.rpart: Single classification tree from package rpart.\n\nregr.rpart: Single regression tree from package rpart.\n\nThis set of baseline learners is usually insufficient for a real data analysis. Thus, we have cherry-picked implementations of the most popular machine learning method and collected them in the mlr3learners package:\n\nLinear (regr.lm) and logistic (classif.log_reg) regression\nPenalized Generalized Linear Models (regr.glmnet, classif.glmnet), possibly with built-in optimization of the penalization parameter (regr.cv_glmnet, classif.cv_glmnet)\n(Kernelized) k-Nearest Neighbors regression (regr.kknn) and classification (classif.kknn).\nKriging / Gaussian Process Regression (regr.km)\nLinear (classif.lda) and Quadratic (classif.qda) Discriminant Analysis\nNaive Bayes Classification (classif.naive_bayes)\nSupport-Vector machines (regr.svm, classif.svm)\nGradient Boosting (regr.xgboost, classif.xgboost)\nRandom Forests for regression and classification (regr.ranger, classif.ranger)\n\nMore machine learning methods and alternative implementations are collected in the mlr3extralearners repository.",
    "crumbs": [
      "statististics",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Machine Learning 2</span>"
    ]
  },
  {
    "objectID": "machine_learning_mlr3.html#setup",
    "href": "machine_learning_mlr3.html#setup",
    "title": "\n17  Machine Learning 2\n",
    "section": "\n17.3 Setup",
    "text": "17.3 Setup\n\nlibrary(mlr3verse)\nlibrary(GEOquery)\nlibrary(mlr3learners) # for knn\nlibrary(ranger) # for randomforest\nset.seed(789)",
    "crumbs": [
      "statististics",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Machine Learning 2</span>"
    ]
  },
  {
    "objectID": "machine_learning_mlr3.html#example-cancer-types",
    "href": "machine_learning_mlr3.html#example-cancer-types",
    "title": "\n17  Machine Learning 2\n",
    "section": "\n17.4 Example: Cancer types",
    "text": "17.4 Example: Cancer types\nIn this exercise, we will be classifying cancer types based on gene expression data. The data we are going to access are from Brouwer-Visser et al. (2018).\nThe data are from the Gene Expression Omnibus (GEO) database, a public repository of functional genomics data. The data are from a study that aimed to identify gene expression signatures that can distinguish between different types of cancer. The data include gene expression profiles from patients with different types of cancer. The goal is to build a machine learning model that can predict the cancer type based on the gene expression data.\n\n17.4.1 Understanding the Problem\nBefore we start building a machine learning model, it’s important to understand the problem we are trying to solve. Here are some key questions to consider:\n\nWhat are the features?\nWhat is the target variable?\nWhat type of machine learning task is this (classification, regression, clustering)?\nWhat is the goal of the analysis?\n\n17.4.2 Data Preparation\nUse the [GEOquery] package to fetch data about [GSE103512].\n\nlibrary(GEOquery)\ngse = getGEO(\"GSE103512\")[[1]]\n\nThe first step, a detail, is to convert from the older Bioconductor data structure (GEOquery was written in 2007), the ExpressionSet, to the newer SummarizedExperiment.\n\nlibrary(SummarizedExperiment)\nse = as(gse, \"SummarizedExperiment\")\n\nExamine two variables of interest, cancer type and tumor/normal status.\n\nwith(colData(se),table(`cancer.type.ch1`,`normal.ch1`))\n\n               normal.ch1\ncancer.type.ch1 no yes\n          BC    65  10\n          CRC   57  12\n          NSCLC 60   9\n          PCA   60   7\n\n\nBefore embarking on a machine learning analysis, we need to make sure that we understand the data. Things like missing values, outliers, and other problems can cause problems for machine learning algorithms.\nIn R, plotting, summaries, and other exploratory data analysis tools are available. PCA analysis, clustering, and other methods can also be used to understand the data. It is worth spending time on this step, as it can save time later.\n\n17.4.3 Feature selection and data cleaning\nWhile we could use all genes in the analysis, we will select the most informative genes using the variance of gene expression across samples. Other methods for feature selection are available, including those based on correlation with the outcome variable.\n\n\n\n\n\n\nFeature selection\n\n\n\nFeature selection should be done on the training data only, not the test data to avoid overfitting. The test data should be used only for evaluation. In other words, the test data should be “unseen” by the model until the final evaluation.\n\n\nRemember that the apply function applies a function to each row or column of a matrix. Here, we apply the sd function to each row of the expression matrix to get a vector of stan\n\nsds = apply(assay(se, 'exprs'),1,sd)\n## filter out normal tissues\nse_small = se[order(sds,decreasing = TRUE)[1:200],\n              colData(se)$characteristics_ch1.1=='normal: no']\n# remove genes with no gene symbol\nse_small = se_small[rowData(se_small)$Gene.Symbol!='',]\n\nTo make the data easier to work with, we will use the opportunity to use one of the rowData columns as the rownames of the data frame. The make.names function is used to make sure that the rownames are valid R variable names and unique.\n\n## convert to matrix for later use\ndat = assay(se_small, 'exprs')\nrownames(dat) = make.names(rowData(se_small)$Gene.Symbol)\n\nWe also need to transpose the data so that the rows are the samples and the columns are the features in order to use the data with mlr3.\n\nfeat_dat = t(dat)\ntumor = data.frame(tumor_type = colData(se_small)$cancer.type.ch1, feat_dat)\n\nThis is another good time to check the data. Make sure that the data is in the format that you expect. Check the dimensions, the column names, and the data types.\n\n17.4.4 Creating the “task”\nThe first step in using mlr3 is to create a task. A task is a data set with a target variable. In this case, the target variable is the cancer type. The mlr3 package provides a function to convert a data frame into a task. These tasks can be used with any machine learning algorithm in mlr3.\nThis is a classification task, so we will use the as_task_classif function to create the task. The classification task requires a target variable that is categorical.\n\nlibrary(mlr3)\ntumor$tumor_type = as.factor(tumor$tumor_type)\ntask = as_task_classif(tumor,target='tumor_type')\n\n\n17.4.5 Splitting the data\nHere, we randomly divide the data into 2/3 training data and 1/3 test data. This is a common split, but other splits can be used. The training data is used to train the model, and the test data is used to evaluate the trained model.\n\nset.seed(7)\ntrain_set = sample(task$row_ids, 0.67 * task$nrow)\ntest_set = setdiff(task$row_ids, train_set)\n\n\n\n\n\n\n\nImportant\n\n\n\nTraining and testing on the same data is a common mistake. We want to test the model on data that it has not seen before. This is the only way to know if the model is overfitting and to get an accurate estimate of the model’s performance.\n\n\nIn the next sections, we will train and evaluate three different models on the data: k-nearest-neighbor, classification tree, and random forest. Remember that the goal is to predict the cancer type based on the gene expression data. The mlr3 package uses the concept of “learners” to encapsulate different machine learning algorithms.\n\n17.4.6 Example learners\n\n17.4.6.1 K-nearest-neighbor\nThe first model we will use is the k-nearest-neighbor model. This model is based on the idea that similar samples have similar outcomes. The number of neighbors to use is a parameter that can be tuned. We’ll use the default value of 7, but you can try other values to see how they affect the results. In fact, mlr3 provides the ability to tune parameters automatically, but we won’t cover that here.\n\n17.4.6.1.1 Create the learner\nIn mlr3, the machine learning algorithms are called learners. To create a learner, we use the lrn function. The lrn function takes the name of the learner as an argument. The lrn function also takes other arguments that are specific to the learner. In this case, we will use the default values for the arguments.\n\nlibrary(mlr3learners)\nlearner = lrn(\"classif.kknn\")\n\nYou can get a list of all the learners available in mlr3 by using the lrn() function without any arguments.\n\nlrn()\n\n&lt;DictionaryLearner&gt; with 49 stored values\nKeys: classif.cv_glmnet, classif.debug, classif.featureless,\n  classif.glmnet, classif.kknn, classif.lda, classif.log_reg,\n  classif.multinom, classif.naive_bayes, classif.nnet, classif.qda,\n  classif.ranger, classif.rpart, classif.svm, classif.xgboost,\n  clust.agnes, clust.ap, clust.cmeans, clust.cobweb, clust.dbscan,\n  clust.dbscan_fpc, clust.diana, clust.em, clust.fanny,\n  clust.featureless, clust.ff, clust.hclust, clust.hdbscan,\n  clust.kkmeans, clust.kmeans, clust.MBatchKMeans, clust.mclust,\n  clust.meanshift, clust.optics, clust.pam, clust.SimpleKMeans,\n  clust.xmeans, regr.cv_glmnet, regr.debug, regr.featureless,\n  regr.glmnet, regr.kknn, regr.km, regr.lm, regr.nnet, regr.ranger,\n  regr.rpart, regr.svm, regr.xgboost\n\n\n\n17.4.6.1.2 Train\nTo train the model, we use the train function. The train function takes the task and the row ids of the training data as arguments.\n\nlearner$train(task, row_ids = train_set)\n\nHere, we can look at the trained model:\n\n# output is large, so do this on your own\nlearner$model\n\n\n17.4.6.1.3 Predict\nLets use our trained model works to predict the classes of the training data. Of course, we already know the classes of the training data, but this is a good way to check that the model is working as expected. It also gives us a measure of performance on the training data that we can compare to the test data to look for overfitting.\n\npred_train = learner$predict(task, row_ids=train_set)\n\nAnd check on the test data:\n\npred_test = learner$predict(task, row_ids=test_set)\n\n\n17.4.6.1.4 Assess\nIn this section, we can look at the accuracy and performance of our model on the training data and the test data. We can also look at the confusion matrix to see which classes are being confused with each other.\n\npred_train$confusion\n\n        truth\nresponse BC CRC NSCLC PCA\n   BC    42   0     0   0\n   CRC    0  40     0   0\n   NSCLC  1   0    44   0\n   PCA    0   0     0  35\n\n\nThis is a multi-class confusion matrix. The rows are the true classes and the columns are the predicted classes. The diagonal shows the number of samples that were correctly classified. The off-diagonal elements show the number of samples that were misclassified.\nWe can also look at the accuracy of the model on the training data and the test data. The accuracy is the number of correctly classified samples divided by the total number of samples.\n\nmeasures = msrs(c('classif.acc'))\npred_train$score(measures)\n\nclassif.acc \n  0.9938272 \n\n\n\npred_test$confusion\n\n        truth\nresponse BC CRC NSCLC PCA\n   BC    22   0     0   0\n   CRC    0  17     1   0\n   NSCLC  0   0    15   0\n   PCA    0   0     0  25\n\npred_test$score(measures)\n\nclassif.acc \n     0.9875 \n\n\nCompare the accuracy on the training data to the accuracy on the test data. Do you see any evidence of overfitting?\n\n17.4.6.2 Classification tree\nWe are going to use a classification tree to classify the data. A classification tree is a series of yes/no questions that are used to classify the data. The questions are based on the features in the data. The classification tree is built by finding the feature that best separates the data into the different classes. Then, the data is split based on the value of that feature. The process is repeated until the data is completely separated into the different classes.\n\n17.4.6.2.1 Train\n\n# in this case, we want to keep the model\n# so we can look at it later\nlearner = lrn(\"classif.rpart\", keep_model = TRUE)\n\n\nlearner$train(task, row_ids = train_set)\n\nWe can take a look at the model.\n\nlearner$model\n\nn= 162 \n\nnode), split, n, loss, yval, (yprob)\n      * denotes terminal node\n\n 1) root 162 118 NSCLC (0.26543210 0.24691358 0.27160494 0.21604938)  \n   2) CDHR5&gt;=5.101625 40   0 CRC (0.00000000 1.00000000 0.00000000 0.00000000) *\n   3) CDHR5&lt; 5.101625 122  78 NSCLC (0.35245902 0.00000000 0.36065574 0.28688525)  \n     6) ACPP&lt; 6.088431 87  43 NSCLC (0.49425287 0.00000000 0.50574713 0.00000000)  \n      12) GATA3&gt;=4.697803 41   1 BC (0.97560976 0.00000000 0.02439024 0.00000000) *\n      13) GATA3&lt; 4.697803 46   3 NSCLC (0.06521739 0.00000000 0.93478261 0.00000000) *\n     7) ACPP&gt;=6.088431 35   0 PCA (0.00000000 0.00000000 0.00000000 1.00000000) *\n\n\nDecision trees are easy to visualize if they are small. Here, we can see that the tree is very simple, with only two splits.\n\nlibrary(mlr3viz)\nlibrary(ggparty)\n\nLoading required package: ggplot2\n\n\nLoading required package: partykit\n\n\nLoading required package: grid\n\n\nLoading required package: libcoin\n\n\nLoading required package: mvtnorm\n\n\n\nAttaching package: 'partykit'\n\n\nThe following object is masked from 'package:SummarizedExperiment':\n\n    width\n\n\nThe following object is masked from 'package:GenomicRanges':\n\n    width\n\n\nThe following object is masked from 'package:IRanges':\n\n    width\n\n\nThe following object is masked from 'package:S4Vectors':\n\n    width\n\n\nThe following object is masked from 'package:BiocGenerics':\n\n    width\n\nautoplot(learner, type='ggparty')\n\n\n\n\n\n\n\n\n17.4.6.2.2 Predict\nNow that we have trained the model on the training data, we can use it to predict the classes of the training data and the test data. The $predict method takes a task and produces a prediction based on the trained model, in this case, called learner.\n\npred_train = learner$predict(task, row_ids=train_set)\n\nRemember that we split the data into training and test sets. We can use the trained model to predict the classes of the test data. Since the test data was not used to train the model, it is not “cheating” like what we just did where we did the prediction on the training data.\n\npred_test = learner$predict(task, row_ids=test_set)\n\n\n17.4.6.2.3 Assess\nFor classification tasks, we often look at a confusion matrix of the truth vs the predicted classes for the samples.\n\n\n\n\n\n\nImportant\n\n\n\nAssessing the performance of a model should always be reported from assessment on an independent test set.\n\n\n\npred_train$confusion\n\n        truth\nresponse BC CRC NSCLC PCA\n   BC    40   0     1   0\n   CRC    0  40     0   0\n   NSCLC  3   0    43   0\n   PCA    0   0     0  35\n\n\n\nWhat does this confusion matrix tell you?\n\nWe can also ask for several “measures” of the performance of the model. Here, we ask for the accuracy of the model. To get a complete list of measures, use msr().\n\nmeasures = msrs(c('classif.acc'))\npred_train$score(measures)\n\nclassif.acc \n  0.9753086 \n\n\n\nHow does the accuracy compare to the confusion matrix?\nHow does this accuracy compare to the accuracy of the k-nearest-neighbor model?\nHow about the decision tree model?\n\n\npred_test$confusion\n\n        truth\nresponse BC CRC NSCLC PCA\n   BC    20   0     1   0\n   CRC    0  17     3   0\n   NSCLC  2   0    12   0\n   PCA    0   0     0  25\n\npred_test$score(measures)\n\nclassif.acc \n      0.925 \n\n\n\nWhat does the confusion matrix in the test set tell you?\nHow do the assessments of the test and training sets differ?\n\n\n\n\n\n\n\nOverfitting\n\n\n\nWhen the assessment of the test set is worse than the evaluation of the training set, the model may be overfit. How to address overfitting varies by model type, but it is a sign that you should pay attention to model selection and parameters.\n\n\n\n17.4.6.3 RandomForest\n\nlearner = lrn(\"classif.ranger\", importance = \"impurity\")\n\n\n17.4.6.3.1 Train\n\nlearner$train(task, row_ids = train_set)\n\nAgain, you can look at the model that was trained.\n\nlearner$model\n\nRanger result\n\nCall:\n ranger::ranger(dependent.variable.name = task$target_names, data = task$data(),      probability = self$predict_type == \"prob\", case.weights = task$weights$weight,      num.threads = 1L, importance = \"impurity\") \n\nType:                             Classification \nNumber of trees:                  500 \nSample size:                      162 \nNumber of independent variables:  192 \nMtry:                             13 \nTarget node size:                 1 \nVariable importance mode:         impurity \nSplitrule:                        gini \nOOB prediction error:             0.62 % \n\n\nFor more details, the mlr3 random forest approach is based ont he ranger package. You can look at the ranger documentation.\n\nWhat is the OOB error in the output?\n\nRandom forests are a collection of decision trees. Since predictors enter the trees in a random order, the trees are different from each other. The random forest procedure gives us a measure of the “importance” of each variable.\n\nhead(learner$importance(), 15)\n\n   CDHR5  TRPS1.1    FABP1   EPS8L3    KRT20    EFHD1   LGALS4    TRPS1 \n4.791870 3.918063 3.692649 3.651422 3.340382 3.314491 2.952969 2.926175 \n   SFTPB  SFTPB.1    GATA3  GATA3.1  TMPRSS2    MUC12    POF1B \n2.805811 2.681004 2.344603 2.271845 2.248734 2.207347 1.806906 \n\n\nMore “important” variables are those that are more often used in the trees. Are the most important variables the same as the ones that were important in the decision tree?\nIf you are interested, look up a few of the important variables in the model to see if they make biological sense.\n\n17.4.6.3.2 Predict\nAgain, we can use the trained model to predict the classes of the training data and the test data.\n\npred_train = learner$predict(task, row_ids=train_set)\n\n\npred_test = learner$predict(task, row_ids=test_set)\n\n\n17.4.6.3.3 Assess\n\npred_train$confusion\n\n        truth\nresponse BC CRC NSCLC PCA\n   BC    43   0     0   0\n   CRC    0  40     0   0\n   NSCLC  0   0    44   0\n   PCA    0   0     0  35\n\n\n\nmeasures = msrs(c('classif.acc'))\npred_train$score(measures)\n\nclassif.acc \n          1 \n\n\n\npred_test$confusion\n\n        truth\nresponse BC CRC NSCLC PCA\n   BC    22   0     0   0\n   CRC    0  17     0   0\n   NSCLC  0   0    16   0\n   PCA    0   0     0  25\n\npred_test$score(measures)\n\nclassif.acc \n          1",
    "crumbs": [
      "statististics",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Machine Learning 2</span>"
    ]
  },
  {
    "objectID": "machine_learning_mlr3.html#example-predicting-age-from-dna-methylation",
    "href": "machine_learning_mlr3.html#example-predicting-age-from-dna-methylation",
    "title": "\n17  Machine Learning 2\n",
    "section": "\n17.5 Example Predicting age from DNA methylation",
    "text": "17.5 Example Predicting age from DNA methylation\nWe will be building a regression model for chronological age prediction, based on DNA methylation. This is based on the work of Jana Naue et al. 2017, in which biomarkers are examined to predict the chronological age of humans by analyzing the DNA methylation patterns. Different machine learning algorithms are used in this study to make an age prediction.\nIt has been recognized that within each individual, the level of DNA methylation changes with age. This knowledge is used to select useful biomarkers from DNA methylation datasets. The CpG sites with the highest correlation to age are selected as the biomarkers (and therefore features for building a regression model). In this tutorial, specific biomarkers are analyzed by machine learning algorithms to create an age prediction model.\nThe data are taken from this tutorial.\n\nlibrary(data.table)\nmeth_age = rbind(\n    fread('https://zenodo.org/record/2545213/files/test_rows_labels.csv'),\n    fread('https://zenodo.org/record/2545213/files/train_rows.csv')\n)\n\nLet’s take a quick look at the data.\n\nhead(meth_age)\n\n   RPA2_3 ZYG11A_4  F5_2 HOXC4_1 NKIRAS2_2 MEIS1_1 SAMD10_2 GRM2_9 TRIM59_5\n    &lt;num&gt;    &lt;num&gt; &lt;num&gt;   &lt;num&gt;     &lt;num&gt;   &lt;num&gt;    &lt;num&gt;  &lt;num&gt;    &lt;num&gt;\n1:  65.96    18.08 41.57   55.46     30.69   63.42    40.86  68.88    44.32\n2:  66.83    20.27 40.55   49.67     29.53   30.47    37.73  53.30    50.09\n3:  50.30    11.74 40.17   33.85     23.39   58.83    38.84  35.08    35.90\n4:  65.54    15.56 33.56   36.79     20.23   56.39    41.75  50.37    41.46\n5:  59.01    14.38 41.95   30.30     24.99   54.40    37.38  30.35    31.28\n6:  81.30    14.68 35.91   50.20     26.57   32.37    32.30  55.19    42.21\n   LDB2_3 ELOVL2_6 DDO_1 KLF14_2   Age\n    &lt;num&gt;    &lt;num&gt; &lt;num&gt;   &lt;num&gt; &lt;int&gt;\n1:  56.17    62.29 40.99    2.30    40\n2:  58.40    61.10 49.73    1.07    44\n3:  58.81    50.38 63.03    0.95    28\n4:  58.05    50.58 62.13    1.99    37\n5:  65.80    48.74 41.88    0.90    24\n6:  70.15    61.36 33.62    1.87    43\n\n\nAs before, we create the task object, but this time we use as_task_regr() to create a regression task.\n\nWhy is this a regression task?\n\n\ntask = as_task_regr(meth_age,target = 'Age')\n\n\nset.seed(7)\ntrain_set = sample(task$row_ids, 0.67 * task$nrow)\ntest_set = setdiff(task$row_ids, train_set)\n\n\n17.5.1 Example learners\n\n17.5.1.1 Linear regression\nWe will start with a simple linear regression model.\n\nlearner = lrn(\"regr.lm\")\n\n\n17.5.1.1.1 Train\n\nlearner$train(task, row_ids = train_set)\n\nWhen you train a linear regression model, we can evaluate some of the diagnostic plots to see if the model is appropriate (Figure 17.4).\n\npar(mfrow=c(2,2))\nplot(learner$model)\n\n\n\n\n\n\nFigure 17.4: Regression diagnostic plots. The top left plot shows the residuals vs. fitted values. The top right plot shows the normal Q-Q plot. The bottom left plot shows the scale-location plot. The bottom right plot shows the residuals vs. leverage.\n\n\n\n\n\n17.5.1.1.2 Predict\n\npred_train = learner$predict(task, row_ids=train_set)\n\n\npred_test = learner$predict(task, row_ids=test_set)\n\n\n17.5.1.1.3 Assess\n\npred_train\n\n&lt;PredictionRegr&gt; for 209 observations:\n    row_ids truth response\n        298    29 31.40565\n        103    58 56.26019\n        194    53 48.96480\n---                       \n        312    48 52.61195\n        246    66 67.66312\n        238    38 39.38414\n\n\nWe can plot the relationship between the truth and response, or predicted value to see visually how our model performs.\n\nlibrary(ggplot2)\nggplot(pred_train,aes(x=truth, y=response)) +\n    geom_point() +\n    geom_smooth(method='lm')\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\nWe can use the r-squared of the fit to roughly compare two models.\n\nmeasures = msrs(c('regr.rsq'))\npred_train$score(measures)\n\n regr.rsq \n0.9376672 \n\n\n\npred_test\n\n&lt;PredictionRegr&gt; for 103 observations:\n    row_ids truth response\n          4    37 37.64301\n          5    24 28.34777\n          7    34 33.22419\n---                       \n        306    42 41.65864\n        307    63 58.68486\n        309    68 70.41987\n\npred_test$score(measures)\n\n regr.rsq \n0.9363526 \n\n\n\n17.5.1.2 Regression tree\n\nlearner = lrn(\"regr.rpart\", keep_model = TRUE)\n\n\n17.5.1.2.1 Train\n\nlearner$train(task, row_ids = train_set)\n\n\nlearner$model\n\nn= 209 \n\nnode), split, n, deviance, yval\n      * denotes terminal node\n\n 1) root 209 45441.4500 43.27273  \n   2) ELOVL2_6&lt; 56.675 98  5512.1220 30.24490  \n     4) ELOVL2_6&lt; 47.24 47   866.4255 24.23404  \n       8) GRM2_9&lt; 31.3 34   289.0588 22.29412 *\n       9) GRM2_9&gt;=31.3 13   114.7692 29.30769 *\n     5) ELOVL2_6&gt;=47.24 51  1382.6270 35.78431  \n      10) F5_2&gt;=39.295 35   473.1429 33.28571 *\n      11) F5_2&lt; 39.295 16   213.0000 41.25000 *\n   3) ELOVL2_6&gt;=56.675 111  8611.3690 54.77477  \n     6) ELOVL2_6&lt; 65.365 63  3101.2700 49.41270  \n      12) KLF14_2&lt; 3.415 37  1059.0270 46.16216 *\n      13) KLF14_2&gt;=3.415 26  1094.9620 54.03846 *\n     7) ELOVL2_6&gt;=65.365 48  1321.3120 61.81250 *\n\n\nWhat is odd about using a regression tree here is that we end up with only a few discrete estimates of age. Each “leaf” has a value.\n\n17.5.1.2.2 Predict\n\npred_train = learner$predict(task, row_ids=train_set)\n\n\npred_test = learner$predict(task, row_ids=test_set)\n\n\n17.5.1.2.3 Assess\n\npred_train\n\n&lt;PredictionRegr&gt; for 209 observations:\n    row_ids truth response\n        298    29 33.28571\n        103    58 61.81250\n        194    53 46.16216\n---                       \n        312    48 54.03846\n        246    66 61.81250\n        238    38 41.25000\n\n\nWe can see the effect of the discrete values much more clearly here.\n\nlibrary(ggplot2)\nggplot(pred_train,aes(x=truth, y=response)) +\n    geom_point() +\n    geom_smooth(method='lm')\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\nAnd the r-squared values for this model prediction shows quite a bit of difference from the linear regression above.\n\nmeasures = msrs(c('regr.rsq'))\npred_train$score(measures)\n\n regr.rsq \n0.8995351 \n\n\n\npred_test\n\n&lt;PredictionRegr&gt; for 103 observations:\n    row_ids truth response\n          4    37 41.25000\n          5    24 33.28571\n          7    34 33.28571\n---                       \n        306    42 46.16216\n        307    63 61.81250\n        309    68 61.81250\n\npred_test$score(measures)\n\n regr.rsq \n0.8545402 \n\n\n\n17.5.1.3 RandomForest\nRandomforest is also tree-based, but unlike the single regression tree above, randomforest is a “forest” of trees which will eliminate the discrete nature of a single tree.\n\nlearner = lrn(\"regr.ranger\", mtry=2, min.node.size=20)\n\n\n17.5.1.3.1 Train\n\nlearner$train(task, row_ids = train_set)\n\n\nlearner$model\n\nRanger result\n\nCall:\n ranger::ranger(dependent.variable.name = task$target_names, data = task$data(),      case.weights = task$weights$weight, num.threads = 1L, mtry = 2L,      min.node.size = 20L) \n\nType:                             Regression \nNumber of trees:                  500 \nSample size:                      209 \nNumber of independent variables:  13 \nMtry:                             2 \nTarget node size:                 20 \nVariable importance mode:         none \nSplitrule:                        variance \nOOB prediction error (MSE):       18.85364 \nR squared (OOB):                  0.9137009 \n\n\n\n17.5.1.3.2 Predict\n\npred_train = learner$predict(task, row_ids=train_set)\n\n\npred_test = learner$predict(task, row_ids=test_set)\n\n\n17.5.1.3.3 Assess\n\npred_train\n\n&lt;PredictionRegr&gt; for 209 observations:\n    row_ids truth response\n        298    29 30.62154\n        103    58 58.05445\n        194    53 48.25661\n---                       \n        312    48 51.49846\n        246    66 64.39315\n        238    38 38.18038\n\n\n\nggplot(pred_train,aes(x=truth, y=response)) +\n    geom_point() +\n    geom_smooth(method='lm')\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\nmeasures = msrs(c('regr.rsq'))\npred_train$score(measures)\n\nregr.rsq \n0.960961 \n\n\n\npred_test\n\n&lt;PredictionRegr&gt; for 103 observations:\n    row_ids truth response\n          4    37 37.79631\n          5    24 29.18371\n          7    34 33.26780\n---                       \n        306    42 40.29101\n        307    63 58.26534\n        309    68 63.15481\n\npred_test$score(measures)\n\n regr.rsq \n0.9208394",
    "crumbs": [
      "statististics",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Machine Learning 2</span>"
    ]
  },
  {
    "objectID": "machine_learning_mlr3.html#example-expression-prediction-from-histone-modification-data",
    "href": "machine_learning_mlr3.html#example-expression-prediction-from-histone-modification-data",
    "title": "\n17  Machine Learning 2\n",
    "section": "\n17.6 Example: Expression prediction from histone modification data",
    "text": "17.6 Example: Expression prediction from histone modification data\nIn this little set of exercises, you will be using histone marks near a gene to predict its expression (Figure 17.5).\n\\[\ny ~ h1 + h2 + h3 + ...\n\\tag{17.1}\\]\n\n\n\n\n\nFigure 17.5: What is the combined effect of histone marks on gene expression?\n\n\nThe data are from a study that aimed to predict gene expression from histone modification data. The data include gene expression levels and histone modification data for a set of genes. The goal is to build a machine learning model that can predict gene expression levels based on the histone modification data. The histone modification data are simply summaries of the histone marks within the promoter, defined as the region 2kb upstream of the transcription start site for this exercise.\nWe will try a couple of different approaches:\n\nPenalized regression\nRandomForest\n\n\n17.6.1 The Data\nThe data in this exercise were developed by Anshul Kundaje. We are not going to focus on the details of the data collection, etc. Instead, this is\n\nfullFeatureSet &lt;- read.table(\"http://seandavi.github.io/ITR/expression-prediction/features.txt\");\n\nWhat are the column names of the predictor variables?\n\ncolnames(fullFeatureSet)\n\n [1] \"Control\"  \"Dnase\"    \"H2az\"     \"H3k27ac\"  \"H3k27me3\" \"H3k36me3\"\n [7] \"H3k4me1\"  \"H3k4me2\"  \"H3k4me3\"  \"H3k79me2\" \"H3k9ac\"   \"H3k9me1\" \n[13] \"H3k9me3\"  \"H4k20me1\"\n\n\nThese are going to be predictors combined into a model. Some of our learners will rely on predictors being on a similar scale. Are our data already there?\nTo perform centering and scaling by column, we can convert to a matrix and then use scale.\n\npar(mfrow=c(1,2))\nscaled_features &lt;- scale(as.matrix(fullFeatureSet))\nboxplot(fullFeatureSet, title='Original data')\nboxplot(scaled_features, title='Centered and scaled data')\n\n\n\n\n\n\nFigure 17.6: Boxplots of original and scaled data.\n\n\n\n\nThere is a row for each gene and a column for each histone mark and we can see that the data are centered and scaled by column. We can also see some patterns in the data (see Figure 17.7).\n\nsampled_features &lt;- fullFeatureSet[sample(nrow(scaled_features), 500),]\nlibrary(ComplexHeatmap)\n\n========================================\nComplexHeatmap version 2.20.0\nBioconductor page: http://bioconductor.org/packages/ComplexHeatmap/\nGithub page: https://github.com/jokergoo/ComplexHeatmap\nDocumentation: http://jokergoo.github.io/ComplexHeatmap-reference\n\nIf you use it in published research, please cite either one:\n- Gu, Z. Complex Heatmap Visualization. iMeta 2022.\n- Gu, Z. Complex heatmaps reveal patterns and correlations in multidimensional \n    genomic data. Bioinformatics 2016.\n\n\nThe new InteractiveComplexHeatmap package can directly export static \ncomplex heatmaps into an interactive Shiny app with zero effort. Have a try!\n\nThis message can be suppressed by:\n  suppressPackageStartupMessages(library(ComplexHeatmap))\n========================================\n\nHeatmap(sampled_features, name='histone marks', show_row_names=FALSE)\n\nWarning: The input is a data frame-like object, convert it to a matrix.\n\n\n\n\n\n\n\nFigure 17.7: Heatmap of 500 randomly sampled rows of the data. Columns are histone marks and there is a row for each gene.\n\n\n\n\nNow, we can read in the associated gene expression measures that will become our “target” for prediction.\n\ntarget &lt;- scan(url(\"http://seandavi.github.io/ITR/expression-prediction/target.txt\"), skip=1)\n# make into a dataframe\nexp_pred_data &lt;- data.frame(gene_expression=target, scaled_features)\n\nAnd the first few rows of the target data frame using:\n\nhead(exp_pred_data,3)\n\n                            gene_expression    Control      Dnase       H2az\nENSG00000000419.7.49575069         6.082343  0.7452926  0.7575546  1.0728432\nENSG00000000457.8.169863093        2.989145  1.9509786  1.0216546  0.3702787\nENSG00000000938.7.27961645        -5.058894 -0.3505542 -1.4482958 -1.0390775\n                               H3k27ac   H3k27me3   H3k36me3    H3k4me1\nENSG00000000419.7.49575069   1.0950440 -0.5125312  1.1334793  0.4127984\nENSG00000000457.8.169863093  0.7142157 -0.4079244  0.8739005  1.1649282\nENSG00000000938.7.27961645  -1.0173283  1.4117293 -0.5157582 -0.5017450\n                               H3k4me2    H3k4me3   H3k79me2     H3k9ac\nENSG00000000419.7.49575069   1.2136176  1.1202901  1.5155803  1.2468256\nENSG00000000457.8.169863093  0.6456572  0.6508561  0.7976487  0.5792891\nENSG00000000938.7.27961645  -0.1878255 -0.6560973 -1.3803974 -1.0067972\n                              H3k9me1   H3k9me3   H4k20me1\nENSG00000000419.7.49575069  0.1426980  1.185622  1.9599992\nENSG00000000457.8.169863093 0.3630902  1.014923 -0.2695111\nENSG00000000938.7.27961645  0.6564520 -1.370871 -1.8773178\n\n\n\n17.6.2 Create task\n\nexp_pred_task = as_task_regr(exp_pred_data, target='gene_expression')\n\nPartition the data into test and training sets. We will use \\(\\frac{1}{3}\\) and \\(\\frac{2}{3}\\) of the data for testing.\n\nsplit = partition(exp_pred_task)\n\n\n17.6.3 Example learners\n\n17.6.3.1 Linear regression\n\nlearner = lrn(\"regr.lm\")\n\n\n17.6.3.1.1 Train\n\nlearner$train(exp_pred_task, split$train)\n\n\n17.6.3.1.2 Predict\n\npred_train = learner$predict(exp_pred_task, split$train)\npred_test = learner$predict(exp_pred_task, split$test)\n\n\n17.6.3.1.3 Assess\n\npred_train\n\n&lt;PredictionRegr&gt; for 5789 observations:\n    row_ids     truth response\n          1  6.082343 5.139251\n          2  2.989145 2.909552\n          7  5.838076 4.563759\n---                           \n       8543  9.016443 6.141272\n       8583  7.475697 2.543423\n       8618 10.049236 5.523896\n\nplot(pred_train)\n\n\n\n\n\n\n\nFor the training data:\n\nmeasures = msrs(c('regr.rsq'))\npred_train$score(measures)\n\n regr.rsq \n0.7495474 \n\n\nAnd the test data:\n\npred_test$score(measures)\n\n regr.rsq \n0.7526609 \n\n\nAnd the plot of the test data predictions:\n\nplot(pred_test)\n\n\n\n\n\n\n\n\n17.6.3.2 Penalized regression\nImagine you want to teach a computer to predict house prices based on various features like size, number of bedrooms, and location. You decide to use regression, which finds a relationship between these features and the house prices. But what if your model becomes too complicated? This is where penalized regression comes in.\n\n17.6.3.2.1 The Problem with Overfitting\nSometimes, the model tries too hard to fit every single data point perfectly. This can make the model very complex, like trying to draw a perfect line through a very bumpy path. This problem is called overfitting. An overfitted model works well on the data it has seen (training data) but performs poorly on new, unseen data (testing data).\n\n17.6.3.2.2 Introducing Penalized Regression\nPenalized regression helps prevent overfitting by adding a “penalty” to the model for being too complex. Think of it as a way to encourage the model to be simpler and more general. There are three common types of penalized regression:\n\n\nRidge Regression (L2 Penalty):\n\nAdds a penalty based on the size of the coefficients. It tries to keep all coefficients small.\nIf the model’s equation looks too complicated, Ridge Regression will push it towards a simpler form by shrinking the coefficients.\nImagine you have a rubber band that pulls the coefficients towards zero, making the model less likely to overfit.\n\n\n\nLasso Regression (L1 Penalty):\n\nAdds a penalty that can shrink some coefficients all the way to zero.\nThis means Lasso Regression can completely remove some features from the model, making it simpler.\nImagine you have a pair of scissors that can cut off the least important features, leaving only the most important ones.\n\n\n\nElastic Net:\n\nCombines both Ridge and Lasso penalties. It adds penalties for both the size and the number of coefficients.\nThis method balances between shrinking coefficients and eliminating some altogether, offering the benefits of both Ridge and Lasso.\nThink of Elastic Net as using both the rubber band (Ridge) and scissors (Lasso) to simplify the model.\n\n\n\nWith our data, the number of predictors is not huge, but we might be interested in 1) reducing overfitting, 2) improving interpretability, or 3) both by minimizing the number of predictors in our model without drastically affecting our prediction accuracy. Without penalized regression, the model might come up with a very complex equation. With Ridge, Lasso, or Elastic Net, the model simplifies this equation by either shrinking the coefficients (Ridge), removing some of them (Lasso), or balancing both (Elastic Net).\nHere’s a simple summary:\n\n\nRidge Regression: Reduces the impact of less important features by shrinking their coefficients.\n\nLasso Regression: Can eliminate some features entirely by setting their coefficients to zero.\n\nElastic Net: Combines the effects of Ridge and Lasso, shrinking some coefficients and eliminating others.\n\nUsing penalized regression in machine learning ensures that your model:\n\n\nPerforms Better on New Data: By avoiding overfitting, the model can make more accurate predictions on new, unseen data.\n\nIs Easier to Interpret: A simpler model with fewer features is easier to understand and explain.\n\n17.6.3.3 Penalized Regression with mlr3\nIn the mlr3 package, you can easily apply penalized regression methods to your tasks. Here’s how:\n\n\nSelect Penalized Regression Learners: mlr3 provides learners for Ridge, Lasso, and Elastic Net Regression.\n\nTrain the Learner: Use your data to train the chosen penalized regression model.\n\nEvaluate and Adjust: Check how well the model performs and make adjustments if needed.\n\nThis description explains penalized regression, including Ridge, Lasso, and Elastic Net, in an intuitive way, highlighting their benefits and how they work, while relating them to familiar concepts and the mlr3 package.\nRecall that we can use penalized regression to select the most important predictors from a large set of predictors. In this case, we will use the glmnet package to perform penalized regression, but we will use the mlr interface to glmnet to make it easier to use.\nThe nfolds parameter is the number of folds to use in the cross-validation procedure.\nWhat is Cross-Validation? Cross-validation is a technique used to assess how well a model will perform on unseen data. It involves splitting the data into multiple parts, training the model on some of these parts, and validating it on the remaining parts. This process is repeated several times to ensure the model’s performance is consistent.\nWhy Use Cross-Validation? Cross-validation helps to:\n\nAvoid Overfitting: By testing the model on different subsets of the data, cross-validation helps ensure that the model does not memorize the training data but learns to generalize from it.\nSelect the Best Model Parameters: Penalized regression models, such as those trained with glmnet, have parameters that control the strength of the penalty (e.g., lambda). Cross-validation helps find the best values for these parameters.\n\nWhen using the glmnet package, cross-validation can be performed using the cv.glmnet function. Here’s how the process works:\n\nSplit the Data: The data is divided into 𝑘 k folds (common choices are 5 or 10 folds). Each fold is a subset of the data.\nTrain and Validate: The model is trained 𝑘 k times. In each iteration, 𝑘 − 1 k−1 folds are used for training, and the remaining fold is used for validation. This process is repeated until each fold has been used as the validation set exactly once.\nCalculate Performance: The performance of the model (e.g., mean squared error for regression) is calculated for each fold. The average performance across all folds is computed to get an overall measure of how well the model is expected to perform on unseen data.\nSelect the Best Parameters: The cv.glmnet function evaluates different values of the penalty parameter (lambda). It selects the lambda value that results in the best average performance across the folds.\n\nIn this case, we will use the cv_glmnet learner, which will automatically select the best value of the penalization parameters. When the alpha parameter is set to 0, the model is a Ridge regression model. When the alpha parameter is set to 1, the model is a Lasso regression model.\n\nlearner = lrn(\"regr.cv_glmnet\", nfolds=10, alpha=0)\n\n\n17.6.3.3.1 Train\n\nlearner$train(exp_pred_task)\n\n\nmeasures = msrs(c('regr.rsq', 'regr.mse', 'regr.rmse'))\npred_train$score(measures)\n\n regr.rsq  regr.mse regr.rmse \n0.7495474 4.8736194 2.2076275 \n\n\nIn the case of the penalized regression, we can also look at the coefficients of the model.\n\ncoef(learner$model)\n\n15 x 1 sparse Matrix of class \"dgCMatrix\"\n                     s1\n(Intercept)  0.10173828\nControl     -0.08042502\nDnase        0.91127090\nH2az         0.33880640\nH3k27ac      0.15845313\nH3k27me3    -0.25171391\nH3k36me3     0.72063384\nH3k4me1     -0.08222957\nH3k4me2      0.13101892\nH3k4me3      0.38905759\nH3k79me2     0.99247076\nH3k9ac       0.52009300\nH3k9me1     -0.09183614\nH3k9me3     -0.17912878\nH4k20me1     0.11235659\n\n\nNote that the coefficients are all zero for the histone marks that were not selected by the model. In this case, we can use the model not to predict new data, but to help us understand the data.\n\npred_train = learner$predict(exp_pred_task, split$train)\npred_test = learner$predict(exp_pred_task, split$test)\n\n\n17.6.3.3.2 Assess\n\npred_train\n\n&lt;PredictionRegr&gt; for 5789 observations:\n    row_ids     truth response\n          1  6.082343 4.923259\n          2  2.989145 2.936421\n          7  5.838076 4.619141\n---                           \n       8543  9.016443 5.580735\n       8583  7.475697 2.565638\n       8618 10.049236 5.226577\n\nplot(pred_train)\n\n\n\n\n\n\n\nFor the training data:\n\nmeasures = msrs(c('regr.rsq'))\npred_train$score(measures)\n\n regr.rsq \n0.7422423 \n\n\nAnd the test data:\n\npred_test$score(measures)\n\n regr.rsq \n0.7481403 \n\n\nAnd the plot of the test data predictions:\n\nplot(pred_test)\n\n\n\n\n\n\n\n\n# Calculate the R-squared value\ntruth &lt;- pred_test$truth\npredicted &lt;- pred_test$response\nrss &lt;- sum((truth - predicted)^2)  # Residual sum of squares\ntss &lt;- sum((truth - mean(truth))^2)  # Total sum of squares\nr_squared &lt;- 1 - (rss / tss)\n\n\n\n\n\nBrouwer-Visser, Jurriaan, Wei-Yi Cheng, Anna Bauer-Mehren, Daniela Maisel, Katharina Lechner, Emilia Andersson, Joel T. Dudley, and Francesca Milletti. 2018. “Regulatory T-Cell Genes Drive Altered Immune Microenvironment in Adult Solid Cancers and Allow for Immune Contextual Patient Subtyping.” Cancer Epidemiology, Biomarkers & Prevention 27 (1): 103–12. https://doi.org/10.1158/1055-9965.EPI-17-0461.",
    "crumbs": [
      "statististics",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Machine Learning 2</span>"
    ]
  },
  {
    "objectID": "geoquery.html",
    "href": "geoquery.html",
    "title": "\n18  Accessing and working with public omics data\n",
    "section": "",
    "text": "18.1 Background\nThe data we are going to access are from this paper.\nIn this little exercise, we will:",
    "crumbs": [
      "Bioconductor",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Accessing and working with public omics data</span>"
    ]
  },
  {
    "objectID": "geoquery.html#background",
    "href": "geoquery.html#background",
    "title": "\n18  Accessing and working with public omics data\n",
    "section": "",
    "text": "Background: The tumor microenvironment is an important factor in cancer immunotherapy response. To further understand how a tumor affects the local immune system, we analyzed immune gene expression differences between matching normal and tumor tissue.Methods: We analyzed public and new gene expression data from solid cancers and isolated immune cell populations. We also determined the correlation between CD8, FoxP3 IHC, and our gene signatures.Results: We observed that regulatory T cells (Tregs) were one of the main drivers of immune gene expression differences between normal and tumor tissue. A tumor-specific CD8 signature was slightly lower in tumor tissue compared with normal of most (12 of 16) cancers, whereas a Treg signature was higher in tumor tissue of all cancers except liver. Clustering by Treg signature found two groups in colorectal cancer datasets. The high Treg cluster had more samples that were consensus molecular subtype 1/4, right-sided, and microsatellite-instable, compared with the low Treg cluster. Finally, we found that the correlation between signature and IHC was low in our small dataset, but samples in the high Treg cluster had significantly more CD8+ and FoxP3+ cells compared with the low Treg cluster.Conclusions: Treg gene expression is highly indicative of the overall tumor immune environment.Impact: In comparison with the consensus molecular subtype and microsatellite status, the Treg signature identifies more colorectal tumors with high immune activation that may benefit from cancer immunotherapy.\n\n\n\nAccess public omics data using the GEOquery package\nGet an opportunity to work with another SummarizedExperiment object.\nPerform a simple unsupervised analysis to visualize these public data.",
    "crumbs": [
      "Bioconductor",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Accessing and working with public omics data</span>"
    ]
  },
  {
    "objectID": "geoquery.html#geoquery-to-pca",
    "href": "geoquery.html#geoquery-to-pca",
    "title": "\n18  Accessing and working with public omics data\n",
    "section": "\n18.2 GEOquery to PCA",
    "text": "18.2 GEOquery to PCA\nThe first step is to install the R package GEOquery. This package allows us to access data from the Gene Expression Omnibus (GEO) database. GEO is a public repository of omics data.\n\nBiocManager::install(\"GEOquery\")\n\nGEOquery has only one commonly used function, getGEO() which takes a GEO accession number as an argument. The GEO accession number is a unique identifier for a dataset.\nUse the GEOquery package to fetch data about GSE103512.\n\nlibrary(GEOquery)\ngse = getGEO(\"GSE103512\")[[1]]\n\nYou might ask why we are using [[1]] at the end of the getGEO() function. The reason is that getGEO() returns a list of GSE objects. We are only interested in the first one (and in this case, the only one). We return a list of GSE objects because in the early days, it was not unusual to have a single GEO accession number represent multiple datasets. While uncommon now, we’ve kept the convention since lots of “older” data is still quite useful.\nAgain, a historically-derived detail, is to convert from the older Bioconductor data structure (GEOquery was written in 2007), the ExpressionSet, to the newer SummarizedExperiment.\n\nlibrary(SummarizedExperiment)\nse = as(gse, \"SummarizedExperiment\")\n\nUse some code to determine the answers to the following:\n\nWhat is the class of se?\nWhat are the dimensions of se?\nWhat are the dimensions of the assay slot of se?\nWhat are the dimensions of the colData slot of se?\nWhat variables are in the colData slot of se?\n\nExamine two variables of interest, cancer type and tumor/normal status. The with function is a convenience to allow us to access variables in a data frame by name (rather than having to do dataframe$variable_name. Recalling that the table function is a convenient way to summarize the counts of unique values in a vector, we can use with to access the variables of interest and table to summarize the counts of unique values.\n\nwith(colData(se),table(`cancer.type.ch1`,`normal.ch1`))\n\n               normal.ch1\ncancer.type.ch1 no yes\n          BC    65  10\n          CRC   57  12\n          NSCLC 60   9\n          PCA   60   7\n\n\n\nHow many samples are there of each cancer type?\nHow many samples are there of each tumor/normal status?\n\nWhen performing unsupervised analysis, it is common to filter genes by variance to find the most informative genes. It is common practice to filter genes by standard deviation or some other measure of variability and keep the top X percent of them when performing dimensionality reduction. There is not a single right answer to what percentage to use, so try a few to see what happens. In the example code, I chose to use the top 500 genes by standard deviation, but you can play with the threshold to see what happens.\nRecall that the assay function is used to access the data matrix of the SummarizedExperiment object.\nThink through the code below and then run it.\n\nsds = apply(assay(se, 'exprs'),1,sd)\ndat = assay(se, 'exprs')[order(sds,decreasing = TRUE)[1:500],]\n\nIf you don’t recognize the function apply, it is a function that applies a function to each row or column of a matrix. In this case, we are applying the sd function to each row of the data matrix. The order function is used to sort the standard deviations in decreasing order (when decreasing=TRUE). And the [1:500] is used to subset the data matrix to the top 500 genes by standard deviation.\nPerform PCA and prepare for plotting. We will be using ggplot2, so we need to make a data.frame before plotting.\n\npca_results &lt;- prcomp(t(dat))\npca_df = as.data.frame(pca_results$x)\npca_df$Type=factor(colData(se)[,'cancer.type.ch1'])\npca_df$Normal = factor(colData(se)[,'normal.ch1'])\n\nNow, we are going to plot the results of the PCA, coloring the points by cancer type and using different shapes for normal and tumor samples.\n\nlibrary(ggplot2)\nggplot(pca_df, aes(x=PC1,y=PC2,shape=Normal,color=Type)) + \n    geom_point( alpha=0.6) + theme(text=element_text(size = 18))\n\n\n\n\n\n\n\nIn this case, the x-axis is the first principal component and the y-axis is the second principal component.\n\nWhat do you see?\nWhat about additional principal components?\nBonus: Try using the GGally package to plot principal components (using the ggpairs function).\nBonus: Calculate the variance explained by each principal component and plot the results.",
    "crumbs": [
      "Bioconductor",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Accessing and working with public omics data</span>"
    ]
  },
  {
    "objectID": "bioc-summarizedexperiment.html",
    "href": "bioc-summarizedexperiment.html",
    "title": "\n19  Introduction to SummarizedExperiment\n",
    "section": "",
    "text": "19.1 Anatomy of a SummarizedExperiment\nThe SummarizedExperiment package contains two classes: SummarizedExperiment and RangedSummarizedExperiment.\nSummarizedExperiment is a matrix-like container where rows represent features of interest (e.g. genes, transcripts, exons, etc.) and columns represent samples. The objects contain one or more assays, each represented by a matrix-like object of numeric or other mode. The rows of a SummarizedExperiment object represent features of interest. Information about these features is stored in a DataFrame object, accessible using the function rowData(). Each row of the DataFrame provides information on the feature in the corresponding row of the SummarizedExperiment object. Columns of the DataFrame represent different attributes of the features of interest, e.g., gene or transcript IDs, etc.\nRangedSummarizedExperiment is the “child”” of the SummarizedExperiment class which means that all the methods on SummarizedExperiment also work on a RangedSummarizedExperiment.\nThe fundamental difference between the two classes is that the rows of a RangedSummarizedExperiment object represent genomic ranges of interest instead of a DataFrame of features. The RangedSummarizedExperiment ranges are described by a GRanges or a GRangesList object, accessible using the rowRanges() function.\nFigure 19.1 displays the class geometry and highlights the vertical (column) and horizontal (row) relationships.",
    "crumbs": [
      "Bioconductor",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Introduction to `SummarizedExperiment`</span>"
    ]
  },
  {
    "objectID": "bioc-summarizedexperiment.html#anatomy-of-a-summarizedexperiment",
    "href": "bioc-summarizedexperiment.html#anatomy-of-a-summarizedexperiment",
    "title": "\n19  Introduction to SummarizedExperiment\n",
    "section": "",
    "text": "Figure 19.1: Summarized Experiment. There are three main components, the colData(), the rowData() and the assays(). The accessors for the various parts of a complete SummarizedExperiment object match the names.\n\n\n\n19.1.1 Assays\nThe airway package contains an example dataset from an RNA-Seq experiment of read counts per gene for airway smooth muscles. These data are stored in a RangedSummarizedExperiment object which contains 8 different experimental and assays 64,102 gene transcripts.\n\n\nLoading required package: airway\n\n\nWarning in library(package, lib.loc = lib.loc, character.only = TRUE,\nlogical.return = TRUE, : there is no package called 'airway'\n\n\nBioconductor version 3.19 (BiocManager 1.30.23), R 4.4.0 (2024-04-24)\n\n\nInstalling package(s) 'airway'\n\n\ninstalling the source package 'airway'\n\n\nOld packages: 'KernSmooth', 'survival'\n\n\n\nlibrary(SummarizedExperiment)\ndata(airway, package=\"airway\")\nse &lt;- airway\nse\n\nclass: RangedSummarizedExperiment \ndim: 63677 8 \nmetadata(1): ''\nassays(1): counts\nrownames(63677): ENSG00000000003 ENSG00000000005 ... ENSG00000273492\n  ENSG00000273493\nrowData names(10): gene_id gene_name ... seq_coord_system symbol\ncolnames(8): SRR1039508 SRR1039509 ... SRR1039520 SRR1039521\ncolData names(9): SampleName cell ... Sample BioSample\n\n\nTo retrieve the experiment data from a SummarizedExperiment object one can use the assays() accessor. An object can have multiple assay datasets each of which can be accessed using the $ operator. The airway dataset contains only one assay (counts). Here each row represents a gene transcript and each column one of the samples.\n\nassays(se)$counts\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSRR1039508\nSRR1039509\nSRR1039512\nSRR1039513\nSRR1039516\nSRR1039517\nSRR1039520\nSRR1039521\n\n\n\nENSG00000000003\n679\n448\n873\n408\n1138\n1047\n770\n572\n\n\nENSG00000000005\n0\n0\n0\n0\n0\n0\n0\n0\n\n\nENSG00000000419\n467\n515\n621\n365\n587\n799\n417\n508\n\n\nENSG00000000457\n260\n211\n263\n164\n245\n331\n233\n229\n\n\nENSG00000000460\n60\n55\n40\n35\n78\n63\n76\n60\n\n\nENSG00000000938\n0\n0\n2\n0\n1\n0\n0\n0\n\n\nENSG00000000971\n3251\n3679\n6177\n4252\n6721\n11027\n5176\n7995\n\n\nENSG00000001036\n1433\n1062\n1733\n881\n1424\n1439\n1359\n1109\n\n\nENSG00000001084\n519\n380\n595\n493\n820\n714\n696\n704\n\n\nENSG00000001167\n394\n236\n464\n175\n658\n584\n360\n269\n\n\n\n\n\n\n19.1.2 ‘Row’ (regions-of-interest) data\nThe rowRanges() accessor is used to view the range information for a RangedSummarizedExperiment. (Note if this were the parent SummarizedExperiment class we’d use rowData()). The data are stored in a GRangesList object, where each list element corresponds to one gene transcript and the ranges in each GRanges correspond to the exons in the transcript.\n\nrowRanges(se)\n\nGRangesList object of length 63677:\n$ENSG00000000003\nGRanges object with 17 ranges and 2 metadata columns:\n       seqnames            ranges strand |   exon_id       exon_name\n          &lt;Rle&gt;         &lt;IRanges&gt;  &lt;Rle&gt; | &lt;integer&gt;     &lt;character&gt;\n   [1]        X 99883667-99884983      - |    667145 ENSE00001459322\n   [2]        X 99885756-99885863      - |    667146 ENSE00000868868\n   [3]        X 99887482-99887565      - |    667147 ENSE00000401072\n   [4]        X 99887538-99887565      - |    667148 ENSE00001849132\n   [5]        X 99888402-99888536      - |    667149 ENSE00003554016\n   ...      ...               ...    ... .       ...             ...\n  [13]        X 99890555-99890743      - |    667156 ENSE00003512331\n  [14]        X 99891188-99891686      - |    667158 ENSE00001886883\n  [15]        X 99891605-99891803      - |    667159 ENSE00001855382\n  [16]        X 99891790-99892101      - |    667160 ENSE00001863395\n  [17]        X 99894942-99894988      - |    667161 ENSE00001828996\n  -------\n  seqinfo: 722 sequences (1 circular) from an unspecified genome\n\n...\n&lt;63676 more elements&gt;\n\n\n\n19.1.3 ‘Column’ (sample) data\nSample meta-data describing the samples can be accessed using colData(), and is a DataFrame that can store any number of descriptive columns for each sample row.\n\ncolData(se)\n\nDataFrame with 8 rows and 9 columns\n           SampleName     cell      dex    albut        Run avgLength\n             &lt;factor&gt; &lt;factor&gt; &lt;factor&gt; &lt;factor&gt;   &lt;factor&gt; &lt;integer&gt;\nSRR1039508 GSM1275862  N61311     untrt    untrt SRR1039508       126\nSRR1039509 GSM1275863  N61311     trt      untrt SRR1039509       126\nSRR1039512 GSM1275866  N052611    untrt    untrt SRR1039512       126\nSRR1039513 GSM1275867  N052611    trt      untrt SRR1039513        87\nSRR1039516 GSM1275870  N080611    untrt    untrt SRR1039516       120\nSRR1039517 GSM1275871  N080611    trt      untrt SRR1039517       126\nSRR1039520 GSM1275874  N061011    untrt    untrt SRR1039520       101\nSRR1039521 GSM1275875  N061011    trt      untrt SRR1039521        98\n           Experiment    Sample    BioSample\n             &lt;factor&gt;  &lt;factor&gt;     &lt;factor&gt;\nSRR1039508  SRX384345 SRS508568 SAMN02422669\nSRR1039509  SRX384346 SRS508567 SAMN02422675\nSRR1039512  SRX384349 SRS508571 SAMN02422678\nSRR1039513  SRX384350 SRS508572 SAMN02422670\nSRR1039516  SRX384353 SRS508575 SAMN02422682\nSRR1039517  SRX384354 SRS508576 SAMN02422673\nSRR1039520  SRX384357 SRS508579 SAMN02422683\nSRR1039521  SRX384358 SRS508580 SAMN02422677\n\n\nThis sample metadata can be accessed using the $ accessor which makes it easy to subset the entire object by a given phenotype.\n\n# subset for only those samples treated with dexamethasone\nse[, se$dex == \"trt\"]\n\nclass: RangedSummarizedExperiment \ndim: 63677 4 \nmetadata(1): ''\nassays(1): counts\nrownames(63677): ENSG00000000003 ENSG00000000005 ... ENSG00000273492\n  ENSG00000273493\nrowData names(10): gene_id gene_name ... seq_coord_system symbol\ncolnames(4): SRR1039509 SRR1039513 SRR1039517 SRR1039521\ncolData names(9): SampleName cell ... Sample BioSample\n\n\n\n19.1.4 Experiment-wide metadata\nMeta-data describing the experimental methods and publication references can be accessed using metadata().\n\nmetadata(se)\n\n[[1]]\nExperiment data\n  Experimenter name: Himes BE \n  Laboratory: NA \n  Contact information:  \n  Title: RNA-Seq transcriptome profiling identifies CRISPLD2 as a glucocorticoid responsive gene that modulates cytokine function in airway smooth muscle cells. \n  URL: http://www.ncbi.nlm.nih.gov/pubmed/24926665 \n  PMIDs: 24926665 \n\n  Abstract: A 226 word abstract is available. Use 'abstract' method.\n\n\nNote that metadata() is just a simple list, so it is appropriate for any experiment wide metadata the user wishes to save, such as storing model formulas.\n\nmetadata(se)$formula &lt;- counts ~ dex + albut\n\nmetadata(se)\n\n[[1]]\nExperiment data\n  Experimenter name: Himes BE \n  Laboratory: NA \n  Contact information:  \n  Title: RNA-Seq transcriptome profiling identifies CRISPLD2 as a glucocorticoid responsive gene that modulates cytokine function in airway smooth muscle cells. \n  URL: http://www.ncbi.nlm.nih.gov/pubmed/24926665 \n  PMIDs: 24926665 \n\n  Abstract: A 226 word abstract is available. Use 'abstract' method.\n\n$formula\ncounts ~ dex + albut",
    "crumbs": [
      "Bioconductor",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Introduction to `SummarizedExperiment`</span>"
    ]
  },
  {
    "objectID": "bioc-summarizedexperiment.html#common-operations-on-summarizedexperiment",
    "href": "bioc-summarizedexperiment.html#common-operations-on-summarizedexperiment",
    "title": "\n19  Introduction to SummarizedExperiment\n",
    "section": "\n19.2 Common operations on SummarizedExperiment\n",
    "text": "19.2 Common operations on SummarizedExperiment\n\n\n19.2.1 Subsetting\n\n\n[ Performs two dimensional subsetting, just like subsetting a matrix or data frame.\n\n\n# subset the first five transcripts and first three samples\nse[1:5, 1:3]\n\nclass: RangedSummarizedExperiment \ndim: 5 3 \nmetadata(2): '' formula\nassays(1): counts\nrownames(5): ENSG00000000003 ENSG00000000005 ENSG00000000419\n  ENSG00000000457 ENSG00000000460\nrowData names(10): gene_id gene_name ... seq_coord_system symbol\ncolnames(3): SRR1039508 SRR1039509 SRR1039512\ncolData names(9): SampleName cell ... Sample BioSample\n\n\n\n\n$ operates on colData() columns, for easy sample extraction.\n\n\nse[, se$cell == \"N61311\"]\n\nclass: RangedSummarizedExperiment \ndim: 63677 2 \nmetadata(2): '' formula\nassays(1): counts\nrownames(63677): ENSG00000000003 ENSG00000000005 ... ENSG00000273492\n  ENSG00000273493\nrowData names(10): gene_id gene_name ... seq_coord_system symbol\ncolnames(2): SRR1039508 SRR1039509\ncolData names(9): SampleName cell ... Sample BioSample\n\n\n\n19.2.2 Getters and setters\n\n\nrowRanges() / (rowData()), colData(), metadata()\n\n\n\ncounts &lt;- matrix(1:15, 5, 3, dimnames=list(LETTERS[1:5], LETTERS[1:3]))\n\ndates &lt;- SummarizedExperiment(assays=list(counts=counts),\n                              rowData=DataFrame(month=month.name[1:5], day=1:5))\n\n# Subset all January assays\ndates[rowData(dates)$month == \"January\", ]\n\nclass: SummarizedExperiment \ndim: 1 3 \nmetadata(0):\nassays(1): counts\nrownames(1): A\nrowData names(2): month day\ncolnames(3): A B C\ncolData names(0):\n\n\n\n\nassay() versus assays() There are two accessor functions for extracting the assay data from a SummarizedExperiment object. assays() operates on the entire list of assay data as a whole, while assay() operates on only one assay at a time. assay(x, i) is simply a convenience function which is equivalent to assays(x)[[i]].\n\n\nassays(se)\n\nList of length 1\nnames(1): counts\n\nassays(se)[[1]][1:5, 1:5]\n\n                SRR1039508 SRR1039509 SRR1039512 SRR1039513 SRR1039516\nENSG00000000003        679        448        873        408       1138\nENSG00000000005          0          0          0          0          0\nENSG00000000419        467        515        621        365        587\nENSG00000000457        260        211        263        164        245\nENSG00000000460         60         55         40         35         78\n\n# assay defaults to the first assay if no i is given\nassay(se)[1:5, 1:5]\n\n                SRR1039508 SRR1039509 SRR1039512 SRR1039513 SRR1039516\nENSG00000000003        679        448        873        408       1138\nENSG00000000005          0          0          0          0          0\nENSG00000000419        467        515        621        365        587\nENSG00000000457        260        211        263        164        245\nENSG00000000460         60         55         40         35         78\n\nassay(se, 1)[1:5, 1:5]\n\n                SRR1039508 SRR1039509 SRR1039512 SRR1039513 SRR1039516\nENSG00000000003        679        448        873        408       1138\nENSG00000000005          0          0          0          0          0\nENSG00000000419        467        515        621        365        587\nENSG00000000457        260        211        263        164        245\nENSG00000000460         60         55         40         35         78\n\n\n\n19.2.3 Range-based operations\n\n\nsubsetByOverlaps() SummarizedExperiment objects support all of the findOverlaps() methods and associated functions. This includes subsetByOverlaps(), which makes it easy to subset a SummarizedExperiment object by an interval.\n\nIn tne next code block, we define a region of interest (or many regions of interest) and then subset our SummarizedExperiment by overlaps with this region.\n\n# Subset for only rows which are in the interval 100,000 to 110,000 of\n# chromosome 1\nroi &lt;- GRanges(seqnames=\"1\", ranges=100000:1100000)\nsub_se = subsetByOverlaps(se, roi)\nsub_se\n\nclass: RangedSummarizedExperiment \ndim: 74 8 \nmetadata(2): '' formula\nassays(1): counts\nrownames(74): ENSG00000131591 ENSG00000177757 ... ENSG00000272512\n  ENSG00000273443\nrowData names(10): gene_id gene_name ... seq_coord_system symbol\ncolnames(8): SRR1039508 SRR1039509 ... SRR1039520 SRR1039521\ncolData names(9): SampleName cell ... Sample BioSample\n\ndim(sub_se)\n\n[1] 74  8",
    "crumbs": [
      "Bioconductor",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Introduction to `SummarizedExperiment`</span>"
    ]
  },
  {
    "objectID": "bioc-summarizedexperiment.html#constructing-a-summarizedexperiment",
    "href": "bioc-summarizedexperiment.html#constructing-a-summarizedexperiment",
    "title": "\n19  Introduction to SummarizedExperiment\n",
    "section": "\n19.3 Constructing a SummarizedExperiment\n",
    "text": "19.3 Constructing a SummarizedExperiment\n\nOften, SummarizedExperiment or RangedSummarizedExperiment objects are returned by functions written by other packages. However it is possible to create them by hand with a call to the SummarizedExperiment() constructor. The code below is simply to illustrate the mechanics of creating an object from scratch. In practice, you will probably have the pieces of the object from other sources such as Excel files or csv files.\nConstructing a RangedSummarizedExperiment with a GRanges as the rowRanges argument:\n\nnrows &lt;- 200\nncols &lt;- 6\ncounts &lt;- matrix(runif(nrows * ncols, 1, 1e4), nrows)\nrowRanges &lt;- GRanges(rep(c(\"chr1\", \"chr2\"), c(50, 150)),\n                     IRanges(floor(runif(200, 1e5, 1e6)), width=100),\n                     strand=sample(c(\"+\", \"-\"), 200, TRUE),\n                     feature_id=sprintf(\"ID%03d\", 1:200))\ncolData &lt;- DataFrame(Treatment=rep(c(\"ChIP\", \"Input\"), 3),\n                     row.names=LETTERS[1:6])\n\nSummarizedExperiment(assays=list(counts=counts),\n                     rowRanges=rowRanges, colData=colData)\n\nclass: RangedSummarizedExperiment \ndim: 200 6 \nmetadata(0):\nassays(1): counts\nrownames: NULL\nrowData names(1): feature_id\ncolnames(6): A B ... E F\ncolData names(1): Treatment\n\n\nA SummarizedExperiment can be constructed with or without supplying a DataFrame for the rowData argument:\n\nSummarizedExperiment(assays=list(counts=counts), colData=colData)\n\nclass: SummarizedExperiment \ndim: 200 6 \nmetadata(0):\nassays(1): counts\nrownames: NULL\nrowData names(0):\ncolnames(6): A B ... E F\ncolData names(1): Treatment",
    "crumbs": [
      "Bioconductor",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Introduction to `SummarizedExperiment`</span>"
    ]
  },
  {
    "objectID": "ranges_exercises.html",
    "href": "ranges_exercises.html",
    "title": "\n20  Ranges Exercises\n",
    "section": "",
    "text": "20.1 Exercise 1\nIn this exercise, we will use DNAse hypersensitivity data to practice working with a GRanges object.\nShow the answerlibrary(AnnotationHub)\nah = AnnotationHub()\nquery(ah, \"goldenpath/hg19/encodeDCC/wgEncodeUwDnase/wgEncodeUwDnaseK562PkRep1.narrowPeak.gz\")\n# the thing above should have only one record, so we can \n# just grab it\ndnase = query(ah, \"goldenpath/hg19/encodeDCC/wgEncodeUwDnase/wgEncodeUwDnaseK562PkRep1.narrowPeak.gz\")[[1]]\nShow the answerdnase\nclass(dnase)\nShow the answermcols(dnase)\nShow the answerlibrary(GenomicFeatures)\ntable(seqnames(dnase))\nShow the answersummary(width(dnase))\nShow the answerseqlevels(dnase)\nseqlevelsStyle(dnase)\nseqlevelsStyle(dnase) = 'ensembl'\nseqlevelsStyle(dnase)\nseqlevels(dnase)\nShow the answersum(width(dnase))\nsum(seqlengths(dnase))\nsum(width(dnase))/sum(seqlengths(dnase))",
    "crumbs": [
      "Bioconductor",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Ranges Exercises</span>"
    ]
  },
  {
    "objectID": "ranges_exercises.html#exercise-1",
    "href": "ranges_exercises.html#exercise-1",
    "title": "\n20  Ranges Exercises\n",
    "section": "",
    "text": "Use the AnnotationHub package to find the goldenpath/hg19/encodeDCC/wgEncodeUwDnase/wgEncodeUwDnaseK562PkRep1.narrowPeak.gz GRanges object. Load that into R as the variable dnase.\n\n\n\nWhat type of object is dnase?\n\n\n\nWhat metadata is stored in dnase?\n\n\n\nHow many peaks are on each chromosome?\n\n\n\nWhat are the mean, min, max, and median widths of the peaks?\n\n\n\nWhat are the sequences that were used in the analysis? Do the names have “chr” or not? Experiment with changing the seqlevelsStyle to adjust the sequence names.\n\n\n\nWhat is the total amount of “landscape” covered by the peaks? Assume that the peaks do not overlap. What portion of the genome does this represent?",
    "crumbs": [
      "Bioconductor",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Ranges Exercises</span>"
    ]
  },
  {
    "objectID": "ranges_exercises.html#exercise-2",
    "href": "ranges_exercises.html#exercise-2",
    "title": "\n20  Ranges Exercises\n",
    "section": "\n20.2 Exercise 2",
    "text": "20.2 Exercise 2\nIn this exercise, we are going to load the second DNAse hypersensitivity replicate to investigate overlaps with the first replicate.\n\nUse the AnnotationHub to find the second replicate, goldenpath/hg19/encodeDCC/wgEncodeUwDnase/wgEncodeUwDnaseK562PkRep2.narrowPeak.gz. Load that as dnase2.\n\n\nShow the answerquery(ah, \"goldenpath/hg19/encodeDCC/wgEncodeUwDnase/wgEncodeUwDnaseK562PkRep2.narrowPeak.gz\")\n# the thing above should have only one record, so we can \n# just grab it\ndnase2 = query(ah, \"goldenpath/hg19/encodeDCC/wgEncodeUwDnase/wgEncodeUwDnaseK562PkRep2.narrowPeak.gz\")[[1]]\n\n\n\nHow many peaks are there in dnase and dnase2? Are there are similar number?\n\n\nShow the answerlength(dnase)\nlength(dnase2)\n\n\n\nWhat are the peak sizes for dnase2?\n\n\nShow the answersummary(width(dnase2))\n\n\n\nWhat proportion of the genome does dnase2 cover?\n\n\nShow the answersum(width(dnase))/sum(seqlengths(dnase))\n\n\n\nCount the number of peaks from dnase that overlap with dnase2.\n\n\nShow the answersum(dnase %over% dnase2)\n\n\n\nAssume that your peak-caller was “too specific” and that you want to expand your peaks by 50 bp on each end (so make them 100 bp larger). Use a combination of resize (and pay attention to the fix argument) and width to do this expansion to dnase and call the new GRanges object “dnase_wide”.\n\n\nShow the answerw = width(dnase)\ndnase_wide = resize(dnase, width=w+100, fix='center') #make a copy\nwidth(dnase_wide)",
    "crumbs": [
      "Bioconductor",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Ranges Exercises</span>"
    ]
  },
  {
    "objectID": "ranges_exercises.html#exercise-3",
    "href": "ranges_exercises.html#exercise-3",
    "title": "\n20  Ranges Exercises\n",
    "section": "\n20.3 Exercise 3",
    "text": "20.3 Exercise 3\nIn this exercise, we are going to look at the overlap of DNAse sites relative to genes. To get started, install and load the TxDb.Hsapiens.UCSC.hg19.knownGene txdb object.\nBiocManager::install(\"TxDb.Hsapiens.UCSC.hg19.knownGene\")\nlibrary(\"TxDb.Hsapiens.UCSC.hg19.knownGene\")\nkg = TxDb.Hsapiens.UCSC.hg19.knownGene\n\nLoad the transcripts from the knownGene txdb into a variable. What is the class of this object?\n\n\nShow the answerlibrary(\"TxDb.Hsapiens.UCSC.hg19.knownGene\")\nkg = TxDb.Hsapiens.UCSC.hg19.knownGene\ngx = genes(kg)\nclass(gx)\nlength(gx)\n\n\n\nRead about the flank method for GRanges objects. How could you use that to get the “promoter” regions of the transcripts? Let’s assume that the promoter region is 2kb upstream of the gene.\n\n\nShow the answerflank(gx,2000)\n\n\n\nInstead of using flank, could you do the same thing with the TxDb object? (See ?promoters).\n\n\nShow the answerproms = promoters(kg)\n\n\n\nDo any of the regions in the promoters overlap with each other?\n\n\nShow the answersummary(countOverlaps(proms))\n\n\n\nTo find overlap of our DNAse sites with promoters, let’s collapse overlapping “promoters” to just keep the contiguous regions by using reduce.\n\n\nShow the answer# reduce takes all overlapping regions and collapses them\n# into a single region that spans all of the overlapping regions\nprom_regions = reduce(proms)\n\n# now we can check for overlaps\nsummary(countOverlaps(prom_regions))\n\n\n\nCount the number of DNAse sites that overlap with our promoter regions.\n\n\nShow the answersum(dnase %over% prom_regions)\n# if you notice no overlap, check the seqlevels\n# and seqlevelsStyle\nseqlevelsStyle(dnase) = \"UCSC\"\nsum(dnase %over% prom_regions)\nsum(dnase2 %over% prom_regions)\n\n\n\nIs this surprising? If we were to assume that the promoter and dnase regions are “independent” of each other, what number of overlaps would we expect?\n\n\nShow the answerprop_proms = sum(width(prom_regions))/sum(seqlengths(prom_regions))\nprop_dnase = sum(width(dnase))/sum(seqlengths(prom_regions))\n# Iff the dnase and promoter regions are \n# not related, then we would expect this number\n# of DNAse overlaps with promoters.\nprop_proms * prop_dnase * length(dnase)",
    "crumbs": [
      "Bioconductor",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Ranges Exercises</span>"
    ]
  },
  {
    "objectID": "ranges_exercises.html#exercise-4",
    "href": "ranges_exercises.html#exercise-4",
    "title": "\n20  Ranges Exercises\n",
    "section": "\n20.4 Exercise 4",
    "text": "20.4 Exercise 4\nWe’ll be using data from histone modification ChIP-seq experiments in human cells to illustrate the concepts of genomic ranges and features. The data consists of genomic intervals representing regions of the genome where specific histone modifications are enriched. These intervals are typically identified using ChIP-seq, a technique that maps protein-DNA interactions across the genome.\nThe ChIP-seq data is stored in a BED file format, which is a tab-delimited text file format commonly used to represent genomic intervals. Each line in the BED file corresponds to a genomic interval and contains information about the chromosome, start and end positions, and strand orientation of the interval. Additional columns may include metadata such as the signal strength or significance of the interval.\nThe AnnotationHub package in Bioconductor provides access to a wide range of genomic datasets, including ChIP-seq data. We can use this package to retrieve the ChIP-seq data for histone modifications in human cells and convert it into a GenomicRanges object for further analysis.\nhttps://www.encodeproject.org/chip-seq/histone/\nLet’s start by loading the AnnotationHub package and retrieving the ChIP-seq data for histone modifications in human cells. You can read more about the AnnotationHub package and how to use it in the Bioconductor documentation.\n\nShow the answerlibrary(AnnotationHub)\nah &lt;- AnnotationHub()\n\n\nThere are multiple ways to search the AnnotationHub database. We’ve done that for you and here are the GRanges objects for each of four histone marks, and one histone mark replicate.\n\nShow the answerh3k4me1 &lt;- ah[['AH25832']]\nh3k4me3 &lt;- ah[['AH25833']]\nh3k9ac &lt;- ah[['AH25834']]\nh3k27me3 &lt;- ah[['AH25835']]\nh3k4me3_2 &lt;- ah[['AH27284']]\n\n\nEach of these variables now represents the peak calls after a chip-seq experiment pulling down the histone mark of interest. In the encode project these records were bed files. The bed files have been converted to GRanges objects to allow computation within R.\n\nShow the answer# Grab cpg islands as well\ncpg = query(ah, c('cpg','UCSC','hg19'))[[1]]\n\n\nLet’s say that we don’t know the behavior of the histone methylation marks with respect to CpG islands. We could ask the question, “What is the overlap of the histone peaks with CpG islands?”\n\nShow the answersum(h3k4me1 %over% cpg)\n\n\nWe might want to actually count the number of bases of overlap between the methyl mark and CpG islands.\n\nShow the answer# The intersection of two peak sets results in the \n# overlapping regions as a new set of regions\n# The width of each peak is the number of overlapping bases\n# And the sum of the widths is the total bases overlapping\nsum(width(intersect(h3k4me1, cpg)))\n\n\nBut some methyl marks are known to have very broad signals, meaning that there is a higher chance of overlapping CpG islands just because there are more methylated bases. We can adjust for this by “normalizing” for all possible bases covered by either set of peaks, using union. We might think of this as a sort of “enrichment score” of one set in another set.\n\nShow the answersum(width(union(h3k4me1, cpg)))\n# and now \"normalize\" \nsum(width(intersect(h3k4me1, cpg)))/sum(width(union(h3k4me1, cpg)))\n\n\nLet’s write a small function to calculate our little enrichment score.\n\nShow the answerrange_enrichment_score &lt;- function(r1, r2) {\n  i = sum(width(intersect(r1, r2)))\n  u = sum(width(union(r1,r2)))\n  return(i/u)\n}\n\n\nAnd give it a try:\n\nShow the answerrange_enrichment_score(h3k4me1, cpg)",
    "crumbs": [
      "Bioconductor",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Ranges Exercises</span>"
    ]
  },
  {
    "objectID": "single-cell-atac-and-rna-transfer-learning.html",
    "href": "single-cell-atac-and-rna-transfer-learning.html",
    "title": "\n21  Transfer Learning in scATAC-seq and scRNA-seq\n",
    "section": "",
    "text": "21.1 Background\nAnalyzing open chromatin regions has been a crucial aspect of understanding gene regulation and cellular identity. Over the years, several techniques have been developed to identify and study these accessible regions of the genome. One of the earliest methods was DNase-seq, which uses the DNase I enzyme to digest exposed DNA, followed by sequencing of the resulting fragments. This method, introduced in the late 1970s and adapted for high-throughput sequencing in 2006, provided valuable insights into the locations of regulatory elements and transcription factor binding sites. Another technique, called FAIRE-seq (Formaldehyde-Assisted Isolation of Regulatory Elements), was developed in 2007. This method relies on the differential crosslinking of proteins to DNA in open and closed chromatin regions, followed by sequencing of the isolated DNA fragments. FAIRE-seq offered a complementary approach to DNase-seq for identifying open chromatin regions. In 2013, a groundbreaking method called ATAC-seq (Assay for Transposase-Accessible Chromatin using sequencing) was introduced by Buenrostro et al. This technique revolutionized the study of open chromatin by providing a simple, fast, and sensitive approach. ATAC-seq employs a hyperactive Tn5 transposase that simultaneously cuts and inserts adapters into accessible DNA regions. The resulting fragments are then sequenced, revealing the locations of open chromatin. ATAC-seq offers several advantages over previous methods. It requires a small number of cells (as few as 500), making it suitable for studying rare cell types or precious samples. Additionally, the protocol is relatively simple and can be completed in a few hours, compared to the multiple days required for DNase-seq or FAIRE-seq. The high resolution and sensitivity of ATAC-seq have made it a widely adopted technique in the field of epigenomics. The introduction of single-cell ATAC-seq (scATAC-seq) in 2015 further expanded the capabilities of this method. By combining ATAC-seq with microfluidic technologies or combinatorial indexing, researchers can now profile open chromatin landscapes at the single-cell level. This advancement allows for the exploration of cellular heterogeneity, the identification of rare cell types, and the study of dynamic changes in chromatin accessibility during processes like differentiation or disease progression.",
    "crumbs": [
      "Bioconductor",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Transfer Learning in scATAC-seq and scRNA-seq</span>"
    ]
  },
  {
    "objectID": "single-cell-atac-and-rna-transfer-learning.html#background",
    "href": "single-cell-atac-and-rna-transfer-learning.html#background",
    "title": "\n21  Transfer Learning in scATAC-seq and scRNA-seq\n",
    "section": "",
    "text": "21.1.1 Protocol\n\n\n\n\n\n\n1. Nuclei Isolation and Tn5 Transposition (Figure 21.1 (a))\n\nNuclei Isolation: The first step involves isolating nuclei from cells while keeping the chromatin intact. This ensures that the native chromatin structure is preserved.\nExposure to Tn5 Transposase: The isolated nuclei are then exposed to Tn5 transposase. The Tn5 enzyme is a hyperactive transposase that simultaneously cuts DNA and inserts sequencing adapters into accessible chromatin regions. This step is crucial as it tags open chromatin areas with sequencing adapters, making them ready for subsequent amplification and sequencing.\nFragment Isolation and Amplification: After transposition, the resulting DNA fragments are isolated. These fragments are then amplified to create a library of transposed sequences. This library represents the accessible regions of the genome and is ready for sequencing.\nSequencing and Identification: The amplified DNA fragments are sequenced using high-throughput sequencing technologies. The resulting sequences are mapped to the reference genome to identify accessible chromatin regions, known as ATAC-seq peaks. These peaks indicate regions where the chromatin is open and potentially active in gene regulation.\n\n2. Detailed Mechanism of Tn5 Transposition (Figure 21.1 (b))\n\nTransposition into Native Chromatin: The Tn5 transposase inserts sequencing adapters into accessible regions of the chromatin. This insertion creates post-transposition DNA fragments, which include the Tn5-induced nick.\nInitial Extension and Amplification: Following transposition, the DNA fragments undergo an initial extension at 72°C. This is followed by amplification, during which barcodes and additional adapter components are added. These steps are essential for the preparation of the final ATAC-seq library.\nPurification and Library Construction: The amplified fragments are purified to construct the final ATAC-seq library. The sites of chromatin accessibility are defined by the Tn5 insertion, which is marked by specific adapter sequences.\n\n3. Data Analysis and Interpretation (Figure 21.1 (c))\n\nATAC-seq Signal and Peaks: The sequenced data is analyzed to generate an ATAC-seq signal, which shows the read density across the genome. Peaks in the ATAC-seq signal correspond to regions of open chromatin. The example in the figure shows differential chromatin accessibility between two cell types (Cell type X and Cell type Y). Each cell type exhibits unique peaks, indicating distinct regulatory regions.\nTranscription Factor Binding and Gene Expression: The open chromatin regions often contain binding sites for transcription factors (TFs). For instance, the motif for a specific TF (TF B) can be identified within a peak. Binding of TF B to its motif within an enhancer or promoter region can regulate the expression of a nearby gene (Gene A). The figure illustrates how the binding of TF B to its motif leads to gene A expression in one cell type but not in another, highlighting the functional impact of chromatin accessibility on gene regulation.\n\n21.1.2 Primary data processing\n\n\n\n\n\n\n\nFigure 21.2: ATAC-seq pipelines universally require several common bioinformatic tools. This figure/table shows tools used in various published ATAC-seq pipelines. The figure also displays the typical steps in an ATAC-seq analysis.\n\n\n\n\n\n21.1.3 Quality control metrics\nIn addition to basic read counts and variant quality scores, there are a number of metrics that are valuable for ATAC-seq (or other regional enrichment experiemnts, like ChIP-seq). Figure Figure 21.3 shows example plots from the pepatac workflow.\n\n\n\n\n\n\n\nFigure 21.3: (A) Library complexity plots the read count versus externally calculated deduplicated read counts. Red line is library complexity curve for SRR5427743. Dashed line represents a completely unique library. Red diamond is the externally calculated duplicate read count. (B) TSS enrichment quality control plot. (C) Fragment length distribution showing characteristic peaks at mono-, di-, and tri-nucleosomes. (D) Cumulative fraction of reads in annotated genomic features (cFRiF). Inset: Fraction of reads in those features (FRiF). (E) Signal tracks including: nucleotide-resolution and smoothed signal tracks. PEPATAC default peaks are called using the default pipeline settings for MACS2 (32). (F) Distribution of peaks over the genome. (G) Distribution of peaks relative to TSS. (H) Distribution of peaks in annotated genomic partitions. Data from SRR5427743.",
    "crumbs": [
      "Bioconductor",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Transfer Learning in scATAC-seq and scRNA-seq</span>"
    ]
  },
  {
    "objectID": "single-cell-atac-and-rna-transfer-learning.html#atac-seq-and-rna-seq-integration",
    "href": "single-cell-atac-and-rna-transfer-learning.html#atac-seq-and-rna-seq-integration",
    "title": "\n21  Transfer Learning in scATAC-seq and scRNA-seq\n",
    "section": "\n21.2 ATAC-seq and RNA-seq integration",
    "text": "21.2 ATAC-seq and RNA-seq integration\nSingle-cell transcriptomics has revolutionized our ability to characterize cell states, but a deeper biological understanding requires more than just clustering cells. As new methods emerge to measure different cellular modalities, integrating these datasets becomes a key challenge in better understanding cellular identity and function. For instance, when performing scRNA-seq and scATAC-seq experiments on the same biological system, consistently annotating both datasets with the same cell type labels can be difficult due to the sparsity of scATAC-seq data and the lack of interpretable gene markers in scRNA-seq data.\nIn a 2019 paper by Stuart, Butler, and colleagues, methods were introduced to integrate scRNA-seq and scATAC-seq datasets from the same biological system. This vignette demonstrates these methods, including:\nUsing an annotated scRNA-seq dataset to label cells from an scATAC-seq experiment Co-visualizing and co-embedding cells from scRNA-seq and scATAC-seq Projecting scATAC-seq cells onto a UMAP derived from an scRNA-seq experiment\nThe Signac package, recently developed for analyzing single-cell resolution chromatin datasets like scATAC-seq, is extensively used in this vignette.\nThe methods are demonstrated using a publicly available ~12,000 human PBMC ‘multiome’ dataset from 10x Genomics, where scRNA-seq and scATAC-seq profiles were simultaneously collected from the same cells. For the purpose of this vignette, the datasets are treated as if they originated from two different experiments and are integrated together. Since they were originally measured in the same cells, this provides a ground truth for assessing the accuracy of the integration. It is emphasized that the use of the multiome dataset here is for demonstration and evaluation purposes, and users should apply these methods to separately collected scRNA-seq and scATAC-seq datasets.\n\n21.2.1 Setup\n\nBiocManager::install('satijalab/seurat-data')\n\nThe following code loads pre-packaged data from the PBMC Multiome dataset from 10x Genomics.\n\nlibrary(SeuratData)\n# install the dataset and load requirements\nInstallData(\"pbmcMultiome\")\n\nWe’ll be using some additional packages. If you get errors here that a package is not available, you can use BiocManager::install to install the missing package and then rerun this step.\n\nlibrary(Seurat)\nlibrary(Signac)\nlibrary(EnsDb.Hsapiens.v86)\nlibrary(ggplot2)\nlibrary(cowplot)\n\nHere, we just load the pre-compiled data. However, if you have your own data, you’d load these data using special data importers or by reading the parts of your data separately.\n\n# load both modalities\npbmc.rna &lt;- LoadData(\"pbmcMultiome\", \"pbmc.rna\")\npbmc.atac &lt;- LoadData(\"pbmcMultiome\", \"pbmc.atac\")\n\n(These next details are taken directly from the Seurat vignette, so I’m going to just blindly follow them.)\n\npbmc.rna[[\"RNA\"]] &lt;- as(pbmc.rna[[\"RNA\"]], Class = \"Assay5\")\n# repeat QC steps performed in the WNN vignette\npbmc.rna &lt;- subset(pbmc.rna, seurat_annotations != \"filtered\")\npbmc.atac &lt;- subset(pbmc.atac, seurat_annotations != \"filtered\")\n\n\n21.2.2 RNA-seq processing\nThis section just follows the Seurat RNA-seq pipeline. At a high level, the steps include:\n\nNormalization: This line normalizes the RNA data. Normalization typically adjusts the expression measurements to account for differences in sequencing depth or other technical variations across cells. In Seurat, the NormalizeData function scales the gene expression measurements for each cell by the total expression, multiplies by a scaling factor (default is 10,000), and log-transforms the result.\nFinding Variable Features: This step identifies the genes that show high variability across cells. These highly variable genes are more likely to capture the biological differences between cells. The FindVariableFeatures function selects these genes for downstream analysis.\nScaling the Data: This line scales the data to have a mean of zero and a variance of one. This standardization step is important for downstream dimensionality reduction techniques like PCA (Principal Component Analysis). The ScaleData function centers and scales the data.\nRunning Principal Component Analysis (PCA): PCA is a dimensionality reduction technique that reduces the data to a set of principal components (PCs). These PCs capture the most significant sources of variation in the data. The RunPCA function in Seurat performs PCA and stores the results in the object.\nRunning Uniform Manifold Approximation and Projection (UMAP): UMAP is another dimensionality reduction technique that is often used for visualization of high-dimensional data. It captures the local and global structure of the data more effectively than PCA for certain types of data. The RunUMAP function runs UMAP on the RNA data, using the first 30 principal components (as specified by dims = 1:30).\n\n\n# Perform standard analysis of each modality independently RNA analysis\npbmc.rna &lt;- NormalizeData(pbmc.rna)\npbmc.rna &lt;- FindVariableFeatures(pbmc.rna)\npbmc.rna &lt;- ScaleData(pbmc.rna)\npbmc.rna &lt;- RunPCA(pbmc.rna)\npbmc.rna &lt;- RunUMAP(pbmc.rna, dims = 1:30)\n\n\n21.2.3 Annotate ATAC-seq regions\n\n# ATAC analysis add gene annotation information\nannotations &lt;- GetGRangesFromEnsDb(ensdb = EnsDb.Hsapiens.v86)\nseqlevelsStyle(annotations) &lt;- \"UCSC\"\ngenome(annotations) &lt;- \"hg38\"\nAnnotation(pbmc.atac) &lt;- annotations\n\nAnd take a look at what we added:\n\nhead(Annotation(pbmc.atac))\n\nGRanges object with 6 ranges and 5 metadata columns:\n                  seqnames        ranges strand |           tx_id   gene_name\n                     &lt;Rle&gt;     &lt;IRanges&gt;  &lt;Rle&gt; |     &lt;character&gt; &lt;character&gt;\n  ENSE00001489430     chrX 276322-276394      + | ENST00000399012      PLCXD1\n  ENSE00001536003     chrX 276324-276394      + | ENST00000484611      PLCXD1\n  ENSE00002160563     chrX 276353-276394      + | ENST00000430923      PLCXD1\n  ENSE00001750899     chrX 281055-281121      + | ENST00000445062      PLCXD1\n  ENSE00001489388     chrX 281192-281684      + | ENST00000381657      PLCXD1\n  ENSE00001719251     chrX 281194-281256      + | ENST00000429181      PLCXD1\n                          gene_id   gene_biotype     type\n                      &lt;character&gt;    &lt;character&gt; &lt;factor&gt;\n  ENSE00001489430 ENSG00000182378 protein_coding     exon\n  ENSE00001536003 ENSG00000182378 protein_coding     exon\n  ENSE00002160563 ENSG00000182378 protein_coding     exon\n  ENSE00001750899 ENSG00000182378 protein_coding     exon\n  ENSE00001489388 ENSG00000182378 protein_coding     exon\n  ENSE00001719251 ENSG00000182378 protein_coding     exon\n  -------\n  seqinfo: 25 sequences (1 circular) from hg38 genome\n\n\n\n21.2.4 ATAC-seq processing\n\n\nNormalization\n\nSignac performs term frequency-inverse document frequency (TF-IDF) normalization. This is a two-step normalization procedure, that both normalizes across cells to correct for differences in cellular sequencing depth, and across peaks to give higher values to more rare peaks.\n\n\n\nFeature selection\n\nThe low dynamic range of scATAC-seq data makes it challenging to perform variable feature selection, as we do for scRNA-seq. Instead, we can choose to use only the top n% of features (peaks) for dimensional reduction, or remove features present in less than n cells with the FindTopFeatures() function. Here we will use all features, though we have seen very similar results when using only a subset of features (try setting min.cutoff to ‘q75’ to use the top 25% all peaks), with faster runtimes. Features used for dimensional reduction are automatically set as VariableFeatures() for the Seurat object by this function.\n\n\n\nDimension reduction\n\nWe next run singular value decomposition (SVD) on the TD-IDF matrix, using the features (peaks) selected above. This returns a reduced dimension representation of the object (for users who are more familiar with scRNA-seq, you can think of this as analogous to the output of PCA).\n\n\n\nThe process described below for dimensionality reduction combining Term Frequency-Inverse Document Frequency (TFIDF) and Singular Value Decomposition (SVD) is called Latent Semantic Indexing (LSI) and was first described here. Suffice it so say that since our ATAC-seq data are very “sparse\n\n# We exclude the first dimension as this is typically correlated with sequencing depth\npbmc.atac &lt;- RunTFIDF(pbmc.atac)\npbmc.atac &lt;- FindTopFeatures(pbmc.atac, min.cutoff = \"q0\")\npbmc.atac &lt;- RunSVD(pbmc.atac)\npbmc.atac &lt;- RunUMAP(pbmc.atac, reduction = \"lsi\", dims = 2:30, reduction.name = \"umap.atac\", reduction.key = \"atacUMAP_\")\n\nNow, plot the results.\n\np1 &lt;- DimPlot(pbmc.rna, group.by = \"seurat_annotations\", label = TRUE) + NoLegend() + ggtitle(\"RNA\")\np2 &lt;- DimPlot(pbmc.atac, group.by = \"orig.ident\", label = FALSE) + NoLegend() + ggtitle(\"ATAC\")\np1 + p2\n\n\n\n\n\n\n\nThe UMAP visualization reveals the presence of multiple cell groups in human blood. If you are familiar with scRNA-seq analyses of PBMC, you may recognize the presence of certain myeloid and lymphoid populations in the scATAC-seq data. However, annotating and interpreting clusters is more challenging in scATAC-seq data as much less is known about the functional roles of noncoding genomic regions than is known about protein coding regions (genes).\nWe can try to quantify the activity of each gene in the genome by assessing the chromatin accessibility associated with the gene, and create a new gene activity assay derived from the scATAC-seq data. Here we will use a simple approach of summing the fragments intersecting the gene body and promoter region (we also recommend exploring the Cicero tool, which can accomplish a similar goal, and we provide a vignette showing how to run Cicero within a Signac workflow here).\nTo create a gene activity matrix, we extract gene coordinates and extend them to include the 2 kb upstream region (as promoter accessibility is often correlated with gene expression). We then count the number of fragments for each cell that map to each of these regions, using the using the FeatureMatrix() function. These steps are automatically performed by the GeneActivity() function:\n\n# quantify gene activity\ngene.activities &lt;- GeneActivity(pbmc.atac, features = VariableFeatures(pbmc.rna))\n\n# add gene activities as a new assay\npbmc.atac[[\"ACTIVITY\"]] &lt;- CreateAssayObject(counts = gene.activities)\n\n\n# normalize gene activities\nDefaultAssay(pbmc.atac) &lt;- \"ACTIVITY\"\npbmc.atac &lt;- NormalizeData(pbmc.atac)\npbmc.atac &lt;- ScaleData(pbmc.atac, features = rownames(pbmc.atac))\n\n\n\n\n\n\n\nTo map cell identities from RNA-seq to ATAC-seq, we follow the steps outlined in the paper by Stuart et al.\nIn Figure 21.4, (A) Representation of two datasets, reference and query, each of which originates from a separate single-cell experiment. The two datasets share cells from similar biological states, but the query dataset contains a unique population (in black). (B) We perform canonical correlation analysis, followed by L2 normalization of the canonical correlation vectors, to project the datasets into a subspace defined by shared correlation structure across datasets. (C) In the shared space, we identify pairs of MNNs across reference and query cells. These should represent cells in a shared biological state across datasets (gray lines) and serve as anchors to guide dataset integration. In principle, cells in unique populations should not participate in anchors, but in practice, we observe “incorrect” anchors at low frequency (red lines). (D) For each anchor pair, we assign a score based on the consistency of anchors across the neighborhood structure of each dataset. (E) We utilize anchors and their scores to compute “correction” vectors for each query cell, transforming its expression so it can be jointly analyzed as part of an integrated reference.\n\n# Identify anchors\ntransfer.anchors &lt;- FindTransferAnchors(reference = pbmc.rna, query = pbmc.atac, features = VariableFeatures(object = pbmc.rna),\n    reference.assay = \"RNA\", query.assay = \"ACTIVITY\", reduction = \"cca\")\n\nAfter identifying anchors, we can transfer annotations from the scRNA-seq dataset onto the scATAC-seq cells. The annotations are stored in the seurat_annotations field, and are provided as input to the refdata parameter. The output will contain a matrix with predictions and confidence scores for each ATAC-seq cell.\n\ncelltype.predictions &lt;- TransferData(anchorset = transfer.anchors, refdata = pbmc.rna$seurat_annotations,\n    weight.reduction = pbmc.atac[[\"lsi\"]], dims = 2:30)\npbmc.atac &lt;- AddMetaData(pbmc.atac, metadata = celltype.predictions)\n\nAfter performing transfer, the ATAC-seq cells have predicted annotations (transferred from the scRNA-seq dataset) stored in the predicted.id field. Since these cells were measured with the multiome kit, we also have a ground-truth annotation that can be used for evaluation. You can see that the predicted and actual annotations are extremely similar.\n\npbmc.atac$annotation_correct &lt;- pbmc.atac$predicted.id == pbmc.atac$seurat_annotations\np1 &lt;- DimPlot(pbmc.atac, group.by = \"predicted.id\", label = TRUE) + NoLegend() + ggtitle(\"Predicted annotation\")\np2 &lt;- DimPlot(pbmc.atac, group.by = \"seurat_annotations\", label = TRUE) + NoLegend() + ggtitle(\"Ground-truth annotation\")\np1 | p2\n\n\n\n\n\n\n\nIn this example, the annotation for an scATAC-seq profile is correctly predicted via scRNA-seq integration ~90% of the time. In addition, the prediction.score.max field quantifies the uncertainty associated with our predicted annotations. We can see that cells that are correctly annotated are typically associated with high prediction scores (&gt;90%), while cells that are incorrectly annotated are associated with sharply lower prediction scores (&lt;50%). Incorrect assignments also tend to reflect closely related cell types (i.e. Intermediate vs. Naive B cells).\n\npredictions &lt;- table(pbmc.atac$seurat_annotations, pbmc.atac$predicted.id)\npredictions &lt;- predictions/rowSums(predictions)  # normalize for number of cells in each cell type\npredictions &lt;- as.data.frame(predictions)\np1 &lt;- ggplot(predictions, aes(Var1, Var2, fill = Freq)) + geom_tile() + scale_fill_gradient(name = \"Fraction of cells\",\n    low = \"#ffffc8\", high = \"#7d0025\") + xlab(\"Cell type annotation (RNA)\") + ylab(\"Predicted cell type label (ATAC)\") +\n    theme_cowplot() + theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust = 1))\n\ncorrect &lt;- length(which(pbmc.atac$seurat_annotations == pbmc.atac$predicted.id))\nincorrect &lt;- length(which(pbmc.atac$seurat_annotations != pbmc.atac$predicted.id))\ndata &lt;- FetchData(pbmc.atac, vars = c(\"prediction.score.max\", \"annotation_correct\"))\np2 &lt;- ggplot(data, aes(prediction.score.max, fill = annotation_correct, colour = annotation_correct)) +\n    geom_density(alpha = 0.5) + theme_cowplot() + scale_fill_discrete(name = \"Annotation Correct\",\n    labels = c(paste0(\"FALSE (n = \", incorrect, \")\"), paste0(\"TRUE (n = \", correct, \")\"))) + scale_color_discrete(name = \"Annotation Correct\",\n    labels = c(paste0(\"FALSE (n = \", incorrect, \")\"), paste0(\"TRUE (n = \", correct, \")\"))) + xlab(\"Prediction Score\")\np1 + p2",
    "crumbs": [
      "Bioconductor",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Transfer Learning in scATAC-seq and scRNA-seq</span>"
    ]
  },
  {
    "objectID": "single-cell-atac-and-rna-transfer-learning.html#transfer-learning",
    "href": "single-cell-atac-and-rna-transfer-learning.html#transfer-learning",
    "title": "\n21  Transfer Learning in scATAC-seq and scRNA-seq\n",
    "section": "\n21.3 Transfer learning",
    "text": "21.3 Transfer learning\nIn this demonstration, we will explore the concept of transfer learning using Principal Component Analysis (PCA). Transfer learning allows us to leverage knowledge gained from one dataset and apply it to another related dataset. We will showcase this by dividing a dataset into two pieces and projecting the second dataset into the principal components derived from the first dataset.\n\n21.3.1 Loading the Data\nFirst, let’s load the required libraries:\n\nlibrary(GEOquery)\nlibrary(SummarizedExperiment)\n\nWe will use the GEOquery package to retrieve a dataset from the Gene Expression Omnibus (GEO) database and convert it into a SummarizedExperiment object:\n\nse = as(getGEO(\"GSE103512\")[[1]], \"SummarizedExperiment\")\n\n\n21.3.2 Selecting the Most Variable Genes\nTo focus on the most informative genes, we will select the top 250 most variable genes based on their standard deviation. Let’s denote the expression matrix as \\(X\\), where rows represent genes and columns represent samples.\n\n# get the top 250 most variable genes\nvariable_rows = order(apply(assays(se)$exprs, 1, sd), decreasing = TRUE)[1:250]\n\nWe subset the SummarizedExperiment object to include only the selected genes:\n\nse_subset &lt;- se[variable_rows,]\n\n\n21.3.3 Splitting the Dataset\nNow, we will split the dataset into two pieces, simulating the collection of two separate datasets with the same genes. This will allow us to demonstrate transfer learning. Let’s denote the subsets as \\(X_1\\) and \\(X_2\\).\n\nsplit_vector = sample(c(TRUE,FALSE), ncol(se_subset), replace=TRUE)\nse_subset_1 = se_subset[,split_vector]\nse_subset_2 = se_subset[,!split_vector]\n\n\n21.3.4 Performing PCA on the First Subset\nWe perform PCA on the first subset (\\(X_1\\)) to obtain the principal components. PCA seeks to find a set of orthogonal vectors (principal components) that capture the maximum variance in the data. The principal components are the eigenvectors of the covariance matrix of \\(X_1\\).\n\npc_subset1 = prcomp(t(assays(se_subset_1)$exprs))\n\nLet’s visualize the samples in the principal component space, colored by their cancer type:\n\nplot(pc_subset1$x, col=as.numeric(as.factor(se_subset_1$cancer.type.ch1))+2)\n\n\n\n\n\n\n\n\n21.3.5 Projecting the Second Subset\nNow, let’s use the PCA model trained on \\(X_1\\) to project the samples from \\(X_2\\) into the same principal component space. This is where transfer learning comes into play. We can represent the projection matrix as \\(P\\), which consists of the top principal components from \\(X_1\\).\n\npred_subset2 &lt;- predict(pc_subset1,t(assay(se_subset_2,'exprs')))\n\nMathematically, the projection of \\(X_2\\) into the principal component space is given by:\n\\(X_2^{(p)} = X_2 \\cdot P\\)\nwhere \\(X_2^{(p)}\\) represents the projected samples from \\(X_2\\) in the principal component space.\nIn PCA, the principal components represent a new coordinate system that is aligned with the directions of maximum variance in the data. The process of finding these principal components can be thought of as a rotation of the original coordinate system. Consider the original feature space, where each dimension corresponds to a variable (gene in our example). The data points (samples) are scattered in this high-dimensional space. PCA identifies the directions in which the data varies the most, and these directions become the principal components. Geometrically, the principal components form a new orthogonal coordinate system. The first principal component (PC1) aligns with the direction of maximum variance, the second principal component (PC2) aligns with the direction of the second-highest variance (orthogonal to PC1), and so on. When we perform PCA on the first subset (\\(X_1\\)), we obtain the principal components \\(P\\). These principal components define the rotation matrix that transforms the original coordinate system to the new PCA coordinate system. Now, let’s consider the “predict” process, where we project the samples from the second subset (\\(X_2\\)) into the principal component space derived from \\(X_1\\). Geometrically, this can be understood as follows:\nThe samples from \\(X_2\\) are originally represented in the same high-dimensional feature space as \\(X_1\\). By using the “predict” function with the PCA model trained on \\(X_1\\), we are essentially applying the rotation matrix \\(P\\) to the samples from \\(X_2\\). The rotation matrix \\(P\\) transforms the coordinates of the samples from \\(X_2\\) into the new PCA coordinate system defined by the principal components of \\(X_1\\). In the PCA coordinate system, the samples from \\(X_2\\) are represented by their projections onto the principal components.\nMathematically, the projection of \\(X_2\\) onto the principal component space is given by: \\(X_2^{(p)} = X_2 \\cdot P\\) where \\(X_2^{(p)}\\) represents the projected samples from \\(X_2\\) in the principal component space. Geometrically, this projection can be visualized as follows:\nEach sample from \\(X_2\\) is represented as a point in the original high-dimensional feature space. The rotation matrix \\(P\\) defines the new PCA coordinate system, where the axes are the principal components. The “predict” process maps each sample from \\(X_2\\) onto the new PCA coordinate system by applying the rotation defined by \\(P\\). The projected samples \\(X_2^{(p)}\\) represent the coordinates of the samples from \\(X_2\\) in the PCA coordinate system.\nBy projecting the samples from \\(X_2\\) into the PCA space derived from \\(X_1\\), we can analyze how well the structure and variability of \\(X_2\\) align with the principal components learned from \\(X_1\\). If the projected samples from \\(X_2\\) exhibit similar patterns or groupings as the samples from \\(X_1\\) in the PCA space, it indicates that the knowledge learned from \\(X_1\\) effectively captures the underlying structure of \\(X_2\\).\nThe “predict” process in PCA can be understood as a rotation of the original coordinate system to align with the directions of maximum variance, followed by a projection of new samples onto the rotated coordinate system defined by the principal components.\n\n21.3.6 Comparing the Subsets in the Principal Component Space\nFinally, we can compare the distribution of samples from both subsets in the principal component space:\n\npar(mfrow=c(1,2))\nplot(pc_subset1$x, col=as.numeric(as.factor(se_subset_1$cancer.type.ch1))+2)\nplot(pred_subset2[,1], pred_subset2[,2], col=as.numeric(as.factor(se_subset_2$cancer.type.ch1))+2)\n\n\n\nIn this plot, we are comparing the subset 1 PCA plot to that produced by projecting the samples from subset 2 into the first two principle components from subset 1.\n\n\n\nBy projecting the samples from \\(X_2\\) into the principal component space derived from \\(X_1\\), we can observe how well the learned principal components capture the structure and variability of the second dataset. This demonstrates the power of transfer learning, where knowledge gained from one dataset can be effectively applied to another related dataset.\nMathematically, transfer learning with PCA can be summarized as follows:\n\nPerform PCA on \\(X_1\\) to obtain the principal components \\(P\\).\nProject \\(X_2\\) into the principal component space using \\(X_2^{(p)} = X_2 \\cdot P\\).\nCompare the distribution of samples from \\(X_1\\) and \\(X_2\\) in the principal component space.\n\nTransfer learning with PCA allows us to leverage the learned principal components from one dataset to analyze and understand another related dataset, even when the datasets are collected separately. This technique can be particularly useful when dealing with limited sample sizes or when trying to integrate information from multiple sources.",
    "crumbs": [
      "Bioconductor",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Transfer Learning in scATAC-seq and scRNA-seq</span>"
    ]
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Bourgon, Richard, Robert Gentleman, and Wolfgang Huber. 2010.\n“Independent Filtering Increases Detection Power for\nHigh-Throughput Experiments.” Proceedings of the National\nAcademy of Sciences 107 (21): 9546–51. https://doi.org/10.1073/pnas.0914005107.\n\n\nBrouwer-Visser, Jurriaan, Wei-Yi Cheng, Anna Bauer-Mehren, Daniela\nMaisel, Katharina Lechner, Emilia Andersson, Joel T. Dudley, and\nFrancesca Milletti. 2018. “Regulatory T-Cell\nGenes Drive Altered\nImmune Microenvironment in Adult\nSolid Cancers and Allow for\nImmune Contextual Patient\nSubtyping.” Cancer Epidemiology, Biomarkers\n& Prevention 27 (1): 103–12. https://doi.org/10.1158/1055-9965.EPI-17-0461.\n\n\nCenter, Pew Research. 2016. “Lifelong Learning and\nTechnology.” Pew Research Center: Internet,\nScience & Tech. https://www.pewresearch.org/internet/2016/03/22/lifelong-learning-and-technology/.\n\n\nDeRisi, J. L., V. R. Iyer, and P. O. Brown. 1997. “Exploring the\nMetabolic and Genetic Control of Gene Expression on a Genomic\nScale.” Science (New York, N.Y.) 278 (5338): 680–86. https://doi.org/10.1126/science.278.5338.680.\n\n\nGreener, Joe G., Shaun M. Kandathil, Lewis Moffat, and David T. Jones.\n2022. “A Guide to Machine Learning for Biologists.”\nNature Reviews Molecular Cell Biology 23 (1): 40–55. https://doi.org/10.1038/s41580-021-00407-0.\n\n\nKnowles, Malcolm S., Elwood F. Holton, and Richard A. Swanson. 2005.\nThe Adult Learner: The Definitive Classic in Adult Education and\nHuman Resource Development. 6th ed. Amsterdam ; Boston: Elsevier.\n\n\nLibbrecht, Maxwell W., and William Stafford Noble. 2015. “Machine\nLearning Applications in Genetics and Genomics.” Nature\nReviews Genetics 16 (6): 321–32. https://doi.org/10.1038/nrg3920.\n\n\nStudent. 1908. “The Probable Error of a\nMean.” Biometrika 6 (1): 1–25. https://doi.org/10.2307/2331554.",
    "crumbs": [
      "References"
    ]
  },
  {
    "objectID": "appendix.html",
    "href": "appendix.html",
    "title": "Appendix A — Appendix",
    "section": "",
    "text": "A.1 Data Sets",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>Appendix</span>"
    ]
  },
  {
    "objectID": "appendix.html#data-sets",
    "href": "appendix.html#data-sets",
    "title": "Appendix A — Appendix",
    "section": "",
    "text": "BRFSS subset\nALL clinical data\nALL expression data",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>Appendix</span>"
    ]
  },
  {
    "objectID": "appendix.html#swirl",
    "href": "appendix.html#swirl",
    "title": "Appendix A — Appendix",
    "section": "\nA.2 Swirl",
    "text": "A.2 Swirl\nThe following is from the swirl website.\n\nThe swirl R package makes it fun and easy to learn R programming and data science. If you are new to R, have no fear.\n\nTo get started, we need to install a new package into R.\n\ninstall.packages('swirl')\n\nOnce installed, we want to load it into the R workspace so we can use it.\n\nlibrary('swirl')\n\nFinally, to get going, start swirl and follow the instructions.\n\nswirl()",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>Appendix</span>"
    ]
  },
  {
    "objectID": "additional_resources.html",
    "href": "additional_resources.html",
    "title": "Appendix B — Additional resources",
    "section": "",
    "text": "Base R Cheat Sheet",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>B</span>  <span class='chapter-title'>Additional resources</span>"
    ]
  },
  {
    "objectID": "eda_overview.html",
    "href": "eda_overview.html",
    "title": "Exploratory data analysis",
    "section": "",
    "text": "Imagine you’re on an adventure, about to embark on a journey into the unknown. You’ve just been handed a treasure map, with the promise of valuable insights waiting to be discovered. This map is your data set, and the journey is exploratory data analysis (EDA).\nAs you begin your exploration, you start by getting a feel for the terrain. You take a broad, bird’s-eye view of the data, examining its structure and dimensions. Are you dealing with a vast landscape or a small, confined area? Are there any missing pieces in the map that you’ll need to account for? Understanding the overall context of your data set is crucial before venturing further.\nWith a sense of the landscape, you now zoom in to identify key landmarks in the data. You might look for unusual patterns, trends, or relationships between variables. As you spot these landmarks, you start asking questions: What’s causing that spike in values? Are these two factors related, or is it just a coincidence? By asking these questions, you’re actively engaging with the data and forming hypotheses that could guide future analysis or experiments.\nAs you continue your journey, you realize that the map alone isn’t enough to fully understand the terrain. You need more tools to bring the data to life. You start visualizing the data using charts, plots, and graphs. These visualizations act as your binoculars, allowing you to see patterns and relationships more clearly. Through them, you can uncover the hidden treasures buried within the data.\nEDA isn’t a linear path from start to finish. As you explore, you’ll find yourself circling back to previous points, refining your questions, and digging deeper. The process is iterative, with each new discovery informing the next. And as you go, you’ll gain a deeper understanding of the data’s underlying structure and potential.\nFinally, after your thorough exploration, you’ll have a solid foundation to build upon. You’ll be better equipped to make informed decisions, test hypotheses, and draw meaningful conclusions. The insights you’ve gained through EDA will serve as a compass, guiding you towards the true value hidden within your data. And with that, you’ve successfully completed your journey through exploratory data analysis.",
    "crumbs": [
      "Exploratory data analysis"
    ]
  },
  {
    "objectID": "license.html",
    "href": "license.html",
    "title": "",
    "section": "",
    "text": "Code\n\n\n\n\n\n    To the extent possible under law,  Sean Davis has waived all copyright and related or neighboring rights to Statistical analysis of functional genomics dataa. This work is published from:  United States."
  },
  {
    "objectID": "data_structures_overview.html",
    "href": "data_structures_overview.html",
    "title": "R Data Structures",
    "section": "",
    "text": "Chapter overview\nAs you progress through these chapters, practice the examples and exercises provided, engage in discussion, and collaborate with your peers to deepen your understanding of R data structures. This solid foundation will serve as the basis for more advanced data manipulation, analysis, and visualization techniques in R.",
    "crumbs": [
      "R Data Structures"
    ]
  },
  {
    "objectID": "data_structures_overview.html#chapter-overview",
    "href": "data_structures_overview.html#chapter-overview",
    "title": "R Data Structures",
    "section": "",
    "text": "Vectors : In this chapter, we will introduce you to the simplest data structure in R, the vector. We will cover how to create, access, and manipulate vectors, as well as discuss their unique properties and limitations.\n\nMatrices\n\nNext, we will explore matrices, which are two-dimensional data structures that extend vectors. You will learn how to create, access, and manipulate matrices, and understand their usefulness in mathematical operations and data organization.\n\n\n\nLists\n\nThe third chapter will focus on lists, a versatile data structure that can store elements of different types and sizes. We will discuss how to create, access, and modify lists, and demonstrate their flexibility in handling complex data structures.\n\n\n\nData.frames\n\nFinally, we will examine data.frames, a widely-used data structure for organizing and manipulating tabular data. You will learn how to create, access, and manipulate data.frames, and understand their advantages over other data structures for data analysis tasks.\n\n\n\nArrays\n\nWhile we will not focus directly on the array data type, which are multidimensional data structures that extend matrices, they are very similar to matrices, but with a third dimension.",
    "crumbs": [
      "R Data Structures"
    ]
  }
]