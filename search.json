[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "The RBioc Book",
    "section": "",
    "text": "Preface"
  },
  {
    "objectID": "index.html#who-is-this-book-for",
    "href": "index.html#who-is-this-book-for",
    "title": "The RBioc Book",
    "section": "Who is this book for?",
    "text": "Who is this book for?\n\nPeople who want to learn data science\nPeople who want to teach data science\nPeople who want to learn how to teach data science\nPeople who want to learn how to learn data science"
  },
  {
    "objectID": "index.html#why-this-book",
    "href": "index.html#why-this-book",
    "title": "The RBioc Book",
    "section": "Why this book?",
    "text": "Why this book?\nThis book is a collection of resources for learning R and Bioconductor. It is meant to be largely self-directed, but for those looking to teach data science, it can also be used as a guide for structuring a course. Material is a bit variable in terms of difficulty, prerequisites, and format which is a reflection of the organic creation of the material.\nStudents are encouraged to work with others to learn the material. Instructors are encouraged to use the material to create a course that is tailored to the needs of their students and to spend lots of time in 1:1 and small groups to support students in their learning. See below for additional thoughts on adult learning and how it relates to this material."
  },
  {
    "objectID": "index.html#adult-learners",
    "href": "index.html#adult-learners",
    "title": "The RBioc Book",
    "section": "Adult learners",
    "text": "Adult learners\nAdult Learning Theory, also known as Andragogy, is the concept and practice of designing, developing, and delivering instructional experiences for adult learners. It is based on the belief that adults learn differently than children, and thus, require distinct approaches to engage, motivate, and retain information (Center 2016). The term was first introduced by Malcolm Knowles, an American educator who is known for his work in adult education (Knowles, Holton, and Swanson 2005).\nOne of the fundamental principles of Adult Learning Theory is that adults are self-directed learners. This means that we prefer to take control of our own learning process and set personal goals for themselves. We are motivated by our desire to solve problems or gain knowledge to improve our lives (see Figure 1). As a result, educational content for adults should be relevant and applicable to real-life situations. Furthermore, adult learners should be given opportunities to actively engage in the learning process by making choices, setting goals, and evaluating their progress.\n\n\nFigure 1: Why do adults choose to learn something?\n\nAnother key aspect of Adult Learning Theory is the role of experience. We bring a wealth of experience to the learning process, which serves as a resource for new learning. We often have well-established beliefs, values, and mental models that can influence our willingness to accept new ideas and concepts. Therefore, it is essential to acknowledge and respect our shared and unique past experiences and create an environment where we all feel comfortable sharing our perspectives.\nTo effectively learn as a group of adult learners, it is crucial to establish a collaborative learning environment that promotes open communication and fosters trust among participants. We all appreciate and strive for a respectful and supportive atmosphere where we can express our opinions without fear of judgment. Instructors should help facilitate discussions, encourage peer-to-peer interactions, and incorporate group activities and collaboration to capitalize on the collective knowledge of participants.\nAdditionally, adult learners often have multiple responsibilities outside of the learning environment, such as work and family commitments. As a result, we require flexible learning opportunities that accommodate busy schedules. Offering a variety of instructional formats, such as online modules, self-paced learning, or evening classes, can help ensure that adult learners have access to education despite any time constraints.\nAdult learners benefit from a learner-centered approach that focuses on the individual needs, preferences, and interests of each participant can greatly enhance the overall learning experience. In addition, we tend to be more intrinsically motivated to learn when we have a sense of autonomy and can practice and experiment (see Figure 2) with new concepts in a safe environment.\n\n\n\n\nFigure 2: How to stay stuck in data science (or anything). The “Read-Do” loop tends to deliver the best results. Too much reading between doing can be somewhat effective. Reading and simply copy-paste is probably the least effective. When working through material, experiment. Try to break things. Incorporate your own experience or applications whenever possible.\n\n\n\nUnderstanding Adult Learning Theory and its principles can significantly enhance the effectiveness of teaching and learning as adults. By respecting our autonomy, acknowledging our experiences, creating a supportive learning environment, offering flexible learning opportunities, and utilizing diverse teaching methods, we can better cater to the unique needs and preferences of adult learners.\nIn practice, that means that we will will not be prescriptive in our approach to teaching data science. We will not tell you what to do, but rather we will provide you with a variety of options and you can choose what works best for you. We will also provide you with a variety of resources and you can choose where to focus your time. Given that we cannot possibly cover everything, we will provide you with a framework for learning and you can fill in the gaps as you see fit. A key component of our success as adult learners is to gain the confidence to ask questions and problem-solve on our own.\n\n\n\n\nCenter, Pew Research. 2016. “Lifelong Learning and Technology.” Pew Research Center: Internet, Science & Tech. https://www.pewresearch.org/internet/2016/03/22/lifelong-learning-and-technology/.\n\n\nKnowles, Malcolm S., Elwood F. Holton, and Richard A. Swanson. 2005. The Adult Learner: The Definitive Classic in Adult Education and Human Resource Development. 6th ed. Amsterdam ; Boston: Elsevier."
  },
  {
    "objectID": "intro.html#questions",
    "href": "intro.html#questions",
    "title": "\n1  Introducing R and RStudio\n",
    "section": "Questions",
    "text": "Questions\n\nWhat is R?\nWhy use R?\nWhy not use R?\nWhy use RStudio and how does it differ from R?"
  },
  {
    "objectID": "intro.html#learning-objectives",
    "href": "intro.html#learning-objectives",
    "title": "\n1  Introducing R and RStudio\n",
    "section": "Learning Objectives",
    "text": "Learning Objectives\n\nKnow advantages of analyzing data in R\nKnow advantages of using RStudio\nBe able to start RStudio on your computer\nIdentify the panels of the RStudio interface\nBe able to customize the RStudio layout"
  },
  {
    "objectID": "intro.html#introduction",
    "href": "intro.html#introduction",
    "title": "\n1  Introducing R and RStudio\n",
    "section": "\n1.1 Introduction",
    "text": "1.1 Introduction\nIn this chapter, we will discuss the basics of R and RStudio, two essential tools in genomics data analysis. We will cover the advantages of using R and RStudio, how to set up RStudio, and the different panels of the RStudio interface."
  },
  {
    "objectID": "intro.html#what-is-r",
    "href": "intro.html#what-is-r",
    "title": "\n1  Introducing R and RStudio\n",
    "section": "\n1.2 What is R?",
    "text": "1.2 What is R?\n[R](https://en.wikipedia.org/wiki/R_(programming_language) is a programming language and software environment designed for statistical computing and graphics. It is widely used by statisticians, data scientists, and researchers for data analysis and visualization. R is an open-source language, which means it is free to use, modify, and distribute. Over the years, R has become particularly popular in the fields of genomics and bioinformatics, owing to its extensive libraries and powerful data manipulation capabilities.\nThe R language is a dialect of the S language, which was developed in the 1970s at Bell Laboratories. The first version of R was written by Robert Gentleman and Ross Ihaka and released in 1995 (see this slide deck for Ross Ihaka’s take on R’s history). Since then, R has been continuously developed by the R Core Team, a group of statisticians and computer scientists. The R Core Team releases a new version of R every year.\n\n\n\n\nFigure 1.1: Google trends showing the popularity of R over time based on Google searches"
  },
  {
    "objectID": "intro.html#why-use-r",
    "href": "intro.html#why-use-r",
    "title": "\n1  Introducing R and RStudio\n",
    "section": "\n1.3 Why use R?",
    "text": "1.3 Why use R?\nThere are several reasons why R is a popular choice for data analysis, particularly in genomics and bioinformatics. These include:\n\n\nOpen-source: R is free to use and has a large community of developers who contribute to its growth and development. What is “open-source”?\n\n\nExtensive libraries: There are thousands of R packages available for a wide range of tasks, including specialized packages for genomics and bioinformatics. These libraries have been extensively tested and ara available for free.\n\nData manipulation: R has powerful data manipulation capabilities, making it easy (or at least possible) to clean, process, and analyze large datasets.\n\nGraphics and visualization: R has excellent tools for creating high-quality graphics and visualizations that can be customized to meet the specific needs of your analysis. In most cases, graphics produced by R are publication-quality.\n\nReproducible research: R enables you to create reproducible research by recording your analysis in a script, which can be easily shared and executed by others. In addition, R does not have a meaningful graphical user interface (GUI), which renders analysis in R much more reproducible than tools that rely on GUI interactions.\n\nCross-platform: R runs on Windows, Mac, and Linux (as well as more obscure systems).\n\nInteroperability with other languages: R can interfact with FORTRAN, C, and many other languages.\n\nScalability: R is useful for small and large projects.\n\nI can develop code for analysis on my Mac laptop. I can then install the same code on our 20k core cluster and run it in parallel on 100 samples, monitor the process, and then update a database (for example) with R when complete."
  },
  {
    "objectID": "intro.html#why-not-use-r",
    "href": "intro.html#why-not-use-r",
    "title": "\n1  Introducing R and RStudio\n",
    "section": "\n1.4 Why not use R?",
    "text": "1.4 Why not use R?\n\nR cannot do everything.\nR is not always the “best” tool for the job.\nR will not hold your hand. Often, it will slap your hand instead.\nThe documentation can be opaque (but there is documentation).\nR can drive you crazy (on a good day) or age you prematurely (on a bad one).\nFinding the right package to do the job you want to do can be challenging; worse, some contributed packages are unreliable.]{}\nR does not have a meaningfully useful graphical user interface (GUI)."
  },
  {
    "objectID": "intro.html#r-license-and-the-open-source-ideal",
    "href": "intro.html#r-license-and-the-open-source-ideal",
    "title": "\n1  Introducing R and RStudio\n",
    "section": "\n1.5 R License and the Open Source Ideal",
    "text": "1.5 R License and the Open Source Ideal\nR is free (yes, totally free!) and distributed under GNU license. In particular, this license allows one to:\n\nDownload the source code\nModify the source code to your heart’s content\nDistribute the modified source code and even charge money for it, but you must distribute the modified source code under the original GNU license]{}\n\nThis license means that R will always be available, will always be open source, and can grow organically without constraint."
  },
  {
    "objectID": "intro.html#rstudio",
    "href": "intro.html#rstudio",
    "title": "\n1  Introducing R and RStudio\n",
    "section": "\n1.6 RStudio",
    "text": "1.6 RStudio\nRStudio is an integrated development environment (IDE) for R. It provides a graphical user interface (GUI) for R, making it easier to write and execute R code. RStudio also provides several other useful features, including a built-in console, syntax-highlighting editor, and tools for plotting, history, debugging, workspace management, and workspace viewing. RStudio is available in both free and commercial editions; the commercial edition provides some additional features, including support for multiple sessions and enhanced debugging\n\n1.6.1 Getting started with RStudio\nTo get started with RStudio, you first need to install both R and RStudio on your computer. Follow these steps:\n\nDownload and install R from the official R website.\nDownload and install RStudio from the official RStudio website.\nLaunch RStudio. You should see the RStudio interface with four panels.\n\n1.6.2 The RStudio Interface\nRStudio’s interface consists of four panels (see Figure 1.2):\n\n\nConsole\n\nThis panel displays the R console, where you can enter and execute R commands directly. The console also shows the output of your code, error messages, and other information.\n\n\n\nSource\n\nThis panel is where you write and edit your R scripts. You can create new scripts, open existing ones, and run your code from this panel.\n\n\n\nEnvironment\n\nThis panel displays your current workspace, including all variables, data objects, and functions that you have created or loaded in your R session.\n\n\n\nPlots, Packages, Help, and Viewer\n\nThese panels display plots, installed packages, help files, and web content, respectively.\n\n\n\n\n\nFigure 1.2: The RStudio interface. In this layout, the source pane is in the upper left, the console is in the lower left, the environment panel is in the top right and the viewer/help/files panel is in the bottom right.\n\n\n\n\n\n\n\nDo I need to use RStudio?\n\n\n\nNo. You can use R without RStudio. However, RStudio makes it easier to write and execute R code, and it provides several useful features that are not available in the basic R console. Note that the only part of RStudio that is actually interacting with R directly is the console. The other panels are simply providing a GUI that enhances the user experience.\n\n\n\n\n\n\n\n\nCustomizing the RStudio Interface\n\n\n\nYou can customize the layout of RStudio to suit your preferences. To do so, go to Tools &gt; Global Options &gt; Appearance. Here, you can change the theme, font size, and panel layout. You can also resize the panels as needed to gain screen real estate (see Figure 1.3).\n\n\n\n\nFigure 1.3: Dealing with limited screen real estate can be a challenge, particularly when you want to open another window to, for example, view a web page. You can resize the panes by sliding the center divider (red arrows) or by clicking on the minimize/maximize buttons (see blue arrow).\n\nIn summary, R and RStudio are powerful tools for genomics data analysis. By understanding the advantages of using R and RStudio and familiarizing yourself with the RStudio interface, you can efficiently analyze and visualize your data. In the following chapters, we will delve deeper into the functionality of R, Bioconductor, and various statistical methods to help you gain a comprehensive understanding of genomics data analysis."
  },
  {
    "objectID": "r_intro_mechanics.html#learning-objectives",
    "href": "r_intro_mechanics.html#learning-objectives",
    "title": "\n2  R mechanics\n",
    "section": "\n2.1 Learning objectives",
    "text": "2.1 Learning objectives\n\nBe able to start R and RStudio\nLearn to interact with the R console\nKnow the difference between expressions and assignment\nRecognize valid and invalid R names\nKnow how to access the R help system\nKnow how to assign values to variables, find what is in R memory, and remove values from R memory"
  },
  {
    "objectID": "r_intro_mechanics.html#starting-r",
    "href": "r_intro_mechanics.html#starting-r",
    "title": "\n2  R mechanics\n",
    "section": "\n2.2 Starting R",
    "text": "2.2 Starting R\nHow to start R depends a bit on the operating system (Mac, Windows, Linux) and interface. In this course, we will largely be using an Integrated Development Environment (IDE) called RStudio, but there is nothing to prohibit using R at the command line or in some other interface (and there are a few)."
  },
  {
    "objectID": "r_intro_mechanics.html#rstudio-a-quick-tour",
    "href": "r_intro_mechanics.html#rstudio-a-quick-tour",
    "title": "\n2  R mechanics\n",
    "section": "\n2.3 RStudio: A Quick Tour",
    "text": "2.3 RStudio: A Quick Tour\nThe RStudio interface has multiple panes. All of these panes are simply for convenience except the “Console” panel, typically in the lower left corner (by default). The console pane contains the running R interface. If you choose to run R outside RStudio, the interaction will be identical to working in the console pane. This is useful to keep in mind as some environments, such as a computer cluster, encourage using R without RStudio.\n\nPanes\nOptions\nHelp\nEnvironment, History, and Files"
  },
  {
    "objectID": "r_intro_mechanics.html#interacting-with-r",
    "href": "r_intro_mechanics.html#interacting-with-r",
    "title": "\n2  R mechanics\n",
    "section": "\n2.4 Interacting with R",
    "text": "2.4 Interacting with R\nThe only meaningful way of interacting with R is by typing into the R console. At the most basic level, anything that we type at the command line will fall into one of two categories:\n\n\nAssignments\n\nx = 1\ny &lt;- 2\n\n\n\nExpressions\n\n1 + pi + sin(42)\n\n[1] 3.225071\n\n\n\n\nThe assignment type is obvious because either the The &lt;- or = are used. Note that when we type expressions, R will return a result. In this case, the result of R evaluating 1 + pi + sin(42) is 3.2250711.\nThe standard R prompt is a “&gt;” sign. When present, R is waiting for the next expression or assignment. If a line is not a complete R command, R will continue the next line with a “+”. For example, typing the fillowing with a “Return” after the second “+” will result in R giving back a “+” on the next line, a prompt to keep typing.\n\n1 + pi +\nsin(3.7)\n\n[1] 3.611757\n\n\nR can be used as a glorified calculator by using R expressions. Mathematical operations include:\n\nAddition: +\n\nSubtraction: -\n\nMultiplication: *\n\nDivision: /\n\nExponentiation: ^\n\nModulo: %%\n\n\nThe ^ operator raises the number to its left to the power of the number to its right: for example 3^2 is 9. The modulo returns the remainder of the division of the number to the left by the number on its right, for example 5 modulo 3 or 5 %% 3 is 2.\n\n2.4.1 Expressions\n\n5 + 2\n28 %% 3\n3^2\n5 + 4 * 4 + 4 ^ 4 / 10\n\nNote that R follows order-of-operations and groupings based on parentheses.\n\n5 + 4 / 9\n(5 + 4) / 9\n\n\n2.4.2 Assignment\nWhile using R as a calculator is interesting, to do useful and interesting things, we need to assign values to objects. To create objects, we need to give it a name followed by the assignment operator &lt;- (or, entirely equivalently, =) and the value we want to give it:\n\nweight_kg &lt;- 55 \n\n&lt;- is the assignment operator. Assigns values on the right to objects on the left, it is like an arrow that points from the value to the object. Using an = is equivalent (in nearly all cases). Learn to use &lt;- as it is good programming practice.\nObjects can be given any name such as x, current_temperature, or subject_id (see below). You want your object names to be explicit and not too long. They cannot start with a number (2x is not valid but x2 is). R is case sensitive (e.g., weight_kg is different from Weight_kg). There are some names that cannot be used because they represent the names of fundamental functions in R (e.g., if, else, for, see here for a complete list). In general, even if it’s allowed, it’s best to not use other function names, which we’ll get into shortly (e.g., c, T, mean, data, df, weights). When in doubt, check the help to see if the name is already in use. It’s also best to avoid dots (.) within a variable name as in my.dataset. It is also recommended to use nouns for variable names, and verbs for function names.\nWhen assigning a value to an object, R does not print anything. You can force to print the value by typing the name:\n\nweight_kg\n\n[1] 55\n\n\nNow that R has weight_kg in memory, which R refers to as the “global environment”, we can do arithmetic with it. For instance, we may want to convert this weight in pounds (weight in pounds is 2.2 times the weight in kg).\n\n2.2 * weight_kg\n\n[1] 121\n\n\nWe can also change a variable’s value by assigning it a new one:\n\nweight_kg &lt;- 57.5\n2.2 * weight_kg\n\n[1] 126.5\n\n\nThis means that assigning a value to one variable does not change the values of other variables. For example, let’s store the animal’s weight in pounds in a variable.\n\nweight_lb &lt;- 2.2 * weight_kg\n\nand then change weight_kg to 100.\n\nweight_kg &lt;- 100\n\nWhat do you think is the current content of the object weight_lb, 126.5 or 220?\nYou can see what objects (variables) are stored by viewing the Environment tab in Rstudio. You can also use the ls() function. You can remove objects (variables) with the rm() function. You can do this one at a time or remove several objects at once. You can also use the little broom button in your environment pane to remove everything from your environment.\n\nls()\nrm(weight_lb, weight_kg)\nls()\n\nWhat happens when you type the following, now?\n\nweight_lb # oops! you should get an error because weight_lb no longer exists!"
  },
  {
    "objectID": "r_intro_mechanics.html#rules-for-names-in-r",
    "href": "r_intro_mechanics.html#rules-for-names-in-r",
    "title": "\n2  R mechanics\n",
    "section": "\n2.5 Rules for Names in R",
    "text": "2.5 Rules for Names in R\nR allows users to assign names to objects such as variables, functions, and even dimensions of data. However, these names must follow a few rules.\n\nNames may contain any combination of letters, numbers, underscore, and “.”\nNames may not start with numbers, underscore.\nR names are case-sensitive.\n\nExamples of valid R names include:\npi\nx\ncamelCaps\nmy_stuff\nMY_Stuff\nthis.is.the.name.of.the.man\nABC123\nabc1234asdf\n.hi"
  },
  {
    "objectID": "r_intro_mechanics.html#resources-for-getting-help",
    "href": "r_intro_mechanics.html#resources-for-getting-help",
    "title": "\n2  R mechanics\n",
    "section": "\n2.6 Resources for Getting Help",
    "text": "2.6 Resources for Getting Help\nThere is extensive built-in help and documentation within R. A separate page contains a collection of additional resources.\nIf the name of the function or object on which help is sought is known, the following approaches with the name of the function or object will be helpful. For a concrete example, examine the help for the print method.\n\nhelp(print)\nhelp('print')\n?print\n\nIf the name of the function or object on which help is sought is not known, the following from within R will be helpful.\n\nhelp.search('microarray')\nRSiteSearch('microarray')\napropos('histogram')\n\nThere are also tons of online resources that Google will include in searches if online searching feels more appropriate.\nI strongly recommend using help(\"newfunction\"\") for all functions that are new or unfamiliar to you.\nThere are also many open and free resources and reference guides for R.\n\n\nQuick-R: a quick online reference for data input, basic statistics and plots\nR reference card PDF by Tom Short\nRstudio cheatsheets"
  },
  {
    "objectID": "r_basics.html#the-r-user-interface",
    "href": "r_basics.html#the-r-user-interface",
    "title": "\n3  Up and Running with R\n",
    "section": "\n3.1 The R User Interface",
    "text": "3.1 The R User Interface\nThe RStudio interface is simple. You type R code into the bottom line of the RStudio console pane and then click Enter to run it. The code you type is called a command, because it will command your computer to do something for you. The line you type it into is called the command line.\n\n\nFigure 3.1: Your computer does your bidding when you type R commands at the prompt in the bottom line of the console pane. Don’t forget to hit the Enter key. When you first open RStudio, the console appears in the pane on your left, but you can change this with File &gt; Tools &gt; Global Options in the menu bar.\n\nWhen you type a command at the prompt and hit Enter, your computer executes the command and shows you the results. Then RStudio displays a fresh prompt for your next command. For example, if you type 1 + 1 and hit Enter, RStudio will display:\n&gt; 1 + 1\n[1] 2\n&gt;\nYou’ll notice that a [1] appears next to your result. R is just letting you know that this line begins with the first value in your result. Some commands return more than one value, and their results may fill up multiple lines. For example, the command 100:130 returns 31 values; it creates a sequence of integers from 100 to 130. Notice that new bracketed numbers appear at the start of the second and third lines of output. These numbers just mean that the second line begins with the 14th value in the result, and the third line begins with the 25th value. You can mostly ignore the numbers that appear in brackets:\n&gt; 100:130\n [1] 100 101 102 103 104 105 106 107 108 109 110 111 112\n[14] 113 114 115 116 117 118 119 120 121 122 123 124 125\n[25] 126 127 128 129 130\n\n\n\n\n\n\nTip\n\n\n\nThe colon operator (:) returns every integer between two integers. It is an easy way to create a sequence of numbers.\n\n\n\n\n\n\n\n\nWhen do we compile?\n\n\n\nIn some languages, like C, Java, and FORTRAN, you have to compile your human-readable code into machine-readable code (often 1s and 0s) before you can run it. If you’ve programmed in such a language before, you may wonder whether you have to compile your R code before you can use it. The answer is no. R is a dynamic programming language, which means R automatically interprets your code as you run it.\n\n\nIf you type an incomplete command and press Enter, R will display a + prompt, which means R is waiting for you to type the rest of your command. Either finish the command or hit Escape to start over:\n&gt; 5 -\n+\n+ 1\n[1] 4\nIf you type a command that R doesn’t recognize, R will return an error message. If you ever see an error message, don’t panic. R is just telling you that your computer couldn’t understand or do what you asked it to do. You can then try a different command at the next prompt:\n&gt; 3 % 5\nError: unexpected input in \"3 % 5\"\n&gt;\n\n\n\n\n\n\nTip\n\n\n\nWhenever you get an error message in R, consider googling the error message. You’ll often find that someone else has had the same problem and has posted a solution online. Simply cutting-and-pasting the error message into a search engine will often work\n\n\nOnce you get the hang of the command line, you can easily do anything in R that you would do with a calculator. For example, you could do some basic arithmetic:\n\n2 * 3   \n\n[1] 6\n\n4 - 1   \n\n[1] 3\n\n# this obeys order-of-operations\n6 / (4 - 1)   \n\n[1] 2\n\n\n\n\n\n\n\n\nTip\n\n\n\nR treats the hashtag character, #, in a special way; R will not run anything that follows a hashtag on a line. This makes hashtags very useful for adding comments and annotations to your code. Humans will be able to read the comments, but your computer will pass over them. The hashtag is known as the commenting symbol in R.\n\n\n\n\n\n\n\n\nCancelling commands\n\n\n\nSome R commands may take a long time to run. You can cancel a command once it has begun by pressing ctrl + c or by clicking the “stop sign” if it is available in Rstudio. Note that it may also take R a long time to cancel the command.\n\n\n\n3.1.1 An exercise\nThat’s the basic interface for executing R code in RStudio. Think you have it? If so, try doing these simple tasks. If you execute everything correctly, you should end up with the same number that you started with:\n\nChoose any number and add 2 to it.\nMultiply the result by 3.\nSubtract 6 from the answer.\nDivide what you get by 3.\n\n\n10 + 2\n\n[1] 12\n\n12 * 3\n\n[1] 36\n\n36 - 6\n\n[1] 30\n\n30 / 3\n\n[1] 10"
  },
  {
    "objectID": "r_basics.html#objects",
    "href": "r_basics.html#objects",
    "title": "\n3  Up and Running with R\n",
    "section": "\n3.2 Objects",
    "text": "3.2 Objects\nNow that you know how to use R, let’s use it to make a virtual die. The : operator from a couple of pages ago gives you a nice way to create a group of numbers from one to six. The : operator returns its results as a vector (we are going to work with vectors in more detail), a one-dimensional set of numbers:\n1:6\n## 1 2 3 4 5 6\nThat’s all there is to how a virtual die looks! But you are not done yet. Running 1:6 generated a vector of numbers for you to see, but it didn’t save that vector anywhere for later use. If we want to use those numbers again, we’ll have to ask your computer to save them somewhere. You can do that by creating an R object.\nR lets you save data by storing it inside an R object. What is an object? Just a name that you can use to call up stored data. For example, you can save data into an object like a or b. Wherever R encounters the object, it will replace it with the data saved inside, like so:\n\na &lt;- 1\na\n\n[1] 1\n\n\n\na + 2\n\n[1] 3\n\n\n\n\n\n\n\n\nWhat just happened?\n\n\n\n\nTo create an R object, choose a name and then use the less-than symbol, &lt;, followed by a minus sign, -, to save data into it. This combination looks like an arrow, &lt;-. R will make an object, give it your name, and store in it whatever follows the arrow. So a &lt;- 1 stores 1 in an object named a.\nWhen you ask R what’s in a, R tells you on the next line.\nYou can use your object in new R commands, too. Since a previously stored the value of 1, you’re now adding 1 to 2.\n\n\n\n\n\n\n\n\n\nAssignment vs expressions\n\n\n\nEverything that you type into the R console can be assigned to one of two categories:\n\nAssignments\nExpressions\n\nAn expression is a command that tells R to do something. For example, 1 + 2 is an expression that tells R to add 1 and 2. When you type an expression into the R console, R will evaluate the expression and return the result. For example, if you type 1 + 2 into the R console, R will return 3. Expressions can have “side effects” but they don’t explicitly result in anything being added to R memory.\n\n5 + 2\n\n[1] 7\n\n28 %% 3\n\n[1] 1\n\n3^2\n\n[1] 9\n\n5 + 4 * 4 + 4 ^ 4 / 10\n\n[1] 46.6\n\n\nWhile using R as a calculator is interesting, to do useful and interesting things, we need to assign values to objects. To create objects, we need to give it a name followed by the assignment operator &lt;- (or, entirely equivalently, =) and the value we want to give it:\n\nweight_kg &lt;- 55\n\n\n\nSo, for another example, the following code would create an object named die that contains the numbers one through six. To see what is stored in an object, just type the object’s name by itself:\n\ndie &lt;- 1:6\ndie\n\n[1] 1 2 3 4 5 6\n\n\nWhen you create an object, the object will appear in the environment pane of RStudio, as shown in Figure 3.2. This pane will show you all of the objects you’ve created since opening RStudio.\n\n\nFigure 3.2: Assignment creates an object in the environment pane.\n\n\n\n\nYou can name an object in R almost anything you want, but there are a few rules. First, a name cannot start with a number. Second, a name cannot use some special symbols, like ^, !, $, @, +, -, /, or *:\n\n\nGood names\nNames that cause errors\n\n\n\na\n1trial\n\n\nb\n$\n\n\nFOO\n^mean\n\n\nmy_var\n2nd\n\n\n.day\n!bad\n\n\n\n\n\n\n\n\n\nCapitalization matters\n\n\n\nR is case-sensitive, so name and Name will refer to different objects:\n&gt; Name = 0\n&gt; Name + 1\n[1] 1\n&gt; name + 1\nError: object 'name' not found\nThe error above is a common one!\n\n\nFinally, R will overwrite any previous information stored in an object without asking you for permission. So, it is a good idea to not use names that are already taken:\n\nmy_number &lt;- 1\nmy_number \n\n[1] 1\n\n\n\nmy_number &lt;- 999\nmy_number\n\n[1] 999\n\n\nYou can see which object names you have already used with the function ls:\nls()\nYour environment will contain different names than mine, because you have probably created different objects.\nYou can also see which names you have used by examining RStudio’s environment pane.\nWe now have a virtual die that is stored in the computer’s memory and which has a name that we can use to refer to it. You can access it whenever you like by typing the word die.\nSo what can you do with this die? Quite a lot. R will replace an object with its contents whenever the object’s name appears in a command. So, for example, you can do all sorts of math with the die. Math isn’t so helpful for rolling dice, but manipulating sets of numbers will be your stock and trade as a data scientist. So let’s take a look at how to do that:\n\ndie - 1\n\n[1] 0 1 2 3 4 5\n\ndie / 2\n\n[1] 0.5 1.0 1.5 2.0 2.5 3.0\n\ndie * die\n\n[1]  1  4  9 16 25 36\n\n\nR uses element-wise execution when working with a vector like die. When you manipulate a set of numbers, R will apply the same operation to each element in the set. So for example, when you run die - 1, R subtracts one from each element of die.\nWhen you use two or more vectors in an operation, R will line up the vectors and perform a sequence of individual operations. For example, when you run die * die, R lines up the two die vectors and then multiplies the first element of vector 1 by the first element of vector 2. R then multiplies the second element of vector 1 by the second element of vector 2, and so on, until every element has been multiplied. The result will be a new vector the same length as the first two {Figure 3.3}.\n\n\nFigure 3.3: “When R performs element-wise execution, it matches up vectors and then manipulates each pair of elements independently.”\n\nIf you give R two vectors of unequal lengths, R will repeat the shorter vector until it is as long as the longer vector, and then do the math, as shown in Figure 3.4. This isn’t a permanent change–the shorter vector will be its original size after R does the math. If the length of the short vector does not divide evenly into the length of the long vector, R will return a warning message. This behavior is known as vector recycling, and it helps R do element-wise operations:\n\n1:2\n\n[1] 1 2\n\n1:4\n\n[1] 1 2 3 4\n\ndie\n\n[1] 1 2 3 4 5 6\n\ndie + 1:2\n\n[1] 2 4 4 6 6 8\n\ndie + 1:4\n\nWarning in die + 1:4: longer object length is not a multiple of shorter object\nlength\n\n\n[1] 2 4 6 8 6 8\n\n\n\n\nFigure 3.4: “R will repeat a short vector to do element-wise operations with two vectors of uneven lengths.”\n\nElement-wise operations are a very useful feature in R because they manipulate groups of values in an orderly way. When you start working with data sets, element-wise operations will ensure that values from one observation or case are only paired with values from the same observation or case. Element-wise operations also make it easier to write your own programs and functions in R.\n\n\n\n\n\n\nElement-wise operations are not matrix operations\n\n\n\nIt is important to know that operations with vectors are not the same that you might expect if you are expecting R to perform “matrix” operations. R can do inner multiplication with the %*% operator and outer multiplication with the %o% operator:\n# Inner product (1*1 + 2*2 + 3*3 + 4*4 + 5*5 + 6*6)\ndie %*% die\n# Outer product\ndie %o% die\n\n\nNow that you can do math with your die object, let’s look at how you could “roll” it. Rolling your die will require something more sophisticated than basic arithmetic; you’ll need to randomly select one of the die’s values. And for that, you will need a function."
  },
  {
    "objectID": "r_basics.html#functions",
    "href": "r_basics.html#functions",
    "title": "\n3  Up and Running with R\n",
    "section": "\n3.3 Functions",
    "text": "3.3 Functions\nR has many functions and puts them all at our disposal. We can use functions to do simple and sophisticated tasks. For example, we can round a number with the round function, or calculate its factorial with the factorial function. Using a function is pretty simple. Just write the name of the function and then the data you want the function to operate on in parentheses:\n\nround(3.1415)\n\n[1] 3\n\nfactorial(3)\n\n[1] 6\n\n\nThe data that you pass into the function is called the function’s argument. The argument can be raw data, an R object, or even the results of another R function. In this last case, R will work from the innermost function to the outermost Figure 3.5.\n\nmean(1:6)\n\n[1] 3.5\n\nmean(die)\n\n[1] 3.5\n\nround(mean(die))\n\n[1] 4\n\n\n\n\nFigure 3.5: “When you link functions together, R will resolve them from the innermost operation to the outermost. Here R first looks up die, then calculates the mean of one through six, then rounds the mean.”\n\nReturning to our die, we can use the sample function to randomly select one of the die’s values; in other words, the sample function can simulate rolling the die.\nThe sample function takes two arguments: a vector named x and a number named size. sample will return size elements from the vector:\n\nsample(x = 1:4, size = 2)\n\n[1] 3 2\n\n\nTo roll your die and get a number back, set x to die and sample one element from it. You’ll get a new (maybe different) number each time you roll it:\n\nsample(x = die, size = 1)\n\n[1] 2\n\nsample(x = die, size = 1)\n\n[1] 1\n\nsample(x = die, size = 1)\n\n[1] 3\n\n\nMany R functions take multiple arguments that help them do their job. You can give a function as many arguments as you like as long as you separate each argument with a comma.\nYou may have noticed that I set die and 1 equal to the names of the arguments in sample, x and size. Every argument in every R function has a name. You can specify which data should be assigned to which argument by setting a name equal to data, as in the preceding code. This becomes important as you begin to pass multiple arguments to the same function; names help you avoid passing the wrong data to the wrong argument. However, using names is optional. You will notice that R users do not often use the name of the first argument in a function. So you might see the previous code written as:\n\nsample(die, size = 1)\n\n[1] 4\n\n\nOften, the name of the first argument is not very descriptive, and it is usually obvious what the first piece of data refers to anyways.\nBut how do you know which argument names to use? If you try to use a name that a function does not expect, you will likely get an error:\nround(3.1415, corners = 2)\n## Error in round(3.1415, corners = 2) : unused argument(s) (corners = 2)\nIf you’re not sure which names to use with a function, you can look up the function’s arguments with args. To do this, place the name of the function in the parentheses behind args. For example, you can see that the round function takes two arguments, one named x and one named digits:\n\nargs(round)\n\nfunction (x, digits = 0) \nNULL\n\n\nDid you notice that args shows that the digits argument of round is already set to 0? Frequently, an R function will take optional arguments like digits. These arguments are considered optional because they come with a default value. You can pass a new value to an optional argument if you want, and R will use the default value if you do not. For example, round will round your number to 0 digits past the decimal point by default. To override the default, supply your own value for digits:\n\nround(3.1415)\n\n[1] 3\n\nround(3.1415, digits = 2)\n\n[1] 3.14\n\n# pi happens to be a built-in value in R\npi\n\n[1] 3.141593\n\nround(pi)\n\n[1] 3\n\n\nYou should write out the names of each argument after the first one or two when you call a function with multiple arguments. Why? First, this will help you and others understand your code. It is usually obvious which argument your first input refers to (and sometimes the second input as well). However, you’d need a large memory to remember the third and fourth arguments of every R function. Second, and more importantly, writing out argument names prevents errors.\nIf you do not write out the names of your arguments, R will match your values to the arguments in your function by order. For example, in the following code, the first value, die, will be matched to the first argument of sample, which is named x. The next value, 1, will be matched to the next argument, size:\n\nsample(die, 1)\n\n[1] 6\n\n\nAs you provide more arguments, it becomes more likely that your order and R’s order may not align. As a result, values may get passed to the wrong argument. Argument names prevent this. R will always match a value to its argument name, no matter where it appears in the order of arguments:\n\nsample(size = 1, x = die)\n\n[1] 2\n\n\n\n3.3.1 Sample with Replacement\nIf you set size = 2, you can almost simulate a pair of dice. Before we run that code, think for a minute why that might be the case. sample will return two numbers, one for each die:\n\nsample(die, size = 2)\n\n[1] 6 2\n\n\nI said this “almost” works because this method does something funny. If you use it many times, you’ll notice that the second die never has the same value as the first die, which means you’ll never roll something like a pair of threes or snake eyes. What is going on?\nBy default, sample builds a sample without replacement. To see what this means, imagine that sample places all of the values of die in a jar or urn. Then imagine that sample reaches into the jar and pulls out values one by one to build its sample. Once a value has been drawn from the jar, sample sets it aside. The value doesn’t go back into the jar, so it cannot be drawn again. So if sample selects a six on its first draw, it will not be able to select a six on the second draw; six is no longer in the jar to be selected. Although sample creates its sample electronically, it follows this seemingly physical behavior.\nOne side effect of this behavior is that each draw depends on the draws that come before it. In the real world, however, when you roll a pair of dice, each die is independent of the other. If the first die comes up six, it does not prevent the second die from coming up six. In fact, it doesn’t influence the second die in any way whatsoever. You can recreate this behavior in sample by adding the argument replace = TRUE:\n\nsample(die, size = 2, replace = TRUE)\n\n[1] 2 5\n\n\nThe argument replace = TRUE causes sample to sample with replacement. Our jar example provides a good way to understand the difference between sampling with replacement and without. When sample uses replacement, it draws a value from the jar and records the value. Then it puts the value back into the jar. In other words, sample replaces each value after each draw. As a result, sample may select the same value on the second draw. Each value has a chance of being selected each time. It is as if every draw were the first draw.\nSampling with replacement is an easy way to create independent random samples. Each value in your sample will be a sample of size one that is independent of the other values. This is the correct way to simulate a pair of dice:\n\nsample(die, size = 2, replace = TRUE)\n\n[1] 2 2\n\n\nCongratulate yourself; you’ve just run your first simulation in R! You now have a method for simulating the result of rolling a pair of dice. If you want to add up the dice, you can feed your result straight into the sum function:\n\ndice &lt;- sample(die, size = 2, replace = TRUE)\ndice\n\n[1] 6 5\n\nsum(dice)\n\n[1] 11\n\n\nWhat would happen if you call dice multiple times? Would R generate a new pair of dice values each time? Let’s give it a try:\n\ndice\n\n[1] 6 5\n\ndice\n\n[1] 6 5\n\ndice\n\n[1] 6 5\n\n\nThe name dice refers to a vector of two numbers. Calling more than once does not change the favlue. Each time you call dice, R will show you the result of that one time you called sample and saved the output to dice. R won’t rerun sample(die, 2, replace = TRUE) to create a new roll of the dice. Once you save a set of results to an R object, those results do not change.\nHowever, it would be convenient to have an object that can re-roll the dice whenever you call it. You can make such an object by writing your own R function."
  },
  {
    "objectID": "r_basics.html#write-functions",
    "href": "r_basics.html#write-functions",
    "title": "\n3  Up and Running with R\n",
    "section": "\n3.4 Writing Your Own Functions",
    "text": "3.4 Writing Your Own Functions\nTo recap, you already have working R code that simulates rolling a pair of dice:\n\ndie &lt;- 1:6\ndice &lt;- sample(die, size = 2, replace = TRUE)\nsum(dice)\n\n[1] 6\n\n\nYou can retype this code into the console anytime you want to re-roll your dice. However, this is an awkward way to work with the code. It would be easier to use your code if you wrapped it into its own function, which is exactly what we’ll do now. We’re going to write a function named roll that you can use to roll your virtual dice. When you’re finished, the function will work like this: each time you call roll(), R will return the sum of rolling two dice:\nroll()\n## 8 \n\nroll()\n## 3\n\nroll()\n## 7\nFunctions may seem mysterious or fancy, but they are just another type of R object. Instead of containing data, they contain code. This code is stored in a special format that makes it easy to reuse the code in new situations. You can write your own functions by recreating this format.\n\n3.4.1 The Function Constructor\nEvery function in R has three basic parts: a name, a body of code, and a set of arguments. To make your own function, you need to replicate these parts and store them in an R object, which you can do with the function function. To do this, call function() and follow it with a pair of braces, {}:\n\nmy_function &lt;- function() {}\n\nThis function, as written, doesn’t do anything (yet). However, it is a valid function. You can call it by typing its name followed by an open and closed parenthesis:\n\nmy_function()\n\nNULL\n\n\nfunction will build a function out of whatever R code you place between the braces. For example, you can turn your dice code into a function by calling:\n\nroll &lt;- function() {\n  die &lt;- 1:6\n  dice &lt;- sample(die, size = 2, replace = TRUE)\n  sum(dice)\n}\n\n\n\n\n\n\n\nIndentation and readability\n\n\n\nNotice each line of code between the braces is indented. This makes the code easier to read but has no impact on how the code runs. R ignores spaces and line breaks and executes one complete expression at a time. Note that in other languages like python, spacing is extremely important and part of the language.\n\n\nJust hit the Enter key between each line after the first brace, {. R will wait for you to type the last brace, }, before it responds.\nDon’t forget to save the output of function to an R object. This object will become your new function. To use it, write the object’s name followed by an open and closed parenthesis:\n\nroll()\n\n[1] 8\n\n\nYou can think of the parentheses as the “trigger” that causes R to run the function. If you type in a function’s name without the parentheses, R will show you the code that is stored inside the function. If you type in the name with the parentheses, R will run that code:\n\nroll\n\nfunction() {\n  die &lt;- 1:6\n  dice &lt;- sample(die, size = 2, replace = TRUE)\n  sum(dice)\n}\n\nroll()\n\n[1] 9\n\n\nThe code that you place inside your function is known as the body of the function. When you run a function in R, R will execute all of the code in the body and then return the result of the last line of code. If the last line of code doesn’t return a value, neither will your function, so you want to ensure that your final line of code returns a value. One way to check this is to think about what would happen if you ran the body of code line by line in the command line. Would R display a result after the last line, or would it not?\nHere’s some code that would display a result:\ndice\n1 + 1\nsqrt(2)\nAnd here’s some code that would not:\ndice &lt;- sample(die, size = 2, replace = TRUE)\ntwo &lt;- 1 + 1\na &lt;- sqrt(2)\nAgain, this is just showing the distinction between expressions and assignments."
  },
  {
    "objectID": "r_basics.html#arguments",
    "href": "r_basics.html#arguments",
    "title": "\n3  Up and Running with R\n",
    "section": "\n3.5 Arguments",
    "text": "3.5 Arguments\nWhat if we removed one line of code from our function and changed the name die to bones (just a name–don’t think of it as important), like this?\n\nroll2 &lt;- function() {\n  dice &lt;- sample(bones, size = 2, replace = TRUE)\n  sum(dice)\n}\n\nNow I’ll get an error when I run the function. The function needs the object bones to do its job, but there is no object named bones to be found (you can check by typing ls() which will show you the names in the environment, or memory).\nroll2()\n## Error in sample(bones, size = 2, replace = TRUE) : \n##   object 'bones' not found\nYou can supply bones when you call roll2 if you make bones an argument of the function. To do this, put the name bones in the parentheses that follow function when you define roll2:\n\nroll2 &lt;- function(bones) {\n  dice &lt;- sample(bones, size = 2, replace = TRUE)\n  sum(dice)\n}\n\nNow roll2 will work as long as you supply bones when you call the function. You can take advantage of this to roll different types of dice each time you call roll2.\nRemember, we’re rolling pairs of dice:\n\nroll2(bones = 1:4)\n\n[1] 6\n\nroll2(bones = 1:6)\n\n[1] 8\n\nroll2(1:20)\n\n[1] 23\n\n\nNotice that roll2 will still give an error if you do not supply a value for the bones argument when you call roll2:\nroll2()\n## Error in sample(bones, size = 2, replace = TRUE) : \n##   argument \"bones\" is missing, with no default\nYou can prevent this error by giving the bones argument a default value. To do this, set bones equal to a value when you define roll2:\n\nroll2 &lt;- function(bones = 1:6) {\n  dice &lt;- sample(bones, size = 2, replace = TRUE)\n  sum(dice)\n}\n\nNow you can supply a new value for bones if you like, and roll2 will use the default if you do not:\n\nroll2()\n\n[1] 8\n\n\nYou can give your functions as many arguments as you like. Just list their names, separated by commas, in the parentheses that follow function. When the function is run, R will replace each argument name in the function body with the value that the user supplies for the argument. If the user does not supply a value, R will replace the argument name with the argument’s default value (if you defined one).\nTo summarize, function helps you construct your own R functions. You create a body of code for your function to run by writing code between the braces that follow function. You create arguments for your function to use by supplying their names in the parentheses that follow function. Finally, you give your function a name by saving its output to an R object, as shown in Figure 3.6.\nOnce you’ve created your function, R will treat it like every other function in R. Think about how useful this is. Have you ever tried to create a new Excel option and add it to Microsoft’s menu bar? Or a new slide animation and add it to Powerpoint’s options? When you work with a programming language, you can do these types of things. As you learn to program in R, you will be able to create new, customized, reproducible tools for yourself whenever you like.\n\n\nFigure 3.6: “Every function in R has the same parts, and you can use function to create these parts. Assign the result to a name, so you can call the function later.”"
  },
  {
    "objectID": "r_basics.html#scripts",
    "href": "r_basics.html#scripts",
    "title": "\n3  Up and Running with R\n",
    "section": "\n3.6 Scripts",
    "text": "3.6 Scripts\nScripts are code that are saved for later reuse or editing. An R script is just a plain text file that you save R code in. You can open an R script in RStudio by going to File &gt; New File &gt; R script in the menu bar. RStudio will then open a fresh script above your console pane, as shown in Figure 3.7.\nI strongly encourage you to write and edit all of your R code in a script before you run it in the console. Why? This habit creates a reproducible record of your work. When you’re finished for the day, you can save your script and then use it to rerun your entire analysis the next day. Scripts are also very handy for editing and proofreading your code, and they make a nice copy of your work to share with others. To save a script, click the scripts pane, and then go to File &gt; Save As in the menu bar.\n\n\nFigure 3.7: “When you open an R Script (File &gt; New File &gt; R Script in the menu bar), RStudio creates a fourth pane (or puts a new tab in the existing pane) above the console where you can write and edit your code.”\n\nRStudio comes with many built-in features that make it easy to work with scripts. First, you can automatically execute a line of code in a script by clicking the Run button at the top of the editor panel.\nR will run whichever line of code your cursor is on. If you have a whole section highlighted, R will run the highlighted code. Alternatively, you can run the entire script by clicking the Source button. Don’t like clicking buttons? You can use Control + Return as a shortcut for the Run button. On Macs, that would be Command + Return.\n\n\n\nIf you’re not convinced about scripts, you soon will be. It becomes a pain to write multi-line code in the console’s single-line command line. Let’s avoid that headache and open your first script now before we move to the next chapter.\n\n\n\n\n\n\nTip\n\n\n\nExtract function\nRStudio comes with a tool that can help you build functions. To use it, highlight the lines of code in your R script that you want to turn into a function. Then click Code &gt; Extract Function in the menu bar. RStudio will ask you for a function name to use and then wrap your code in a function call. It will scan the code for undefined variables and use these as arguments.\nYou may want to double-check RStudio’s work. It assumes that your code is correct, so if it does something surprising, you may have a problem in your code."
  },
  {
    "objectID": "r_basics.html#summary",
    "href": "r_basics.html#summary",
    "title": "\n3  Up and Running with R\n",
    "section": "\n3.7 Summary",
    "text": "3.7 Summary\nWe’ve covered a lot of ground already. You now have a virtual die stored in your computer’s memory, as well as your own R function that rolls a pair of dice. You’ve also begun speaking the R language.\nThe two most important components of the R language are objects, which store data, and functions, which manipulate data. R also uses a host of operators like +, -, *, /, and &lt;- to do basic tasks. As a data scientist, you will use R objects to store data in your computer’s memory, and you will use functions to automate tasks and do complicated calculations."
  },
  {
    "objectID": "packages_and_dice.html#packages",
    "href": "packages_and_dice.html#packages",
    "title": "\n4  Packages and more dice\n",
    "section": "\n4.1 Packages",
    "text": "4.1 Packages\nR is a powerful language for data science and programming, allowing beginners and experts alike to manipulate, analyze, and visualize data effectively. One of the most appealing features of R is its extensive library of packages, which are essential tools for expanding its capabilities and streamlining the coding process.\nAn R package is a collection of reusable functions, datasets, and compiled code created by other users and developers to extend the functionality of the base R language. These packages cover a wide range of applications, such as data manipulation, statistical analysis, machine learning, and data visualization. By utilizing existing R packages, you can leverage the expertise of others and save time by avoiding the need to create custom functions from scratch.\nUsing others’ R packages is incredibly beneficial as it allows you to take advantage of the collective knowledge of the R community. Developers often create packages to address specific challenges, optimize performance, or implement popular algorithms or methodologies. By incorporating these packages into your projects, you can enhance your productivity, reduce development time, and ensure that you are using well-tested and reliable code.\n\n4.1.1 install.packages\nTo install an R package, you can use the install.packages() function in the R console or script. For example, to install the popular data manipulation package “dplyr,” simply type install.packages(“dplyr”). This command will download the package from the Comprehensive R Archive Network (CRAN) and install it on your local machine. Keep in mind that you only need to install a package once, unless you want to update it to a newer version.\nIn our case, we want to install the ggplot2 package.\n\ninstall.packages('ggplot2')\n\n\n4.1.2 library\nAfter installing an R package, you will need to load it into your R session before using its functions. To load a package, use the library() function followed by the package name, such as library(dplyr). Loading a package makes its functions and datasets available for use in your current R session. Note that you need to load a package every time you start a new R session.\n\nlibrary(ggplot2)\n\nNow, the functionality of the ggplot2 package is available in our R session.\n\n\n\n\n\n\nInstalling vs loading packages\n\n\n\nThe main thing to remember is that you only need to install a package once, but you need to load it with library each time you wish to use it in a new R session. R will unload all of its packages each time you close RStudio.\n\n\n\n4.1.3 Finding R packages\nFinding useful R packages can be done in several ways. First, browsing CRAN (https://cran.r-project.org/) and Bioconductor (more later, https://bioconductor.org) are an excellent starting points, as they host thousands of packages categorized by topic. Additionally, online forums like Stack Overflow and R-bloggers can provide valuable recommendations based on user experiences. Social media platforms such as Twitter, where developers and data scientists often share new packages and updates, can also be a helpful resource. Finally, don’t forget to ask your colleagues or fellow R users for their favorite packages, as they may have insights on which ones best suit your specific needs."
  },
  {
    "objectID": "packages_and_dice.html#are-our-dice-fair",
    "href": "packages_and_dice.html#are-our-dice-fair",
    "title": "\n4  Packages and more dice\n",
    "section": "\n4.2 Are our dice fair?",
    "text": "4.2 Are our dice fair?\nWell, let’s review our code.\n\nroll2 &lt;- function(bones = 1:6) {\n  dice = sample(bones, size = 2, replace = TRUE)\n  sum(dice)\n}\n\nIf our dice are fair, then each number should show up equally. What does the sum look like with our two dice?\n\n\nFigure 4.1: In an ideal world, a histogram of the results would look like this\n\nRead the help page for replicate (i.e., help(\"replicate\")). In short, it suggests that we can repeat our dice rolling as many times as we like and replicate will return a vector of the sums for each roll.\n\nrolls = replicate(n = 100, roll2())\n\nWhat does rolls look like?\n\nhead(rolls)\n\n[1]  5  8 10  6  7  5\n\nlength(rolls)\n\n[1] 100\n\nmean(rolls)\n\n[1] 6.89\n\nsummary(rolls)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n   2.00    5.00    7.00    6.89    8.25   12.00 \n\n\nThis looks like it roughly agrees with our sketched out ideal histogram in Figure 4.1. However, now that we’ve loaded the qplot function from the ggplot2 package, we can make a histogram of the data themselves.\n\nqplot(rolls, binwidth=1)\n\nWarning: `qplot()` was deprecated in ggplot2 3.4.0.\n\n\n\n\nFigure 4.2: Histogram of the sums from 100 rolls of our fair dice\n\n\n\nHow does your histogram look (and yours will be different from mine since we are sampling random values)? Is it what you expect?\nWhat happens to our histogram as we increase the number of replicates?\n\nrolls = replicate(n = 100000, roll2())\nqplot(rolls, binwidth=1)\n\n\n\nFigure 4.3: Histogram with 100000 rolls much more closely approximates the pyramidal shape we anticipated"
  },
  {
    "objectID": "packages_and_dice.html#bonus-exercise",
    "href": "packages_and_dice.html#bonus-exercise",
    "title": "\n4  Packages and more dice\n",
    "section": "\n4.3 Bonus exercise",
    "text": "4.3 Bonus exercise\nHow would you change the roll2 function to weight the dice?"
  },
  {
    "objectID": "data_structures_overview.html#chapter-overview",
    "href": "data_structures_overview.html#chapter-overview",
    "title": "Overview of R Data Structures",
    "section": "Chapter overview",
    "text": "Chapter overview\n\nVectors : In this chapter, we will introduce you to the simplest data structure in R, the vector. We will cover how to create, access, and manipulate vectors, as well as discuss their unique properties and limitations.\n\nMatrices\n\nNext, we will explore matrices, which are two-dimensional data structures that extend vectors. You will learn how to create, access, and manipulate matrices, and understand their usefulness in mathematical operations and data organization.\n\n\n\nLists\n\nThe third chapter will focus on lists, a versatile data structure that can store elements of different types and sizes. We will discuss how to create, access, and modify lists, and demonstrate their flexibility in handling complex data structures.\n\n\n\nData.frames\n\nFinally, we will examine data.frames, a widely-used data structure for organizing and manipulating tabular data. You will learn how to create, access, and manipulate data.frames, and understand their advantages over other data structures for data analysis tasks.\n\n\n\nArrays\n\nWhile we will not focus directly on the array data type, which are multidimensional data structures that extend matrices, they are very similar to matrices, but with a third dimension.\n\n\n\nAs you progress through these chapters, we encourage you to practice the examples and exercises provided, engage in discussion, and collaborate with your peers to deepen your understanding of R data structures. This solid foundation will serve as the basis for more advanced data manipulation, analysis, and visualization techniques in R."
  },
  {
    "objectID": "vectors.html#what-is-a-vector",
    "href": "vectors.html#what-is-a-vector",
    "title": "\n5  Vectors\n",
    "section": "\n5.1 What is a Vector?",
    "text": "5.1 What is a Vector?\nA vector is the simplest and most basic data structure in R. It is a one-dimensional, ordered collection of elements, where all the elements are of the same data type. Vectors can store various types of data, such as numeric, character, or logical values. Figure 5.1 shows a pictorial representation of three vector examples.\n\n\nFigure 5.1: “Pictorial representation of three vector examples. The first vector is a numeric vector. The second is a ‘logical’ vector. The third is a character vector. Vectors also have indices and, optionally, names.”\n\nIn this chapter, we will provide a comprehensive overview of vectors, including how to create, access, and manipulate them. We will also discuss some unique properties and rules associated with vectors, and explore their applications in data analysis tasks.\nIn R, even a single value is a vector with length=1.\n\nz = 1\nz\n\n[1] 1\n\nlength(z)\n\n[1] 1\n\n\nIn the code above, we “assigned” the value 1 to the variable named z. Typing z by itself is an “expression” that returns a result which is, in this case, the value that we just assigned. The length method takes an R object and returns the R length. There are numerous ways of asking R about what an object represents, and length is one of them.\nVectors can contain numbers, strings (character data), or logical values (TRUE and FALSE) or other “atomic” data types Table 5.1. Vectors cannot contain a mix of types! We will introduce another data structure, the R list for situations when we need to store a mix of base R data types.\n\n\n\n\nData type\nStores\n\n\n\nnumeric\nfloating point numbers\n\n\ninteger\nintegers\n\n\ncomplex\ncomplex numbers\n\n\nfactor\ncategorical data\n\n\ncharacter\nstrings\n\n\nlogical\nTRUE or FALSE\n\n\nNA\nmissing\n\n\nNULL\nempty\n\n\nfunction\nfunction type\n\n\n\nTable 5.1: Atomic (simplest) data types in R."
  },
  {
    "objectID": "vectors.html#creating-vectors",
    "href": "vectors.html#creating-vectors",
    "title": "\n5  Vectors\n",
    "section": "\n5.2 Creating vectors",
    "text": "5.2 Creating vectors\nCharacter vectors (also sometimes called “string” vectors) are entered with each value surrounded by single or double quotes; either is acceptable, but they must match. They are always displayed by R with double quotes. Here are some examples of creating vectors:\n\n# examples of vectors\nc('hello','world')\n\n[1] \"hello\" \"world\"\n\nc(1,3,4,5,1,2)\n\n[1] 1 3 4 5 1 2\n\nc(1.12341e7,78234.126)\n\n[1] 11234100.00    78234.13\n\nc(TRUE,FALSE,TRUE,TRUE)\n\n[1]  TRUE FALSE  TRUE  TRUE\n\n# note how in the next case the TRUE is converted to \"TRUE\"\n# with quotes around it.\nc(TRUE,'hello')\n\n[1] \"TRUE\"  \"hello\"\n\n\nWe can also create vectors as “regular sequences” of numbers. For example:\n\n# create a vector of integers from 1 to 10\nx = 1:10\n# and backwards\nx = 10:1\n\nThe seq function can create more flexible regular sequences.\n\n# create a vector of numbers from 1 to 4 skipping by 0.3\ny = seq(1,4,0.3)\n\nAnd creating a new vector by concatenating existing vectors is possible, as well.\n\n# create a sequence by concatenating two other sequences\nz = c(y,x)\nz\n\n [1]  1.0  1.3  1.6  1.9  2.2  2.5  2.8  3.1  3.4  3.7  4.0 10.0  9.0  8.0  7.0\n[16]  6.0  5.0  4.0  3.0  2.0  1.0"
  },
  {
    "objectID": "vectors.html#vector-operations",
    "href": "vectors.html#vector-operations",
    "title": "\n5  Vectors\n",
    "section": "\n5.3 Vector Operations",
    "text": "5.3 Vector Operations\nOperations on a single vector are typically done element-by-element. For example, we can add 2 to a vector, 2 is added to each element of the vector and a new vector of the same length is returned.\n\nx = 1:10\nx + 2\n\n [1]  3  4  5  6  7  8  9 10 11 12\n\n\nIf the operation involves two vectors, the following rules apply. If the vectors are the same length: R simply applies the operation to each pair of elements.\n\nx + x\n\n [1]  2  4  6  8 10 12 14 16 18 20\n\n\nIf the vectors are different lengths, but one length a multiple of the other, R reuses the shorter vector as needed.\n\nx = 1:10\ny = c(1,2)\nx * y\n\n [1]  1  4  3  8  5 12  7 16  9 20\n\n\nIf the vectors are different lengths, but one length not a multiple of the other, R reuses the shorter vector as needed and delivers a warning.\n\nx = 1:10\ny = c(2,3,4)\nx * y\n\nWarning in x * y: longer object length is not a multiple of shorter object\nlength\n\n\n [1]  2  6 12  8 15 24 14 24 36 20\n\n\nTypical operations include multiplication (“*”), addition, subtraction, division, exponentiation (“^”), but many operations in R operate on vectors and are then called “vectorized”.\nBe aware of the recycling rule when working with vectors of different lengths, as it may lead to unexpected results if you’re not careful."
  },
  {
    "objectID": "vectors.html#logical-vectors",
    "href": "vectors.html#logical-vectors",
    "title": "\n5  Vectors\n",
    "section": "\n5.4 Logical Vectors",
    "text": "5.4 Logical Vectors\nLogical vectors are vectors composed on only the values TRUE and FALSE. Note the all-upper-case and no quotation marks.\n\na = c(TRUE,FALSE,TRUE)\n\n# we can also create a logical vector from a numeric vector\n# 0 = false, everything else is 1\nb = c(1,0,217)\nd = as.logical(b)\nd\n\n[1]  TRUE FALSE  TRUE\n\n# test if a and d are the same at every element\nall.equal(a,d)\n\n[1] TRUE\n\n# We can also convert from logical to numeric\nas.numeric(a)\n\n[1] 1 0 1\n\n\n\n5.4.1 Logical Operators\nSome operators like &lt;, &gt;, ==, &gt;=, &lt;=, != can be used to create logical vectors.\n\n# create a numeric vector\nx = 1:10\n# testing whether x &gt; 5 creates a logical vector\nx &gt; 5\n\n [1] FALSE FALSE FALSE FALSE FALSE  TRUE  TRUE  TRUE  TRUE  TRUE\n\nx &lt;= 5\n\n [1]  TRUE  TRUE  TRUE  TRUE  TRUE FALSE FALSE FALSE FALSE FALSE\n\nx != 5\n\n [1]  TRUE  TRUE  TRUE  TRUE FALSE  TRUE  TRUE  TRUE  TRUE  TRUE\n\nx == 5\n\n [1] FALSE FALSE FALSE FALSE  TRUE FALSE FALSE FALSE FALSE FALSE\n\n\nWe can also assign the results to a variable:\n\ny = (x == 5)\ny\n\n [1] FALSE FALSE FALSE FALSE  TRUE FALSE FALSE FALSE FALSE FALSE"
  },
  {
    "objectID": "vectors.html#indexing-vectors",
    "href": "vectors.html#indexing-vectors",
    "title": "\n5  Vectors\n",
    "section": "\n5.5 Indexing Vectors",
    "text": "5.5 Indexing Vectors\nIn R, an index is used to refer to a specific element or set of elements in an vector (or other data structure). [R uses [ and ] to perform indexing, although other approaches to getting subsets of larger data structures are common in R.\n\nx = seq(0,1,0.1)\n# create a new vector from the 4th element of x\nx[4]\n\n[1] 0.3\n\n\nWe can even use other vectors to perform the “indexing”.\n\nx[c(3,5,6)]\n\n[1] 0.2 0.4 0.5\n\ny = 3:6\nx[y]\n\n[1] 0.2 0.3 0.4 0.5\n\n\nCombining the concept of indexing with the concept of logical vectors results in a very power combination.\n\n# use help('rnorm') to figure out what is happening next\nmyvec = rnorm(10)\n\n# create logical vector that is TRUE where myvec is &gt;0.25\ngt1 = (myvec &gt; 0.25)\nsum(gt1)\n\n[1] 3\n\n# and use our logical vector to create a vector of myvec values that are &gt;0.25\nmyvec[gt1]\n\n[1] 0.4574919 1.1630095 0.5459320\n\n# or &lt;=0.25 using the logical \"not\" operator, \"!\"\nmyvec[!gt1]\n\n[1] -0.59597571 -1.85019670 -2.19753321  0.02355410  0.19790134 -0.09821473\n[7] -0.01932891\n\n# shorter, one line approach\nmyvec[myvec &gt; 0.25]\n\n[1] 0.4574919 1.1630095 0.5459320"
  },
  {
    "objectID": "vectors.html#named-vectors",
    "href": "vectors.html#named-vectors",
    "title": "\n5  Vectors\n",
    "section": "\n5.6 Named Vectors",
    "text": "5.6 Named Vectors\nNamed vectors are vectors with labels or names assigned to their elements. These names can be used to access and manipulate the elements in a more meaningful way.\nTo create a named vector, use the names() function:\n\nfruit_prices &lt;- c(0.5, 0.75, 1.25)\nnames(fruit_prices) &lt;- c(\"apple\", \"banana\", \"cherry\")\nprint(fruit_prices)\n\n apple banana cherry \n  0.50   0.75   1.25 \n\n\nYou can also access and modify elements using their names:\n\nbanana_price &lt;- fruit_prices[\"banana\"]\nprint(banana_price)\n\nbanana \n  0.75 \n\nfruit_prices[\"apple\"] &lt;- 0.6\nprint(fruit_prices)\n\n apple banana cherry \n  0.60   0.75   1.25"
  },
  {
    "objectID": "vectors.html#character-vectors-a.k.a.-strings",
    "href": "vectors.html#character-vectors-a.k.a.-strings",
    "title": "\n5  Vectors\n",
    "section": "\n5.7 Character Vectors, A.K.A. Strings",
    "text": "5.7 Character Vectors, A.K.A. Strings\nR uses the paste function to concatenate strings.\n\npaste(\"abc\",\"def\")\n\n[1] \"abc def\"\n\npaste(\"abc\",\"def\",sep=\"THISSEP\")\n\n[1] \"abcTHISSEPdef\"\n\npaste0(\"abc\",\"def\")\n\n[1] \"abcdef\"\n\n## [1] \"abcdef\"\npaste(c(\"X\",\"Y\"),1:10)\n\n [1] \"X 1\"  \"Y 2\"  \"X 3\"  \"Y 4\"  \"X 5\"  \"Y 6\"  \"X 7\"  \"Y 8\"  \"X 9\"  \"Y 10\"\n\npaste(c(\"X\",\"Y\"),1:10,sep=\"_\")\n\n [1] \"X_1\"  \"Y_2\"  \"X_3\"  \"Y_4\"  \"X_5\"  \"Y_6\"  \"X_7\"  \"Y_8\"  \"X_9\"  \"Y_10\"\n\n\nWe can count the number of characters in a string.\n\nnchar('abc')\n\n[1] 3\n\nnchar(c('abc','d',123456))\n\n[1] 3 1 6\n\n\nPulling out parts of strings is also sometimes useful.\n\nsubstr('This is a good sentence.',start=10,stop=15)\n\n[1] \" good \"\n\n\nAnother common operation is to replace something in a string with something (a find-and-replace).\n\nsub('This','That','This is a good sentence.')\n\n[1] \"That is a good sentence.\"\n\n\nWhen we want to find all strings that match some other string, we can use grep, or “grab regular expression”.\n\ngrep('bcd',c('abcdef','abcd','bcde','cdef','defg'))\n\n[1] 1 2 3\n\ngrep('bcd',c('abcdef','abcd','bcde','cdef','defg'),value=TRUE)\n\n[1] \"abcdef\" \"abcd\"   \"bcde\"  \n\n\nRead about the grepl function (?grepl). Use that function to return a logical vector (TRUE/FALSE) for each entry above with an a in it."
  },
  {
    "objectID": "vectors.html#missing-values-aka-na",
    "href": "vectors.html#missing-values-aka-na",
    "title": "\n5  Vectors\n",
    "section": "\n5.8 Missing Values, AKA “NA”",
    "text": "5.8 Missing Values, AKA “NA”\nR has a special value, “NA”, that represents a “missing” value, or Not Available, in a vector or other data structure. Here, we just create a vector to experiment.\n\nx = 1:5\nx\n\n[1] 1 2 3 4 5\n\nlength(x)\n\n[1] 5\n\n\n\nis.na(x)\n\n[1] FALSE FALSE FALSE FALSE FALSE\n\nx[2] = NA\nx\n\n[1]  1 NA  3  4  5\n\n\nThe length of x is unchanged, but there is one value that is marked as “missing” by virtue of being NA.\n\nlength(x)\n\n[1] 5\n\nis.na(x)\n\n[1] FALSE  TRUE FALSE FALSE FALSE\n\n\nWe can remove NA values by using indexing. In the following, is.na(x) returns a logical vector the length of x. The ! is the logical NOT operator and converts TRUE to FALSE and vice-versa.\n\nx[!is.na(x)]\n\n[1] 1 3 4 5"
  },
  {
    "objectID": "vectors.html#exercises",
    "href": "vectors.html#exercises",
    "title": "\n5  Vectors\n",
    "section": "\n5.9 Exercises",
    "text": "5.9 Exercises\n\n\nCreate a numeric vector called temperatures containing the following values: 72, 75, 78, 81, 76, 73.\n\nShow answertemperatures &lt;- c(72, 75, 78, 81, 76, 73, 93)\n\n\n\n\nCreate a character vector called days containing the following values: “Monday”, “Tuesday”, “Wednesday”, “Thursday”, “Friday”, “Saturday”, “Sunday”.\n\nShow answerdays &lt;- c(\"Monday\", \"Tuesday\", \"Wednesday\", \"Thursday\", \"Friday\", \"Saturday\", \"Sunday\")\n\n\n\n\nCalculate the average temperature for the week and store it in a variable called average_temperature.\n\nShow answeraverage_temperature &lt;- mean(temperatures)\n\n\n\n\nCreate a named vector called weekly_temperatures, where the names are the days of the week and the values are the temperatures from the temperatures vector.\n\nShow answerweekly_temperatures &lt;- temperatures\nnames(weekly_temperatures) &lt;- days\n\n\n\n\nCreate a numeric vector called ages containing the following values: 25, 30, 35, 40, 45, 50, 55, 60.\n\nShow answerages &lt;- c(25, 30, 35, 40, 45, 50, 55, 60)\n\n\n\n\nCreate a logical vector called is_adult by checking if the elements in the ages vector are greater than or equal to 18.\n\nShow answeris_adult &lt;- ages &gt;= 18\n\n\n\n\nCalculate the sum and product of the ages vector.\n\nShow answersum_ages &lt;- sum(ages)\nproduct_ages &lt;- prod(ages)\n\n\n\n\nExtract the ages greater than or equal to 40 from the ages vector and store them in a variable called older_ages.\n\nShow answerolder_ages &lt;- ages[ages &gt;= 40]"
  },
  {
    "objectID": "matrices.html#creating-a-matrix",
    "href": "matrices.html#creating-a-matrix",
    "title": "\n6  Matrices\n",
    "section": "\n6.1 Creating a matrix",
    "text": "6.1 Creating a matrix\nThere are many ways to create a matrix in R. One of the simplest is to use the matrix() function. In the code below, we’ll create a matrix from a vector from 1:16.\n\nmat1 &lt;- matrix(1:16,nrow=4)\nmat1\n\n     [,1] [,2] [,3] [,4]\n[1,]    1    5    9   13\n[2,]    2    6   10   14\n[3,]    3    7   11   15\n[4,]    4    8   12   16\n\n\nThe same is possible, but specifying that the matrix be “filled” by row.\n\nmat1 &lt;- matrix(1:16,nrow=4,byrow = TRUE)\nmat1\n\n     [,1] [,2] [,3] [,4]\n[1,]    1    2    3    4\n[2,]    5    6    7    8\n[3,]    9   10   11   12\n[4,]   13   14   15   16\n\n\nNotice the subtle difference in the order that the numbers go into the matrix.\nWe can also build a matrix from parts by “binding” vectors together:\n\nx &lt;- 1:10 \ny &lt;- rnorm(10)\n\nEach of the vectors above is of length 10 and both are “numeric”, so we can make them into a matrix. Using rbind binds rows (r) into a matrix.\n\nmat &lt;- rbind(x,y)\nmat\n\n       [,1]      [,2]    [,3]      [,4]      [,5]     [,6]       [,7]      [,8]\nx 1.0000000 2.0000000 3.00000  4.000000  5.000000 6.000000  7.0000000  8.000000\ny 0.4213429 0.4326436 1.28799 -2.153729 -0.955121 1.369986 -0.1474719 -1.594401\n      [,9]      [,10]\nx 9.000000 10.0000000\ny 1.166565  0.5907749\n\n\nThe alternative to rbind is cbind that binds columns (c) together.\n\nmat &lt;- cbind(x,y)\nmat\n\n       x          y\n [1,]  1  0.4213429\n [2,]  2  0.4326436\n [3,]  3  1.2879903\n [4,]  4 -2.1537293\n [5,]  5 -0.9551210\n [6,]  6  1.3699864\n [7,]  7 -0.1474719\n [8,]  8 -1.5944011\n [9,]  9  1.1665650\n[10,] 10  0.5907749\n\n\nInspecting the names associated with rows and columns is often useful, particularly if the names have human meaning.\n\nrownames(mat)\n\nNULL\n\ncolnames(mat)\n\n[1] \"x\" \"y\"\n\n\nWe can also change the names of the matrix by assigning valid names to the columns or rows.\n\ncolnames(mat) = c('apples','oranges')\ncolnames(mat)\n\n[1] \"apples\"  \"oranges\"\n\nmat\n\n      apples    oranges\n [1,]      1  0.4213429\n [2,]      2  0.4326436\n [3,]      3  1.2879903\n [4,]      4 -2.1537293\n [5,]      5 -0.9551210\n [6,]      6  1.3699864\n [7,]      7 -0.1474719\n [8,]      8 -1.5944011\n [9,]      9  1.1665650\n[10,]     10  0.5907749\n\n\nMatrices have dimensions.\n\ndim(mat)\n\n[1] 10  2\n\nnrow(mat)\n\n[1] 10\n\nncol(mat)\n\n[1] 2"
  },
  {
    "objectID": "matrices.html#accessing-elements-of-a-matrix",
    "href": "matrices.html#accessing-elements-of-a-matrix",
    "title": "\n6  Matrices\n",
    "section": "\n6.2 Accessing elements of a matrix",
    "text": "6.2 Accessing elements of a matrix\nIndexing for matrices works as for vectors except that we now need to include both the row and column (in that order). We can access elements of a matrix using the square bracket [ indexing method. Elements can be accessed as var[r, c]. Here, r and c are vectors describing the elements of the matrix to select.\n\n\n\n\n\n\nImportant\n\n\n\nThe indices in R start with one, meaning that the first element of a vector or the first row/column of a matrix is indexed as one.\nThis is different from some other programming languages, such as Python, which use zero-based indexing, meaning that the first element of a vector or the first row/column of a matrix is indexed as zero.\nIt is important to be aware of this difference when working with data in R, especially if you are coming from a programming background that uses zero-based indexing. Using the wrong index can lead to unexpected results or errors in your code.\n\n\n\n# The 2nd element of the 1st row of mat\nmat[1,2]\n\n  oranges \n0.4213429 \n\n# The first ROW of mat\nmat[1,]\n\n   apples   oranges \n1.0000000 0.4213429 \n\n# The first COLUMN of mat\nmat[,1]\n\n [1]  1  2  3  4  5  6  7  8  9 10\n\n# and all elements of mat that are &gt; 4; note no comma\nmat[mat&gt;4]\n\n[1]  5  6  7  8  9 10\n\n## [1]  5  6  7  8  9 10\n\n\n\n\n\n\n\nCaution\n\n\n\nNote that in the last case, there is no “,”, so R treats the matrix as a long vector (length=20). This is convenient, sometimes, but it can also be a source of error, as some code may “work” but be doing something unexpected.\n\n\nWe can also use indexing to exclude a row or column by prefixing the selection with a - sign.\n\nmat[,-1]       # remove first column\n\n [1]  0.4213429  0.4326436  1.2879903 -2.1537293 -0.9551210  1.3699864\n [7] -0.1474719 -1.5944011  1.1665650  0.5907749\n\nmat[-c(1:5),]  # remove first five rows\n\n     apples    oranges\n[1,]      6  1.3699864\n[2,]      7 -0.1474719\n[3,]      8 -1.5944011\n[4,]      9  1.1665650\n[5,]     10  0.5907749"
  },
  {
    "objectID": "matrices.html#changing-values-in-a-matrix",
    "href": "matrices.html#changing-values-in-a-matrix",
    "title": "\n6  Matrices\n",
    "section": "\n6.3 Changing values in a matrix",
    "text": "6.3 Changing values in a matrix\nWe can create a matrix filled with random values drawn from a normal distribution for our work below.\n\nm = matrix(rnorm(20),nrow=10)\nsummary(m)\n\n       V1                V2         \n Min.   :-1.6420   Min.   :-0.7456  \n 1st Qu.:-0.8393   1st Qu.:-0.5711  \n Median : 0.2474   Median :-0.2029  \n Mean   : 0.2034   Mean   : 0.1043  \n 3rd Qu.: 0.5573   3rd Qu.: 0.2300  \n Max.   : 2.9066   Max.   : 2.1126  \n\n\nMultiplication and division works similarly to vectors. When multiplying by a vector, for example, the values of the vector are reused. In the simplest case, let’s multiply the matrix by a constant (vector of length 1).\n\n# multiply all values in the matrix by 20\nm2 = m*20\nsummary(m2)\n\n       V1                V2         \n Min.   :-32.840   Min.   :-14.912  \n 1st Qu.:-16.786   1st Qu.:-11.422  \n Median :  4.947   Median : -4.059  \n Mean   :  4.068   Mean   :  2.086  \n 3rd Qu.: 11.147   3rd Qu.:  4.599  \n Max.   : 58.131   Max.   : 42.251  \n\n\nBy combining subsetting with assignment, we can make changes to just part of a matrix.\n\n# and add 100 to the first column of m\nm2[,1] = m2[,1] + 100\n# summarize m\nsummary(m2)\n\n       V1               V2         \n Min.   : 67.16   Min.   :-14.912  \n 1st Qu.: 83.21   1st Qu.:-11.422  \n Median :104.95   Median : -4.059  \n Mean   :104.07   Mean   :  2.086  \n 3rd Qu.:111.15   3rd Qu.:  4.599  \n Max.   :158.13   Max.   : 42.251  \n\n\nA somewhat common transformation for a matrix is to transpose which changes rows to columns. One might need to do this if an assay output from a lab machine puts samples in rows and genes in columns, for example, while in Bioconductor/R, we often want the samples in columns and the genes in rows.\n\nt(m2)\n\n          [,1]        [,2]      [,3]       [,4]     [,5]       [,6]      [,7]\n[1,] 107.96863 104.0960114 101.49678 158.131297 105.7981 133.993020 72.711332\n[2,] -14.33319  -0.4561012  29.42504  -7.661384 -12.3741  -8.565006  5.457562\n          [,8]     [,9]     [,10]\n[1,] 77.119850 67.15979 112.20585\n[2,]  2.023542 42.25148 -14.91248"
  },
  {
    "objectID": "matrices.html#calculations-on-matrix-rows-and-columns",
    "href": "matrices.html#calculations-on-matrix-rows-and-columns",
    "title": "\n6  Matrices\n",
    "section": "\n6.4 Calculations on matrix rows and columns",
    "text": "6.4 Calculations on matrix rows and columns\nAgain, we just need a matrix to play with. We’ll use rnorm again, but with a slight twist.\n\nm3 = matrix(rnorm(100,5,2),ncol=10) # what does the 5 mean here? And the 2?\n\nSince these data are from a normal distribution, we can look at a row (or column) to see what the mean and standard deviation are.\n\nmean(m3[,1])\n\n[1] 4.720436\n\nsd(m3[,1])\n\n[1] 2.204187\n\n# or a row\nmean(m3[1,])\n\n[1] 4.330586\n\nsd(m3[1,])\n\n[1] 1.328035\n\n\nThere are some useful convenience functions for computing means and sums of data in all of the columns and rows of matrices.\n\ncolMeans(m3)\n\n [1] 4.720436 4.901195 4.912108 5.326346 5.300867 4.462975 3.948073 4.793714\n [9] 5.383735 4.405058\n\nrowMeans(m3)\n\n [1] 4.330586 4.098086 5.515398 4.497942 5.170548 5.157355 4.960425 5.033872\n [9] 4.735140 4.655157\n\nrowSums(m3)\n\n [1] 43.30586 40.98086 55.15398 44.97942 51.70548 51.57355 49.60425 50.33872\n [9] 47.35140 46.55157\n\ncolSums(m3)\n\n [1] 47.20436 49.01195 49.12108 53.26346 53.00867 44.62975 39.48073 47.93714\n [9] 53.83735 44.05058\n\n\nWe can look at the distribution of column means:\n\n# save as a variable\ncmeans = colMeans(m3)\nsummary(cmeans)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  3.948   4.527   4.847   4.815   5.204   5.384 \n\n\nNote that this is centered pretty closely around the selected mean of 5 above.\nHow about the standard deviation? There is not a colSd function, but it turns out that we can easily apply functions that take vectors as input, like sd and “apply” them across either the rows (the first dimension) or columns (the second) dimension.\n\ncsds = apply(m3, 2, sd)\nsummary(csds)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  1.106   1.710   1.995   1.869   2.193   2.286 \n\n\nAgain, take a look at the distribution which is centered quite close to the selected standard deviation when we created our matrix."
  },
  {
    "objectID": "matrices.html#exercises",
    "href": "matrices.html#exercises",
    "title": "\n6  Matrices\n",
    "section": "\n6.5 Exercises",
    "text": "6.5 Exercises\n\n6.5.1 Data preparation\nFor this set of exercises, we are going to rely on a dataset that comes with R. It gives the number of sunspots per month from 1749-1983. The dataset comes as a ts or time series data type which I convert to a matrix using the following code.\nJust run the code as is and focus on the rest of the exercises.\n\ndata(sunspots)\nsunspot_mat &lt;- matrix(as.vector(sunspots),ncol=12,byrow = TRUE)\ncolnames(sunspot_mat) &lt;- as.character(1:12)\nrownames(sunspot_mat) &lt;- as.character(1749:1983)\n\n\n6.5.2 Questions\n\n\nAfter the conversion above, what does sunspot_mat look like? Use functions to find the number of rows, the number of columns, the class, and some basic summary statistics.\n\nShow answerncol(sunspot_mat)\nnrow(sunspot_mat)\ndim(sunspot_mat)\nsummary(sunspot_mat)\nhead(sunspot_mat)\ntail(sunspot_mat)\n\n\n\n\nPractice subsetting the matrix a bit by selecting:\n\nThe first 10 years (rows)\nThe month of July (7th column)\nThe value for July, 1979 using the rowname to do the selection.\n\n\nShow answersunspot_mat[1:10,]\nsunspot_mat[,7]\nsunspot_mat['1979',7]\n\n\n\n\n\n\nThese next few exercises take advantage of the fact that calling a univariate statistical function (one that expects a vector) works for matrices by just making a vector of all the values in the matrix. What is the highest (max) number of sunspots recorded in these data?\n\nShow answermax(sunspot_mat)\n\n\n\n\nAnd the minimum?\n\nShow answermin(sunspot_mat)\n\n\n\n\nAnd the overall mean and median?\n\nShow answermean(sunspot_mat)\nmedian(sunspot_mat)\n\n\n\n\nUse the hist() function to look at the distribution of all the monthly sunspot data.\n\nShow answerhist(sunspot_mat)\n\n\n\n\nRead about the breaks argument to hist() to try to increase the number of breaks in the histogram to increase the resolution slightly. Adjust your hist() and breaks to your liking.\n\nShow answerhist(sunspot_mat, breaks=40)\n\n\n\n\nNow, let’s move on to summarizing the data a bit to learn about the pattern of sunspots varies by month or by year. Examine the dataset again. What do the columns represent? And the rows?\n\nShow answer# just a quick glimpse of the data will give us a sense\nhead(sunspot_mat)\n\n\n\n\nWe’d like to look at the distribution of sunspots by month. How can we do that?\n\nShow answer# the mean of the columns is the mean number of sunspots per month.\ncolMeans(sunspot_mat)\n\n# Another way to write the same thing:\napply(sunspot_mat, 2, mean)\n\n\n\n\nAssign the month summary above to a variable and summarize it to get a sense of the spread over months.\n\nShow answermonthmeans = colMeans(sunspot_mat)\nsummary(monthmeans)\n\n\n\n\nPlay the same game for years to get the per-year mean?\n\nShow answerymeans = rowMeans(sunspot_mat)\nsummary(ymeans)\n\n\n\n\nMake a plot of the yearly means. Do you see a pattern?\n\nShow answerplot(ymeans)\n# or make it clearer\nplot(ymeans, type='l')"
  },
  {
    "objectID": "dataframes_intro.html#learning-goals",
    "href": "dataframes_intro.html#learning-goals",
    "title": "\n7  Data Frames\n",
    "section": "\n7.1 Learning goals",
    "text": "7.1 Learning goals\n\nUnderstand how data.frames are different from matrices.\nKnow a few functions for examing the contents of a data.frame.\nList approaches for subsetting data.frames.\nBe able to load and save tabular data from and to disk.\nShow how to create a data.frames from scratch."
  },
  {
    "objectID": "dataframes_intro.html#learning-objectives",
    "href": "dataframes_intro.html#learning-objectives",
    "title": "\n7  Data Frames\n",
    "section": "\n7.2 Learning objectives",
    "text": "7.2 Learning objectives\n\nLoad the yeast growth dataset into R using read.csv.\nExamine the contents of the dataset.\nUse subsetting to find genes that may be involved with nutrient metabolism and transport.\nSummarize data measurements by categories."
  },
  {
    "objectID": "dataframes_intro.html#dataset",
    "href": "dataframes_intro.html#dataset",
    "title": "\n7  Data Frames\n",
    "section": "\n7.3 Dataset",
    "text": "7.3 Dataset\nThe data used here are borrowed directly from the fantastic Bioconnector tutorials and are a cleaned up version of the data from Brauer et al. Coordination of Growth Rate, Cell Cycle, Stress Response, and Metabolic Activity in Yeast (2008) Mol Biol Cell 19:352-367. These data are from a gene expression microarray, and in this paper the authors examine the relationship between growth rate and gene expression in yeast cultures limited by one of six different nutrients (glucose, leucine, ammonium, sulfate, phosphate, uracil). If you give yeast a rich media loaded with nutrients except restrict the supply of a single nutrient, you can control the growth rate to any rate you choose. By starving yeast of specific nutrients you can find genes that:\n\nRaise or lower their expression in response to growth rate. Growth-rate dependent expression patterns can tell us a lot about cell cycle control, and how the cell responds to stress. The authors found that expression of &gt;25% of all yeast genes is linearly correlated with growth rate, independent of the limiting nutrient. They also found that the subset of negatively growth-correlated genes is enriched for peroxisomal functions, and positively correlated genes mainly encode ribosomal functions.\nRespond differently when different nutrients are being limited. If you see particular genes that respond very differently when a nutrient is sharply restricted, these genes might be involved in the transport or metabolism of that specific nutrient.\n\nThe dataset can be downloaded directly from:\n\nbrauer2007_tidy.csv\n\nWe are going to read this dataset into R and then use it as a playground for learning about data.frames."
  },
  {
    "objectID": "dataframes_intro.html#reading-in-data",
    "href": "dataframes_intro.html#reading-in-data",
    "title": "\n7  Data Frames\n",
    "section": "\n7.4 Reading in data",
    "text": "7.4 Reading in data\nR has many capabilities for reading in data. Many of the functions have names that help us to understand what data format is to be expected. In this case, the filename that we want to read ends in .csv, meaning comma-separated-values. The read.csv() function reads in .csv files. As usual, it is worth reading help('read.csv') to get a better sense of the possible bells-and-whistles.\nThe read.csv() function can read directly from a URL, so we do not need to download the file directly. This dataset is relatively large (about 16MB), so this may take a bit depending on your network connection speed.\n\noptions(width=60)\n\n\nurl = paste0(\n    'https://raw.githubusercontent.com',\n    '/bioconnector/workshops/master/data/brauer2007_tidy.csv'\n)\nydat &lt;- read.csv(url)\n\nOur variable, ydat, now “contains” the downloaded and read data. We can check to see what data type read.csv gave us:\n\nclass(ydat)\n\n[1] \"data.frame\""
  },
  {
    "objectID": "dataframes_intro.html#inspecting-data.frames",
    "href": "dataframes_intro.html#inspecting-data.frames",
    "title": "\n7  Data Frames\n",
    "section": "\n7.5 Inspecting data.frames",
    "text": "7.5 Inspecting data.frames\nOur ydat variable is a data.frame. As I mentioned, the dataset is fairly large, so we will not be able to look at it all at once on the screen. However, R gives us many tools to inspect a data.frame.\n\nOverviews of content\n\n\nhead() to show first few rows\n\ntail() to show last few rows\n\n\nSize\n\n\ndim() for dimensions (rows, columns)\nnrow()\nncol()\n\nobject.size() for power users interested in the memory used to store an object\n\n\nData and attribute summaries\n\n\ncolnames() to get the names of the columns\n\nrownames() to get the “names” of the rows–may not be present\n\nsummary() to get per-column summaries of the data in the data.frame.\n\n\n\n\nhead(ydat)\n\n  symbol systematic_name nutrient rate expression\n1   SFB2         YNL049C  Glucose 0.05      -0.24\n2   &lt;NA&gt;         YNL095C  Glucose 0.05       0.28\n3   QRI7         YDL104C  Glucose 0.05      -0.02\n4   CFT2         YLR115W  Glucose 0.05      -0.33\n5   SSO2         YMR183C  Glucose 0.05       0.05\n6   PSP2         YML017W  Glucose 0.05      -0.69\n                            bp\n1        ER to Golgi transport\n2   biological process unknown\n3 proteolysis and peptidolysis\n4      mRNA polyadenylylation*\n5              vesicle fusion*\n6   biological process unknown\n                             mf\n1    molecular function unknown\n2    molecular function unknown\n3 metalloendopeptidase activity\n4                   RNA binding\n5              t-SNARE activity\n6    molecular function unknown\n\ntail(ydat)\n\n       symbol systematic_name nutrient rate expression\n198425   DOA1         YKL213C   Uracil  0.3       0.14\n198426   KRE1         YNL322C   Uracil  0.3       0.28\n198427   MTL1         YGR023W   Uracil  0.3       0.27\n198428   KRE9         YJL174W   Uracil  0.3       0.43\n198429   UTH1         YKR042W   Uracil  0.3       0.19\n198430   &lt;NA&gt;         YOL111C   Uracil  0.3       0.04\n                                               bp\n198425    ubiquitin-dependent protein catabolism*\n198426      cell wall organization and biogenesis\n198427      cell wall organization and biogenesis\n198428     cell wall organization and biogenesis*\n198429 mitochondrion organization and biogenesis*\n198430                 biological process unknown\n                                        mf\n198425          molecular function unknown\n198426 structural constituent of cell wall\n198427          molecular function unknown\n198428          molecular function unknown\n198429          molecular function unknown\n198430          molecular function unknown\n\ndim(ydat)\n\n[1] 198430      7\n\nnrow(ydat)\n\n[1] 198430\n\nncol(ydat)\n\n[1] 7\n\ncolnames(ydat)\n\n[1] \"symbol\"          \"systematic_name\" \"nutrient\"       \n[4] \"rate\"            \"expression\"      \"bp\"             \n[7] \"mf\"             \n\nsummary(ydat)\n\n    symbol          systematic_name      nutrient        \n Length:198430      Length:198430      Length:198430     \n Class :character   Class :character   Class :character  \n Mode  :character   Mode  :character   Mode  :character  \n                                                         \n                                                         \n                                                         \n      rate          expression             bp           \n Min.   :0.0500   Min.   :-6.500000   Length:198430     \n 1st Qu.:0.1000   1st Qu.:-0.290000   Class :character  \n Median :0.2000   Median : 0.000000   Mode  :character  \n Mean   :0.1752   Mean   : 0.003367                     \n 3rd Qu.:0.2500   3rd Qu.: 0.290000                     \n Max.   :0.3000   Max.   : 6.640000                     \n      mf           \n Length:198430     \n Class :character  \n Mode  :character  \n                   \n                   \n                   \n\n\nIn RStudio, there is an additional function, View() (note the capital “V”) that opens the first 1000 rows (default) in the RStudio window, akin to a spreadsheet view.\n\nView(ydat)"
  },
  {
    "objectID": "dataframes_intro.html#accessing-variables-columns-and-subsetting",
    "href": "dataframes_intro.html#accessing-variables-columns-and-subsetting",
    "title": "\n7  Data Frames\n",
    "section": "\n7.6 Accessing variables (columns) and subsetting",
    "text": "7.6 Accessing variables (columns) and subsetting\nIn R, data.frames can be subset similarly to other two-dimensional data structures. The [ in R is used to denote subsetting of any kind. When working with two-dimensional data, we need two values inside the [ ] to specify the details. The specification is [rows, columns]. For example, to get the first three rows of ydat, use:\n\nydat[1:3, ]\n\n  symbol systematic_name nutrient rate expression\n1   SFB2         YNL049C  Glucose 0.05      -0.24\n2   &lt;NA&gt;         YNL095C  Glucose 0.05       0.28\n3   QRI7         YDL104C  Glucose 0.05      -0.02\n                            bp\n1        ER to Golgi transport\n2   biological process unknown\n3 proteolysis and peptidolysis\n                             mf\n1    molecular function unknown\n2    molecular function unknown\n3 metalloendopeptidase activity\n\n\nNote how the second number, the columns, is blank. R takes that to mean “all the columns”. Similarly, we can combine rows and columns specification arbitrarily.\n\nydat[1:3, 1:3]\n\n  symbol systematic_name nutrient\n1   SFB2         YNL049C  Glucose\n2   &lt;NA&gt;         YNL095C  Glucose\n3   QRI7         YDL104C  Glucose\n\n\nBecause selecting a single variable, or column, is such a common operation, there are two shortcuts for doing so with data.frames. The first, the $ operator works like so:\n\n# Look at the column names, just to refresh memory\ncolnames(ydat)\n\n[1] \"symbol\"          \"systematic_name\" \"nutrient\"       \n[4] \"rate\"            \"expression\"      \"bp\"             \n[7] \"mf\"             \n\n# Note that I am using \"head\" here to limit the output\nhead(ydat$symbol)\n\n[1] \"SFB2\" NA     \"QRI7\" \"CFT2\" \"SSO2\" \"PSP2\"\n\n# What is the actual length of \"symbol\"?\nlength(ydat$symbol)\n\n[1] 198430\n\n\nThe second is related to the fact that, in R, data.frames are also lists. We subset a list by using [[]] notation. To get the second column of ydat, we can use:\n\nhead(ydat[[2]])\n\n[1] \"YNL049C\" \"YNL095C\" \"YDL104C\" \"YLR115W\" \"YMR183C\"\n[6] \"YML017W\"\n\n\nAlternatively, we can use the column name:\n\nhead(ydat[[\"systematic_name\"]])\n\n[1] \"YNL049C\" \"YNL095C\" \"YDL104C\" \"YLR115W\" \"YMR183C\"\n[6] \"YML017W\"\n\n\n\n7.6.1 Some data exploration\nThere are a couple of columns that include numeric values. Which columns are numeric?\n\nclass(ydat$symbol)\n\n[1] \"character\"\n\nclass(ydat$rate)\n\n[1] \"numeric\"\n\nclass(ydat$expression)\n\n[1] \"numeric\"\n\n\nMake histograms of: - the expression values - the rate values\nWhat does the table() function do? Could you use that to look a the rate column given that that column appears to have repeated values?\nWhat rate corresponds to the most nutrient-starved condition?\n\n7.6.2 More advanced indexing and subsetting\nWe can use, for example, logical values (TRUE/FALSE) to subset data.frames.\n\nhead(ydat[ydat$symbol == 'LEU1', ])\n\n     symbol systematic_name nutrient rate expression   bp\nNA     &lt;NA&gt;            &lt;NA&gt;     &lt;NA&gt;   NA         NA &lt;NA&gt;\nNA.1   &lt;NA&gt;            &lt;NA&gt;     &lt;NA&gt;   NA         NA &lt;NA&gt;\nNA.2   &lt;NA&gt;            &lt;NA&gt;     &lt;NA&gt;   NA         NA &lt;NA&gt;\nNA.3   &lt;NA&gt;            &lt;NA&gt;     &lt;NA&gt;   NA         NA &lt;NA&gt;\nNA.4   &lt;NA&gt;            &lt;NA&gt;     &lt;NA&gt;   NA         NA &lt;NA&gt;\nNA.5   &lt;NA&gt;            &lt;NA&gt;     &lt;NA&gt;   NA         NA &lt;NA&gt;\n       mf\nNA   &lt;NA&gt;\nNA.1 &lt;NA&gt;\nNA.2 &lt;NA&gt;\nNA.3 &lt;NA&gt;\nNA.4 &lt;NA&gt;\nNA.5 &lt;NA&gt;\n\ntail(ydat[ydat$symbol == 'LEU1', ])\n\n         symbol systematic_name nutrient rate expression\nNA.47244   &lt;NA&gt;            &lt;NA&gt;     &lt;NA&gt;   NA         NA\nNA.47245   &lt;NA&gt;            &lt;NA&gt;     &lt;NA&gt;   NA         NA\nNA.47246   &lt;NA&gt;            &lt;NA&gt;     &lt;NA&gt;   NA         NA\nNA.47247   &lt;NA&gt;            &lt;NA&gt;     &lt;NA&gt;   NA         NA\nNA.47248   &lt;NA&gt;            &lt;NA&gt;     &lt;NA&gt;   NA         NA\nNA.47249   &lt;NA&gt;            &lt;NA&gt;     &lt;NA&gt;   NA         NA\n           bp   mf\nNA.47244 &lt;NA&gt; &lt;NA&gt;\nNA.47245 &lt;NA&gt; &lt;NA&gt;\nNA.47246 &lt;NA&gt; &lt;NA&gt;\nNA.47247 &lt;NA&gt; &lt;NA&gt;\nNA.47248 &lt;NA&gt; &lt;NA&gt;\nNA.47249 &lt;NA&gt; &lt;NA&gt;\n\n\nWhat is the problem with this approach? It appears that there are a bunch of NA values. Taking a quick look at the symbol column, we see what the problem.\n\nsummary(ydat$symbol)\n\n   Length     Class      Mode \n   198430 character character \n\n\nUsing the is.na() function, we can make filter further to get down to values of interest.\n\nhead(ydat[ydat$symbol == 'LEU1' & !is.na(ydat$symbol), ])\n\n      symbol systematic_name nutrient rate expression\n1526    LEU1         YGL009C  Glucose 0.05      -1.12\n7043    LEU1         YGL009C  Glucose 0.10      -0.77\n12555   LEU1         YGL009C  Glucose 0.15      -0.67\n18071   LEU1         YGL009C  Glucose 0.20      -0.59\n23603   LEU1         YGL009C  Glucose 0.25      -0.20\n29136   LEU1         YGL009C  Glucose 0.30       0.03\n                        bp\n1526  leucine biosynthesis\n7043  leucine biosynthesis\n12555 leucine biosynthesis\n18071 leucine biosynthesis\n23603 leucine biosynthesis\n29136 leucine biosynthesis\n                                          mf\n1526  3-isopropylmalate dehydratase activity\n7043  3-isopropylmalate dehydratase activity\n12555 3-isopropylmalate dehydratase activity\n18071 3-isopropylmalate dehydratase activity\n23603 3-isopropylmalate dehydratase activity\n29136 3-isopropylmalate dehydratase activity\n\n\nSometimes, looking at the data themselves is not that important. Using dim() is one possibility to look at the number of rows and columns after subsetting.\n\ndim(ydat[ydat$expression &gt; 3, ])\n\n[1] 714   7\n\n\nFind the high expressed genes when leucine-starved. For this task we can also use subset which allows us to treat column names as R variables (no $ needed).\n\nsubset(ydat, nutrient == 'Leucine' & rate == 0.05 & expression &gt; 3)\n\n       symbol systematic_name nutrient rate expression\n133768   QDR2         YIL121W  Leucine 0.05       4.61\n133772   LEU1         YGL009C  Leucine 0.05       3.84\n133858   BAP3         YDR046C  Leucine 0.05       4.29\n135186   &lt;NA&gt;         YPL033C  Leucine 0.05       3.43\n135187   &lt;NA&gt;         YLR267W  Leucine 0.05       3.23\n135288   HXT3         YDR345C  Leucine 0.05       5.16\n135963   TPO2         YGR138C  Leucine 0.05       3.75\n135965   YRO2         YBR054W  Leucine 0.05       4.40\n136102   GPG1         YGL121C  Leucine 0.05       3.08\n136109  HSP42         YDR171W  Leucine 0.05       3.07\n136119   HXT5         YHR096C  Leucine 0.05       4.90\n136151   &lt;NA&gt;         YJL144W  Leucine 0.05       3.06\n136152   MOH1         YBL049W  Leucine 0.05       3.43\n136153   &lt;NA&gt;         YBL048W  Leucine 0.05       3.95\n136189  HSP26         YBR072W  Leucine 0.05       4.86\n136231   NCA3         YJL116C  Leucine 0.05       4.03\n136233   &lt;NA&gt;         YBR116C  Leucine 0.05       3.28\n136486   &lt;NA&gt;         YGR043C  Leucine 0.05       3.07\n137443   ADH2         YMR303C  Leucine 0.05       4.15\n137448   ICL1         YER065C  Leucine 0.05       3.54\n137451   SFC1         YJR095W  Leucine 0.05       3.72\n137569   MLS1         YNL117W  Leucine 0.05       3.76\n                                              bp\n133768                       multidrug transport\n133772                      leucine biosynthesis\n133858                      amino acid transport\n135186                                  meiosis*\n135187                biological process unknown\n135288                          hexose transport\n135963                       polyamine transport\n135965                biological process unknown\n136102                       signal transduction\n136109                       response to stress*\n136119                          hexose transport\n136151                   response to dessication\n136152                biological process unknown\n136153                                      &lt;NA&gt;\n136189                       response to stress*\n136231 mitochondrion organization and biogenesis\n136233                                      &lt;NA&gt;\n136486                biological process unknown\n137443                             fermentation*\n137448                          glyoxylate cycle\n137451                       fumarate transport*\n137569                          glyoxylate cycle\n                                           mf\n133768         multidrug efflux pump activity\n133772 3-isopropylmalate dehydratase activity\n133858        amino acid transporter activity\n135186             molecular function unknown\n135187             molecular function unknown\n135288          glucose transporter activity*\n135963          spermine transporter activity\n135965             molecular function unknown\n136102             signal transducer activity\n136109               unfolded protein binding\n136119          glucose transporter activity*\n136151             molecular function unknown\n136152             molecular function unknown\n136153                                   &lt;NA&gt;\n136189               unfolded protein binding\n136231             molecular function unknown\n136233                                   &lt;NA&gt;\n136486                 transaldolase activity\n137443         alcohol dehydrogenase activity\n137448              isocitrate lyase activity\n137451 succinate:fumarate antiporter activity\n137569               malate synthase activity"
  },
  {
    "objectID": "dataframes_intro.html#aggregating-data",
    "href": "dataframes_intro.html#aggregating-data",
    "title": "\n7  Data Frames\n",
    "section": "\n7.7 Aggregating data",
    "text": "7.7 Aggregating data\nAggregating data, or summarizing by category, is a common way to look for trends or differences in measurements between categories. Use aggregate to find the mean expression by gene symbol.\n\nhead(aggregate(ydat$expression, by=list( ydat$symbol), mean))\n\n  Group.1           x\n1    AAC1  0.52888889\n2    AAC3 -0.21628571\n3   AAD10  0.43833333\n4   AAD14 -0.07166667\n5   AAD16  0.24194444\n6    AAD4 -0.79166667\n\n# or \nhead(aggregate(expression ~ symbol, mean, data=ydat))\n\n  symbol  expression\n1   AAC1  0.52888889\n2   AAC3 -0.21628571\n3  AAD10  0.43833333\n4  AAD14 -0.07166667\n5  AAD16  0.24194444\n6   AAD4 -0.79166667"
  },
  {
    "objectID": "dataframes_intro.html#creating-a-data.frame-from-scratch",
    "href": "dataframes_intro.html#creating-a-data.frame-from-scratch",
    "title": "\n7  Data Frames\n",
    "section": "\n7.8 Creating a data.frame from scratch",
    "text": "7.8 Creating a data.frame from scratch\nSometimes it is useful to combine related data into one object. For example, let’s simulate some data.\n\nsmoker = factor(rep(c(\"smoker\", \"non-smoker\"), each=50))\nsmoker_numeric = as.numeric(smoker)\nx = rnorm(100)\nrisk = x + 2*smoker_numeric\n\nWe have two varibles, risk and smoker that are related. We can make a data.frame out of them:\n\nsmoker_risk = data.frame(smoker = smoker, risk = risk)\nhead(smoker_risk)\n\n  smoker     risk\n1 smoker 2.676388\n2 smoker 4.354609\n3 smoker 4.755054\n4 smoker 4.670494\n5 smoker 3.945609\n6 smoker 3.516762\n\n\nR also has plotting shortcuts that work with data.frames to simplify plotting\n\nplot( risk ~ smoker, data=smoker_risk)"
  },
  {
    "objectID": "dataframes_intro.html#saving-a-data.frame",
    "href": "dataframes_intro.html#saving-a-data.frame",
    "title": "\n7  Data Frames\n",
    "section": "\n7.9 Saving a data.frame",
    "text": "7.9 Saving a data.frame\nOnce we have a data.frame of interest, we may want to save it. The most portable way to save a data.frame is to use one of the write functions. In this case, let’s save the data as a .csv file.\n\nwrite.csv(smoker_risk, \"smoker_risk.csv\")"
  },
  {
    "objectID": "factors.html#factors",
    "href": "factors.html#factors",
    "title": "\n8  Factors\n",
    "section": "\n8.1 Factors",
    "text": "8.1 Factors\nA factor is a special type of vector, normally used to hold a categorical variable–such as smoker/nonsmoker, state of residency, zipcode–in many statistical functions. Such vectors have class “factor”. Factors are primarily used in Analysis of Variance (ANOVA) or other situations when “categories” are needed. When a factor is used as a predictor variable, the corresponding indicator variables are created (more later).\nNote of caution that factors in R often appear to be character vectors when printed, but you will notice that they do not have double quotes around them. They are stored in R as numbers with a key name, so sometimes you will note that the factor behaves like a numeric vector.\n\n# create the character vector\ncitizen&lt;-c(\"uk\",\"us\",\"no\",\"au\",\"uk\",\"us\",\"us\",\"no\",\"au\") \n\n# convert to factor\ncitizenf&lt;-factor(citizen)                                \ncitizen             \n\n[1] \"uk\" \"us\" \"no\" \"au\" \"uk\" \"us\" \"us\" \"no\" \"au\"\n\ncitizenf\n\n[1] uk us no au uk us us no au\nLevels: au no uk us\n\n# convert factor back to character vector\nas.character(citizenf)\n\n[1] \"uk\" \"us\" \"no\" \"au\" \"uk\" \"us\" \"us\" \"no\" \"au\"\n\n# convert to numeric vector\nas.numeric(citizenf)\n\n[1] 3 4 2 1 3 4 4 2 1\n\n\nR stores many data structures as vectors with “attributes” and “class” (just so you have seen this).\n\nattributes(citizenf)\n\n$levels\n[1] \"au\" \"no\" \"uk\" \"us\"\n\n$class\n[1] \"factor\"\n\nclass(citizenf)\n\n[1] \"factor\"\n\n# note that after unclassing, we can see the \n# underlying numeric structure again\nunclass(citizenf)\n\n[1] 3 4 2 1 3 4 4 2 1\nattr(,\"levels\")\n[1] \"au\" \"no\" \"uk\" \"us\"\n\n\nTabulating factors is a useful way to get a sense of the “sample” set available.\n\ntable(citizenf)\n\ncitizenf\nau no uk us \n 2  2  2  3"
  },
  {
    "objectID": "eda_overview.html",
    "href": "eda_overview.html",
    "title": "Exploratory data analysis",
    "section": "",
    "text": "Imagine you’re on an adventure, about to embark on a journey into the unknown. You’ve just been handed a treasure map, with the promise of valuable insights waiting to be discovered. This map is your data set, and the journey is exploratory data analysis (EDA).\nAs you begin your exploration, you start by getting a feel for the terrain. You take a broad, bird’s-eye view of the data, examining its structure and dimensions. Are you dealing with a vast landscape or a small, confined area? Are there any missing pieces in the map that you’ll need to account for? Understanding the overall context of your data set is crucial before venturing further.\nWith a sense of the landscape, you now zoom in to identify key landmarks in the data. You might look for unusual patterns, trends, or relationships between variables. As you spot these landmarks, you start asking questions: What’s causing that spike in values? Are these two factors related, or is it just a coincidence? By asking these questions, you’re actively engaging with the data and forming hypotheses that could guide future analysis or experiments.\nAs you continue your journey, you realize that the map alone isn’t enough to fully understand the terrain. You need more tools to bring the data to life. You start visualizing the data using charts, plots, and graphs. These visualizations act as your binoculars, allowing you to see patterns and relationships more clearly. Through them, you can uncover the hidden treasures buried within the data.\nEDA isn’t a linear path from start to finish. As you explore, you’ll find yourself circling back to previous points, refining your questions, and digging deeper. The process is iterative, with each new discovery informing the next. And as you go, you’ll gain a deeper understanding of the data’s underlying structure and potential.\nFinally, after your thorough exploration, you’ll have a solid foundation to build upon. You’ll be better equipped to make informed decisions, test hypotheses, and draw meaningful conclusions. The insights you’ve gained through EDA will serve as a compass, guiding you towards the true value hidden within your data. And with that, you’ve successfully completed your journey through exploratory data analysis."
  },
  {
    "objectID": "dplyr_intro_msleep.html#learning-goals",
    "href": "dplyr_intro_msleep.html#learning-goals",
    "title": "\n9  Introduction to dplyr: mammal sleep dataset\n",
    "section": "\n9.1 Learning goals",
    "text": "9.1 Learning goals\n\nKnow that dplyr is just a different approach to manipulating data in data.frames.\nList the commonly used dplyr verbs and how they can be used to manipulate data.frames.\nShow how to aggregate and summarized data using dplyr\n\nKnow what the piping operator, |&gt;, is and how it can be used."
  },
  {
    "objectID": "dplyr_intro_msleep.html#learning-objectives",
    "href": "dplyr_intro_msleep.html#learning-objectives",
    "title": "\n9  Introduction to dplyr: mammal sleep dataset\n",
    "section": "\n9.2 Learning objectives",
    "text": "9.2 Learning objectives\n\nSelect subsets of the mammal sleep dataset.\nReorder the dataset.\nAdd columns to the dataset based on existing columns.\nSummarize the amount of sleep by categorical variables using group_by and summarize."
  },
  {
    "objectID": "dplyr_intro_msleep.html#what-is-dplyr",
    "href": "dplyr_intro_msleep.html#what-is-dplyr",
    "title": "\n9  Introduction to dplyr: mammal sleep dataset\n",
    "section": "\n9.3 What is dplyr?",
    "text": "9.3 What is dplyr?\nThe dplyr package is a specialized package for working with data.frames (and the related tibble) to transform and summarize tabular data with rows and columns. For another explanation of dplyr see the dplyr package vignette: Introduction to dplyr"
  },
  {
    "objectID": "dplyr_intro_msleep.html#why-is-dplyr-userful",
    "href": "dplyr_intro_msleep.html#why-is-dplyr-userful",
    "title": "\n9  Introduction to dplyr: mammal sleep dataset\n",
    "section": "\n9.4 Why Is dplyr userful?",
    "text": "9.4 Why Is dplyr userful?\ndplyr contains a set of functions–commonly called the dplyr “verbs”–that perform common data manipulations such as filtering for rows, selecting specific columns, re-ordering rows, adding new columns and summarizing data. In addition, dplyr contains a useful function to perform another common task which is the “split-apply-combine” concept.\nCompared to base functions in R, the functions in dplyr are often easier to work with, are more consistent in the syntax and are targeted for data analysis around data frames, instead of just vectors."
  },
  {
    "objectID": "dplyr_intro_msleep.html#data-mammals-sleep",
    "href": "dplyr_intro_msleep.html#data-mammals-sleep",
    "title": "\n9  Introduction to dplyr: mammal sleep dataset\n",
    "section": "\n9.5 Data: Mammals Sleep",
    "text": "9.5 Data: Mammals Sleep\nThe msleep (mammals sleep) data set contains the sleep times and weights for a set of mammals and is available in the dagdata repository on github. This data set contains 83 rows and 11 variables. The data happen to be available as a dataset in the ggplot2 package. To get access to the msleep dataset, we need to first install the ggplot2 package.\n\ninstall.packages('ggplot2')\n\nThen, we can load the library.\n\nlibrary(ggplot2)\ndata(msleep)\n\nAs with many datasets in R, “help” is available to describe the dataset itself.\n\n?msleep\n\nThe columns are described in the help page, but are included here, also.\n\n\ncolumn name\nDescription\n\n\n\nname\ncommon name\n\n\ngenus\ntaxonomic rank\n\n\nvore\ncarnivore, omnivore or herbivore?\n\n\norder\ntaxonomic rank\n\n\nconservation\nthe conservation status of the mammal\n\n\nsleep_total\ntotal amount of sleep, in hours\n\n\nsleep_rem\nrem sleep, in hours\n\n\nsleep_cycle\nlength of sleep cycle, in hours\n\n\nawake\namount of time spent awake, in hours\n\n\nbrainwt\nbrain weight in kilograms\n\n\nbodywt\nbody weight in kilograms"
  },
  {
    "objectID": "dplyr_intro_msleep.html#dplyr-verbs",
    "href": "dplyr_intro_msleep.html#dplyr-verbs",
    "title": "\n9  Introduction to dplyr: mammal sleep dataset\n",
    "section": "\n9.6 dplyr verbs",
    "text": "9.6 dplyr verbs\nThe dplyr verbs are listed here. There are many other functions available in dplyr, but we will focus on just these.\n\n\n\n\n\n\ndplyr verbs\nDescription\n\n\n\nselect()\nselect columns\n\n\nfilter()\nfilter rows\n\n\narrange()\nre-order or arrange rows\n\n\nmutate()\ncreate new columns\n\n\nsummarise()\nsummarise values\n\n\ngroup_by()\nallows for group operations in the “split-apply-combine” concept"
  },
  {
    "objectID": "dplyr_intro_msleep.html#using-the-dplyr-verbs",
    "href": "dplyr_intro_msleep.html#using-the-dplyr-verbs",
    "title": "\n9  Introduction to dplyr: mammal sleep dataset\n",
    "section": "\n9.7 Using the dplyr verbs",
    "text": "9.7 Using the dplyr verbs\nThe two most basic functions are select() and filter(), which selects columns and filters rows respectively. What are the equivalent ways to select columns without dplyr? And filtering to include only specific rows?\nBefore proceeding, we need to install the dplyr package:\n\ninstall.packages('dplyr')\n\nAnd then load the library:\n\nlibrary(dplyr)\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\n\n\n9.7.1 Selecting columns: select()\n\nSelect a set of columns such as the name and the sleep_total columns.\n\nsleepData &lt;- select(msleep, name, sleep_total)\nhead(sleepData)\n\n# A tibble: 6 × 2\n  name                       sleep_total\n  &lt;chr&gt;                            &lt;dbl&gt;\n1 Cheetah                           12.1\n2 Owl monkey                        17  \n3 Mountain beaver                   14.4\n4 Greater short-tailed shrew        14.9\n5 Cow                                4  \n6 Three-toed sloth                  14.4\n\n\nTo select all the columns except a specific column, use the “-” (subtraction) operator (also known as negative indexing). For example, to select all columns except name:\n\nhead(select(msleep, -name))\n\n# A tibble: 6 × 10\n  genus      vore  order    conservation sleep_total sleep_rem sleep_cycle awake\n  &lt;chr&gt;      &lt;chr&gt; &lt;chr&gt;    &lt;chr&gt;              &lt;dbl&gt;     &lt;dbl&gt;       &lt;dbl&gt; &lt;dbl&gt;\n1 Acinonyx   carni Carnivo… lc                  12.1      NA        NA      11.9\n2 Aotus      omni  Primates &lt;NA&gt;                17         1.8      NA       7  \n3 Aplodontia herbi Rodentia nt                  14.4       2.4      NA       9.6\n4 Blarina    omni  Soricom… lc                  14.9       2.3       0.133   9.1\n5 Bos        herbi Artioda… domesticated         4         0.7       0.667  20  \n6 Bradypus   herbi Pilosa   &lt;NA&gt;                14.4       2.2       0.767   9.6\n# ℹ 2 more variables: brainwt &lt;dbl&gt;, bodywt &lt;dbl&gt;\n\n\nTo select a range of columns by name, use the “:” operator. Note that dplyr allows us to use the column names without quotes and as “indices” of the columns.\n\nhead(select(msleep, name:order))\n\n# A tibble: 6 × 4\n  name                       genus      vore  order       \n  &lt;chr&gt;                      &lt;chr&gt;      &lt;chr&gt; &lt;chr&gt;       \n1 Cheetah                    Acinonyx   carni Carnivora   \n2 Owl monkey                 Aotus      omni  Primates    \n3 Mountain beaver            Aplodontia herbi Rodentia    \n4 Greater short-tailed shrew Blarina    omni  Soricomorpha\n5 Cow                        Bos        herbi Artiodactyla\n6 Three-toed sloth           Bradypus   herbi Pilosa      \n\n\nTo select all columns that start with the character string “sl”, use the function starts_with().\n\nhead(select(msleep, starts_with(\"sl\")))\n\n# A tibble: 6 × 3\n  sleep_total sleep_rem sleep_cycle\n        &lt;dbl&gt;     &lt;dbl&gt;       &lt;dbl&gt;\n1        12.1      NA        NA    \n2        17         1.8      NA    \n3        14.4       2.4      NA    \n4        14.9       2.3       0.133\n5         4         0.7       0.667\n6        14.4       2.2       0.767\n\n\nSome additional options to select columns based on a specific criteria include:\n\n\nends_with() = Select columns that end with a character string\n\ncontains() = Select columns that contain a character string\n\nmatches() = Select columns that match a regular expression\n\none_of() = Select column names that are from a group of names\n\n9.7.2 Selecting rows: filter()\n\nThe filter() function allows us to filter rows to include only those rows that match the filter. For example, we can filter the rows for mammals that sleep a total of more than 16 hours.\n\nfilter(msleep, sleep_total &gt;= 16)\n\n# A tibble: 8 × 11\n  name    genus vore  order conservation sleep_total sleep_rem sleep_cycle awake\n  &lt;chr&gt;   &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt;              &lt;dbl&gt;     &lt;dbl&gt;       &lt;dbl&gt; &lt;dbl&gt;\n1 Owl mo… Aotus omni  Prim… &lt;NA&gt;                17         1.8      NA       7  \n2 Long-n… Dasy… carni Cing… lc                  17.4       3.1       0.383   6.6\n3 North … Dide… omni  Dide… lc                  18         4.9       0.333   6  \n4 Big br… Epte… inse… Chir… lc                  19.7       3.9       0.117   4.3\n5 Thick-… Lutr… carni Dide… lc                  19.4       6.6      NA       4.6\n6 Little… Myot… inse… Chir… &lt;NA&gt;                19.9       2         0.2     4.1\n7 Giant … Prio… inse… Cing… en                  18.1       6.1      NA       5.9\n8 Arctic… Sper… herbi Rode… lc                  16.6      NA        NA       7.4\n# ℹ 2 more variables: brainwt &lt;dbl&gt;, bodywt &lt;dbl&gt;\n\n\nFilter the rows for mammals that sleep a total of more than 16 hours and have a body weight of greater than 1 kilogram.\n\nfilter(msleep, sleep_total &gt;= 16, bodywt &gt;= 1)\n\n# A tibble: 3 × 11\n  name    genus vore  order conservation sleep_total sleep_rem sleep_cycle awake\n  &lt;chr&gt;   &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt;              &lt;dbl&gt;     &lt;dbl&gt;       &lt;dbl&gt; &lt;dbl&gt;\n1 Long-n… Dasy… carni Cing… lc                  17.4       3.1       0.383   6.6\n2 North … Dide… omni  Dide… lc                  18         4.9       0.333   6  \n3 Giant … Prio… inse… Cing… en                  18.1       6.1      NA       5.9\n# ℹ 2 more variables: brainwt &lt;dbl&gt;, bodywt &lt;dbl&gt;\n\n\nFilter the rows for mammals in the Perissodactyla and Primates taxonomic order. The %in% operator is a logical operator that returns TRUE for values of a vector that are present in a second vector.\n\nfilter(msleep, order %in% c(\"Perissodactyla\", \"Primates\"))\n\n# A tibble: 15 × 11\n   name   genus vore  order conservation sleep_total sleep_rem sleep_cycle awake\n   &lt;chr&gt;  &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt;              &lt;dbl&gt;     &lt;dbl&gt;       &lt;dbl&gt; &lt;dbl&gt;\n 1 Owl m… Aotus omni  Prim… &lt;NA&gt;                17         1.8      NA       7  \n 2 Grivet Cerc… omni  Prim… lc                  10         0.7      NA      14  \n 3 Horse  Equus herbi Peri… domesticated         2.9       0.6       1      21.1\n 4 Donkey Equus herbi Peri… domesticated         3.1       0.4      NA      20.9\n 5 Patas… Eryt… omni  Prim… lc                  10.9       1.1      NA      13.1\n 6 Galago Gala… omni  Prim… &lt;NA&gt;                 9.8       1.1       0.55   14.2\n 7 Human  Homo  omni  Prim… &lt;NA&gt;                 8         1.9       1.5    16  \n 8 Mongo… Lemur herbi Prim… vu                   9.5       0.9      NA      14.5\n 9 Macaq… Maca… omni  Prim… &lt;NA&gt;                10.1       1.2       0.75   13.9\n10 Slow … Nyct… carni Prim… &lt;NA&gt;                11        NA        NA      13  \n11 Chimp… Pan   omni  Prim… &lt;NA&gt;                 9.7       1.4       1.42   14.3\n12 Baboon Papio omni  Prim… &lt;NA&gt;                 9.4       1         0.667  14.6\n13 Potto  Pero… omni  Prim… lc                  11        NA        NA      13  \n14 Squir… Saim… omni  Prim… &lt;NA&gt;                 9.6       1.4      NA      14.4\n15 Brazi… Tapi… herbi Peri… vu                   4.4       1         0.9    19.6\n# ℹ 2 more variables: brainwt &lt;dbl&gt;, bodywt &lt;dbl&gt;\n\n\nYou can use the boolean operators (e.g. &gt;, &lt;, &gt;=, &lt;=, !=, %in%) to create the logical tests."
  },
  {
    "objectID": "dplyr_intro_msleep.html#piping-with",
    "href": "dplyr_intro_msleep.html#piping-with",
    "title": "\n9  Introduction to dplyr: mammal sleep dataset\n",
    "section": "\n9.8 “Piping”” with |>\n",
    "text": "9.8 “Piping”” with |&gt;\n\nIt is not unusual to want to perform a set of operations using dplyr. The pipe operator |&gt; allows us to “pipe” the output from one function into the input of the next. While there is nothing special about how R treats operations that are written in a pipe, the idea of piping is to allow us to read multiple functions operating one after another from left-to-right. Without piping, one would either 1) save each step in set of functions as a temporary variable and then pass that variable along the chain or 2) have to “nest” functions, which can be hard to read.\nHere’s an example we have already used:\n\nhead(select(msleep, name, sleep_total))\n\n# A tibble: 6 × 2\n  name                       sleep_total\n  &lt;chr&gt;                            &lt;dbl&gt;\n1 Cheetah                           12.1\n2 Owl monkey                        17  \n3 Mountain beaver                   14.4\n4 Greater short-tailed shrew        14.9\n5 Cow                                4  \n6 Three-toed sloth                  14.4\n\n\nNow in this case, we will pipe the msleep data frame to the function that will select two columns (name and sleep\\_total) and then pipe the new data frame to the function head(), which will return the head of the new data frame.\n\nmsleep |&gt; \n    select(name, sleep_total) |&gt; \n    head()\n\n# A tibble: 6 × 2\n  name                       sleep_total\n  &lt;chr&gt;                            &lt;dbl&gt;\n1 Cheetah                           12.1\n2 Owl monkey                        17  \n3 Mountain beaver                   14.4\n4 Greater short-tailed shrew        14.9\n5 Cow                                4  \n6 Three-toed sloth                  14.4\n\n\nYou will soon see how useful the pipe operator is when we start to combine many functions.\nNow that you know about the pipe operator (|&gt;), we will use it throughout the rest of this tutorial.\n\n9.8.1 Arrange Or Re-order Rows Using arrange()\n\nTo arrange (or re-order) rows by a particular column, such as the taxonomic order, list the name of the column you want to arrange the rows by:\n\nmsleep |&gt; arrange(order) |&gt; head()\n\n# A tibble: 6 × 11\n  name    genus vore  order conservation sleep_total sleep_rem sleep_cycle awake\n  &lt;chr&gt;   &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt;              &lt;dbl&gt;     &lt;dbl&gt;       &lt;dbl&gt; &lt;dbl&gt;\n1 Tenrec  Tenr… omni  Afro… &lt;NA&gt;                15.6       2.3      NA       8.4\n2 Cow     Bos   herbi Arti… domesticated         4         0.7       0.667  20  \n3 Roe de… Capr… herbi Arti… lc                   3        NA        NA      21  \n4 Goat    Capri herbi Arti… lc                   5.3       0.6      NA      18.7\n5 Giraffe Gira… herbi Arti… cd                   1.9       0.4      NA      22.1\n6 Sheep   Ovis  herbi Arti… domesticated         3.8       0.6      NA      20.2\n# ℹ 2 more variables: brainwt &lt;dbl&gt;, bodywt &lt;dbl&gt;\n\n\nNow we will select three columns from msleep, arrange the rows by the taxonomic order and then arrange the rows by sleep_total. Finally, show the head of the final data frame:\n\nmsleep |&gt; \n    select(name, order, sleep_total) |&gt;\n    arrange(order, sleep_total) |&gt; \n    head()\n\n# A tibble: 6 × 3\n  name     order        sleep_total\n  &lt;chr&gt;    &lt;chr&gt;              &lt;dbl&gt;\n1 Tenrec   Afrosoricida        15.6\n2 Giraffe  Artiodactyla         1.9\n3 Roe deer Artiodactyla         3  \n4 Sheep    Artiodactyla         3.8\n5 Cow      Artiodactyla         4  \n6 Goat     Artiodactyla         5.3\n\n\nSame as above, except here we filter the rows for mammals that sleep for 16 or more hours, instead of showing the head of the final data frame:\n\nmsleep |&gt; \n    select(name, order, sleep_total) |&gt;\n    arrange(order, sleep_total) |&gt; \n    filter(sleep_total &gt;= 16)\n\n# A tibble: 8 × 3\n  name                   order           sleep_total\n  &lt;chr&gt;                  &lt;chr&gt;                 &lt;dbl&gt;\n1 Big brown bat          Chiroptera             19.7\n2 Little brown bat       Chiroptera             19.9\n3 Long-nosed armadillo   Cingulata              17.4\n4 Giant armadillo        Cingulata              18.1\n5 North American Opossum Didelphimorphia        18  \n6 Thick-tailed opposum   Didelphimorphia        19.4\n7 Owl monkey             Primates               17  \n8 Arctic ground squirrel Rodentia               16.6\n\n\nFor something slightly more complicated do the same as above, except arrange the rows in the sleep_total column in a descending order. For this, use the function desc()\n\nmsleep |&gt; \n    select(name, order, sleep_total) |&gt;\n    arrange(order, desc(sleep_total)) |&gt; \n    filter(sleep_total &gt;= 16)\n\n# A tibble: 8 × 3\n  name                   order           sleep_total\n  &lt;chr&gt;                  &lt;chr&gt;                 &lt;dbl&gt;\n1 Little brown bat       Chiroptera             19.9\n2 Big brown bat          Chiroptera             19.7\n3 Giant armadillo        Cingulata              18.1\n4 Long-nosed armadillo   Cingulata              17.4\n5 Thick-tailed opposum   Didelphimorphia        19.4\n6 North American Opossum Didelphimorphia        18  \n7 Owl monkey             Primates               17  \n8 Arctic ground squirrel Rodentia               16.6"
  },
  {
    "objectID": "dplyr_intro_msleep.html#create-new-columns-using-mutate",
    "href": "dplyr_intro_msleep.html#create-new-columns-using-mutate",
    "title": "\n9  Introduction to dplyr: mammal sleep dataset\n",
    "section": "\n9.9 Create New Columns Using mutate()\n",
    "text": "9.9 Create New Columns Using mutate()\n\nThe mutate() function will add new columns to the data frame. Create a new column called rem_proportion, which is the ratio of rem sleep to total amount of sleep.\n\nmsleep |&gt; \n    mutate(rem_proportion = sleep_rem / sleep_total) |&gt;\n    head()\n\n# A tibble: 6 × 12\n  name    genus vore  order conservation sleep_total sleep_rem sleep_cycle awake\n  &lt;chr&gt;   &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt;              &lt;dbl&gt;     &lt;dbl&gt;       &lt;dbl&gt; &lt;dbl&gt;\n1 Cheetah Acin… carni Carn… lc                  12.1      NA        NA      11.9\n2 Owl mo… Aotus omni  Prim… &lt;NA&gt;                17         1.8      NA       7  \n3 Mounta… Aplo… herbi Rode… nt                  14.4       2.4      NA       9.6\n4 Greate… Blar… omni  Sori… lc                  14.9       2.3       0.133   9.1\n5 Cow     Bos   herbi Arti… domesticated         4         0.7       0.667  20  \n6 Three-… Brad… herbi Pilo… &lt;NA&gt;                14.4       2.2       0.767   9.6\n# ℹ 3 more variables: brainwt &lt;dbl&gt;, bodywt &lt;dbl&gt;, rem_proportion &lt;dbl&gt;\n\n\nYou can add many new columns using mutate (separated by commas). Here we add a second column called bodywt_grams which is the bodywt column in grams.\n\nmsleep |&gt; \n    mutate(rem_proportion = sleep_rem / sleep_total, \n           bodywt_grams = bodywt * 1000) |&gt;\n    head()\n\n# A tibble: 6 × 13\n  name    genus vore  order conservation sleep_total sleep_rem sleep_cycle awake\n  &lt;chr&gt;   &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt;              &lt;dbl&gt;     &lt;dbl&gt;       &lt;dbl&gt; &lt;dbl&gt;\n1 Cheetah Acin… carni Carn… lc                  12.1      NA        NA      11.9\n2 Owl mo… Aotus omni  Prim… &lt;NA&gt;                17         1.8      NA       7  \n3 Mounta… Aplo… herbi Rode… nt                  14.4       2.4      NA       9.6\n4 Greate… Blar… omni  Sori… lc                  14.9       2.3       0.133   9.1\n5 Cow     Bos   herbi Arti… domesticated         4         0.7       0.667  20  \n6 Three-… Brad… herbi Pilo… &lt;NA&gt;                14.4       2.2       0.767   9.6\n# ℹ 4 more variables: brainwt &lt;dbl&gt;, bodywt &lt;dbl&gt;, rem_proportion &lt;dbl&gt;,\n#   bodywt_grams &lt;dbl&gt;\n\n\nIs there a relationship between rem_proportion and bodywt? How about sleep_total?\n\n9.9.1 Create summaries: summarise()\n\nThe summarise() function will create summary statistics for a given column in the data frame such as finding the mean. For example, to compute the average number of hours of sleep, apply the mean() function to the column sleep_total and call the summary value avg_sleep.\n\nmsleep |&gt; \n    summarise(avg_sleep = mean(sleep_total))\n\n# A tibble: 1 × 1\n  avg_sleep\n      &lt;dbl&gt;\n1      10.4\n\n\nThere are many other summary statistics you could consider such sd(), min(), max(), median(), sum(), n() (returns the length of vector), first() (returns first value in vector), last() (returns last value in vector) and n_distinct() (number of distinct values in vector).\n\nmsleep |&gt; \n    summarise(avg_sleep = mean(sleep_total), \n              min_sleep = min(sleep_total),\n              max_sleep = max(sleep_total),\n              total = n())\n\n# A tibble: 1 × 4\n  avg_sleep min_sleep max_sleep total\n      &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt; &lt;int&gt;\n1      10.4       1.9      19.9    83"
  },
  {
    "objectID": "dplyr_intro_msleep.html#grouping-data-group_by",
    "href": "dplyr_intro_msleep.html#grouping-data-group_by",
    "title": "\n9  Introduction to dplyr: mammal sleep dataset\n",
    "section": "\n9.10 Grouping data: group_by()\n",
    "text": "9.10 Grouping data: group_by()\n\nThe group_by() verb is an important function in dplyr. The group_by allows us to use the concept of “split-apply-combine”. We literally want to split the data frame by some variable (e.g. taxonomic order), apply a function to the individual data frames and then combine the output. This approach is similar to the aggregate function from R, but group_by integrates with dplyr.\nLet’s do that: split the msleep data frame by the taxonomic order, then ask for the same summary statistics as above. We expect a set of summary statistics for each taxonomic order.\n\nmsleep |&gt; \n    group_by(order) |&gt;\n    summarise(avg_sleep = mean(sleep_total), \n              min_sleep = min(sleep_total), \n              max_sleep = max(sleep_total),\n              total = n())\n\n# A tibble: 19 × 5\n   order           avg_sleep min_sleep max_sleep total\n   &lt;chr&gt;               &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt; &lt;int&gt;\n 1 Afrosoricida        15.6       15.6      15.6     1\n 2 Artiodactyla         4.52       1.9       9.1     6\n 3 Carnivora           10.1        3.5      15.8    12\n 4 Cetacea              4.5        2.7       5.6     3\n 5 Chiroptera          19.8       19.7      19.9     2\n 6 Cingulata           17.8       17.4      18.1     2\n 7 Didelphimorphia     18.7       18        19.4     2\n 8 Diprotodontia       12.4       11.1      13.7     2\n 9 Erinaceomorpha      10.2       10.1      10.3     2\n10 Hyracoidea           5.67       5.3       6.3     3\n11 Lagomorpha           8.4        8.4       8.4     1\n12 Monotremata          8.6        8.6       8.6     1\n13 Perissodactyla       3.47       2.9       4.4     3\n14 Pilosa              14.4       14.4      14.4     1\n15 Primates            10.5        8        17      12\n16 Proboscidea          3.6        3.3       3.9     2\n17 Rodentia            12.5        7        16.6    22\n18 Scandentia           8.9        8.9       8.9     1\n19 Soricomorpha        11.1        8.4      14.9     5"
  },
  {
    "objectID": "eda_and_univariate_brfss.html#a-case-study-on-the-behavioral-risk-factor-surveillance-system",
    "href": "eda_and_univariate_brfss.html#a-case-study-on-the-behavioral-risk-factor-surveillance-system",
    "title": "\n10  Case Study: Behavioral Risk Factor Surveillance System\n",
    "section": "\n10.1 A Case Study on the Behavioral Risk Factor Surveillance System",
    "text": "10.1 A Case Study on the Behavioral Risk Factor Surveillance System\nThe Behavioral Risk Factor Surveillance System (BRFSS) is a large-scale health survey conducted annually by the Centers for Disease Control and Prevention (CDC) in the United States. The BRFSS collects information on various health-related behaviors, chronic health conditions, and the use of preventive services among the adult population (18 years and older) through telephone interviews. The main goal of the BRFSS is to identify and monitor the prevalence of risk factors associated with chronic diseases, inform public health policies, and evaluate the effectiveness of health promotion and disease prevention programs. The data collected through BRFSS is crucial for understanding the health status and needs of the population, and it serves as a valuable resource for researchers, policy makers, and healthcare professionals in making informed decisions and designing targeted interventions.\nIn this chapter, we will walk through an exploratory data analysis (EDA) of the Behavioral Risk Factor Surveillance System dataset using R. EDA is an important step in the data analysis process, as it helps you to understand your data, identify trends, and detect any anomalies before performing more advanced analyses. We will use various R functions and packages to explore the dataset, with a focus on active learning and hands-on experience."
  },
  {
    "objectID": "eda_and_univariate_brfss.html#loading-the-dataset",
    "href": "eda_and_univariate_brfss.html#loading-the-dataset",
    "title": "\n10  Case Study: Behavioral Risk Factor Surveillance System\n",
    "section": "\n10.2 Loading the Dataset",
    "text": "10.2 Loading the Dataset\nFirst, let’s load the dataset into R. We will use the read.csv() function from the base R package to read the data and store it in a data frame called brfss. Make sure the CSV file is in your working directory, or provide the full path to the file.\nFirst, we need to get the data. Either download the data from THIS LINK or have R do it directly from the command-line (preferred):\n\ndownload.file('https://raw.githubusercontent.com/seandavi/ITR/master/BRFSS-subset.csv',\n              destfile = 'BRFSS-subset.csv')\n\n\n\npath &lt;- file.choose()    # look for BRFSS-subset.csv\n\n\nstopifnot(file.exists(path))\nbrfss &lt;- read.csv(path)"
  },
  {
    "objectID": "eda_and_univariate_brfss.html#inspecting-the-data",
    "href": "eda_and_univariate_brfss.html#inspecting-the-data",
    "title": "\n10  Case Study: Behavioral Risk Factor Surveillance System\n",
    "section": "\n10.3 Inspecting the Data",
    "text": "10.3 Inspecting the Data\nOnce the data is loaded, let’s take a look at the first few rows of the dataset using the head() function:\n\nhead(brfss)\n\n  Age   Weight    Sex Height Year\n1  31 48.98798 Female 157.48 1990\n2  57 81.64663 Female 157.48 1990\n3  43 80.28585   Male 177.80 1990\n4  72 70.30682   Male 170.18 1990\n5  31 49.89516 Female 154.94 1990\n6  58 54.43108 Female 154.94 1990\n\n\nThis will display the first six rows of the dataset, allowing you to get a feel for the data structure and variable types.\nNext, let’s check the dimensions of the dataset using the dim() function:\n\ndim(brfss)\n\n[1] 20000     5\n\n\nThis will return the number of rows and columns in the dataset, which is important to know for subsequent analyses."
  },
  {
    "objectID": "eda_and_univariate_brfss.html#summary-statistics",
    "href": "eda_and_univariate_brfss.html#summary-statistics",
    "title": "\n10  Case Study: Behavioral Risk Factor Surveillance System\n",
    "section": "\n10.4 Summary Statistics",
    "text": "10.4 Summary Statistics\nNow that we have a basic understanding of the data structure, let’s calculate some summary statistics. The summary() function in R provides a quick overview of the main statistics for each variable in the dataset:\n\nsummary(brfss)\n\n      Age            Weight           Sex                Height     \n Min.   :18.00   Min.   : 34.93   Length:20000       Min.   :105.0  \n 1st Qu.:36.00   1st Qu.: 61.69   Class :character   1st Qu.:162.6  \n Median :51.00   Median : 72.57   Mode  :character   Median :168.0  \n Mean   :50.99   Mean   : 75.42                      Mean   :169.2  \n 3rd Qu.:65.00   3rd Qu.: 86.18                      3rd Qu.:177.8  \n Max.   :99.00   Max.   :278.96                      Max.   :218.0  \n NA's   :139     NA's   :649                         NA's   :184    \n      Year     \n Min.   :1990  \n 1st Qu.:1990  \n Median :2000  \n Mean   :2000  \n 3rd Qu.:2010  \n Max.   :2010  \n               \n\n\nThis will display the minimum, first quartile, median, mean, third quartile, and maximum for each numeric variable, and the frequency counts for each factor level for categorical variables."
  },
  {
    "objectID": "eda_and_univariate_brfss.html#data-visualization",
    "href": "eda_and_univariate_brfss.html#data-visualization",
    "title": "\n10  Case Study: Behavioral Risk Factor Surveillance System\n",
    "section": "\n10.5 Data Visualization",
    "text": "10.5 Data Visualization\nVisualizing the data can help you identify patterns and trends in the dataset. Let’s start by creating a histogram of the Age variable using the hist() function.\nThis will create a histogram showing the frequency distribution of ages in the dataset. You can customize the appearance of the histogram by adjusting the parameters within the hist() function.\n\nhist(brfss$Age, main = \"Age Distribution\", \n     xlab = \"Age\", col = \"lightblue\")\n\n\n\n\n\n\n\n\n\n\nWhat are the options for a histogram?\n\n\n\nThe hist() function has many options. For example, you can change the number of bins, the color of the bars, the title, and the x-axis label. You can also add a vertical line at the mean or median, or add a normal curve to the histogram. For more information, type ?hist in the R console.\nMore generally, it is important to understand the options available for each function you use. You can do this by reading the documentation for the function, which can be accessed by typing ?function_name or help(\"function_name\")in the R console.\n\n\nNext, let’s create a boxplot to compare the distribution of Weight between males and females. We will use the boxplot() function for this. This will create a boxplot comparing the weight distribution between males and females. You can customize the appearance of the boxplot by adjusting the parameters within the boxplot() function.\n\nboxplot(brfss$Weight ~ brfss$Sex, main = \"Weight Distribution by Sex\", \n        xlab = \"Sex\", ylab = \"Weight\", col = c(\"pink\", \"lightblue\"))"
  },
  {
    "objectID": "eda_and_univariate_brfss.html#analyzing-relationships-between-variables",
    "href": "eda_and_univariate_brfss.html#analyzing-relationships-between-variables",
    "title": "\n10  Case Study: Behavioral Risk Factor Surveillance System\n",
    "section": "\n10.6 Analyzing Relationships Between Variables",
    "text": "10.6 Analyzing Relationships Between Variables\nTo further explore the data, let’s investigate the relationship between age and weight using a scatterplot. We will use the plot() function for this:\nThis will create a scatterplot of age and weight, allowing you to visually assess the relationship between these two variables.\n\nplot(brfss$Age, brfss$Weight, main = \"Scatterplot of Age and Weight\", \n     xlab = \"Age\", ylab = \"Weight\", col = \"darkblue\")  \n\n\n\n\nTo quantify the strength of the relationship between age and weight, we can calculate the correlation coefficient using the cor() function:\nThis will return the correlation coefficient between age and weight, which can help you determine whether there is a linear relationship between these variables.\n\ncor(brfss$Age, brfss$Weight)\n\n[1] NA\n\n\nWhy does cor() give a value of NA? What can we do about it? A quick glance at help(\"cor\") will give you the answer.\n\ncor(brfss$Age, brfss$Weight, use = \"complete.obs\")\n\n[1] 0.02699989"
  },
  {
    "objectID": "eda_and_univariate_brfss.html#exercises",
    "href": "eda_and_univariate_brfss.html#exercises",
    "title": "\n10  Case Study: Behavioral Risk Factor Surveillance System\n",
    "section": "\n10.7 Exercises",
    "text": "10.7 Exercises\n\n\nWhat is the mean weight in this dataset? How about the median? What is the difference between the two? What does this tell you about the distribution of weights in the dataset?\n\nShow answermean(brfss$Weight, na.rm = TRUE)\n\n[1] 75.42455\n\nShow answermedian(brfss$Weight, na.rm = TRUE)\n\n[1] 72.57478\n\nShow answermean(brfss$Weight, na.rm=TRUE) - median(brfss$Weight, na.rm = TRUE)\n\n[1] 2.849774\n\n\n\n\nGiven the findings about the mean and median in the previous exercise, use the hist() function to create a histogram of the weight distribution in this dataset. How would you describe the shape of this distribution?\n\nShow answerhist(brfss$Weight, xlab=\"Weight (kg)\", breaks = 30)\n\n\n\n\n\n\nUse plot() to examine the relationship between height and weight in this dataset.\n\nShow answerplot(brfss$Height, brfss$Weight)\n\n\n\n\n\n\nWhat is the correlation between height and weight? What does this tell you about the relationship between these two variables?\n\nShow answercor(brfss$Height, brfss$Weight, use = \"complete.obs\")\n\n[1] 0.5140928\n\n\n\n\nCreate a histogram of the height distribution in this dataset. How would you describe the shape of this distribution?\n\nShow answerhist(brfss$Height, xlab=\"Height (cm)\", breaks = 30)"
  },
  {
    "objectID": "eda_and_univariate_brfss.html#conclusion",
    "href": "eda_and_univariate_brfss.html#conclusion",
    "title": "\n10  Case Study: Behavioral Risk Factor Surveillance System\n",
    "section": "\n10.8 Conclusion",
    "text": "10.8 Conclusion\nIn this chapter, we have demonstrated how to perform an exploratory data analysis on the Behavioral Risk Factor Surveillance System dataset using R. We covered data loading, inspection, summary statistics, visualization, and the analysis of relationships between variables. By actively engaging with the R code and data, you have gained valuable experience in using R for EDA and are well-equipped to tackle more complex analyses in your future work.\nRemember that EDA is just the beginning of the data analysis process, and further statistical modeling and hypothesis testing will likely be necessary to draw meaningful conclusions from your data. However, EDA is a crucial step in understanding your data and informing your subsequent analyses."
  },
  {
    "objectID": "eda_and_univariate_brfss.html#learn-about-the-data",
    "href": "eda_and_univariate_brfss.html#learn-about-the-data",
    "title": "\n10  Case Study: Behavioral Risk Factor Surveillance System\n",
    "section": "\n10.9 Learn about the data",
    "text": "10.9 Learn about the data\nUsing the data exploration techniques you have seen to explore the brfss dataset.\n\nsummary()\ndim()\ncolnames()\nhead()\ntail()\nclass()\nView()\n\nYou may want to investigate individual columns visually using plotting like hist(). For categorical data, consider using something like table()."
  },
  {
    "objectID": "eda_and_univariate_brfss.html#clean-data",
    "href": "eda_and_univariate_brfss.html#clean-data",
    "title": "\n10  Case Study: Behavioral Risk Factor Surveillance System\n",
    "section": "\n10.10 Clean data",
    "text": "10.10 Clean data\nR read Year as an integer value, but it’s really a factor\n\nbrfss$Year &lt;- factor(brfss$Year)"
  },
  {
    "objectID": "eda_and_univariate_brfss.html#weight-in-1990-vs.-2010-females",
    "href": "eda_and_univariate_brfss.html#weight-in-1990-vs.-2010-females",
    "title": "\n10  Case Study: Behavioral Risk Factor Surveillance System\n",
    "section": "\n10.11 Weight in 1990 vs. 2010 Females",
    "text": "10.11 Weight in 1990 vs. 2010 Females\n\nCreate a subset of the data\n\n\nbrfssFemale &lt;- brfss[brfss$Sex == \"Female\",]\nsummary(brfssFemale)\n\n      Age            Weight           Sex                Height     \n Min.   :18.00   Min.   : 36.29   Length:12039       Min.   :105.0  \n 1st Qu.:37.00   1st Qu.: 57.61   Class :character   1st Qu.:157.5  \n Median :52.00   Median : 65.77   Mode  :character   Median :163.0  \n Mean   :51.92   Mean   : 69.05                      Mean   :163.3  \n 3rd Qu.:67.00   3rd Qu.: 77.11                      3rd Qu.:168.0  \n Max.   :99.00   Max.   :272.16                      Max.   :200.7  \n NA's   :103     NA's   :560                         NA's   :140    \n   Year     \n 1990:5718  \n 2010:6321  \n            \n            \n            \n            \n            \n\n\n\nVisualize\n\n\nplot(Weight ~ Year, brfssFemale)\n\n\n\n\n\nStatistical test\n\n\nt.test(Weight ~ Year, brfssFemale)\n\n\n    Welch Two Sample t-test\n\ndata:  Weight by Year\nt = -27.133, df = 11079, p-value &lt; 2.2e-16\nalternative hypothesis: true difference in means between group 1990 and group 2010 is not equal to 0\n95 percent confidence interval:\n -8.723607 -7.548102\nsample estimates:\nmean in group 1990 mean in group 2010 \n          64.81838           72.95424"
  },
  {
    "objectID": "eda_and_univariate_brfss.html#weight-and-height-in-2010-males",
    "href": "eda_and_univariate_brfss.html#weight-and-height-in-2010-males",
    "title": "\n10  Case Study: Behavioral Risk Factor Surveillance System\n",
    "section": "\n10.12 Weight and height in 2010 Males",
    "text": "10.12 Weight and height in 2010 Males\n\nCreate a subset of the data\n\n\nbrfss2010Male &lt;- subset(brfss,  Year == 2010 & Sex == \"Male\")\nsummary(brfss2010Male)\n\n      Age            Weight           Sex                Height      Year     \n Min.   :18.00   Min.   : 36.29   Length:3679        Min.   :135   1990:   0  \n 1st Qu.:45.00   1st Qu.: 77.11   Class :character   1st Qu.:173   2010:3679  \n Median :57.00   Median : 86.18   Mode  :character   Median :178              \n Mean   :56.25   Mean   : 88.85                      Mean   :178              \n 3rd Qu.:68.00   3rd Qu.: 99.79                      3rd Qu.:183              \n Max.   :99.00   Max.   :278.96                      Max.   :218              \n NA's   :30      NA's   :49                          NA's   :31               \n\n\n\nVisualize the relationship\n\n\nhist(brfss2010Male$Weight)\n\n\n\nhist(brfss2010Male$Height)\n\n\n\nplot(Weight ~ Height, brfss2010Male)\n\n\n\n\n\nFit a linear model (regression)\n\n\nfit &lt;- lm(Weight ~ Height, brfss2010Male)\nfit\n\n\nCall:\nlm(formula = Weight ~ Height, data = brfss2010Male)\n\nCoefficients:\n(Intercept)       Height  \n   -86.8747       0.9873  \n\n\nSummarize as ANOVA table\n\nanova(fit)\n\nAnalysis of Variance Table\n\nResponse: Weight\n            Df  Sum Sq Mean Sq F value    Pr(&gt;F)    \nHeight       1  197664  197664   693.8 &lt; 2.2e-16 ***\nResiduals 3617 1030484     285                      \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\nPlot points, superpose fitted regression line; where am I?\n\n\nplot(Weight ~ Height, brfss2010Male)\nabline(fit, col=\"blue\", lwd=2)\n# Substitute your own weight and height...\npoints(73 * 2.54, 178 / 2.2, col=\"red\", cex=4, pch=20)\n\n\n\n\n\nClass and available ‘methods’\n\n\nclass(fit)                 # 'noun'\nmethods(class=class(fit))  # 'verb'\n\n\nDiagnostics\n\n\nplot(fit)\n# Note that the \"plot\" above does not have a \".lm\"\n# However, R will use \"plot.lm\". Why?\n?plot.lm"
  },
  {
    "objectID": "norm.html#pnorm",
    "href": "norm.html#pnorm",
    "title": "\n11  Working with distribution functions\n",
    "section": "\n11.1 pnorm",
    "text": "11.1 pnorm\nThis function gives the probability function for a normal distribution. If you do not specify the mean and standard deviation, R defaults to standard normal. Figure 11.1\n\npnorm(q, mean = 0, sd = 1, lower.tail = TRUE, log.p = FALSE)\n\nThe R help file for pnorm provides the template above. The value you input for q is a value on the x-axis, and the returned value is the area under the distribution curve to the left of that point.\n\n\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\nℹ Please use `linewidth` instead.\n\n\n\n\n\n\n\n\n\n\n\nThis function gives the probability function for a normal distribution. If you do not specify the mean and standard deviation, R defaults to standard normal.\npnorm(q, mean = 0, sd = 1, lower.tail = TRUE, log.p = FALSE) The R help file for pnorm provides the template above. The value you input for q is a value on the x-axis, and the returned value is the area under the distribution curve to the left of that point.\nThe option lower.tail = TRUE tells R to use the area to the left of the given point. This is the default, so will remain true even without entering it. In order to compute the area to the right of the given point, you can either switch to lower.tail = FALSE, or simply calculate 1-pnorm() instead. This is demonstrated below.\n\n\n\n\nFigure 11.1: The pnorm function takes a quantile (value on the x-axis) and returns the area under the curve to the left of that value.\n\n\n\n\n\nFigure 11.2: The pnorm function takes a quantile (value on the x-axis) and returns the area under the curve to the left of that value.\n\n\n\n\n\nFigure 11.3: The pnorm function takes a quantile (value on the x-axis) and returns the area under the curve to the left of that value.\n\n\n\n\n\nFigure 11.4: The pnorm function takes a quantile (value on the x-axis) and returns the area under the curve to the left of that value.\n\n\n\nThe option lower.tail = TRUE tells R to use the area to the left of the given point. This is the default, so will remain true even without entering it. In order to compute the area to the right of the given point, you can either switch to lower.tail = FALSE, or simply calculate 1-pnorm() instead."
  },
  {
    "objectID": "norm.html#dnorm",
    "href": "norm.html#dnorm",
    "title": "\n11  Working with distribution functions\n",
    "section": "\n11.2 dnorm",
    "text": "11.2 dnorm\nThis function calculates the probability density function (PDF) for the normal distribution. It gives the probability density (height of the curve) at a specified value (x).\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 11.5: The dnorm function returns the height of the normal distribution at a given point.\n\n\n\n\n\nFigure 11.6: The dnorm function returns the height of the normal distribution at a given point.\n\n\n\n\n\nFigure 11.7: The dnorm function returns the height of the normal distribution at a given point."
  },
  {
    "objectID": "norm.html#qnorm",
    "href": "norm.html#qnorm",
    "title": "\n11  Working with distribution functions\n",
    "section": "\n11.3 qnorm",
    "text": "11.3 qnorm\nThis function calculates the quantiles of the normal distribution. It returns the value (x) corresponding to a specified probability (p). It is the inverse of thepnorm function.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 11.8: The qnorm function is the inverse of the pnorm function in that it takes a probability and gives the quantile.\n\n\n\n\n\nFigure 11.9: The qnorm function is the inverse of the pnorm function in that it takes a probability and gives the quantile.\n\n\n\n\n\n\n\nFigure 11.10: The qnorm function is the inverse of the pnorm function in that it takes a probability and gives the quantile.\n\n\n\n\n\nFigure 11.11: The qnorm function is the inverse of the pnorm function in that it takes a probability and gives the quantile.\n\n\n\n\n\n\n\nFigure 11.12: The qnorm function is the inverse of the pnorm function in that it takes a probability and gives the quantile."
  },
  {
    "objectID": "norm.html#rnorm",
    "href": "norm.html#rnorm",
    "title": "\n11  Working with distribution functions\n",
    "section": "\n11.4 rnorm",
    "text": "11.4 rnorm\n\n\n\n\nprint(r1)\n\n\n\nFigure 11.13: The rnorm function takes a number of samples and returns a vector of random numbers from the normal distribution (with mean=0, sd=1 as defaults)"
  },
  {
    "objectID": "norm.html#iq-scores",
    "href": "norm.html#iq-scores",
    "title": "\n11  Working with distribution functions\n",
    "section": "\n11.5 IQ scores",
    "text": "11.5 IQ scores\nNormal Distribution and its Application with IQ\nThe normal distribution, also known as the Gaussian distribution, is a continuous probability distribution characterized by its bell-shaped curve. It is defined by two parameters: the mean (µ) and the standard deviation (σ). The mean represents the central tendency of the distribution, while the standard deviation represents the dispersion or spread of the data.\nThe IQ scores are an excellent example of the normal distribution, as they are designed to follow this distribution pattern. The mean IQ score is set at 100, and the standard deviation is set at 15. This means that the majority of the population (about 68%) have an IQ score between 85 and 115, while 95% of the population have an IQ score between 70 and 130.\n\n\nWhat is the probability of having an IQ score between 85 and 115?\n\nShow answerpnorm(115, mean = 100, sd = 15) - pnorm(85, mean = 100, sd = 15)\n\n\n\n\nWhat is the 90th percentile of the IQ scores?\n\nShow answerqnorm(0.9, mean = 100, sd = 15)\n\n\n\n\nWhat is the probability of having an IQ score above 130?\n\nShow answer1 - pnorm(130, mean = 100, sd = 15)\n\n\n\n\nWhat is the probability of having an IQ score below 70?\n\nShow answerpnorm(70, mean = 100, sd = 15)"
  },
  {
    "objectID": "t-stats-and-tests.html#background",
    "href": "t-stats-and-tests.html#background",
    "title": "\n12  The t-statistic and t-distribution\n",
    "section": "\n12.1 Background",
    "text": "12.1 Background\nThe t-test is a statistical hypothesis test that is commonly used when the data are normally distributed (follow a normal distribution) if the value of the population standard deviation were known. When the population standard deviation is not known and is replaced by an estimate based no the data, the test statistic follows a Student’s t distribution.\nT-tests are handy hypothesis tests in statistics when you want to compare means. You can compare a sample mean to a hypothesized or target value using a one-sample t-test. You can compare the means of two groups with a two-sample t-test. If you have two groups with paired observations (e.g., before and after measurements), use the paired t-test.\nA t-test looks at the t-statistic, the t-distribution values, and the degrees of freedom to determine the statistical significance. To conduct a test with three or more means, we would use an analysis of variance.\nThe distriubution that the t-statistic follows was described in a famous paper (Student 1908) by “Student”, a pseudonym for William Sealy Gosset."
  },
  {
    "objectID": "t-stats-and-tests.html#the-z-score-and-probability",
    "href": "t-stats-and-tests.html#the-z-score-and-probability",
    "title": "\n12  The t-statistic and t-distribution\n",
    "section": "\n12.2 The Z-score and probability",
    "text": "12.2 The Z-score and probability\nBefore talking about the t-distribution and t-scores, lets review the Z-score, its relation to the normal distribution, and probability.\nThe Z-score is defined as:\n\\[Z = \\frac{x - \\mu}{\\sigma} \\tag{12.1}\\]\nwhere \\(\\mu\\) is a the population mean from which \\(x\\) is drawn and \\(\\sigma\\) is the population standard deviation (taken as known, not estimated from the data).\nThe probability of observing a \\(Z\\) score of \\(z\\) or greater can be calculated by \\(pnorm(z,\\mu,\\sigma)\\).\nFor example, let’s assume that our “population” is known and it truly has a mean 0 and standard deviation 1. If we have observations drawn from that population, we can assign a probability of seeing that observation by random chance under the assumption that the null hypothesis is TRUE.\n\nzscore = seq(-5,5,1)\n\nFor each value of zscore, let’s calculate the p-value and put the results in a data.frame.\n\ndf = data.frame(\n    zscore = zscore,\n    pval   = pnorm(zscore, 0, 1)\n)\ndf\n\n   zscore         pval\n1      -5 2.866516e-07\n2      -4 3.167124e-05\n3      -3 1.349898e-03\n4      -2 2.275013e-02\n5      -1 1.586553e-01\n6       0 5.000000e-01\n7       1 8.413447e-01\n8       2 9.772499e-01\n9       3 9.986501e-01\n10      4 9.999683e-01\n11      5 9.999997e-01\n\n\nWhy is the p-value of something 5 population standard deviations away from the mean (zscore=5) nearly 1 in this calculation? What is the default for pnorm with respect to being one-sided or two-sided?\nLet’s plot the values of probability vs z-score:\n\nplot(df$zscore, df$pval, type='b')\n\n\n\n\nThis plot is the empirical cumulative density function (cdf) for our data. How can we use it? If we know the z-score, we can look up the probability of observing that value. Since we have constructed our experiment to follow the standard normal distribution, this cdf also represents the cdf of the standard normal distribution.\n\n12.2.1 Small diversion: two-sided pnorm function\nThe pnorm function returns the “one-sided” probability of having a value at least as extreme as the observed \\(x\\) and uses the “lower” tail by default. Let’s create a function that computes two-sided p-values.\n\nTake the absolute value of x\nCompute pnorm with lower.tail=FALSE so we get lower p-values with larger values of \\(x\\).\nSince we want to include both tails, we need to multiply the area (probability) returned by pnorm by 2.\n\n\ntwosidedpnorm = function(x,mu=0,sd=1) {\n    2*pnorm(abs(x),mu,sd,lower.tail=FALSE)\n}\n\nAnd we can test this to see how likely it is to be 2 or 3 standard deviations from the mean:\n\ntwosidedpnorm(2)\n\n[1] 0.04550026\n\ntwosidedpnorm(3)\n\n[1] 0.002699796"
  },
  {
    "objectID": "t-stats-and-tests.html#the-t-distribution",
    "href": "t-stats-and-tests.html#the-t-distribution",
    "title": "\n12  The t-statistic and t-distribution\n",
    "section": "\n12.3 The t-distribution",
    "text": "12.3 The t-distribution\nWe spent time above working with z-scores and probability. An important aspect of working with the normal distribution is that we MUST assume that we know the standard deviation. Remember that the Z-score is defined as:\n\\[Z = \\frac{x - \\mu}{\\sigma}\\]\nThe formula for the population standard deviation is:\n\\[\\sigma = \\sqrt{\\frac{1}{N}\\sum_{i=1}^{N}({xi - \\mu)^2}} \\tag{12.2}\\]\nIn general, the population standard deviation is taken as “known” as we did above.\nIf we do not but only have a sample from the population, instead of using the Z-score, we use the t-score defined as:\n\\[t = \\frac{x - \\bar{x}}{s} \\tag{12.3}\\]\nThis looks quite similar to the formula for Z-score, but here we have to estimate the standard deviation, \\(s\\) from the data. The formula for \\(s\\) is:\n\\[s = \\sqrt{\\frac{1}{N-1}\\sum_{i=1}^{N}({x_{i} - \\bar{x})^2}} \\tag{12.4}\\]\nSince we are estimating the standard deviation from the data, this leads to extra variability that shows up as “fatter tails” for smaller sample sizes than for larger sample sizes. We can see this by comparing the t-distribution for various numbers of degrees of freedom (sample sizes).\nWe can look at the effect of sample size on the distributions graphically by looking at the densities for 3, 5, 10, 20 degrees of freedom and the normal distribution:\n\nlibrary(dplyr)\nlibrary(ggplot2)\nt_values = seq(-6,6,0.01)\ndf = data.frame(\n    value = t_values,\n    t_3   = dt(t_values,3),\n    t_6   = dt(t_values,6),\n    t_10  = dt(t_values,10),\n    t_20  = dt(t_values,20),\n    Normal= dnorm(t_values)\n) |&gt;\n    tidyr::gather(\"Distribution\", \"density\", -value)\nggplot(df, aes(x=value, y=density, color=Distribution)) + \n    geom_line()\n\n\n\nFigure 12.1: t-distributions for various degrees of freedom. Note that the tails are fatter for smaller degrees of freedom, which is a result of estimating the standard deviation from the data.\n\n\n\nThe dt and dnorm functions give the density of the distributions for each point.\n\ndf2 = df |&gt; \n    group_by(Distribution) |&gt;\n    arrange(value) |&gt; \n    mutate(cdf=cumsum(density))\nggplot(df2, aes(x=value, y=cdf, color=Distribution)) + \n    geom_line()\n\n\n\n\n\n12.3.1 p-values based on Z vs t\nWhen we have a “sample” of data and want to compute the statistical significance of the difference of the mean from the population mean, we calculate the standard deviation of the sample means (standard error).\n\\[z = \\frac{x - \\mu}{\\sigma/\\sqrt{n}}\\]\nLet’s look at the relationship between the p-values of Z (from the normal distribution) vs t for a sample of data.\n\nset.seed(5432)\nsamp = rnorm(5,mean = 0.5)\nz = sqrt(length(samp)) * mean(samp) #simplifying assumption (sigma=1, mu=0)\n\nAnd the p-value if we assume we know the standard deviation:\n\npnorm(z, lower.tail = FALSE)\n\n[1] 0.02428316\n\n\nIn reality, we don’t know the standard deviation, so we have to estimate it from the data. We can do this by calculating the sample standard deviation:\n\nts = sqrt(length(samp)) * mean(samp) / sd(samp)\npnorm(ts, lower.tail = FALSE)\n\n[1] 0.0167297\n\npt(ts,df = length(samp)-1, lower.tail = FALSE)\n\n[1] 0.0503001\n\n\n\n12.3.2 Experiment\nWhen sampling from a normal distribution, we often calculate p-values to test hypotheses or determine the statistical significance of our results. The p-value represents the probability of obtaining a test statistic as extreme or more extreme than the one observed, under the null hypothesis.\nIn a typical scenario, we assume that the population mean and standard deviation are known. However, in many real-life situations, we don’t know the true population standard deviation, and we have to estimate it using the sample standard deviation (Equation 12.4). This estimation introduces some uncertainty into our calculations, which affects the p-values. When we include an estimate of the standard deviation, we switch from using the standard normal (z) distribution to the t-distribution for calculating p-values.\nWhat would happen if we used the normal distribution to calculate p-values when we use the sample standard deviation? Let’s find out!\n\nSimulate a bunch of samples of size n from the standard normal distribution\nCalculate the p-value distribution for those samples based on the normal.\nCalculate the p-value distribution for those samples based on the normal, but with the estimated standard deviation.\nCalculate the p-value distribution for those samples based on the t-distribution.\n\nCreate a function that draws a sample of size n from the standard normal distribution.\n\nzf = function(n) {\n    samp = rnorm(n)\n    z = sqrt(length(samp)) * mean(samp) / 1 #simplifying assumption (sigma=1, mu=0)\n    z\n}\n\nAnd give it a try:\n\nzf(5)\n\n[1] 0.7406094\n\n\nPerform 10000 replicates of our sampling and z-scoring. We are using the assumption that we know the population standard deviation; in this case, we do know since we are sampling from the standard normal distribution.\n\nz10k = replicate(10000,zf(5))\nhist(pnorm(z10k))\n\n\n\n\nAnd do the same, but now creating a t-score function. We are using the assumption that we don’t know the population standard deviation; in this case, we must estimate it from the data. Note the difference in the calculation of the t-score (ts) as compared to the z-score (z).\n\ntf = function(n) {\n    samp = rnorm(n)\n    # now, using the sample standard deviation since we \n    # \"don't know\" the population standard deviation\n    ts = sqrt(length(samp)) * mean(samp) / sd(samp)\n    ts\n}\n\nIf we use those t-scores and calculate the p-values based on the normal distribution, the histogram of those p-values looks like:\n\nt10k = replicate(10000,tf(5))\nhist(pnorm(t10k))\n\n\n\n\nSince we are using the normal distribution to calculate the p-values, we are, in effect, assuming that we know the population standard deviation. This assumption is incorrect, and we can see that the p-values are not uniformly distributed between 0 and 1.\nIf we use those t-scores and calculate the p-values based on the t-distribution, the histogram of those p-values looks like:\n\nhist(pt(t10k,5))\n\n\n\n\nNow, the p-values are uniformly distributed between 0 and 1, as expected.\nWhat is a qqplot and how do we use it? A qqplot is a plot of the quantiles of two distributions against each other. If the two distributions are identical, the points will fall on a straight line. If the two distributions are different, the points will deviate from the straight line. We can use a qqplot to compare the t-distribution to the normal distribution. If the t-distribution is identical to the normal distribution, the points will fall on a straight line. If the t-distribution is different from the normal distribution, the points will deviate from the straight line. In this case, we can see that the t-distribution is different from the normal distribution, as the points deviate from the straight line. What would happen if we increased the sample size? The t-distribution would approach the normal distribution, and the points would fall closer and closer to the straight line.\n\nqqplot(z10k,t10k)\nabline(0,1)"
  },
  {
    "objectID": "t-stats-and-tests.html#summary-of-t-distribution-vs-normal-distribution",
    "href": "t-stats-and-tests.html#summary-of-t-distribution-vs-normal-distribution",
    "title": "\n12  The t-statistic and t-distribution\n",
    "section": "\n12.4 Summary of t-distribution vs normal distribution",
    "text": "12.4 Summary of t-distribution vs normal distribution\nThe t-distribution is a family of probability distributions that depends on a parameter called degrees of freedom, which is related to the sample size. The t-distribution approaches the standard normal distribution as the sample size increases but has heavier tails for smaller sample sizes. This means that the t-distribution is more conservative in calculating p-values for small samples, making it harder to reject the null hypothesis. Including an estimate of the standard deviation changes the way we calculate p-values by switching from the standard normal distribution to the t-distribution, which accounts for the uncertainty introduced by estimating the population standard deviation from the sample. This adjustment is particularly important for small sample sizes, as it provides a more accurate assessment of the statistical significance of our results."
  },
  {
    "objectID": "t-stats-and-tests.html#t.test",
    "href": "t-stats-and-tests.html#t.test",
    "title": "\n12  The t-statistic and t-distribution\n",
    "section": "\n12.5 t.test",
    "text": "12.5 t.test\n\n12.5.1 One-sample\nWe are going to use the t.test function to perform a one-sample t-test. The t.test function takes a vector of values as input that represents the sample values. In this case, we’ll simulate our sample using the rnorm function and presume that our “effect-size” is 1.\n\nx = rnorm(20,1)\n# small sample\n# Just use the first 5 values of the sample\nt.test(x[1:5])\n\n\n    One Sample t-test\n\ndata:  x[1:5]\nt = 0.97599, df = 4, p-value = 0.3843\nalternative hypothesis: true mean is not equal to 0\n95 percent confidence interval:\n -1.029600  2.145843\nsample estimates:\nmean of x \n0.5581214 \n\n\nIn this case, we set up the experiment so that the null hypothesis is true (the true mean is not zero, but actually 1). However, we only have a small sample size that leads to a modest p-value.\nIncreasing the sample size allows us to see the effect more clearly.\n\nt.test(x[1:20])\n\n\n    One Sample t-test\n\ndata:  x[1:20]\nt = 3.8245, df = 19, p-value = 0.001144\nalternative hypothesis: true mean is not equal to 0\n95 percent confidence interval:\n 0.3541055 1.2101894\nsample estimates:\nmean of x \n0.7821474 \n\n\n\n12.5.2 two-sample\n\nx = rnorm(10,0.5)\ny = rnorm(10,-0.5)\nt.test(x,y)\n\n\n    Welch Two Sample t-test\n\ndata:  x and y\nt = 3.4296, df = 17.926, p-value = 0.003003\nalternative hypothesis: true difference in means is not equal to 0\n95 percent confidence interval:\n 0.5811367 2.4204048\nsample estimates:\n mean of x  mean of y \n 0.7039205 -0.7968502 \n\n\n\n12.5.3 from a data.frame\nIn some situations, you may have data and groups as columns in a data.frame. See the following data.frame, for example\n\ndf = data.frame(value=c(x,y),group=as.factor(rep(c('g1','g2'),each=10)))\ndf\n\n         value group\n1   1.12896674    g1\n2  -1.26838101    g1\n3   1.04577597    g1\n4   1.69075585    g1\n5   0.18672204    g1\n6   1.99715092    g1\n7   1.15424947    g1\n8   0.37671442    g1\n9  -0.09565723    g1\n10  0.82290783    g1\n11 -1.48530261    g2\n12 -1.29200440    g2\n13 -0.18778362    g2\n14  0.59205742    g2\n15 -2.10065248    g2\n16 -0.29961560    g2\n17 -0.38985115    g2\n18 -2.47126235    g2\n19 -0.63654380    g2\n20  0.30245611    g2\n\n\nR allows us to perform a t-test using the formula notation.\n\nt.test(value ~ group, data=df)\n\n\n    Welch Two Sample t-test\n\ndata:  value by group\nt = 3.4296, df = 17.926, p-value = 0.003003\nalternative hypothesis: true difference in means between group g1 and group g2 is not equal to 0\n95 percent confidence interval:\n 0.5811367 2.4204048\nsample estimates:\nmean in group g1 mean in group g2 \n       0.7039205       -0.7968502 \n\n\nYou read that as value is a function of group. In practice, this will do a t-test between the values in g1 vs g2.\n\n12.5.4 Equivalence to linear model\n\nt.test(value ~ group, data=df, var.equal=TRUE)\n\n\n    Two Sample t-test\n\ndata:  value by group\nt = 3.4296, df = 18, p-value = 0.002989\nalternative hypothesis: true difference in means between group g1 and group g2 is not equal to 0\n95 percent confidence interval:\n 0.5814078 2.4201337\nsample estimates:\nmean in group g1 mean in group g2 \n       0.7039205       -0.7968502 \n\n\nThis is equivalent to:\n\nres = lm(value ~ group, data=df)\nsummary(res)\n\n\nCall:\nlm(formula = value ~ group, data = df)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-1.9723 -0.5600  0.2511  0.5252  1.3889 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)   \n(Intercept)   0.7039     0.3094   2.275  0.03538 * \ngroupg2      -1.5008     0.4376  -3.430  0.00299 **\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.9785 on 18 degrees of freedom\nMultiple R-squared:  0.3952,    Adjusted R-squared:  0.3616 \nF-statistic: 11.76 on 1 and 18 DF,  p-value: 0.002989"
  },
  {
    "objectID": "t-stats-and-tests.html#power-calculations",
    "href": "t-stats-and-tests.html#power-calculations",
    "title": "\n12  The t-statistic and t-distribution\n",
    "section": "\n12.6 Power calculations",
    "text": "12.6 Power calculations\nThe power of a statistical test is the probability that the test will reject the null hypothesis when the alternative hypothesis is true. In other words, the power of a statistical test is the probability of not making a Type II error. The power of a statistical test depends on the significance level (alpha), the sample size, and the effect size.\nThe power.t.test function can be used to calculate the power of a one-sample t-test.\nLooking at help(\"power.t.test\"), we see that the function takes the following arguments:\n\n\nn - sample size\n\ndelta - effect size\n\nsd - standard deviation of the sample\n\nsig.level - significance level\n\npower - power\n\nWe need to supply four of these arguments to calculate the fifth. For example, if we want to calculate the power of a one-sample t-test with a sample size of 5, a standard deviation of 1, and an effect size of 1, we can use the following command:\n\npower.t.test(n = 5, delta = 1, sd = 1, sig.level = 0.05)\n\n\n     Two-sample t test power calculation \n\n              n = 5\n          delta = 1\n             sd = 1\n      sig.level = 0.05\n          power = 0.2859276\n    alternative = two.sided\n\nNOTE: n is number in *each* group\n\n\nThis gives a nice summary of the power calculation. We can also extract the power value from the result:\n\npower.t.test(n = 5, delta = 1, sd = 1, \n             sig.level = 0.05, type='one.sample')$power\n\n[1] 0.4013203\n\n\n\n\n\n\n\n\nTip\n\n\n\nWhen getting results from a function that don’t look “computable” such as those from power.t.test, you can use the $ operator to extract the value you want. In this case, we want the power value from the result of power.t.test.\nHow would you know what to extract? You can use the names function or the str function to see the structure of the result. For example:\n\nnames(power.t.test(n = 5, delta = 1, sd = 1, \n             sig.level = 0.05, type='one.sample'))\n\n[1] \"n\"           \"delta\"       \"sd\"          \"sig.level\"   \"power\"      \n[6] \"alternative\" \"note\"        \"method\"     \n\n# or \nstr(power.t.test(n = 5, delta = 1, sd = 1, \n             sig.level = 0.05, type='one.sample'))\n\nList of 8\n $ n          : num 5\n $ delta      : num 1\n $ sd         : num 1\n $ sig.level  : num 0.05\n $ power      : num 0.401\n $ alternative: chr \"two.sided\"\n $ note       : NULL\n $ method     : chr \"One-sample t test power calculation\"\n - attr(*, \"class\")= chr \"power.htest\"\n\n\n\n\nAlternatively, we may know a lot about our experimental system and want to calculate the sample size needed to achieve a certain power. For example, if we want to achieve a power of 0.8 with a standard deviation of 1 and an effect size of 1, we can use the following command:\n\npower.t.test(delta = 1, sd = 1, sig.level = 0.05, power = 0.8, type = \"one.sample\")\n\n\n     One-sample t test power calculation \n\n              n = 9.937864\n          delta = 1\n             sd = 1\n      sig.level = 0.05\n          power = 0.8\n    alternative = two.sided\n\n\nThe power.t.test function is convenient and quite fast. As we’ve seen before, though, sometimes the distribution of the test statistics is now easily calculated. In those cases, we can use simulation to calculate the power of a statistical test. For example, if we want to calculate the power of a one-sample t-test with a sample size of 5, a standard deviation of 1, and an effect size of 1, we can use the following command:\n\nsim_t_test_pval &lt;- function(n = 5, delta = 1, sd = 1, sig.level = 0.05) {\n    x = rnorm(n, delta, sd)\n    t.test(x)$p.value &lt;= sig.level\n}\npow = mean(replicate(1000, sim_t_test_pval()))\npow\n\n[1] 0.405\n\n\nLet’s break this down. First, we define a function called sim_t_test_pval that takes the same arguments as the power.t.test function. Inside the function, we simulate a sample of size n from a normal distribution with mean delta and standard deviation sd. Then, we perform a one-sample t-test on the sample and return a logical value indicating whether the p-value is less than the significance level. Next, we use the replicate function to repeat the simulation 1000 times. Finally, we calculate the proportion of simulations in which the p-value was less than the significance level. This proportion is an estimate of the power of the one-sample t-test.\nLet’s compare the results of the power.t.test function and our simulation-based approach:\n\npower.t.test(n = 5, delta = 1, sd = 1, sig.level = 0.05, type='one.sample')$power\n\n[1] 0.4013203\n\nmean(replicate(1000, sim_t_test_pval(n = 5, delta = 1, sd = 1, sig.level = 0.05)))\n\n[1] 0.414"
  },
  {
    "objectID": "t-stats-and-tests.html#resources",
    "href": "t-stats-and-tests.html#resources",
    "title": "\n12  The t-statistic and t-distribution\n",
    "section": "\n12.7 Resources",
    "text": "12.7 Resources\nSee the pwr package for more information on power calculations.\n\n\n\n\nStudent. 1908. “The Probable Error of a Mean.” Biometrika 6 (1): 1–25. https://doi.org/10.2307/2331554."
  },
  {
    "objectID": "kmeans.html#history-of-the-k-means-algorithm",
    "href": "kmeans.html#history-of-the-k-means-algorithm",
    "title": "\n13  K-means clustering\n",
    "section": "\n13.1 History of the k-means algorithm",
    "text": "13.1 History of the k-means algorithm\nThe k-means clustering algorithm was first proposed by Stuart Lloyd in 1957 as a technique for pulse-code modulation. However, it was not published until 1982. In 1965, Edward W. Forgy published an essentially identical method, which became widely known as the k-means algorithm. Since then, k-means clustering has become one of the most popular unsupervised learning techniques in data analysis and machine learning.\nK-means clustering is a method for finding patterns or groups in a dataset. It is an unsupervised learning technique, meaning that it doesn’t rely on previously labeled data for training. Instead, it identifies structures or patterns directly from the data based on the similarity between data points (see Figure 13.1).\n\n\nFigure 13.1: K-means clustering takes a dataset and divides it into k clusters.\n\nIn simple terms, k-means clustering aims to divide a dataset into k distinct groups or clusters, where each data point belongs to the cluster with the nearest mean (average). The goal is to minimize the variability within each cluster while maximizing the differences between clusters. This helps to reveal hidden patterns or relationships in the data that might not be apparent otherwise."
  },
  {
    "objectID": "kmeans.html#the-k-means-algorithm",
    "href": "kmeans.html#the-k-means-algorithm",
    "title": "\n13  K-means clustering\n",
    "section": "\n13.2 The k-means algorithm",
    "text": "13.2 The k-means algorithm\nThe k-means algorithm follows these general steps:\n\nChoose the number of clusters k.\nInitialize the cluster centroids randomly by selecting k data points from the dataset.\nAssign each data point to the nearest centroid.\nUpdate the centroids by computing the mean of all the data points assigned to each centroid.\nRepeat steps 3 and 4 until the centroids no longer change or a certain stopping criterion is met (e.g., a maximum number of iterations).\n\nThe algorithm converges when the centroids stabilize or no longer change significantly. The final clusters represent the underlying patterns or structures in the data. Advantages and disadvantages of k-means clustering"
  },
  {
    "objectID": "kmeans.html#pros-and-cons-of-k-means-clustering",
    "href": "kmeans.html#pros-and-cons-of-k-means-clustering",
    "title": "\n13  K-means clustering\n",
    "section": "\n13.3 Pros and cons of k-means clustering",
    "text": "13.3 Pros and cons of k-means clustering\nCompared to other clustering algorithms, k-means has several advantages:\n\n\nSimplicity and ease of implementation\n\nThe k-means algorithm is relatively straightforward and can be easily implemented, even for large datasets.\n\n\n\nScalability\n\nThe algorithm can be adapted for large datasets using various optimization techniques or parallel processing.\n\n\n\nSpeed\n\nK-means is generally faster than other clustering algorithms, especially when the number of clusters k is small.\n\n\n\nInterpretability\n\nThe results of k-means clustering are easy to understand, as the algorithm assigns each data point to a specific cluster based on its similarity to the cluster’s centroid.\n\n\n\nHowever, k-means clustering has several disadvantages as well:\n\n\nChoice of k\n\nSelecting the appropriate number of clusters can be challenging and often requires domain knowledge or experimentation. A poor choice of k may yield poor results.\n\n\n\nSensitivity to initial conditions\n\nThe algorithm’s results can vary depending on the initial placement of centroids. To overcome this issue, the algorithm can be run multiple times with different initializations and the best solution can be chosen based on a criterion (e.g., minimizing within-cluster variation).\n\n\n\nAssumes spherical clusters\n\nK-means assumes that clusters are spherical and evenly sized, which may not always be the case in real-world datasets. This can lead to poor performance if the underlying clusters have different shapes or densities.\n\n\n\nSensitivity to outliers\n\nThe algorithm is sensitive to outliers, which can heavily influence the position of centroids and the final clustering result. Preprocessing the data to remove or mitigate the impact of outliers can help improve the performance of k-means clustering.\n\n\n\nDespite limitations, k-means clustering remains a popular and widely used method for exploring and analyzing data, particularly in biological data analysis, where identifying patterns and relationships can provide valuable insights into complex systems and processes."
  },
  {
    "objectID": "kmeans.html#an-example-of-k-means-clustering",
    "href": "kmeans.html#an-example-of-k-means-clustering",
    "title": "\n13  K-means clustering\n",
    "section": "\n13.4 An example of k-means clustering",
    "text": "13.4 An example of k-means clustering\n\n13.4.1 The data and experimental background\nThe data we are going to use are from DeRisi, Iyer, and Brown (1997). From their abstract:\n\nDNA microarrays containing virtually every gene of Saccharomyces cerevisiae were used to carry out a comprehensive investigation of the temporal program of gene expression accompanying the metabolic shift from fermentation to respiration. The expression profiles observed for genes with known metabolic functions pointed to features of the metabolic reprogramming that occur during the diauxic shift, and the expression patterns of many previously uncharacterized genes provided clues to their possible functions.\n\nThese data are available from NCBI GEO as GSE28.\nIn the case of the baker’s or brewer’s yeast Saccharomyces cerevisiae growing on glucose with plenty of aeration, the diauxic growth pattern is commonly observed in batch culture. During the first growth phase, when there is plenty of glucose and oxygen available, the yeast cells prefer glucose fermentation to aerobic respiration even though aerobic respiration is the more efficient pathway to grow on glucose. This experiment profiles gene expression for 6400 genes over a time course during which the cells are undergoing a diauxic shift.\nThe data in deRisi et al. have no replicates and are time course data. Sometimes, seeing how groups of genes behave can give biological insight into the experimental system or the function of individual genes. We can use clustering to group genes that have a similar expression pattern over time and then potentially look at the genes that do so.\nOur goal, then, is to use kmeans clustering to divide highly variable (informative) genes into groups and then to visualize those groups."
  },
  {
    "objectID": "kmeans.html#getting-data",
    "href": "kmeans.html#getting-data",
    "title": "\n13  K-means clustering\n",
    "section": "\n13.5 Getting data",
    "text": "13.5 Getting data\nThese data were deposited at NCBI GEO back in 2002. GEOquery can pull them out easily.\n\nlibrary(GEOquery)\ngse = getGEO(\"GSE28\")[[1]]\nclass(gse)\n\n[1] \"ExpressionSet\"\nattr(,\"package\")\n[1] \"Biobase\"\n\n\nGEOquery is a little dated and was written before the SummarizedExperiment existed. However, Bioconductor makes a conversion from the old ExpressionSet that GEOquery uses to the SummarizedExperiment that we see so commonly used now.\n\nlibrary(SummarizedExperiment)\ngse = as(gse, \"SummarizedExperiment\")\ngse\n\nclass: SummarizedExperiment \ndim: 6400 7 \nmetadata(3): experimentData annotation protocolData\nassays(1): exprs\nrownames(6400): 1 2 ... 6399 6400\nrowData names(20): ID ORF ... FAILED IS_CONTAMINATED\ncolnames(7): GSM887 GSM888 ... GSM892 GSM893\ncolData names(33): title geo_accession ... supplementary_file\n  data_row_count\n\n\nTaking a quick look at the colData(), it might be that we want to reorder the columns a bit.\n\ncolData(gse)$title\n\n[1] \"diauxic shift timecourse: 15.5 hr\" \"diauxic shift timecourse: 0 hr\"   \n[3] \"diauxic shift timecourse: 18.5 hr\" \"diauxic shift timecourse: 9.5 hr\" \n[5] \"diauxic shift timecourse: 11.5 hr\" \"diauxic shift timecourse: 13.5 hr\"\n[7] \"diauxic shift timecourse: 20.5 hr\"\n\n\nSo, we can reorder by hand to get the time course correct:\n\ngse = gse[, c(2,4,5,6,1,3,7)]"
  },
  {
    "objectID": "kmeans.html#preprocessing",
    "href": "kmeans.html#preprocessing",
    "title": "\n13  K-means clustering\n",
    "section": "\n13.6 Preprocessing",
    "text": "13.6 Preprocessing\nIn gene expression data analysis, the primary objective is often to identify genes that exhibit significant differences in expression levels across various conditions, such as diseased vs. healthy samples or different time points in a time-course experiment. However, gene expression datasets are typically large, noisy, and contain numerous genes that do not exhibit substantial changes in expression levels. Analyzing all genes in the dataset can be computationally intensive and may introduce noise or false positives in the results.\nOne common approach to reduce the complexity of the dataset and focus on the most informative genes is to subset the genes based on their standard deviation in expression levels across the samples. The standard deviation is a measure of dispersion or variability in the data, and genes with high standard deviations have more variation in their expression levels across the samples.\nBy selecting genes with high standard deviations, we focus on genes that show relatively large changes in expression levels across different conditions. These genes are more likely to be biologically relevant and involved in the underlying processes or pathways of interest. In contrast, genes with low standard deviations exhibit little or no change in expression levels and are less likely to be informative for the analysis. It turns out that applying filtering based on criteria such as standard deviation can also increase power and reduce false positives in the analysis (Bourgon, Gentleman, and Huber 2010).\nTo subset the genes for analysis based on their standard deviation, the following steps can be followed: Calculate the standard deviation of each gene’s expression levels across all samples. Set a threshold for the standard deviation, which can be determined based on domain knowledge, data distribution, or a specific percentile of the standard deviation values (e.g., selecting the top 10% or 25% of genes with the highest standard deviations). Retain only the genes with a standard deviation above the chosen threshold for further analysis.\nBy subsetting the genes based on their standard deviation, we can reduce the complexity of the dataset, speed up the subsequent analysis, and increase the likelihood of detecting biologically meaningful patterns and relationships in the gene expression data. The threshold for the standard deviation cutoff is rather arbitrary, so it may be beneficial to try a few to check for sensitivity of findings.\n\nsds = apply(assays(gse)[[1]], 1, sd)\nhist(sds)\n\n\n\nFigure 13.2: Histogram of standard deviations for all genes in the deRisi dataset.\n\n\n\nExamining the plot, we can see that the most highly variable genes have an sd &gt; 0.8 or so (arbitrary). We can, for convenience, create a new SummarizedExperiment that contains only our most highly variable genes.\n\nidx = sds&gt;0.8 & !is.na(sds)\ngse_sub = gse[idx,]"
  },
  {
    "objectID": "kmeans.html#clustering",
    "href": "kmeans.html#clustering",
    "title": "\n13  K-means clustering\n",
    "section": "\n13.7 Clustering",
    "text": "13.7 Clustering\nNow, gse_sub contains a subset of our data.\nThe kmeans function takes a matrix and the number of clusters as arguments.\n\nk = 4\nkm = kmeans(assays(gse_sub)[[1]], 4)\n\nThe km kmeans result contains a vector, km$cluster, which gives the cluster associated with each gene. We can plot the genes for each cluster to see how these different genes behave.\n\nexpression_values = assays(gse_sub)[[1]]\npar(mfrow=c(2,2), mar=c(3,4,1,2)) # this allows multiple plots per page\nfor(i in 1:k) {\n    matplot(t(expression_values[km$cluster==i, ]), type='l', ylim=c(-3,3),\n            ylab = paste(\"cluster\", i))\n}\n\n\n\nFigure 13.3: Gene expression profiles for the four clusters identified by k-means clustering. Each line represents a gene in the cluster, and each column represents a time point in the experiment. Each cluster shows a distinct trend where the genes in the cluster are potentially co-regulated.\n\n\n\nTry this with different size k. Perhaps go back to choose more genes (using a smaller cutoff for sd)."
  },
  {
    "objectID": "kmeans.html#summary",
    "href": "kmeans.html#summary",
    "title": "\n13  K-means clustering\n",
    "section": "\n13.8 Summary",
    "text": "13.8 Summary\nIn this lesson, we have learned how to use k-means clustering to identify groups of genes that behave similarly over time. We have also learned how to subset our data to focus on the most informative genes.\n\n\n\n\nBourgon, Richard, Robert Gentleman, and Wolfgang Huber. 2010. “Independent Filtering Increases Detection Power for High-Throughput Experiments.” Proceedings of the National Academy of Sciences 107 (21): 9546–51. https://doi.org/10.1073/pnas.0914005107.\n\n\nDeRisi, J. L., V. R. Iyer, and P. O. Brown. 1997. “Exploring the Metabolic and Genetic Control of Gene Expression on a Genomic Scale.” Science (New York, N.Y.) 278 (5338): 680–86. https://doi.org/10.1126/science.278.5338.680."
  },
  {
    "objectID": "ml_practical.html#what-is-machine-learning",
    "href": "ml_practical.html#what-is-machine-learning",
    "title": "\n14  Machine Learning\n",
    "section": "\n14.1 What is Machine Learning?",
    "text": "14.1 What is Machine Learning?\nMachine learning is a subfield of artificial intelligence that focuses on the development of algorithms and models that enable computers to learn and make decisions or predictions without explicit programming. It has emerged as a powerful tool for solving complex problems across various industries, including healthcare, finance, marketing, and natural language processing. This chapter provides an overview of machine learning, its types, key concepts, applications, and challenges.\nMachine learning in biology is a really broad topic. Greener et al. (2022) present a nice overview of the different types of machine learning methods that are used in biology. Libbrecht and Noble (2015) also present an early review of machine learning in genetics and genomics."
  },
  {
    "objectID": "ml_practical.html#classes-of-machine-learning",
    "href": "ml_practical.html#classes-of-machine-learning",
    "title": "\n14  Machine Learning\n",
    "section": "\n14.2 Classes of Machine Learning",
    "text": "14.2 Classes of Machine Learning\n\n14.2.1 Supervised learning\nSupervised learning is a type of machine learning where the model learns from labeled data, i.e., input-output pairs, to make predictions. It includes tasks like regression (predicting continuous values) and classification (predicting discrete classes or categories).\n\n14.2.2 Unsupervised learning\nUnsupervised learning involves learning from unlabeled data, where the model discovers patterns or structures within the data. Common unsupervised learning tasks include clustering (grouping similar data points), dimensionality reduction (reducing the number of features or variables), and anomaly detection (identifying unusual data points).\n\n\n\n\n\n\nTerminology and Concepts\n\n\n\n\n\nData\n\nData is the foundation of machine learning and can be structured (tabular) or unstructured (text, images, audio). It is usually divided into training, validation, and testing sets for model development and evaluation.\n\n\n\nFeatures\n\nFeatures are the variables or attributes used to describe the data points. Feature engineering and selection are crucial steps in machine learning to improve model performance and interpretability.\n\n\n\nModels and Algorithms\n\nModels are mathematical representations of the relationship between features and the target variable(s). Algorithms are the methods used to train models, such as linear regression, decision trees, and neaural networks.\n\n\n\nHyperparameters and Tuning\n\nHyperparameters are adjustable parameters that control the learning process of an algorithm. Tuning involves finding the optimal set of hyperparameters to improve model performance.\n\n\n\nEvaluation Metrics\n\nEvaluation metrics quantify the performance of a model, such as accuracy, precision, recall, F1-score (for classification), and mean squared error, R-squared (for regression).\n\n\n\n\n\n\nCodeset.seed(123)\nsinsim &lt;- function(n,sd=0.1) {\n  x &lt;- seq(0,1,length.out=n)\n  y &lt;- sin(2*pi*x) + rnorm(n,0,sd)\n  return(data.frame(x=x,y=y))\n}\ndat &lt;- sinsim(100,0.25)\nlibrary(ggplot2)\nlibrary(patchwork)\np_base &lt;- ggplot(dat,aes(x=x,y=y)) +\n geom_point(alpha=0.7) +\n theme_bw()\np_lm &lt;- p_base + \n geom_smooth(method=\"lm\", se=FALSE, alpha=0.6, formula = y ~ x) \np_lmsin &lt;- p_base +\n geom_smooth(method=\"lm\",formula=y~sin(2*pi*x), se=FALSE, alpha=0.6) \np_loess_wide &lt;- p_base +\n  geom_smooth(method=\"loess\",span=0.5, se=FALSE, alpha=0.6, formula = y ~ x) \np_loess_narrow &lt;- p_base + \n geom_smooth(method=\"loess\",span=0.1, se=FALSE, alpha=0.6, formula = y ~ x) \np_lm + p_lmsin + p_loess_wide + p_loess_narrow + plot_layout(ncol=2) +\n  plot_annotation(tag_levels = 'A') & \n  theme(plot.tag = element_text(size = 8))\n\n\n\nFigure 14.1: Data simulated according to the function \\(f(x) = sin(2 \\pi x) + N(0,0.25)\\) fitted with four different models. A) A simple linear model demonstrates underfitting. B) A linear model with a sin function (\\(y = sin(2 \\pi x)\\)) and C) a loess model with a wide span (0.5) demonstrate good fits. D) A loess model with a narrow span (0.1) is a good example of overfitting.\n\n\n\nIn Figure 14.1, we simulate data according to the function \\(f(x) = sin(2 \\pi x) + N(0,0.25)\\) and fit four different models. Choosing a model that is too simple (A) will result in underfitting the data, while choosing a model that is too complex (D) will result in overfitting the data.\nWhen thinking about machine learning, it can help to have a simple framework in mind. In Figure 14.2, we present a simple view of machine learning according to the scikit-learn package.\n\n\nFigure 14.2: A simple view of machine learning according the sklearn.\n\nWe’re going to focus on supervised learning here. Here is a rough schematic (see Figure 14.3) of the supervised learning process from the mlr3 book.\n\n\nFigure 14.3: A schematic of the supervised learning process.\n\nIn nearly all cases, we will have a training set and a test set. The training set is used to train the model, and the test set is used to evaluate the model (see Figure 14.4). Even when we don’t have a separate test set, we will usually create one by splitting the data.\n\n\nFigure 14.4: Training and testing sets."
  },
  {
    "objectID": "ml_practical.html#supervised-learning-1",
    "href": "ml_practical.html#supervised-learning-1",
    "title": "\n14  Machine Learning\n",
    "section": "\n14.3 Supervised Learning",
    "text": "14.3 Supervised Learning\n\n14.3.1 Linear regression\nIn statistics, linear regression is a linear approach for modelling the relationship between a scalar response and one or more explanatory variables (also known as dependent and independent variables). The case of one explanatory variable is called simple linear regression; for more than one, the process is called multiple linear regression. This term is distinct from multivariate linear regression, where multiple correlated dependent variables are predicted, rather than a single scalar variable.\nIn linear regression, the relationships are modeled using linear predictor functions whose unknown model parameters are estimated from the data. Such models are called linear models. Most commonly, the conditional mean of the response given the values of the explanatory variables (or predictors) is assumed to be an affine function of those values; less commonly, the conditional median or some other quantile is used. Like all forms of regression analysis, linear regression focuses on the conditional probability distribution of the response given the values of the predictors, rather than on the joint probability distribution of all of these variables, which is the domain of multivariate analysis.\nLinear regression was the first type of regression analysis to be studied rigorously, and to be used extensively in practical applications. This is because models which depend linearly on their unknown parameters are easier to fit than models which are non-linearly related to their parameters and because the statistical properties of the resulting estimators are easier to determine.\n\n14.3.2 K-nearest Neighbor\n\n\n\n\nFigure. The k-nearest neighbor algorithm can be used for regression or classification.\n\n\n\nThe k-nearest neighbors algorithm (k-NN) is a non-parametric supervised learning method first developed by Evelyn Fix and Joseph Hodges in 1951, and later expanded by Thomas Cover. It is used for classification and regression. In both cases, the input consists of the k closest training examples in a data set.\nThe k-nearest neighbor (k-NN) algorithm is a simple, yet powerful, supervised machine learning method used for classification and regression tasks. It is an instance-based, non-parametric learning method that stores the entire training dataset and makes predictions based on the similarity between data points. The underlying principle of the k-NN algorithm is that similar data points (those that are close to each other in multidimensional space) are likely to have similar outcomes or belong to the same class.\nHere’s a description of how the k-NN algorithm works:\n\nDetermine the value of k: The first step is to choose the number of nearest neighbors (k) to consider when making predictions. The value of k is a user-defined hyperparameter and can significantly impact the algorithm’s performance. A small value of k can lead to overfitting, while a large value may result in underfitting.\nCompute distance: Calculate the distance between the new data point (query point) and each data point in the training dataset. The most common distance metrics used are Euclidean, Manhattan, and Minkowski distance. The choice of distance metric depends on the problem and the nature of the data.\nFind k-nearest neighbors: Identify the k data points in the training dataset that are closest to the query point, based on the chosen distance metric.\nMake predictions: Once the k-nearest neighbors are identified, the final step is to make predictions. The prediction for the query point can be made in two ways:\n\nFor classification, determine the class labels of the k-nearest neighbors and assign the class label with the highest frequency (majority vote) to the query point. In case of a tie, one can choose the class with the smallest average distance to the query point or randomly select one among the tied classes.\nFor regression tasks, the k-NN algorithm follows a similar process, but instead of majority voting, it calculates the mean (or median) of the target values of the k-nearest neighbors and assigns it as the prediction for the query point.\n\n\n\nThe k-NN algorithm is known for its simplicity, ease of implementation, and ability to handle multi-class problems. However, it has some drawbacks, such as high computational cost (especially for large datasets), sensitivity to the choice of k and distance metric, and poor performance with high-dimensional or noisy data. Scaling and preprocessing the data, as well as using dimensionality reduction techniques, can help mitigate some of these issues.\n\nIn k-NN classification, the output is a class membership. An object is classified by a plurality vote of its neighbors, with the object being assigned to the class most common among its k nearest neighbors (k is a positive integer, typically small). If k = 1, then the object is simply assigned to the class of that single nearest neighbor.\nIn k-NN regression, the output is the property value for the object. This value is the average of the values of k nearest neighbors.\n\nk-NN is a type of classification where the function is only approximated locally and all computation is deferred until function evaluation. Since this algorithm relies on distance for classification, if the features represent different physical units or come in vastly different scales then normalizing the training data can improve its accuracy dramatically.\nBoth for classification and regression, a useful technique can be to assign weights to the contributions of the neighbors, so that the nearer neighbors contribute more to the average than the more distant ones. For example, a common weighting scheme consists in giving each neighbor a weight of 1/d, where d is the distance to the neighbor.\nThe neighbors are taken from a set of objects for which the class (for k-NN classification) or the object property value (for k-NN regression) is known. This can be thought of as the training set for the algorithm, though no explicit training step is required."
  },
  {
    "objectID": "ml_practical.html#penalized-regression",
    "href": "ml_practical.html#penalized-regression",
    "title": "\n14  Machine Learning\n",
    "section": "\n14.4 Penalized regression",
    "text": "14.4 Penalized regression\nAdapted from http://www.sthda.com/english/articles/37-model-selection-essentials-in-r/153-penalized-regression-essentials-ridge-lasso-elastic-net/.\nPenalized regression is a type of regression analysis that introduces a penalty term to the loss function in order to prevent overfitting and improve the model’s ability to generalize. Remember that in regression, the loss function is the sum of squares Equation 14.1.\n\\[L = \\sum_{i=0}^{n}{(\\hat{y}_i - y_i)}^2 \\tag{14.1}\\]\nIn Equation 14.1, \\(\\hat{y}_i\\) is the predicted output, \\(y_i\\) is the actual output, and n is the number of observations. The goal of regression is to minimize the loss function by finding the optimal values of the model parameters or coefficients. The model parameters are estimated using the training data. The model is then evaluated using the test data. If the model performs well on the training data but poorly on the test data, it is said to be overfit. Overfitting occurs when the model learns the training data too well, including the noise, and is not able to generalize well to new data. This is a common problem in machine learning, particularly when there are a large number of predictors compared to the number of observations, and can be addressed by penalized regression.\nThe two most common types of penalized regression are Ridge Regression (L2 penalty) and LASSO Regression (L1 penalty). Both Ridge and LASSO help to reduce model complexity and prevent over-fitting which may result from simple linear regression. However, the choice between Ridge and LASSO depends on the situation and the dataset at hand. If feature selection is important for the interpretation of the model, LASSO might be preferred. If the goal is prediction accuracy and the model needs to retain all features, Ridge might be the better choice.\n\n14.4.1 Ridge regression\nRidge regression shrinks the regression coefficients, so that variables, with minor contribution to the outcome, have their coefficients close to zero. The shrinkage of the coefficients is achieved by penalizing the regression model with a penalty term called L2-norm, which is the sum of the squared coefficients. The amount of the penalty can be fine-tuned using a constant called lambda (λ). Selecting a good value for λ is critical. When λ=0, the penalty term has no effect, and ridge regression will produce the classical least square coefficients. However, as λ increases to infinite, the impact of the shrinkage penalty grows, and the ridge regression coefficients will get close zero. The loss function for Ridge Regression is:\n\\[L = \\sum_{i=0}^{n}{(\\hat{y}_i - y_i)}^2 + \\lambda\\sum_{j=0}^{k} \\beta_j^2 \\tag{14.2}\\]\nHere, \\(\\hat{y}_i\\) is the predicted output, \\(y_i\\) is the actual output, \\({\\beta_j}\\) represents the model parameters or coefficients, and λ is the regularization parameter. The second term, λ∑βj^2, is the penalty term where all parameters are squared and summed. Ridge regression tends to shrink the coefficients but doesn’t necessarily zero them.\nNote that, in contrast to the ordinary least square regression, ridge regression is highly affected by the scale of the predictors. Therefore, it is better to standardize (i.e., scale) the predictors before applying the ridge regression (James et al. 2014), so that all the predictors are on the same scale. The standardization of a predictor x, can be achieved using the formula \\(x' = \\frac{x}{sd(x)}\\), where \\(sd(x)\\) is the standard deviation of \\(x\\). The consequence of this is that, all standardized predictors will have a standard deviation of one allowing the final fit to not depend on the scale on which the predictors are measured.\nOne important advantage of the ridge regression, is that it still performs well, compared to the ordinary least square method (see Equation 14.1), in a situation where you have a large multivariate data with the number of predictors (p) larger than the number of observations (n). One disadvantage of the ridge regression is that, it will include all the predictors in the final model, unlike the stepwise regression methods, which will generally select models that involve a reduced set of variables. Ridge regression shrinks the coefficients towards zero, but it will not set any of them exactly to zero. The LASSO regression is an alternative that overcomes this drawback.\n\n14.4.2 LASSO regression\nLASSO stands for Least Absolute Shrinkage and Selection Operator. It shrinks the regression coefficients toward zero by penalizing the regression model with a penalty term called L1-norm, which is the sum of the absolute coefficients. In the case of LASSO regression, the penalty has the effect of forcing some of the coefficient estimates, with a minor contribution to the model, to be exactly equal to zero. This means that, LASSO can be also seen as an alternative to the subset selection methods for performing variable selection in order to reduce the complexity of the model. As in ridge regression, selecting a good value of \\(\\lambda\\) for the LASSO is critical. The loss function for LASSO Regression is:\n\\[L = \\sum_{i=0}^{n}{(\\hat{y}_i - y_i)}^2 + \\lambda\\sum_{j=0}^{k} |\\beta_j| \\tag{14.3}\\]\nSimilar to Ridge, ŷi is the predicted output, yi is the actual output, βj represents the model parameters or coefficients, and λ is the regularization parameter. The second term, λ∑|βj|, is the penalty term where the absolute values of all parameters are summed. LASSO regression tends to shrink the coefficients and can zero out some of them, effectively performing variable selection.\nOne obvious advantage of LASSO regression over ridge regression, is that it produces simpler and more interpretable models that incorporate only a reduced set of the predictors. However, neither ridge regression nor the LASSO will universally dominate the other. Generally, LASSO might perform better in a situation where some of the predictors have large coefficients, and the remaining predictors have very small coefficients. Ridge regression will perform better when the outcome is a function of many predictors, all with coefficients of roughly equal size (James et al. 2014).\nCross-validation methods can be used for identifying which of these two techniques is better on a particular data set.\n\n14.4.3 Elastic Net\nElastic Net produces a regression model that is penalized with both the L1-norm and L2-norm. The consequence of this is to effectively shrink coefficients (like in ridge regression) and to set some coefficients to zero (as in LASSO).\n\n14.4.4 Classification and Regression Trees (CART)\nDecision Tree Learning is supervised learning approach used in statistics, data mining and machine learning. In this formalism, a classification or regression decision tree is used as a predictive model to draw conclusions about a set of observations. Decision trees are a popular machine learning method used for both classification and regression tasks. They are hierarchical, tree-like structures that model the relationship between features and the target variable by recursively splitting the data into subsets based on the feature values. Each internal node in the tree represents a decision or test on a feature, and each branch represents the outcome of that test. The leaf nodes contain the final prediction, which is the majority class for classification tasks or the mean/median of the target values for regression tasks.\n\n\n\n\nFigure 14.5: An example of a decision tree that performs classification, also sometimes called a classification tree.\n\n\n\nHere’s an overview of the decision tree learning process:\n\nSelect the best feature and split value: Start at the root node and choose the feature and split value that results in the maximum reduction of impurity (or increase in information gain) in the child nodes. For classification tasks, impurity measures like Gini index or entropy are commonly used, while for regression tasks, mean squared error (MSE) or mean absolute error (MAE) can be used.\nSplit the data: Partition the dataset into subsets based on the chosen feature and split value.\nRecursion: Repeat steps 1 and 2 for each subset until a stopping criterion is met. Stopping criteria can include reaching a maximum tree depth, a minimum number of samples per leaf, or no further improvement in impurity.\nPrune the tree (optional): To reduce overfitting, decision trees can be pruned by removing branches that do not significantly improve the model’s performance on the validation dataset. This can be done using techniques like reduced error pruning or cost-complexity pruning.\n\nDecision trees have several advantages, such as:\n\n\nInterpretability\n\nThey are easy to understand, visualize, and explain, even for non-experts.\n\n\n\nMinimal data preprocessing\n\nDecision trees can handle both numerical and categorical data, and they are robust to outliers and missing values.\n\n\n\nNon-linear relationships\n\nThey can capture complex non-linear relationships between features and the target variable.\n\n\n\nHowever, decision trees also have some drawbacks:\n\n\nOverfitting\n\nThey are prone to overfitting, especially when the tree is deep or has few samples per leaf. Pruning and setting stopping criteria can help mitigate this issue.\n\n\n\nInstability\n\nSmall changes in the data can lead to different tree structures. This can be addressed by using ensemble methods like random forests or gradient boosting machines (GBMs).\n\n\n\nGreedy learning\n\nDecision tree algorithms use a greedy approach, meaning they make locally optimal choices at each node. This may not always result in a globally optimal tree.\n\n\n\nDespite these limitations, decision trees are widely used in various applications due to their simplicity, interpretability, and ability to handle diverse data types.\n\n14.4.5 RandomForest\nRandom forests or random decision forests is an ensemble learning method for classification, regression and other tasks that operates by constructing a multitude of decision trees at training time. For classification tasks, the output of the random forest is the class selected by most trees. For regression tasks, the mean or average prediction of the individual trees is returned. Random decision forests correct for decision trees’ habit of overfitting to their training set. Random forests generally outperform decision trees, but their accuracy is lower than gradient boosted trees[citation needed]. However, data characteristics can affect their performance.\nThe first algorithm for random decision forests was created in 1995 by Tin Kam Ho using the random subspace method, which, in Ho’s formulation, is a way to implement the “stochastic discrimination” approach to classification proposed by Eugene Kleinberg.\nAn extension of the algorithm was developed by Leo Breiman and Adele Cutler, who registered “Random Forests” as a trademark in 2006 (as of 2019[update], owned by Minitab, Inc.). The extension combines Breiman’s “bagging” idea and random selection of features, introduced first by Ho and later independently by Amit and Geman in order to construct a collection of decision trees with controlled variance.\nRandom forests are frequently used as “blackbox” models in businesses, as they generate reasonable predictions across a wide range of data while requiring little configuration.\n\n\n\n\nFigure 14.6: Random forests or random decision forests is an ensemble learning method for classification, regression and other tasks that operates by constructing a multitude of decision trees at training time.\n\n\n\n\n\n\n\nGreener, Joe G., Shaun M. Kandathil, Lewis Moffat, and David T. Jones. 2022. “A Guide to Machine Learning for Biologists.” Nature Reviews Molecular Cell Biology 23 (1): 40–55. https://doi.org/10.1038/s41580-021-00407-0.\n\n\nLibbrecht, Maxwell W., and William Stafford Noble. 2015. “Machine Learning Applications in Genetics and Genomics.” Nature Reviews Genetics 16 (6): 321–32. https://doi.org/10.1038/nrg3920."
  },
  {
    "objectID": "machine_learning_mlr3.html#practical-machine-learning-with-r-and-mlr3",
    "href": "machine_learning_mlr3.html#practical-machine-learning-with-r-and-mlr3",
    "title": "\n15  Machine Learning 2\n",
    "section": "\n15.1 Practical Machine Learning with R and mlr3",
    "text": "15.1 Practical Machine Learning with R and mlr3\nThe mlr3 R package is a modern, object-oriented machine learning framework in R that builds on the success of its predecessor, the mlr package. It provides a flexible and extensible platform for handling common machine learning tasks such as data preprocessing, model training, hyperparameter tuning, and model evaluation Figure 15.1. The package is designed to simplify the process of creating and deploying complex machine learning pipelines.\n\n\n\n\nFigure 15.1: The mlr3 ecosystem.\n\n\n\n\n15.1.1 Key features of mlr3\n\n\nTask abstraction\n\nmlr3 encapsulates different types of learning problems like classification, regression, and survival analysis into “Task” objects, making it easier to handle various learning scenarios.\n\n\n\nModular design\n\nThe package follows a modular design, allowing users to quickly swap out different components such as learners (algorithms), measures (performance metrics), and resampling strategies.\n\n\n\nExtensibility\n\nUsers can extend the functionality of mlr3 by adding custom components like learners, measures, and preprocessing steps via the R6 object-oriented system.\n\n\n\nPreprocessing\n\nmlr3 provides a flexible way to preprocess data using “PipeOps” (pipeline operations), allowing users to create reusable preprocessing pipelines.\n\n\n\nTuning and model selection\n\nmlr3 supports hyperparameter tuning and model selection using various search strategies like grid search, random search, and Bayesian optimization.\n\n\n\nParallelization\n\nThe package allows for parallelization of model training and evaluation, making it suitable for large-scale machine learning tasks.\n\n\n\nBenchmarking\n\nmlr3 facilitates benchmarking of multiple algorithms on multiple tasks, simplifying the process of comparing and selecting the best models.\n\n\n\nYou can find more information, including tutorials and examples, on the official mlr3 GitHub repository1 and the mlr3 book2.1 https://github.com/mlr-org/mlr32 https://mlr3book.mlr-org.com/"
  },
  {
    "objectID": "machine_learning_mlr3.html#the-mlr3-workflow",
    "href": "machine_learning_mlr3.html#the-mlr3-workflow",
    "title": "\n15  Machine Learning 2\n",
    "section": "\n15.2 The mlr3 workflow",
    "text": "15.2 The mlr3 workflow\nThe mlr3 package is designed to simplify the process of creating and deploying complex machine learning pipelines. The package follows a modular design, which means that users can quickly swap out different components such as learners (algorithms), measures (performance metrics), and resampling strategies. The package also supports parallelization of model training and evaluation, making it suitable for large-scale machine learning tasks.\nThe mlr3 workflow consists of the following steps:\n\nLoad data\nCreate a task to define the learning problem.\nSplit data into training and test sets.\nChoose a learner object to specify the learning algorithm.\n\nTrain the model on the training set.\nPredict the target variable for the test set.\n\n\nAssess the performance of the model.\nInterpret the model.\n\nThe following sections describe each of these steps in detail.\n\n15.2.1 Tasks\nTasks are objects that contain the (usually tabular) data and additional metadata to define a machine learning problem. The meta-data is, for example, the name of the target variable for supervised machine learning problems, or the type of the dataset (e.g. a spatial or survival task). This information is used by specific operations that can be performed on a task.\nTasks are objects that contain the (usually tabular) data and additional meta-data to define a machine learning problem. The meta-data is, for example, the name of the target variable for supervised machine learning problems, or the type of the dataset (e.g. a spatial or survival task). This information is used by specific operations that can be performed on a task.\nThere are a number of Task Types that are supported by mlr3. To create a task from a data.frame(), data.table() or Matrix(), you first need to select the right task type:\n\nClassification Task: The target is a label (stored as character or factor) with only relatively few distinct values → TaskClassif.\nRegression Task: The target is a numeric quantity (stored as integer or numeric) → TaskRegr.\nSurvival Task: The target is the (right-censored) time to an event. More censoring types are currently in development → mlr3proba::TaskSurv in add-on package mlr3proba.\nDensity Task: An unsupervised task to estimate the density → mlr3proba::TaskDens in add-on package mlr3proba.\nCluster Task: An unsupervised task type; there is no target and the aim is to identify similar groups within the feature space → mlr3cluster::TaskClust in add-on package mlr3cluster.\nSpatial Task: Observations in the task have spatio-temporal information (e.g. coordinates) → mlr3spatiotempcv::TaskRegrST or mlr3spatiotempcv::TaskClassifST in add-on package mlr3spatiotempcv.\nOrdinal Regression Task: The target is ordinal → TaskOrdinal in add-on package mlr3ordinal (still in development).\n\n15.2.2 Learners\nObjects of class Learner provide a unified interface to many popular machine learning algorithms in R. They consist of methods to train and predict a model for a Task and provide meta-information about the learners, such as the hyperparameters (which control the behavior of the learner) you can set.\nThe base class of each learner is Learner, specialized for regression as LearnerRegr and for classification as LearnerClassif. Other types of learners, provided by extension packages, also inherit from the Learner base class, e.g. mlr3proba::LearnerSurv or mlr3cluster::LearnerClust.\nAll Learners work in a two-stage procedure:\n\n\nFigure 15.2: Two stages of a learner. Top: data (features and a target) are passed to an (untrained) learner. Bottom: new data are passed to the trained model which makes predictions for the ‘missing’ target column.\n\n\n\nTraining stage: The training data (features and target) is passed to the Learner’s $train() function which trains and stores a model, i.e. the relationship of the target and features.\n\nPredict stage: The new data, usually a different slice of the original data than used for training, is passed to the $predict() method of the Learner. The model trained in the first step is used to predict the missing target, e.g. labels for classification problems or the numerical value for regression problems.\n\nThere are a number of predefined learners. The mlr3 package ships with the following set of classification and regression learners. We deliberately keep this small to avoid unnecessary dependencies:\n\n\nclassif.featureless: Simple baseline classification learner. The default is to always predict the label that is most frequent in the training set. While this is not very useful by itself, it can be used as a “fallback learner” to make predictions in case another, more sophisticated, learner failed for some reason.\n\nregr.featureless: Simple baseline regression learner. The default is to always predict the mean of the target in training set. Similar to mlr_learners_classif.featureless, it makes for a good “fallback learner”\n\nclassif.rpart: Single classification tree from package rpart.\n\nregr.rpart: Single regression tree from package rpart.\n\nThis set of baseline learners is usually insufficient for a real data analysis. Thus, we have cherry-picked implementations of the most popular machine learning method and collected them in the mlr3learners package:\n\nLinear (regr.lm) and logistic (classif.log_reg) regression\nPenalized Generalized Linear Models (regr.glmnet, classif.glmnet), possibly with built-in optimization of the penalization parameter (regr.cv_glmnet, classif.cv_glmnet)\n(Kernelized) k-Nearest Neighbors regression (regr.kknn) and classification (classif.kknn).\nKriging / Gaussian Process Regression (regr.km)\nLinear (classif.lda) and Quadratic (classif.qda) Discriminant Analysis\nNaive Bayes Classification (classif.naive_bayes)\nSupport-Vector machines (regr.svm, classif.svm)\nGradient Boosting (regr.xgboost, classif.xgboost)\nRandom Forests for regression and classification (regr.ranger, classif.ranger)\n\nMore machine learning methods and alternative implementations are collected in the mlr3extralearners repository.\n\n15.2.3 mlr3 Workflow\n\nLoad data\nSplit data into training and test sets\nCreate a task.\nChoose a learner. See mlr_learners.\nTrain\nPredict\nAssess\nInterpret"
  },
  {
    "objectID": "machine_learning_mlr3.html#setup",
    "href": "machine_learning_mlr3.html#setup",
    "title": "\n15  Machine Learning 2\n",
    "section": "\n15.3 Setup",
    "text": "15.3 Setup\n\nlibrary(mlr3verse)\nlibrary(GEOquery)\nlibrary(mlr3learners) # for knn\nlibrary(ranger) # for randomforest\nset.seed(789)"
  },
  {
    "objectID": "machine_learning_mlr3.html#example-1-cancer-types",
    "href": "machine_learning_mlr3.html#example-1-cancer-types",
    "title": "\n15  Machine Learning 2\n",
    "section": "\n15.4 Example 1: cancer types",
    "text": "15.4 Example 1: cancer types\nIn this exercise, we will be classifying cancer types based on gene expression data. The data we are going to access are from Brouwer-Visser et al. (2018).\n\n15.4.1 Data Preparation\nUse the [GEOquery] package to fetch data about [GSE103512].\n\nlibrary(GEOquery)\ngse = getGEO(\"GSE103512\")[[1]]\n\nThe first step, a detail, is to convert from the older Bioconductor data structure (GEOquery was written in 2007), the ExpressionSet, to the newer SummarizedExperiment.\n\nlibrary(SummarizedExperiment)\nse = as(gse, \"SummarizedExperiment\")\n\nExamine two variables of interest, cancer type and tumor/normal status.\n\nwith(colData(se),table(`cancer.type.ch1`,`normal.ch1`))\n\n               normal.ch1\ncancer.type.ch1 no yes\n          BC    65  10\n          CRC   57  12\n          NSCLC 60   9\n          PCA   60   7\n\n\nBefore embarking on a machine learning analysis, we need to make sure that we understand the data. Things like missing values, outliers, and other problems can cause problems for machine learning algorithms.\nIn R, plotting, summaries, and other exploratory data analysis tools are available. PCA analysis, clustering, and other methods can also be used to understand the data. It is worth spending time on this step, as it can save time later.\n\n15.4.2 Feature selection and data cleaning\nWhile we could use all genes in the analysis, we will select the most informative genes using the variance of gene expression across samples. Other methods for feature selection are available, including those based on correlation with the outcome variable.\n\n\n\n\n\n\nFeature selection\n\n\n\nFeature selection should be done on the training data only, not the test data to avoid overfitting.\n\n\nRemember that the apply function applies a function to each row or column of a matrix. Here, we apply the sd function to each row of the expression matrix to get a vector of stan\n\nsds = apply(assay(se, 'exprs'),1,sd)\n## filter out normal tissues\nse_small = se[order(sds,decreasing = TRUE)[1:200],\n              colData(se)$characteristics_ch1.1=='normal: no']\n# remove genes with no gene symbol\nse_small = se_small[rowData(se_small)$Gene.Symbol!='',]\n\nTo make the data easier to work with, we will use the opportunity to use one of the rowData columns as the rownames of the data frame. The make.names function is used to make sure that the rownames are valid R variable names and unique.\n\n## convert to matrix for later use\ndat = assay(se_small, 'exprs')\nrownames(dat) = make.names(rowData(se_small)$Gene.Symbol)\n\nWe also need to transpose the data so that the rows are the samples and the columns are the features in order to use the data with mlr3.\n\nfeat_dat = t(dat)\ntumor = data.frame(tumor_type = colData(se_small)$cancer.type.ch1, feat_dat)\n\nThis is another good time to check the data. Make sure that the data is in the format that you expect. Check the dimensions, the column names, and the data types.\n\n15.4.3 Creating the “task”\nThe first step in using mlr3 is to create a task. A task is a data set with a target variable. In this case, the target variable is the cancer type. The mlr3 package provides a function to convert a data frame into a task. These tasks can be used with any machine learning algorithm in mlr3.\n\ntumor$tumor_type = as.factor(tumor$tumor_type)\ntask = as_task_classif(tumor,target='tumor_type')\n\nHere, we randomly divide the data into 2/3 training data and 1/3 test data.\n\nset.seed(7)\ntrain_set = sample(task$row_ids, 0.67 * task$nrow)\ntest_set = setdiff(task$row_ids, train_set)\n\n\n\n\n\n\n\nImportant\n\n\n\nTraining and testing on the same data is a common mistake. We want to test the model on data that it has not seen before. This is the only way to know if the model is overfitting.\n\n\n\n15.4.4 K-nearest-neighbor\nThe first model we will use is the k-nearest-neighbor model. This model is based on the idea that similar samples have similar outcomes. The number of neighbors to use is a parameter that can be tuned. We’ll use the default value of 7, but you can try other values to see how they affect the results. In fact, mlr3 provides the ability to tune parameters automatically, but we won’t cover that here.\n\n15.4.4.1 Create the learner\nIn mlr3, the machine learning algorithms are called learners. To create a learner, we use the lrn function. The lrn function takes the name of the learner as an argument. The lrn function also takes other arguments that are specific to the learner. In this case, we will use the default values for the arguments.\n\nlearner = lrn(\"classif.kknn\")\n\nYou can get a list of all the learners available in mlr3 by using the lrn() function without any arguments.\n\nlrn()\n\n&lt;DictionaryLearner&gt; with 46 stored values\nKeys: classif.cv_glmnet, classif.debug, classif.featureless,\n  classif.glmnet, classif.kknn, classif.lda, classif.log_reg,\n  classif.multinom, classif.naive_bayes, classif.nnet, classif.qda,\n  classif.ranger, classif.rpart, classif.svm, classif.xgboost,\n  clust.agnes, clust.ap, clust.cmeans, clust.cobweb, clust.dbscan,\n  clust.diana, clust.em, clust.fanny, clust.featureless, clust.ff,\n  clust.hclust, clust.kkmeans, clust.kmeans, clust.MBatchKMeans,\n  clust.mclust, clust.meanshift, clust.pam, clust.SimpleKMeans,\n  clust.xmeans, regr.cv_glmnet, regr.debug, regr.featureless,\n  regr.glmnet, regr.kknn, regr.km, regr.lm, regr.nnet, regr.ranger,\n  regr.rpart, regr.svm, regr.xgboost\n\n\n\n15.4.4.2 Train\nTo train the model, we use the train function. The train function takes the task and the row ids of the training data as arguments.\n\nlearner$train(task, row_ids = train_set)\n\nHere, we can look at the trained model:\n\n# output is large, so do this on your own\nlearner$model\n\n\n15.4.4.3 Predict\nLets use our trained model works to predict the classes of the training data. Of course, we already know the classes of the training data, but this is a good way to check that the model is working as expected. It also gives us a measure of performance on the training data that we can compare to the test data to look for overfitting.\n\npred_train = learner$predict(task, row_ids=train_set)\n\nAnd check on the test data:\n\npred_test = learner$predict(task, row_ids=test_set)\n\n\n15.4.4.4 Assess\nIn this section, we can look at the accuracy and performance of our model on the training data and the test data. We can also look at the confusion matrix to see which classes are being confused with each other.\n\npred_train$confusion\n\n        truth\nresponse BC CRC NSCLC PCA\n   BC    42   0     0   0\n   CRC    0  40     0   0\n   NSCLC  1   0    44   0\n   PCA    0   0     0  35\n\n\nThis is a multi-class confusion matrix. The rows are the true classes and the columns are the predicted classes. The diagonal shows the number of samples that were correctly classified. The off-diagonal elements show the number of samples that were misclassified.\nWe can also look at the accuracy of the model on the training data and the test data. The accuracy is the number of correctly classified samples divided by the total number of samples.\n\nmeasures = msrs(c('classif.acc'))\npred_train$score(measures)\n\nclassif.acc \n  0.9938272 \n\n\n\npred_test$confusion\n\n        truth\nresponse BC CRC NSCLC PCA\n   BC    22   0     0   0\n   CRC    0  17     1   0\n   NSCLC  0   0    15   0\n   PCA    0   0     0  25\n\npred_test$score(measures)\n\nclassif.acc \n     0.9875 \n\n\nCompare the accuracy on the training data to the accuracy on the test data. Do you see any evidence of overfitting?\n\n15.4.5 Classification tree\nWe are going to use a classification tree to classify the data. A classification tree is a series of yes/no questions that are used to classify the data. The questions are based on the features in the data. The classification tree is built by finding the feature that best separates the data into the different classes. Then, the data is split based on the value of that feature. The process is repeated until the data is completely separated into the different classes.\n\n15.4.5.1 Train\n\n# in this case, we want to keep the model\n# so we can look at it later\nlearner = lrn(\"classif.rpart\", keep_model = TRUE)\n\n\nlearner$train(task, row_ids = train_set)\n\nWe can take a look at the model.\n\nlearner$model\n\nn= 162 \n\nnode), split, n, loss, yval, (yprob)\n      * denotes terminal node\n\n 1) root 162 118 NSCLC (0.26543210 0.24691358 0.27160494 0.21604938)  \n   2) CDHR5&gt;=5.101625 40   0 CRC (0.00000000 1.00000000 0.00000000 0.00000000) *\n   3) CDHR5&lt; 5.101625 122  78 NSCLC (0.35245902 0.00000000 0.36065574 0.28688525)  \n     6) ACPP&lt; 6.088431 87  43 NSCLC (0.49425287 0.00000000 0.50574713 0.00000000)  \n      12) GATA3&gt;=4.697803 41   1 BC (0.97560976 0.00000000 0.02439024 0.00000000) *\n      13) GATA3&lt; 4.697803 46   3 NSCLC (0.06521739 0.00000000 0.93478261 0.00000000) *\n     7) ACPP&gt;=6.088431 35   0 PCA (0.00000000 0.00000000 0.00000000 1.00000000) *\n\n\nDecision trees are easy to visualize if they are small. Here, we can see that the tree is very simple, with only two splits.\n\nautoplot(learner)\n\n\n\n\n\n15.4.5.2 Predict\nNow that we have trained the model on the training data, we can use it to predict the classes of the training data and the test data. The $predict method takes a task and produces a prediction based on the trained model, in this case, called learner.\n\npred_train = learner$predict(task, row_ids=train_set)\n\nRemember that we split the data into training and test sets. We can use the trained model to predict the classes of the test data. Since the test data was not used to train the model, it is not “cheating” like what we just did where we did the prediction on the training data.\n\npred_test = learner$predict(task, row_ids=test_set)\n\n\n15.4.5.3 Assess\nFor classification tasks, we often look at a confusion matrix of the truth vs the predicted classes for the samples.\n\n\n\n\n\n\nImportant\n\n\n\nAssessing the performance of a model should always be reported from assessment on an independent test set.\n\n\n\npred_train$confusion\n\n        truth\nresponse BC CRC NSCLC PCA\n   BC    40   0     1   0\n   CRC    0  40     0   0\n   NSCLC  3   0    43   0\n   PCA    0   0     0  35\n\n\n\nWhat does this confusion matrix tell you?\n\nWe can also ask for several “measures” of the performance of the model. Here, we ask for the accuracy of the model. To get a complete list of measures, use msr().\n\nmeasures = msrs(c('classif.acc'))\npred_train$score(measures)\n\nclassif.acc \n  0.9753086 \n\n\n\nHow does the accuracy compare to the confusion matrix?\nHow does this accuracy compare to the accuracy of the k-nearest-neighbor model?\nHow about the decision tree model?\n\n\npred_test$confusion\n\n        truth\nresponse BC CRC NSCLC PCA\n   BC    20   0     1   0\n   CRC    0  17     3   0\n   NSCLC  2   0    12   0\n   PCA    0   0     0  25\n\npred_test$score(measures)\n\nclassif.acc \n      0.925 \n\n\n\nWhat does the confusion matrix in the test set tell you?\nHow do the assessments of the test and training sets differ?\n\n\n\n\n\n\n\nOverfitting\n\n\n\nWhen the assessment of the test set is worse than the evaluation of the training set, the model may be overfit. How to address overfitting varies by model type, but it is a sign that you should pay attention to model selection and parameters.\n\n\n\n15.4.6 RandomForest\n\nlearner = lrn(\"classif.ranger\", importance = \"impurity\")\n\n\n15.4.6.1 Train\n\nlearner$train(task, row_ids = train_set)\n\nAgain, you can look at the model that was trained.\n\nlearner$model\n\nRanger result\n\nCall:\n ranger::ranger(dependent.variable.name = task$target_names, data = task$data(),      probability = self$predict_type == \"prob\", case.weights = task$weights$weight,      num.threads = 1L, importance = \"impurity\") \n\nType:                             Classification \nNumber of trees:                  500 \nSample size:                      162 \nNumber of independent variables:  192 \nMtry:                             13 \nTarget node size:                 1 \nVariable importance mode:         impurity \nSplitrule:                        gini \nOOB prediction error:             0.62 % \n\n\nFor more details, the mlr3 random forest approach is based ont he ranger package. You can look at the ranger documentation.\n\nWhat is the OOB error in the output?\n\nRandom forests are a collection of decision trees. Since predictors enter the trees in a random order, the trees are different from each other. The random forest procedure gives us a measure of the “importance” of each variable.\n\nhead(learner$importance(), 15)\n\n   CDHR5  TRPS1.1    FABP1   EPS8L3    KRT20    EFHD1   LGALS4    TRPS1 \n4.791870 3.918063 3.692649 3.651422 3.340382 3.314491 2.952969 2.926175 \n   SFTPB  SFTPB.1    GATA3  GATA3.1  TMPRSS2    MUC12    POF1B \n2.805811 2.681004 2.344603 2.271845 2.248734 2.207347 1.806906 \n\n\nMore “important” variables are those that are more often used in the trees. Are the most important variables the same as the ones that were important in the decision tree?\nIf you are interested, look up a few of the important variables in the model to see if they make biological sense.\n\n15.4.6.2 Predict\nAgain, we can use the trained model to predict the classes of the training data and the test data.\n\npred_train = learner$predict(task, row_ids=train_set)\n\n\npred_test = learner$predict(task, row_ids=test_set)\n\n\n15.4.6.3 Assess\n\npred_train$confusion\n\n        truth\nresponse BC CRC NSCLC PCA\n   BC    43   0     0   0\n   CRC    0  40     0   0\n   NSCLC  0   0    44   0\n   PCA    0   0     0  35\n\n\n\nmeasures = msrs(c('classif.acc'))\npred_train$score(measures)\n\nclassif.acc \n          1 \n\n\n\npred_test$confusion\n\n        truth\nresponse BC CRC NSCLC PCA\n   BC    22   0     0   0\n   CRC    0  17     0   0\n   NSCLC  0   0    16   0\n   PCA    0   0     0  25\n\npred_test$score(measures)\n\nclassif.acc \n          1"
  },
  {
    "objectID": "machine_learning_mlr3.html#exercise-predicting-age-from-dna-methylation",
    "href": "machine_learning_mlr3.html#exercise-predicting-age-from-dna-methylation",
    "title": "\n15  Machine Learning 2\n",
    "section": "\n15.5 Exercise: Predicting age from DNA methylation",
    "text": "15.5 Exercise: Predicting age from DNA methylation\nWe will be building a regression model for chronological age prediction, based on DNA methylation. This is based on the work of Jana Naue et al. 2017, in which biomarkers are examined to predict the chronological age of humans by analyzing the DNA methylation patterns. Different machine learning algorithms are used in this study to make an age prediction.\nIt has been recognized that within each individual, the level of DNA methylation changes with age. This knowledge is used to select useful biomarkers from DNA methylation datasets. The CpG sites with the highest correlation to age are selected as the biomarkers (and therefore features for building a regression model). In this tutorial, specific biomarkers are analyzed by machine learning algorithms to create an age prediction model.\nThe data are taken from this tutorial.\n\nlibrary(data.table)\nmeth_age = rbind(\n    fread('https://zenodo.org/record/2545213/files/test_rows_labels.csv'),\n    fread('https://zenodo.org/record/2545213/files/train_rows.csv')\n)\n\nLet’s take a quick look at the data.\n\nhead(meth_age)\n\n   RPA2_3 ZYG11A_4  F5_2 HOXC4_1 NKIRAS2_2 MEIS1_1 SAMD10_2 GRM2_9 TRIM59_5\n1:  65.96    18.08 41.57   55.46     30.69   63.42    40.86  68.88    44.32\n2:  66.83    20.27 40.55   49.67     29.53   30.47    37.73  53.30    50.09\n3:  50.30    11.74 40.17   33.85     23.39   58.83    38.84  35.08    35.90\n4:  65.54    15.56 33.56   36.79     20.23   56.39    41.75  50.37    41.46\n5:  59.01    14.38 41.95   30.30     24.99   54.40    37.38  30.35    31.28\n6:  81.30    14.68 35.91   50.20     26.57   32.37    32.30  55.19    42.21\n   LDB2_3 ELOVL2_6 DDO_1 KLF14_2 Age\n1:  56.17    62.29 40.99    2.30  40\n2:  58.40    61.10 49.73    1.07  44\n3:  58.81    50.38 63.03    0.95  28\n4:  58.05    50.58 62.13    1.99  37\n5:  65.80    48.74 41.88    0.90  24\n6:  70.15    61.36 33.62    1.87  43\n\n\nAs before, we create the task object, but this time we use as_task_regr() to create a regression task.\n\nWhy is this a regression task?\n\n\ntask = as_task_regr(meth_age,target = 'Age')\n\n\nset.seed(7)\ntrain_set = sample(task$row_ids, 0.67 * task$nrow)\ntest_set = setdiff(task$row_ids, train_set)\n\n\n15.5.1 Linear regression\nWe will start with a simple linear regression model.\n\nlearner = lrn(\"regr.lm\")\n\n\n15.5.1.1 Train\n\nlearner$train(task, row_ids = train_set)\n\nWhen you train a linear regression model, we can evaluate some of the diagnostic plots to see if the model is appropriate (Figure 15.3).\n\npar(mfrow=c(2,2))\nplot(learner$model)\n\n\n\nFigure 15.3: Regression diagnostic plots. The top left plot shows the residuals vs. fitted values. The top right plot shows the normal Q-Q plot. The bottom left plot shows the scale-location plot. The bottom right plot shows the residuals vs. leverage.\n\n\n\n\n15.5.1.2 Predict\n\npred_train = learner$predict(task, row_ids=train_set)\n\n\npred_test = learner$predict(task, row_ids=test_set)\n\n\n15.5.1.3 Assess\n\npred_train\n\n&lt;PredictionRegr&gt; for 209 observations:\n    row_ids truth response\n        298    29 31.40565\n        103    58 56.26019\n        194    53 48.96480\n---                       \n        312    48 52.61195\n        246    66 67.66312\n        238    38 39.38414\n\n\nWe can plot the relationship between the truth and response, or predicted value to see visually how our model performs.\n\nlibrary(ggplot2)\nggplot(pred_train,aes(x=truth, y=response)) +\n    geom_point() +\n    geom_smooth(method='lm')\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\nWe can use the r-squared of the fit to roughly compare two models.\n\nmeasures = msrs(c('regr.rsq'))\npred_train$score(measures)\n\n regr.rsq \n0.9376672 \n\n\n\npred_test\n\n&lt;PredictionRegr&gt; for 103 observations:\n    row_ids truth response\n          4    37 37.64301\n          5    24 28.34777\n          7    34 33.22419\n---                       \n        306    42 41.65864\n        307    63 58.68486\n        309    68 70.41987\n\npred_test$score(measures)\n\n regr.rsq \n0.9363526 \n\n\n\n15.5.2 Regression tree\n\nlearner = lrn(\"regr.rpart\", keep_model = TRUE)\n\n\n15.5.2.1 Train\n\nlearner$train(task, row_ids = train_set)\n\n\nlearner$model\n\nn= 209 \n\nnode), split, n, deviance, yval\n      * denotes terminal node\n\n 1) root 209 45441.4500 43.27273  \n   2) ELOVL2_6&lt; 56.675 98  5512.1220 30.24490  \n     4) ELOVL2_6&lt; 47.24 47   866.4255 24.23404  \n       8) GRM2_9&lt; 31.3 34   289.0588 22.29412 *\n       9) GRM2_9&gt;=31.3 13   114.7692 29.30769 *\n     5) ELOVL2_6&gt;=47.24 51  1382.6270 35.78431  \n      10) F5_2&gt;=39.295 35   473.1429 33.28571 *\n      11) F5_2&lt; 39.295 16   213.0000 41.25000 *\n   3) ELOVL2_6&gt;=56.675 111  8611.3690 54.77477  \n     6) ELOVL2_6&lt; 65.365 63  3101.2700 49.41270  \n      12) KLF14_2&lt; 3.415 37  1059.0270 46.16216 *\n      13) KLF14_2&gt;=3.415 26  1094.9620 54.03846 *\n     7) ELOVL2_6&gt;=65.365 48  1321.3120 61.81250 *\n\n\nWhat is odd about using a regression tree here is that we end up with only a few discrete estimates of age. Each “leaf” has a value.\n\n15.5.2.2 Predict\n\npred_train = learner$predict(task, row_ids=train_set)\n\n\npred_test = learner$predict(task, row_ids=test_set)\n\n\n15.5.2.3 Assess\n\npred_train\n\n&lt;PredictionRegr&gt; for 209 observations:\n    row_ids truth response\n        298    29 33.28571\n        103    58 61.81250\n        194    53 46.16216\n---                       \n        312    48 54.03846\n        246    66 61.81250\n        238    38 41.25000\n\n\nWe can see the effect of the discrete values much more clearly here.\n\nlibrary(ggplot2)\nggplot(pred_train,aes(x=truth, y=response)) +\n    geom_point() +\n    geom_smooth(method='lm')\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\nAnd the r-squared values for this model prediction shows quite a bit of difference from the linear regression above.\n\nmeasures = msrs(c('regr.rsq'))\npred_train$score(measures)\n\n regr.rsq \n0.8995351 \n\n\n\npred_test\n\n&lt;PredictionRegr&gt; for 103 observations:\n    row_ids truth response\n          4    37 41.25000\n          5    24 33.28571\n          7    34 33.28571\n---                       \n        306    42 46.16216\n        307    63 61.81250\n        309    68 61.81250\n\npred_test$score(measures)\n\n regr.rsq \n0.8545402 \n\n\n\n15.5.3 RandomForest\nRandomforest is also tree-based, but unlike the single regression tree above, randomforest is a “forest” of trees which will eliminate the discrete nature of a single tree.\n\nlearner = lrn(\"regr.ranger\", mtry=2, min.node.size=20)\n\n\n15.5.3.1 Train\n\nlearner$train(task, row_ids = train_set)\n\n\nlearner$model\n\nRanger result\n\nCall:\n ranger::ranger(dependent.variable.name = task$target_names, data = task$data(),      case.weights = task$weights$weight, num.threads = 1L, mtry = 2L,      min.node.size = 20L) \n\nType:                             Regression \nNumber of trees:                  500 \nSample size:                      209 \nNumber of independent variables:  13 \nMtry:                             2 \nTarget node size:                 20 \nVariable importance mode:         none \nSplitrule:                        variance \nOOB prediction error (MSE):       18.84172 \nR squared (OOB):                  0.9137554 \n\n\n\n15.5.3.2 Predict\n\npred_train = learner$predict(task, row_ids=train_set)\n\n\npred_test = learner$predict(task, row_ids=test_set)\n\n\n15.5.3.3 Assess\n\npred_train\n\n&lt;PredictionRegr&gt; for 209 observations:\n    row_ids truth response\n        298    29 30.62154\n        103    58 57.99398\n        194    53 48.32491\n---                       \n        312    48 51.49846\n        246    66 64.39315\n        238    38 38.18038\n\n\n\nggplot(pred_train,aes(x=truth, y=response)) +\n    geom_point() +\n    geom_smooth(method='lm')\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\nmeasures = msrs(c('regr.rsq'))\npred_train$score(measures)\n\n regr.rsq \n0.9609739 \n\n\n\npred_test\n\n&lt;PredictionRegr&gt; for 103 observations:\n    row_ids truth response\n          4    37 37.79631\n          5    24 29.18371\n          7    34 33.26780\n---                       \n        306    42 40.27428\n        307    63 58.26534\n        309    68 63.15481\n\npred_test$score(measures)\n\n regr.rsq \n0.9208186"
  },
  {
    "objectID": "machine_learning_mlr3.html#expression-prediction-from-histone-modification-data",
    "href": "machine_learning_mlr3.html#expression-prediction-from-histone-modification-data",
    "title": "\n15  Machine Learning 2\n",
    "section": "\n15.6 Expression prediction from histone modification data",
    "text": "15.6 Expression prediction from histone modification data\nIn this little set of exercises, you will be using histone marks near a gene to predict its expression (Figure 15.4).\n\\[y = h1 + h2 + h3 + ... \\tag{15.1}\\]\n\n\nFigure 15.4: What is the combined effect of histone marks on gene expression?\n\nWe will try a couple of different approaches:\n\nPenalized regression\nRandomForest\n\n\n15.6.1 The Data\nThe data in this\n\nfullFeatureSet &lt;- read.table(\"http://seandavi.github.io/ITR/expression-prediction/features.txt\");\n\nWhat are the column names of the predictor variables?\n\ncolnames(fullFeatureSet)\n\n [1] \"Control\"  \"Dnase\"    \"H2az\"     \"H3k27ac\"  \"H3k27me3\" \"H3k36me3\"\n [7] \"H3k4me1\"  \"H3k4me2\"  \"H3k4me3\"  \"H3k79me2\" \"H3k9ac\"   \"H3k9me1\" \n[13] \"H3k9me3\"  \"H4k20me1\"\n\n\nThese are going to be predictors combined into a model. Some of our learners will rely on predictors being on a similar scale. Are our data already there?\nTo perform centering and scaling by column, we can convert to a matrix and then use scale.\n\npar(mfrow=c(1,2))\nscaled_features &lt;- scale(as.matrix(fullFeatureSet))\nboxplot(fullFeatureSet, title='Original data')\nboxplot(scaled_features, title='Centered and scaled data')\n\n\n\nFigure 15.5: Boxplots of original and scaled data.\n\n\n\nThere is a row for each gene and a column for each histone mark and we can see that the data are centered and scaled by column. We can also see some patterns in the data (see Figure 15.6).\n\nsampled_features &lt;- fullFeatureSet[sample(nrow(scaled_features), 500),]\nlibrary(ComplexHeatmap)\n\nLoading required package: grid\n\n\n========================================\nComplexHeatmap version 2.16.0\nBioconductor page: http://bioconductor.org/packages/ComplexHeatmap/\nGithub page: https://github.com/jokergoo/ComplexHeatmap\nDocumentation: http://jokergoo.github.io/ComplexHeatmap-reference\n\nIf you use it in published research, please cite either one:\n- Gu, Z. Complex Heatmap Visualization. iMeta 2022.\n- Gu, Z. Complex heatmaps reveal patterns and correlations in multidimensional \n    genomic data. Bioinformatics 2016.\n\n\nThe new InteractiveComplexHeatmap package can directly export static \ncomplex heatmaps into an interactive Shiny app with zero effort. Have a try!\n\nThis message can be suppressed by:\n  suppressPackageStartupMessages(library(ComplexHeatmap))\n========================================\n\nHeatmap(sampled_features, name='histone marks', show_row_names=FALSE)\n\nWarning: The input is a data frame-like object, convert it to a matrix.\n\n\n\n\nFigure 15.6: Heatmap of 500 randomly sampled rows of the data. Columns are histone marks and there is a row for each gene.\n\n\n\nNow, we can read in the associated gene expression measures that will become our “target” for prediction.\n\ntarget &lt;- scan(url(\"http://seandavi.github.io/ITR/expression-prediction/target.txt\"), skip=1)\n# make into a dataframe\nexp_pred_data &lt;- data.frame(gene_expression=target, scaled_features)\n\nAnd the first few rows of the target data frame using:\n\nhead(exp_pred_data,3)\n\n                            gene_expression    Control      Dnase       H2az\nENSG00000000419.7.49575069         6.082343  0.7452926  0.7575546  1.0728432\nENSG00000000457.8.169863093        2.989145  1.9509786  1.0216546  0.3702787\nENSG00000000938.7.27961645        -5.058894 -0.3505542 -1.4482958 -1.0390775\n                               H3k27ac   H3k27me3   H3k36me3    H3k4me1\nENSG00000000419.7.49575069   1.0950440 -0.5125312  1.1334793  0.4127984\nENSG00000000457.8.169863093  0.7142157 -0.4079244  0.8739005  1.1649282\nENSG00000000938.7.27961645  -1.0173283  1.4117293 -0.5157582 -0.5017450\n                               H3k4me2    H3k4me3   H3k79me2     H3k9ac\nENSG00000000419.7.49575069   1.2136176  1.1202901  1.5155803  1.2468256\nENSG00000000457.8.169863093  0.6456572  0.6508561  0.7976487  0.5792891\nENSG00000000938.7.27961645  -0.1878255 -0.6560973 -1.3803974 -1.0067972\n                              H3k9me1   H3k9me3   H4k20me1\nENSG00000000419.7.49575069  0.1426980  1.185622  1.9599992\nENSG00000000457.8.169863093 0.3630902  1.014923 -0.2695111\nENSG00000000938.7.27961645  0.6564520 -1.370871 -1.8773178\n\n\n\n15.6.2 Create task\n\nexp_pred_task = as_task_regr(exp_pred_data, target='gene_expression')\n\nPartition the data into test and training sets. We will use \\(\\frac{1}{3}\\) and \\(\\frac{2}{3}\\) of the data for testing.\n\nsplit = partition(exp_pred_task)\n\n\n15.6.3 Linear regression\n\nlearner = lrn(\"regr.lm\")\n\n\n15.6.3.1 Train\n\nlearner$train(exp_pred_task, split$train)\n\n\n15.6.3.2 Predict\n\npred_train = learner$predict(exp_pred_task, split$train)\npred_test = learner$predict(exp_pred_task, split$test)\n\n\n15.6.3.3 Assess\n\npred_train\n\n&lt;PredictionRegr&gt; for 5789 observations:\n    row_ids     truth response\n          1  6.082343 5.139251\n          2  2.989145 2.909552\n          7  5.838076 4.563759\n---                           \n       8543  9.016443 6.141272\n       8583  7.475697 2.543423\n       8618 10.049236 5.523896\n\n\nFor the training data:\n\nmeasures = msrs(c('regr.rsq'))\npred_train$score(measures)\n\n regr.rsq \n0.7495474 \n\n\nAnd the test data:\n\npred_test$score(measures)\n\n regr.rsq \n0.7526609 \n\n\n\n15.6.4 Penalized regression\nRecall that we can use penalized regression to select the most important predictors from a large set of predictors. In this case, we will use the glmnet package to perform penalized regression, but we will use the mlr interface to glmnet to make it easier to use.\n\nlearner = lrn(\"regr.cv_glmnet\", nfolds=10, alpha=1)\n\n\n15.6.4.1 Train\n\nlearner$train(exp_pred_task)\n\n\nmeasures = msrs(c('regr.rsq'))\npred_train$score(measures)\n\n regr.rsq \n0.7495474 \n\n\nIn the case of the penalized regression, we can also look at the coefficients of the model.\n\ncoef(learner$model)\n\n15 x 1 sparse Matrix of class \"dgCMatrix\"\n                     s1\n(Intercept)  0.10173828\nControl     -0.06138687\nDnase        1.15560095\nH2az         0.25382598\nH3k27ac      .         \nH3k27me3    -0.17000065\nH3k36me3     0.67803937\nH3k4me1     -0.06934505\nH3k4me2      .         \nH3k4me3      0.22513201\nH3k79me2     1.47587175\nH3k9ac       0.51449187\nH3k9me1     -0.11580672\nH3k9me3     -0.17270444\nH4k20me1     .         \n\n\nNote that the coefficients are all zero for the histone marks that were not selected by the model. In this case, we can use the model not to predict new data, but to help us understand the data."
  },
  {
    "objectID": "machine_learning_mlr3.html#cross-validation",
    "href": "machine_learning_mlr3.html#cross-validation",
    "title": "\n15  Machine Learning 2\n",
    "section": "\n15.7 Cross-validation",
    "text": "15.7 Cross-validation\n\nas.data.table(mlr_resamplings)\n\n           key                         label        params iters\n1:   bootstrap                     Bootstrap ratio,repeats    30\n2:      custom                 Custom Splits                  NA\n3:   custom_cv Custom Split Cross-Validation                  NA\n4:          cv              Cross-Validation         folds    10\n5:     holdout                       Holdout         ratio     1\n6:    insample           Insample Resampling                   1\n7:         loo                 Leave-One-Out                  NA\n8: repeated_cv     Repeated Cross-Validation folds,repeats   100\n9: subsampling                   Subsampling ratio,repeats    30\n\n\n\n\n\n\nBrouwer-Visser, Jurriaan, Wei-Yi Cheng, Anna Bauer-Mehren, Daniela Maisel, Katharina Lechner, Emilia Andersson, Joel T. Dudley, and Francesca Milletti. 2018. “Regulatory T-Cell Genes Drive Altered Immune Microenvironment in Adult Solid Cancers and Allow for Immune Contextual Patient Subtyping.” Cancer Epidemiology, Biomarkers & Prevention 27 (1): 103–12. https://doi.org/10.1158/1055-9965.EPI-17-0461."
  },
  {
    "objectID": "geoquery.html",
    "href": "geoquery.html",
    "title": "\n16  Accessing and working with public omics data\n",
    "section": "",
    "text": "17 The data\nThe data we are going to access are from this paper.\nIn this little exercise, we will:"
  },
  {
    "objectID": "geoquery.html#geoquery-to-multidimensional-scaling",
    "href": "geoquery.html#geoquery-to-multidimensional-scaling",
    "title": "\n16  Accessing and working with public omics data\n",
    "section": "\n17.1 GEOquery to multidimensional scaling",
    "text": "17.1 GEOquery to multidimensional scaling\nThe first step is to install the R package GEOquery. This package allows us to access data from the Gene Expression Omnibus (GEO) database. GEO is a public repository of omics data.\n\nBiocManager::install(\"GEOquery\")\n\nGEOquery has only one commonly used function, getGEO() which takes a GEO accession number as an argument. The GEO accession number is a unique identifier for a dataset.\nUse the GEOquery package to fetch data about GSE103512.\n\nlibrary(GEOquery)\ngse = getGEO(\"GSE103512\")[[1]]\n\nThe first step, a detail, is to convert from the older Bioconductor data structure (GEOquery was written in 2007), the ExpressionSet, to the newer SummarizedExperiment.\n\nlibrary(SummarizedExperiment)\nse = as(gse, \"SummarizedExperiment\")\n\n\nWhat is the class of se?\nWhat are the dimensions of se?\nWhat are the dimensions of the assay slot of se?\nWhat are the dimensions of the colData slot of se?\nWhat variables are in the colData slot of se?\n\nExamine two variables of interest, cancer type and tumor/normal status. The with function is a convenience to allow us to access variables in a data frame by name (rather than having to do dataframe$variable_name.\n\nwith(colData(se),table(`cancer.type.ch1`,`normal.ch1`))\n\n               normal.ch1\ncancer.type.ch1 no yes\n          BC    65  10\n          CRC   57  12\n          NSCLC 60   9\n          PCA   60   7\n\n\nFilter gene expression by variance to find most informative genes. It is common practice to filter genes by standard deviation or some other measure of variability and keep the top X percent of them when performing dimensionality reduction. There is not a single right answer to what percentage to use, so try a few to see what happens.\nIn the example code, I chose to use the top 500 genes by standard deviation.\n\nsds = apply(assay(se, 'exprs'),1,sd)\ndat = assay(se, 'exprs')[order(sds,decreasing = TRUE)[1:500],]\n\nPerform multidimensional scaling and prepare for plotting. We will be using ggplot2, so we need to make a data.frame before plotting.\n\nmdsvals = cmdscale(dist(t(dat)))\nmdsvals = as.data.frame(mdsvals)\nmdsvals$Type=factor(colData(se)[,'cancer.type.ch1'])\nmdsvals$Normal = factor(colData(se)[,'normal.ch1'])\n\nAnd do the plot.\n\nlibrary(ggplot2)\nggplot(mdsvals, aes(x=V1,y=V2,shape=Normal,color=Type)) + \n    geom_point( alpha=0.6) + theme(text=element_text(size = 18))\n\n\n\n\n\n\n\n\nWhat do you see?"
  },
  {
    "objectID": "bioc-summarizedexperiment.html#anatomy-of-a-summarizedexperiment",
    "href": "bioc-summarizedexperiment.html#anatomy-of-a-summarizedexperiment",
    "title": "\n17  Introduction to SummarizedExperiment\n",
    "section": "\n17.1 Anatomy of a SummarizedExperiment\n",
    "text": "17.1 Anatomy of a SummarizedExperiment\n\nThe SummarizedExperiment package contains two classes: SummarizedExperiment and RangedSummarizedExperiment.\nSummarizedExperiment is a matrix-like container where rows represent features of interest (e.g. genes, transcripts, exons, etc.) and columns represent samples. The objects contain one or more assays, each represented by a matrix-like object of numeric or other mode. The rows of a SummarizedExperiment object represent features of interest. Information about these features is stored in a DataFrame object, accessible using the function rowData(). Each row of the DataFrame provides information on the feature in the corresponding row of the SummarizedExperiment object. Columns of the DataFrame represent different attributes of the features of interest, e.g., gene or transcript IDs, etc.\nRangedSummarizedExperiment is the “child”” of the SummarizedExperiment class which means that all the methods on SummarizedExperiment also work on a RangedSummarizedExperiment.\nThe fundamental difference between the two classes is that the rows of a RangedSummarizedExperiment object represent genomic ranges of interest instead of a DataFrame of features. The RangedSummarizedExperiment ranges are described by a GRanges or a GRangesList object, accessible using the rowRanges() function.\nFigure 17.1 displays the class geometry and highlights the vertical (column) and horizontal (row) relationships.\n\n\nFigure 17.1: Summarized Experiment. There are three main components, the colData(), the rowData() and the assays(). The accessors for the various parts of a complete SummarizedExperiment object match the names.\n\n\n17.1.1 Assays\nThe airway package contains an example dataset from an RNA-Seq experiment of read counts per gene for airway smooth muscles. These data are stored in a RangedSummarizedExperiment object which contains 8 different experimental and assays 64,102 gene transcripts.\n\n\nLoading required package: airway\n\n\n\nlibrary(SummarizedExperiment)\ndata(airway, package=\"airway\")\nse &lt;- airway\nse\n\nclass: RangedSummarizedExperiment \ndim: 63677 8 \nmetadata(1): ''\nassays(1): counts\nrownames(63677): ENSG00000000003 ENSG00000000005 ... ENSG00000273492\n  ENSG00000273493\nrowData names(10): gene_id gene_name ... seq_coord_system symbol\ncolnames(8): SRR1039508 SRR1039509 ... SRR1039520 SRR1039521\ncolData names(9): SampleName cell ... Sample BioSample\n\n\nTo retrieve the experiment data from a SummarizedExperiment object one can use the assays() accessor. An object can have multiple assay datasets each of which can be accessed using the $ operator. The airway dataset contains only one assay (counts). Here each row represents a gene transcript and each column one of the samples.\n\nassays(se)$counts\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSRR1039508\nSRR1039509\nSRR1039512\nSRR1039513\nSRR1039516\nSRR1039517\nSRR1039520\nSRR1039521\n\n\n\nENSG00000000003\n679\n448\n873\n408\n1138\n1047\n770\n572\n\n\nENSG00000000005\n0\n0\n0\n0\n0\n0\n0\n0\n\n\nENSG00000000419\n467\n515\n621\n365\n587\n799\n417\n508\n\n\nENSG00000000457\n260\n211\n263\n164\n245\n331\n233\n229\n\n\nENSG00000000460\n60\n55\n40\n35\n78\n63\n76\n60\n\n\nENSG00000000938\n0\n0\n2\n0\n1\n0\n0\n0\n\n\nENSG00000000971\n3251\n3679\n6177\n4252\n6721\n11027\n5176\n7995\n\n\nENSG00000001036\n1433\n1062\n1733\n881\n1424\n1439\n1359\n1109\n\n\nENSG00000001084\n519\n380\n595\n493\n820\n714\n696\n704\n\n\nENSG00000001167\n394\n236\n464\n175\n658\n584\n360\n269\n\n\n\n\n\n\n17.1.2 ‘Row’ (regions-of-interest) data\nThe rowRanges() accessor is used to view the range information for a RangedSummarizedExperiment. (Note if this were the parent SummarizedExperiment class we’d use rowData()). The data are stored in a GRangesList object, where each list element corresponds to one gene transcript and the ranges in each GRanges correspond to the exons in the transcript.\n\nrowRanges(se)\n\nGRangesList object of length 63677:\n$ENSG00000000003\nGRanges object with 17 ranges and 2 metadata columns:\n       seqnames            ranges strand |   exon_id       exon_name\n          &lt;Rle&gt;         &lt;IRanges&gt;  &lt;Rle&gt; | &lt;integer&gt;     &lt;character&gt;\n   [1]        X 99883667-99884983      - |    667145 ENSE00001459322\n   [2]        X 99885756-99885863      - |    667146 ENSE00000868868\n   [3]        X 99887482-99887565      - |    667147 ENSE00000401072\n   [4]        X 99887538-99887565      - |    667148 ENSE00001849132\n   [5]        X 99888402-99888536      - |    667149 ENSE00003554016\n   ...      ...               ...    ... .       ...             ...\n  [13]        X 99890555-99890743      - |    667156 ENSE00003512331\n  [14]        X 99891188-99891686      - |    667158 ENSE00001886883\n  [15]        X 99891605-99891803      - |    667159 ENSE00001855382\n  [16]        X 99891790-99892101      - |    667160 ENSE00001863395\n  [17]        X 99894942-99894988      - |    667161 ENSE00001828996\n  -------\n  seqinfo: 722 sequences (1 circular) from an unspecified genome\n\n...\n&lt;63676 more elements&gt;\n\n\n\n17.1.3 ‘Column’ (sample) data\nSample meta-data describing the samples can be accessed using colData(), and is a DataFrame that can store any number of descriptive columns for each sample row.\n\ncolData(se)\n\nDataFrame with 8 rows and 9 columns\n           SampleName     cell      dex    albut        Run avgLength\n             &lt;factor&gt; &lt;factor&gt; &lt;factor&gt; &lt;factor&gt;   &lt;factor&gt; &lt;integer&gt;\nSRR1039508 GSM1275862  N61311     untrt    untrt SRR1039508       126\nSRR1039509 GSM1275863  N61311     trt      untrt SRR1039509       126\nSRR1039512 GSM1275866  N052611    untrt    untrt SRR1039512       126\nSRR1039513 GSM1275867  N052611    trt      untrt SRR1039513        87\nSRR1039516 GSM1275870  N080611    untrt    untrt SRR1039516       120\nSRR1039517 GSM1275871  N080611    trt      untrt SRR1039517       126\nSRR1039520 GSM1275874  N061011    untrt    untrt SRR1039520       101\nSRR1039521 GSM1275875  N061011    trt      untrt SRR1039521        98\n           Experiment    Sample    BioSample\n             &lt;factor&gt;  &lt;factor&gt;     &lt;factor&gt;\nSRR1039508  SRX384345 SRS508568 SAMN02422669\nSRR1039509  SRX384346 SRS508567 SAMN02422675\nSRR1039512  SRX384349 SRS508571 SAMN02422678\nSRR1039513  SRX384350 SRS508572 SAMN02422670\nSRR1039516  SRX384353 SRS508575 SAMN02422682\nSRR1039517  SRX384354 SRS508576 SAMN02422673\nSRR1039520  SRX384357 SRS508579 SAMN02422683\nSRR1039521  SRX384358 SRS508580 SAMN02422677\n\n\nThis sample metadata can be accessed using the $ accessor which makes it easy to subset the entire object by a given phenotype.\n\n# subset for only those samples treated with dexamethasone\nse[, se$dex == \"trt\"]\n\nclass: RangedSummarizedExperiment \ndim: 63677 4 \nmetadata(1): ''\nassays(1): counts\nrownames(63677): ENSG00000000003 ENSG00000000005 ... ENSG00000273492\n  ENSG00000273493\nrowData names(10): gene_id gene_name ... seq_coord_system symbol\ncolnames(4): SRR1039509 SRR1039513 SRR1039517 SRR1039521\ncolData names(9): SampleName cell ... Sample BioSample\n\n\n\n17.1.4 Experiment-wide metadata\nMeta-data describing the experimental methods and publication references can be accessed using metadata().\n\nmetadata(se)\n\n[[1]]\nExperiment data\n  Experimenter name: Himes BE \n  Laboratory: NA \n  Contact information:  \n  Title: RNA-Seq transcriptome profiling identifies CRISPLD2 as a glucocorticoid responsive gene that modulates cytokine function in airway smooth muscle cells. \n  URL: http://www.ncbi.nlm.nih.gov/pubmed/24926665 \n  PMIDs: 24926665 \n\n  Abstract: A 226 word abstract is available. Use 'abstract' method.\n\n\nNote that metadata() is just a simple list, so it is appropriate for any experiment wide metadata the user wishes to save, such as storing model formulas.\n\nmetadata(se)$formula &lt;- counts ~ dex + albut\n\nmetadata(se)\n\n[[1]]\nExperiment data\n  Experimenter name: Himes BE \n  Laboratory: NA \n  Contact information:  \n  Title: RNA-Seq transcriptome profiling identifies CRISPLD2 as a glucocorticoid responsive gene that modulates cytokine function in airway smooth muscle cells. \n  URL: http://www.ncbi.nlm.nih.gov/pubmed/24926665 \n  PMIDs: 24926665 \n\n  Abstract: A 226 word abstract is available. Use 'abstract' method.\n\n$formula\ncounts ~ dex + albut"
  },
  {
    "objectID": "bioc-summarizedexperiment.html#common-operations-on-summarizedexperiment",
    "href": "bioc-summarizedexperiment.html#common-operations-on-summarizedexperiment",
    "title": "\n17  Introduction to SummarizedExperiment\n",
    "section": "\n17.2 Common operations on SummarizedExperiment\n",
    "text": "17.2 Common operations on SummarizedExperiment\n\n\n17.2.1 Subsetting\n\n\n[ Performs two dimensional subsetting, just like subsetting a matrix or data frame.\n\n\n# subset the first five transcripts and first three samples\nse[1:5, 1:3]\n\nclass: RangedSummarizedExperiment \ndim: 5 3 \nmetadata(2): '' formula\nassays(1): counts\nrownames(5): ENSG00000000003 ENSG00000000005 ENSG00000000419\n  ENSG00000000457 ENSG00000000460\nrowData names(10): gene_id gene_name ... seq_coord_system symbol\ncolnames(3): SRR1039508 SRR1039509 SRR1039512\ncolData names(9): SampleName cell ... Sample BioSample\n\n\n\n\n$ operates on colData() columns, for easy sample extraction.\n\n\nse[, se$cell == \"N61311\"]\n\nclass: RangedSummarizedExperiment \ndim: 63677 2 \nmetadata(2): '' formula\nassays(1): counts\nrownames(63677): ENSG00000000003 ENSG00000000005 ... ENSG00000273492\n  ENSG00000273493\nrowData names(10): gene_id gene_name ... seq_coord_system symbol\ncolnames(2): SRR1039508 SRR1039509\ncolData names(9): SampleName cell ... Sample BioSample\n\n\n\n17.2.2 Getters and setters\n\n\nrowRanges() / (rowData()), colData(), metadata()\n\n\n\ncounts &lt;- matrix(1:15, 5, 3, dimnames=list(LETTERS[1:5], LETTERS[1:3]))\n\ndates &lt;- SummarizedExperiment(assays=list(counts=counts),\n                              rowData=DataFrame(month=month.name[1:5], day=1:5))\n\n# Subset all January assays\ndates[rowData(dates)$month == \"January\", ]\n\nclass: SummarizedExperiment \ndim: 1 3 \nmetadata(0):\nassays(1): counts\nrownames(1): A\nrowData names(2): month day\ncolnames(3): A B C\ncolData names(0):\n\n\n\n\nassay() versus assays() There are two accessor functions for extracting the assay data from a SummarizedExperiment object. assays() operates on the entire list of assay data as a whole, while assay() operates on only one assay at a time. assay(x, i) is simply a convenience function which is equivalent to assays(x)[[i]].\n\n\nassays(se)\n\nList of length 1\nnames(1): counts\n\nassays(se)[[1]][1:5, 1:5]\n\n                SRR1039508 SRR1039509 SRR1039512 SRR1039513 SRR1039516\nENSG00000000003        679        448        873        408       1138\nENSG00000000005          0          0          0          0          0\nENSG00000000419        467        515        621        365        587\nENSG00000000457        260        211        263        164        245\nENSG00000000460         60         55         40         35         78\n\n# assay defaults to the first assay if no i is given\nassay(se)[1:5, 1:5]\n\n                SRR1039508 SRR1039509 SRR1039512 SRR1039513 SRR1039516\nENSG00000000003        679        448        873        408       1138\nENSG00000000005          0          0          0          0          0\nENSG00000000419        467        515        621        365        587\nENSG00000000457        260        211        263        164        245\nENSG00000000460         60         55         40         35         78\n\nassay(se, 1)[1:5, 1:5]\n\n                SRR1039508 SRR1039509 SRR1039512 SRR1039513 SRR1039516\nENSG00000000003        679        448        873        408       1138\nENSG00000000005          0          0          0          0          0\nENSG00000000419        467        515        621        365        587\nENSG00000000457        260        211        263        164        245\nENSG00000000460         60         55         40         35         78\n\n\n\n17.2.3 Range-based operations\n\n\nsubsetByOverlaps() SummarizedExperiment objects support all of the findOverlaps() methods and associated functions. This includes subsetByOverlaps(), which makes it easy to subset a SummarizedExperiment object by an interval.\n\nIn tne next code block, we define a region of interest (or many regions of interest) and then subset our SummarizedExperiment by overlaps with this region.\n\n# Subset for only rows which are in the interval 100,000 to 110,000 of\n# chromosome 1\nroi &lt;- GRanges(seqnames=\"1\", ranges=100000:1100000)\nsub_se = subsetByOverlaps(se, roi)\nsub_se\n\nclass: RangedSummarizedExperiment \ndim: 74 8 \nmetadata(2): '' formula\nassays(1): counts\nrownames(74): ENSG00000131591 ENSG00000177757 ... ENSG00000272512\n  ENSG00000273443\nrowData names(10): gene_id gene_name ... seq_coord_system symbol\ncolnames(8): SRR1039508 SRR1039509 ... SRR1039520 SRR1039521\ncolData names(9): SampleName cell ... Sample BioSample\n\ndim(sub_se)\n\n[1] 74  8"
  },
  {
    "objectID": "bioc-summarizedexperiment.html#constructing-a-summarizedexperiment",
    "href": "bioc-summarizedexperiment.html#constructing-a-summarizedexperiment",
    "title": "\n17  Introduction to SummarizedExperiment\n",
    "section": "\n17.3 Constructing a SummarizedExperiment\n",
    "text": "17.3 Constructing a SummarizedExperiment\n\nOften, SummarizedExperiment or RangedSummarizedExperiment objects are returned by functions written by other packages. However it is possible to create them by hand with a call to the SummarizedExperiment() constructor. The code below is simply to illustrate the mechanics of creating an object from scratch. In practice, you will probably have the pieces of the object from other sources such as Excel files or csv files.\nConstructing a RangedSummarizedExperiment with a GRanges as the rowRanges argument:\n\nnrows &lt;- 200\nncols &lt;- 6\ncounts &lt;- matrix(runif(nrows * ncols, 1, 1e4), nrows)\nrowRanges &lt;- GRanges(rep(c(\"chr1\", \"chr2\"), c(50, 150)),\n                     IRanges(floor(runif(200, 1e5, 1e6)), width=100),\n                     strand=sample(c(\"+\", \"-\"), 200, TRUE),\n                     feature_id=sprintf(\"ID%03d\", 1:200))\ncolData &lt;- DataFrame(Treatment=rep(c(\"ChIP\", \"Input\"), 3),\n                     row.names=LETTERS[1:6])\n\nSummarizedExperiment(assays=list(counts=counts),\n                     rowRanges=rowRanges, colData=colData)\n\nclass: RangedSummarizedExperiment \ndim: 200 6 \nmetadata(0):\nassays(1): counts\nrownames: NULL\nrowData names(1): feature_id\ncolnames(6): A B ... E F\ncolData names(1): Treatment\n\n\nA SummarizedExperiment can be constructed with or without supplying a DataFrame for the rowData argument:\n\nSummarizedExperiment(assays=list(counts=counts), colData=colData)\n\nclass: SummarizedExperiment \ndim: 200 6 \nmetadata(0):\nassays(1): counts\nrownames: NULL\nrowData names(0):\ncolnames(6): A B ... E F\ncolData names(1): Treatment"
  },
  {
    "objectID": "eda_with_pca.html#introduction",
    "href": "eda_with_pca.html#introduction",
    "title": "\n18  EDA with PCA\n",
    "section": "\n18.1 Introduction",
    "text": "18.1 Introduction\nIn this tutorial, we will use the GEOquery package to download a dataset from the Gene Expression Omnibus (GEO) and perform some exploratory data analysis (EDA) using principal components analysis (PCA)."
  },
  {
    "objectID": "eda_with_pca.html#downloading-data-from-geo",
    "href": "eda_with_pca.html#downloading-data-from-geo",
    "title": "\n18  EDA with PCA\n",
    "section": "\n18.2 Downloading data from GEO",
    "text": "18.2 Downloading data from GEO\nThe GEOquery package can be used to download data from GEO. The getGEO function takes a GEO accession number as an argument and returns a list of ExpressionSet objects. The [[1]] at the end of the getGEO call is used to extract the first (and only) ExpressionSet object from the list.\nHistorically, it was not uncommon for GEO datasets to contain multiple separate experiments. In those cases, the [[1]] would need to be replaced with the index of the experiment of interest. However, it is now uncommon for GEO datasets to contain multiple experiments, but the [[1]] is still needed to extract the ExpressionSet object from the list.\n\nlibrary(GEOquery)\nlibrary(SummarizedExperiment)\n\nExpressionSet objects are a type of Bioconductor object that is used to store gene expression data. The as function can be used to convert the ExpressionSet object to a SummarizedExperiment object, which is a newer Bioconductor object that is used to store gene expression data. The SummarizedExperiment object is preferred over the ExpressionSet object so we immediately convert the ExpressionSet object to a SummarizedExperiment.\n\ngse &lt;- getGEO(\"GSE30219\")[[1]]\n\nFound 1 file(s)\n\n\nGSE30219_series_matrix.txt.gz\n\nse &lt;- as(gse, \"SummarizedExperiment\")"
  },
  {
    "objectID": "eda_with_pca.html#filtering-genes",
    "href": "eda_with_pca.html#filtering-genes",
    "title": "\n18  EDA with PCA\n",
    "section": "\n18.3 Filtering genes",
    "text": "18.3 Filtering genes\nWhen performing PCA, it is common to filter to the most variable genes before performing the PCA. Limiting genes to the most variable genes can help to reduce the computational burden of the PCA.\nWe can calculate the standard deviation of each gene using the apply function. The apply function takes a matrix as the first argument and a 1 or 2 to indicate whether the function should be applied to the rows or columns of the matrix. The sd function calculates the standard deviation of a vector and is performed on each row of the matrix.\nA histogram of the standard deviations is not that useful, but it is easy to make.\n\nsds = apply(assay(se, 'exprs'), 1, sd)\nhist(sds)\n\n\n\n\nHere, we produce a subset of the SummarizedExperiment object that contains only the 500 most variable genes. We’ll use this subset for the rest of the tutorial. Feel free to revisit the number of genes you choose to keep and see how it affects the PCA.\n\nsub_se = se[order(sds,decreasing = TRUE)[1:500],]"
  },
  {
    "objectID": "eda_with_pca.html#pca",
    "href": "eda_with_pca.html#pca",
    "title": "\n18  EDA with PCA\n",
    "section": "\n18.4 PCA",
    "text": "18.4 PCA\nPCA is a method for dimensionality reduction. It is a linear transformation that finds the directions of maximum variance in a dataset and projects it onto a new subspace with equal or fewer dimensions than the original one. The orthogonal axes (principal components) of the new subspace can be interpreted as the directions of maximum variance given the constraint that the new feature axes are orthogonal to each other.\n\n\nThe matrix decomposition of the first PC and how we can use it to construct the dimensionally-reduced dataset.\n\n\n# read the help for prcomp here to see what the arguments are\n# ?prcomp\npca = prcomp(t(assay(sub_se,'exprs')))\n\nThe PCA algorithm results in a rotation matrix that can be used to transform the original data into the new subspace. The rotation matrix is stored in the rotation slot of the prcomp object and represents the loadings of each gene for each principle component. The prcomp function also stores the coordinates of the samples in the new subspace in the x slot, which represents the locations of the samples in principle component space.\n\nstr(pca)\n\nList of 5\n $ sdev    : num [1:307] 27.01 22.78 13.46 10.43 9.35 ...\n $ rotation: num [1:500, 1:307] -0.1091 0.0598 -0.0474 -0.0513 -0.0903 ...\n  ..- attr(*, \"dimnames\")=List of 2\n  .. ..$ : chr [1:500] \"209125_at\" \"209988_s_at\" \"223678_s_at\" \"218835_at\" ...\n  .. ..$ : chr [1:307] \"PC1\" \"PC2\" \"PC3\" \"PC4\" ...\n $ center  : Named num [1:500] 6.86 5.56 7.99 10.39 7.82 ...\n  ..- attr(*, \"names\")= chr [1:500] \"209125_at\" \"209988_s_at\" \"223678_s_at\" \"218835_at\" ...\n $ scale   : logi FALSE\n $ x       : num [1:307, 1:307] 0.571 -3.528 5.289 -31.486 1.273 ...\n  ..- attr(*, \"dimnames\")=List of 2\n  .. ..$ : chr [1:307] \"GSM748053\" \"GSM748054\" \"GSM748055\" \"GSM748056\" ...\n  .. ..$ : chr [1:307] \"PC1\" \"PC2\" \"PC3\" \"PC4\" ...\n - attr(*, \"class\")= chr \"prcomp\"\n\n\nThe prcomp function also centers the data by default. The centering values are stored in the center slot. The x slot contains the coordinates of the samples in the new subspace. The\nWe can plot the samples using the first two PCs as the x and y axes.\n\nplot(pca$x[,1], pca$x[,2], pch=20)\n\n\n\nFigure 18.1: PCA plot of samples in the first two PCs.\n\n\n\nIf we use ALL the PCs, we can perform a matrix multiplication to get the original data back.\n\norig_data = pca$rotation %*% t(pca$x) + pca$center\norig_data[1:5,1:5]\n\n            GSM748053 GSM748054 GSM748055 GSM748056 GSM748057\n209125_at    4.400830  4.349534  3.661922 10.289194  3.354648\n209988_s_at  3.078285  3.079797  3.467936  3.447669  3.141168\n223678_s_at 11.389715 10.637554  5.956832  9.311594 10.467811\n218835_at   13.541261 12.944545  9.725079 12.744177 12.896846\n201820_at    5.056486  5.030666  4.986433 11.284134  5.132626\n\n\nCompare to the original data:\n\nassay(sub_se,'exprs')[1:5,1:5]\n\n            GSM748053 GSM748054 GSM748055 GSM748056 GSM748057\n209125_at    4.400830  4.349534  3.661922 10.289194  3.354648\n209988_s_at  3.078285  3.079797  3.467936  3.447669  3.141168\n223678_s_at 11.389715 10.637554  5.956831  9.311594 10.467811\n218835_at   13.541261 12.944545  9.725079 12.744177 12.896846\n201820_at    5.056486  5.030666  4.986433 11.284134  5.132626\n\n\nAnd the same thing, but using only the first 3 PCs:\n\norig_data_3pcs = pca$rotation[,1:3] %*% t(pca$x[,1:3]) + pca$center\norig_data_3pcs[1:5,1:5]\n\n            GSM748053 GSM748054 GSM748055 GSM748056 GSM748057\n209125_at    4.302207  5.319141  4.660544  9.765880  4.368261\n209988_s_at  3.509123  4.372072  4.644271  4.552902  4.251617\n223678_s_at 12.637489 11.381274 10.702921  9.672075 11.985243\n218835_at   14.587942 13.535895 12.807482 12.279706 14.037817\n201820_at    5.300632  6.248694  5.741837 10.170421  5.391310"
  },
  {
    "objectID": "eda_with_pca.html#variance-explained",
    "href": "eda_with_pca.html#variance-explained",
    "title": "\n18  EDA with PCA\n",
    "section": "\n18.5 Variance explained",
    "text": "18.5 Variance explained\nOften, we want to know how much of the variance in the data is explained by each PC. The pca object has a slot called sdev that represents the standard deviation of the principle component. Variance is the square of sdev, so we can calculate the variance by squaring sdev.\n\nvar_explained = pca$sdev ^ 2\n\nThe total variance is just the sum of all the variances:\n\ntot_variance = sum(var_explained)\n\nAnd the proportion of the variance explained by each PC is then\n\nprop_var_explained = var_explained/tot_variance\nhead(prop_var_explained)\n\n[1] 0.27748451 0.19747527 0.06892190 0.04138543 0.03325101 0.02956515\n\n\nIf we plot the prop_var_explained, it is called a scree plot and can help us to choose an appropriate number of PCs to “keep” in order to reduce the dimensionality.\n\nplot(prop_var_explained[1:15], type='b')\n\n\n\n\nExamine the plot. How many PCs would you keep?"
  },
  {
    "objectID": "eda_with_pca.html#add-pcs-to-our-summarizedexperiment-object",
    "href": "eda_with_pca.html#add-pcs-to-our-summarizedexperiment-object",
    "title": "\n18  EDA with PCA\n",
    "section": "\n18.6 Add PCs to our SummarizedExperiment object",
    "text": "18.6 Add PCs to our SummarizedExperiment object\nRecall that the x matrix stored in the pca object represent the coordinates of the samples in the new subspace. We can look at the first five rows and columns of the x matrix to see what it looks like.\n\npca$x[1:5,1:5]\n\n                  PC1       PC2       PC3        PC4        PC5\nGSM748053   0.5712872 34.105929 15.208118  -4.738482  3.4101384\nGSM748054  -3.5283223 24.664382  7.632319 -11.746590  0.1405872\nGSM748055   5.2892621 21.841834  9.092392  -3.022823 11.7810845\nGSM748056 -31.4864198  3.762922 -5.332805   9.695495 -8.8944295\nGSM748057   1.2726085 30.640997 10.562888   6.294335  7.1601434\n\n\nSo, PC components for each sample are in columns and samples are in rows. For colData, the samples are also in rows. So, we can join the PC values to the SummarizedExperiment, sub_se, for later use and for comparison to other sample metadata.\n\n# We can use cbind to join the PC values to the colData\n# note that the names of the rows are the same for both\ncolData(sub_se) = cbind(colData(sub_se), pca$x)\n\nWe now have the PCs stored conveniently with our SummarizedExperiment.\n\ncolnames(colData(sub_se))\n\n  [1] \"title\"                              \n  [2] \"geo_accession\"                      \n  [3] \"status\"                             \n  [4] \"submission_date\"                    \n  [5] \"last_update_date\"                   \n  [6] \"type\"                               \n  [7] \"channel_count\"                      \n  [8] \"source_name_ch1\"                    \n  [9] \"organism_ch1\"                       \n [10] \"characteristics_ch1\"                \n [11] \"characteristics_ch1.1\"              \n [12] \"characteristics_ch1.2\"              \n [13] \"characteristics_ch1.3\"              \n [14] \"characteristics_ch1.4\"              \n [15] \"characteristics_ch1.5\"              \n [16] \"characteristics_ch1.6\"              \n [17] \"characteristics_ch1.7\"              \n [18] \"characteristics_ch1.8\"              \n [19] \"characteristics_ch1.9\"              \n [20] \"characteristics_ch1.10\"             \n [21] \"molecule_ch1\"                       \n [22] \"extract_protocol_ch1\"               \n [23] \"label_ch1\"                          \n [24] \"label_protocol_ch1\"                 \n [25] \"taxid_ch1\"                          \n [26] \"hyb_protocol\"                       \n [27] \"scan_protocol\"                      \n [28] \"description\"                        \n [29] \"data_processing\"                    \n [30] \"platform_id\"                        \n [31] \"contact_name\"                       \n [32] \"contact_laboratory\"                 \n [33] \"contact_department\"                 \n [34] \"contact_institute\"                  \n [35] \"contact_address\"                    \n [36] \"contact_city\"                       \n [37] \"contact_zip.postal_code\"            \n [38] \"contact_country\"                    \n [39] \"supplementary_file\"                 \n [40] \"data_row_count\"                     \n [41] \"age.at.surgery.ch1\"                 \n [42] \"disease.free.survival.in.months.ch1\"\n [43] \"follow.up.time..months..ch1\"        \n [44] \"gender.ch1\"                         \n [45] \"histology.ch1\"                      \n [46] \"pm.stage.ch1\"                       \n [47] \"pn.stage.ch1\"                       \n [48] \"pt.stage.ch1\"                       \n [49] \"relapse..event.1..no.event.0..ch1\"  \n [50] \"status.ch1\"                         \n [51] \"tissue.ch1\"                         \n [52] \"PC1\"                                \n [53] \"PC2\"                                \n [54] \"PC3\"                                \n [55] \"PC4\"                                \n [56] \"PC5\"                                \n [57] \"PC6\"                                \n [58] \"PC7\"                                \n [59] \"PC8\"                                \n [60] \"PC9\"                                \n [61] \"PC10\"                               \n [62] \"PC11\"                               \n [63] \"PC12\"                               \n [64] \"PC13\"                               \n [65] \"PC14\"                               \n [66] \"PC15\"                               \n [67] \"PC16\"                               \n [68] \"PC17\"                               \n [69] \"PC18\"                               \n [70] \"PC19\"                               \n [71] \"PC20\"                               \n [72] \"PC21\"                               \n [73] \"PC22\"                               \n [74] \"PC23\"                               \n [75] \"PC24\"                               \n [76] \"PC25\"                               \n [77] \"PC26\"                               \n [78] \"PC27\"                               \n [79] \"PC28\"                               \n [80] \"PC29\"                               \n [81] \"PC30\"                               \n [82] \"PC31\"                               \n [83] \"PC32\"                               \n [84] \"PC33\"                               \n [85] \"PC34\"                               \n [86] \"PC35\"                               \n [87] \"PC36\"                               \n [88] \"PC37\"                               \n [89] \"PC38\"                               \n [90] \"PC39\"                               \n [91] \"PC40\"                               \n [92] \"PC41\"                               \n [93] \"PC42\"                               \n [94] \"PC43\"                               \n [95] \"PC44\"                               \n [96] \"PC45\"                               \n [97] \"PC46\"                               \n [98] \"PC47\"                               \n [99] \"PC48\"                               \n[100] \"PC49\"                               \n[101] \"PC50\"                               \n[102] \"PC51\"                               \n[103] \"PC52\"                               \n[104] \"PC53\"                               \n[105] \"PC54\"                               \n[106] \"PC55\"                               \n[107] \"PC56\"                               \n[108] \"PC57\"                               \n[109] \"PC58\"                               \n[110] \"PC59\"                               \n[111] \"PC60\"                               \n[112] \"PC61\"                               \n[113] \"PC62\"                               \n[114] \"PC63\"                               \n[115] \"PC64\"                               \n[116] \"PC65\"                               \n[117] \"PC66\"                               \n[118] \"PC67\"                               \n[119] \"PC68\"                               \n[120] \"PC69\"                               \n[121] \"PC70\"                               \n[122] \"PC71\"                               \n[123] \"PC72\"                               \n[124] \"PC73\"                               \n[125] \"PC74\"                               \n[126] \"PC75\"                               \n[127] \"PC76\"                               \n[128] \"PC77\"                               \n[129] \"PC78\"                               \n[130] \"PC79\"                               \n[131] \"PC80\"                               \n[132] \"PC81\"                               \n[133] \"PC82\"                               \n[134] \"PC83\"                               \n[135] \"PC84\"                               \n[136] \"PC85\"                               \n[137] \"PC86\"                               \n[138] \"PC87\"                               \n[139] \"PC88\"                               \n[140] \"PC89\"                               \n[141] \"PC90\"                               \n[142] \"PC91\"                               \n[143] \"PC92\"                               \n[144] \"PC93\"                               \n[145] \"PC94\"                               \n[146] \"PC95\"                               \n[147] \"PC96\"                               \n[148] \"PC97\"                               \n[149] \"PC98\"                               \n[150] \"PC99\"                               \n[151] \"PC100\"                              \n[152] \"PC101\"                              \n[153] \"PC102\"                              \n[154] \"PC103\"                              \n[155] \"PC104\"                              \n[156] \"PC105\"                              \n[157] \"PC106\"                              \n[158] \"PC107\"                              \n[159] \"PC108\"                              \n[160] \"PC109\"                              \n[161] \"PC110\"                              \n[162] \"PC111\"                              \n[163] \"PC112\"                              \n[164] \"PC113\"                              \n[165] \"PC114\"                              \n[166] \"PC115\"                              \n[167] \"PC116\"                              \n[168] \"PC117\"                              \n[169] \"PC118\"                              \n[170] \"PC119\"                              \n[171] \"PC120\"                              \n[172] \"PC121\"                              \n[173] \"PC122\"                              \n[174] \"PC123\"                              \n[175] \"PC124\"                              \n[176] \"PC125\"                              \n[177] \"PC126\"                              \n[178] \"PC127\"                              \n[179] \"PC128\"                              \n[180] \"PC129\"                              \n[181] \"PC130\"                              \n[182] \"PC131\"                              \n[183] \"PC132\"                              \n[184] \"PC133\"                              \n[185] \"PC134\"                              \n[186] \"PC135\"                              \n[187] \"PC136\"                              \n[188] \"PC137\"                              \n[189] \"PC138\"                              \n[190] \"PC139\"                              \n[191] \"PC140\"                              \n[192] \"PC141\"                              \n[193] \"PC142\"                              \n[194] \"PC143\"                              \n[195] \"PC144\"                              \n[196] \"PC145\"                              \n[197] \"PC146\"                              \n[198] \"PC147\"                              \n[199] \"PC148\"                              \n[200] \"PC149\"                              \n[201] \"PC150\"                              \n[202] \"PC151\"                              \n[203] \"PC152\"                              \n[204] \"PC153\"                              \n[205] \"PC154\"                              \n[206] \"PC155\"                              \n[207] \"PC156\"                              \n[208] \"PC157\"                              \n[209] \"PC158\"                              \n[210] \"PC159\"                              \n[211] \"PC160\"                              \n[212] \"PC161\"                              \n[213] \"PC162\"                              \n[214] \"PC163\"                              \n[215] \"PC164\"                              \n[216] \"PC165\"                              \n[217] \"PC166\"                              \n[218] \"PC167\"                              \n[219] \"PC168\"                              \n[220] \"PC169\"                              \n[221] \"PC170\"                              \n[222] \"PC171\"                              \n[223] \"PC172\"                              \n[224] \"PC173\"                              \n[225] \"PC174\"                              \n[226] \"PC175\"                              \n[227] \"PC176\"                              \n[228] \"PC177\"                              \n[229] \"PC178\"                              \n[230] \"PC179\"                              \n[231] \"PC180\"                              \n[232] \"PC181\"                              \n[233] \"PC182\"                              \n[234] \"PC183\"                              \n[235] \"PC184\"                              \n[236] \"PC185\"                              \n[237] \"PC186\"                              \n[238] \"PC187\"                              \n[239] \"PC188\"                              \n[240] \"PC189\"                              \n[241] \"PC190\"                              \n[242] \"PC191\"                              \n[243] \"PC192\"                              \n[244] \"PC193\"                              \n[245] \"PC194\"                              \n[246] \"PC195\"                              \n[247] \"PC196\"                              \n[248] \"PC197\"                              \n[249] \"PC198\"                              \n[250] \"PC199\"                              \n[251] \"PC200\"                              \n[252] \"PC201\"                              \n[253] \"PC202\"                              \n[254] \"PC203\"                              \n[255] \"PC204\"                              \n[256] \"PC205\"                              \n[257] \"PC206\"                              \n[258] \"PC207\"                              \n[259] \"PC208\"                              \n[260] \"PC209\"                              \n[261] \"PC210\"                              \n[262] \"PC211\"                              \n[263] \"PC212\"                              \n[264] \"PC213\"                              \n[265] \"PC214\"                              \n[266] \"PC215\"                              \n[267] \"PC216\"                              \n[268] \"PC217\"                              \n[269] \"PC218\"                              \n[270] \"PC219\"                              \n[271] \"PC220\"                              \n[272] \"PC221\"                              \n[273] \"PC222\"                              \n[274] \"PC223\"                              \n[275] \"PC224\"                              \n[276] \"PC225\"                              \n[277] \"PC226\"                              \n[278] \"PC227\"                              \n[279] \"PC228\"                              \n[280] \"PC229\"                              \n[281] \"PC230\"                              \n[282] \"PC231\"                              \n[283] \"PC232\"                              \n[284] \"PC233\"                              \n[285] \"PC234\"                              \n[286] \"PC235\"                              \n[287] \"PC236\"                              \n[288] \"PC237\"                              \n[289] \"PC238\"                              \n[290] \"PC239\"                              \n[291] \"PC240\"                              \n[292] \"PC241\"                              \n[293] \"PC242\"                              \n[294] \"PC243\"                              \n[295] \"PC244\"                              \n[296] \"PC245\"                              \n[297] \"PC246\"                              \n[298] \"PC247\"                              \n[299] \"PC248\"                              \n[300] \"PC249\"                              \n[301] \"PC250\"                              \n[302] \"PC251\"                              \n[303] \"PC252\"                              \n[304] \"PC253\"                              \n[305] \"PC254\"                              \n[306] \"PC255\"                              \n[307] \"PC256\"                              \n[308] \"PC257\"                              \n[309] \"PC258\"                              \n[310] \"PC259\"                              \n[311] \"PC260\"                              \n[312] \"PC261\"                              \n[313] \"PC262\"                              \n[314] \"PC263\"                              \n[315] \"PC264\"                              \n[316] \"PC265\"                              \n[317] \"PC266\"                              \n[318] \"PC267\"                              \n[319] \"PC268\"                              \n[320] \"PC269\"                              \n[321] \"PC270\"                              \n[322] \"PC271\"                              \n[323] \"PC272\"                              \n[324] \"PC273\"                              \n[325] \"PC274\"                              \n[326] \"PC275\"                              \n[327] \"PC276\"                              \n[328] \"PC277\"                              \n[329] \"PC278\"                              \n[330] \"PC279\"                              \n[331] \"PC280\"                              \n[332] \"PC281\"                              \n[333] \"PC282\"                              \n[334] \"PC283\"                              \n[335] \"PC284\"                              \n[336] \"PC285\"                              \n[337] \"PC286\"                              \n[338] \"PC287\"                              \n[339] \"PC288\"                              \n[340] \"PC289\"                              \n[341] \"PC290\"                              \n[342] \"PC291\"                              \n[343] \"PC292\"                              \n[344] \"PC293\"                              \n[345] \"PC294\"                              \n[346] \"PC295\"                              \n[347] \"PC296\"                              \n[348] \"PC297\"                              \n[349] \"PC298\"                              \n[350] \"PC299\"                              \n[351] \"PC300\"                              \n[352] \"PC301\"                              \n[353] \"PC302\"                              \n[354] \"PC303\"                              \n[355] \"PC304\"                              \n[356] \"PC305\"                              \n[357] \"PC306\"                              \n[358] \"PC307\""
  },
  {
    "objectID": "eda_with_pca.html#variable-relationships",
    "href": "eda_with_pca.html#variable-relationships",
    "title": "\n18  EDA with PCA\n",
    "section": "\n18.7 Variable relationships",
    "text": "18.7 Variable relationships\nLooking at relationships between variables can be a really useful way of generating hypotheses, performing quality control, and suggesting areas to focus in analysis. One common approach to looking at a few variables and their relationships is the “pairs” plot.\nThe GGally package has a function called ggpairs that can be used to generate a pairs plot for a few variables.\n\nlibrary(GGally)\n\nTake a look at this website and examine some variable relationships in the colData(sub_se). When working with ggplot (and ggpairs), you’ll likely want to convert the colData() to a data.frame first. See Figure 18.2 for an example.\n\nggpairs(as.data.frame(colData(sub_se)),columns=(c(\"PC1\",\"PC2\",\"PC3\",\"gender.ch1\")))\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\nFigure 18.2: A pairs plot of a few variables.\n\n\n\nThe ggpairs function is very flexible and plays well with ggplot. Therefore, you can add aes() to the ggpairs function to add colors, etc. to the plot (see Figure 18.3). Look at other variables that you might want to include and style the plot to your liking.\n\nggpairs(as.data.frame(colData(sub_se)),columns=(c(\"PC1\",\"PC2\",\"PC3\",\"gender.ch1\")), \n        aes(color=gender.ch1, alpha=0.5))\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\nFigure 18.3: A pairs plot colored by a variable of interest."
  },
  {
    "objectID": "ranges_and_signals.html#introduction",
    "href": "ranges_and_signals.html#introduction",
    "title": "\n19  Genomic ranges and features\n",
    "section": "\n19.1 Introduction",
    "text": "19.1 Introduction\nGenomic ranges are a way of describing regions on the genome (or any other linear object, such as a transcript, or even a protein). This functionality is typically found in the GenomicRanges package (Lawrence et al. 2013).\n\nlibrary(GenomicRanges)\n\nThe Bioconductor GenomicRanges package is a comprehensive toolkit designed to handle and manipulate genomic intervals and variables systematized on these intervals. Developed by Bioconductor, this package simplifies the complexity of managing genomic data, facilitating the efficient exploration, manipulation, and visualization of such data. GenomicRanges aids in dealing with the challenges of genomic data, including its massive size, intricate relationships, and high dimensionality.\nThe GenomicRanges package in Bioconductor covers a wide range of use cases related to the management and analysis of genomic data. Here are some key examples:\n\n\nGenomic Feature Manipulation\n\nThe GenomicRanges and GRanges classes can be used to represent and manipulate various genomic features such as genes, transcripts, exons, or single-nucleotide polymorphisms (SNPs). Users can query, subset, resize, shift, or sort these features based on their genomic coordinates.\n\n\n\nGenomic Interval Operations\n\nThe GenomicRanges package provides functions for performing operations on genomic intervals, such as finding overlaps, nearest neighbors, or disjoint intervals. These operations are fundamental to many types of genomic data analyses, such as identifying genes that overlap with ChIP-seq peaks, or finding variants that are in close proximity to each other.\n\n\n\nAlignments and Coverage\n\nThe GAlignments and GAlignmentPairs classes can be used to represent alignments of sequencing reads to a reference genome, such as those produced by a read aligner. Users can then compute coverage of these alignments over genomic ranges of interest, which is a common task in RNA-seq or ChIP-seq analysis.\n\n\n\nAnnotation and Metadata Handling\n\nThe metadata column of a GRanges object can be used to store various types of annotation data associated with genomic ranges, such as gene names, gene biotypes, or experimental scores. This makes it easy to perform analyses that integrate genomic coordinates with other types of biological information.\n\n\n\nGenome Arithmetic\n\nThe GenomicRanges package supports a version of “genome arithmetic”, which includes set operations (union, intersection, set difference) as well as other operations (like coverage, complement, or reduction) that are adapted to the specificities of genomic data.\n\n\n\nEfficient Data Handling\n\nThe CompressedGRangesList class provides a space-efficient way to represent a large list of GRanges objects, which is particularly useful when working with large genomic datasets, such as whole-genome sequencing data.\n\n\n\nThe GenomicRanges package in Bioconductor uses the S4 class system (see Table 19.1), which is a part of the methods package in R. The S4 system is a more rigorous and formal approach to object-oriented programming in R, providing enhanced capabilities for object design and function dispatch.\n\n\n\n\n\n\n\n\n\nClass Name\nDescription\nPotential Use\n\n\n\nGRanges\nRepresents a collection of genomic ranges and associated variables.\nChipSeq peaks, CpG islands, etc.\n\n\nGRangesList\nRepresents a list of GenomicRanges objects.\ntranscript models (exons, introns)\n\n\nRangesList\nRepresents a list of Ranges objects.\n\n\n\nIRanges\nRepresents a collection of integer ranges.\nused mainly to build GRanges, etc.\n\n\nGPos\nRepresents genomic positions.\nSNPs or other single nicleotide locations\n\n\nGAlignments\nRepresents alignments against a reference genome.\nSequence read locations from a BAM file\n\n\nGAlignmentPairs\nRepresents pairs of alignments, typically representing a single fragment of DNA.\nPaired-end sequence alignments\n\n\n\nTable 19.1: Classes within the GenomicRanges package. Each class has a slightly different use case.\nIn the context of the GenomicRanges package, the S4 class system allows for the creation of complex, structured data objects that can effectively encapsulate genomic intervals and associated data. This system enables the package to handle the complexity and intricacy of genomic data.\nFor example, the GenomicRanges class in the package is an S4 class that combines several basic data types into a composite object. It includes slots for sequence names (seqnames), ranges (start and end positions), strand information, and metadata. Each slot in the S4 class corresponds to a specific component of the genomic data, and methods (see Table 19.2 and Table 19.3) can be defined to interact with these slots in a structured and predictable way.\n\n\n\n\n\n\n\n\nMethod\nDescription\n\n\n\nlength\nReturns the number of ranges in the GRanges object.\n\n\nseqnames\nRetrieves the sequence names of the ranges.\n\n\nranges\nRetrieves the start and end positions of the ranges.\n\n\nstrand\nRetrieves the strand information of the ranges.\n\n\nelementMetadata\nRetrieves the metadata associated with the ranges.\n\n\nseqlevels\nReturns the levels of the factor that the seqnames slot is derived from.\n\n\nseqinfo\nRetrieves the Seqinfo (sequence information) object associated with the GRanges object.\n\n\nstart, end, width\nRetrieve or set the start or end positions, or the width of the ranges.\n\n\nresize\nResizes the ranges.\n\n\nsubset, [, [[, $\nSubset or extract elements from the GRanges object.\n\n\nsort\nSorts the GRanges object.\n\n\nshift\nShifts the ranges by a certain number of base pairs.\n\n\n\nTable 19.2: Methods for accessing, manipulating single objects\nThe S4 class system also supports inheritance, which allows for the creation of specialized subclasses that share certain characteristics with a parent class but have additional features or behaviors.\nThe S4 system’s formalism and rigor make it well-suited to the complexities of bioinformatics and genomic data analysis. It allows for the creation of robust, reliable code that can handle complex data structures and operations, making it a key part of the GenomicRanges package and other Bioconductor packages.\n\n\n\n\n\n\n\n\nMethod\nDescription\n\n\n\nfindOverlaps\nFinds overlaps between different sets of ranges.\n\n\ncountOverlaps\nCounts overlaps between different sets of ranges.\n\n\nsubsetByOverlaps\nSubsets the ranges based on overlaps.\n\n\ndistanceToNearest\nComputes the distance to the nearest range in another set of ranges.\n\n\n\nTable 19.3: Methods for comparing and combining multiple GenomicRanges-class objects\n\n\nFigure 19.1: The structure of a GRangesList, which is a list of GRanges objects. While the analogy is not perfect, a GRangesList behaves a bit like a list. Each element in the GRangesList is a Granges object. A common use case for a GRangesList is to store a list of transcripts, each of which have exons as the regions in the GRanges."
  },
  {
    "objectID": "ranges_and_signals.html#the-granges-class",
    "href": "ranges_and_signals.html#the-granges-class",
    "title": "\n19  Genomic ranges and features\n",
    "section": "\n19.2 The GRanges class",
    "text": "19.2 The GRanges class\nTo get going, we can construct a GRanges object by hand as an example.\nThe GRanges class represents a collection of genomic ranges that each have a single start and end location on the genome. It can be used to store the location of genomic features such as contiguous binding sites, transcripts, and exons. These objects can be created by using the GRanges constructor function. The following code just creates a GRanges object from scratch.\n\ngr &lt;- GRanges(\n    seqnames = Rle(c(\"chr1\", \"chr2\", \"chr1\", \"chr3\"), c(1, 3, 2, 4)),\n    ranges = IRanges(101:110, end = 111:120, names = head(letters, 10)),\n    strand = Rle(strand(c(\"-\", \"+\", \"*\", \"+\", \"-\")), c(1, 2, 2, 3, 2)),\n    score = 1:10,\n    GC = seq(1, 0, length=10))\ngr\n\nGRanges object with 10 ranges and 2 metadata columns:\n    seqnames    ranges strand |     score        GC\n       &lt;Rle&gt; &lt;IRanges&gt;  &lt;Rle&gt; | &lt;integer&gt; &lt;numeric&gt;\n  a     chr1   101-111      - |         1  1.000000\n  b     chr2   102-112      + |         2  0.888889\n  c     chr2   103-113      + |         3  0.777778\n  d     chr2   104-114      * |         4  0.666667\n  e     chr1   105-115      * |         5  0.555556\n  f     chr1   106-116      + |         6  0.444444\n  g     chr3   107-117      + |         7  0.333333\n  h     chr3   108-118      + |         8  0.222222\n  i     chr3   109-119      - |         9  0.111111\n  j     chr3   110-120      - |        10  0.000000\n  -------\n  seqinfo: 3 sequences from an unspecified genome; no seqlengths\n\n\nThis creates a GRanges object with 10 genomic ranges. The output of the GRanges show() method separates the information into a left and right hand region that are separated by | symbols (see Figure 19.2). The genomic coordinates (seqnames, ranges, and strand) are located on the left-hand side and the metadata columns are located on the right. For this example, the metadata is comprised of score and GC information, but almost anything can be stored in the metadata portion of a GRanges object.\n\n\nFigure 19.2: The structure of a GRanges object, which behaves a bit like a vector of ranges, although the analogy is not perfect. A GRanges object is composed of the “Ranges” part the lefthand box, the “metadata” columns (the righthand box), and a “seqinfo” part that describes the names and lengths of associated sequences. Only the “Ranges” part is required. The figure also shows a few of the “accessors” and approaches to subsetting a GRanges object.\n\nThe components of the genomic coordinates within a GRanges object can be extracted using the seqnames, ranges, and strand accessor functions.\n\nseqnames(gr)\n\nfactor-Rle of length 10 with 4 runs\n  Lengths:    1    3    2    4\n  Values : chr1 chr2 chr1 chr3\nLevels(3): chr1 chr2 chr3\n\nranges(gr)\n\nIRanges object with 10 ranges and 0 metadata columns:\n        start       end     width\n    &lt;integer&gt; &lt;integer&gt; &lt;integer&gt;\n  a       101       111        11\n  b       102       112        11\n  c       103       113        11\n  d       104       114        11\n  e       105       115        11\n  f       106       116        11\n  g       107       117        11\n  h       108       118        11\n  i       109       119        11\n  j       110       120        11\n\nstrand(gr)\n\nfactor-Rle of length 10 with 5 runs\n  Lengths: 1 2 2 3 2\n  Values : - + * + -\nLevels(3): + - *\n\n\nNote that the GRanges object has information to the “left” side of the | that has special accessors. The information to the right side of the |, when it is present, is the metadata and is accessed using mcols(), for “metadata columns”.\n\nclass(mcols(gr))\n\n[1] \"DFrame\"\nattr(,\"package\")\n[1] \"S4Vectors\"\n\nmcols(gr)\n\nDataFrame with 10 rows and 2 columns\n      score        GC\n  &lt;integer&gt; &lt;numeric&gt;\na         1  1.000000\nb         2  0.888889\nc         3  0.777778\nd         4  0.666667\ne         5  0.555556\nf         6  0.444444\ng         7  0.333333\nh         8  0.222222\ni         9  0.111111\nj        10  0.000000\n\n\nSince the class of mcols(gr) is DFrame, we can use our DataFrame approaches to work with the data.\n\nmcols(gr)$score\n\n [1]  1  2  3  4  5  6  7  8  9 10\n\n\nWe can even assign a new column.\n\nmcols(gr)$AT = 1-mcols(gr)$GC\ngr\n\nGRanges object with 10 ranges and 3 metadata columns:\n    seqnames    ranges strand |     score        GC        AT\n       &lt;Rle&gt; &lt;IRanges&gt;  &lt;Rle&gt; | &lt;integer&gt; &lt;numeric&gt; &lt;numeric&gt;\n  a     chr1   101-111      - |         1  1.000000  0.000000\n  b     chr2   102-112      + |         2  0.888889  0.111111\n  c     chr2   103-113      + |         3  0.777778  0.222222\n  d     chr2   104-114      * |         4  0.666667  0.333333\n  e     chr1   105-115      * |         5  0.555556  0.444444\n  f     chr1   106-116      + |         6  0.444444  0.555556\n  g     chr3   107-117      + |         7  0.333333  0.666667\n  h     chr3   108-118      + |         8  0.222222  0.777778\n  i     chr3   109-119      - |         9  0.111111  0.888889\n  j     chr3   110-120      - |        10  0.000000  1.000000\n  -------\n  seqinfo: 3 sequences from an unspecified genome; no seqlengths\n\n\nAnother common way to create a GRanges object is to start with a data.frame, perhaps created by hand like below or read in using read.csv or read.table. We can convert from a data.frame, when columns are named appropriately, to a GRanges object.\n\ndf_regions = data.frame(chromosome = rep(\"chr1\",10),\n                        start=seq(1000,10000,1000),\n                        end=seq(1100, 10100, 1000))\nas(df_regions,'GRanges') # note that names have to match with GRanges slots\n\nGRanges object with 10 ranges and 0 metadata columns:\n       seqnames      ranges strand\n          &lt;Rle&gt;   &lt;IRanges&gt;  &lt;Rle&gt;\n   [1]     chr1   1000-1100      *\n   [2]     chr1   2000-2100      *\n   [3]     chr1   3000-3100      *\n   [4]     chr1   4000-4100      *\n   [5]     chr1   5000-5100      *\n   [6]     chr1   6000-6100      *\n   [7]     chr1   7000-7100      *\n   [8]     chr1   8000-8100      *\n   [9]     chr1   9000-9100      *\n  [10]     chr1 10000-10100      *\n  -------\n  seqinfo: 1 sequence from an unspecified genome; no seqlengths\n\n## fix column name\ncolnames(df_regions)[1] = 'seqnames'\ngr2 = as(df_regions,'GRanges')\ngr2\n\nGRanges object with 10 ranges and 0 metadata columns:\n       seqnames      ranges strand\n          &lt;Rle&gt;   &lt;IRanges&gt;  &lt;Rle&gt;\n   [1]     chr1   1000-1100      *\n   [2]     chr1   2000-2100      *\n   [3]     chr1   3000-3100      *\n   [4]     chr1   4000-4100      *\n   [5]     chr1   5000-5100      *\n   [6]     chr1   6000-6100      *\n   [7]     chr1   7000-7100      *\n   [8]     chr1   8000-8100      *\n   [9]     chr1   9000-9100      *\n  [10]     chr1 10000-10100      *\n  -------\n  seqinfo: 1 sequence from an unspecified genome; no seqlengths\n\n\nGRanges have one-dimensional-like behavior. For instance, we can check the length and even give GRanges names.\n\nnames(gr)\n\n [1] \"a\" \"b\" \"c\" \"d\" \"e\" \"f\" \"g\" \"h\" \"i\" \"j\"\n\nlength(gr)\n\n[1] 10\n\n\n\n19.2.1 Subsetting GRanges objects\nWhile GRanges objects look a bit like a data.frame, they can be thought of as vectors with associated ranges. Subsetting, then, works very similarly to vectors. To subset a GRanges object to include only second and third regions:\n\ngr[2:3]\n\nGRanges object with 2 ranges and 3 metadata columns:\n    seqnames    ranges strand |     score        GC        AT\n       &lt;Rle&gt; &lt;IRanges&gt;  &lt;Rle&gt; | &lt;integer&gt; &lt;numeric&gt; &lt;numeric&gt;\n  b     chr2   102-112      + |         2  0.888889  0.111111\n  c     chr2   103-113      + |         3  0.777778  0.222222\n  -------\n  seqinfo: 3 sequences from an unspecified genome; no seqlengths\n\n\nThat said, if the GRanges object has metadata columns, we can also treat it like a two-dimensional object kind of like a data.frame. Note that the information to the left of the | is not like a data.frame, so we cannot do something like gr$seqnames. Here is an example of subsetting with the subset of one metadata column.\n\ngr[2:3, \"GC\"]\n\nGRanges object with 2 ranges and 1 metadata column:\n    seqnames    ranges strand |        GC\n       &lt;Rle&gt; &lt;IRanges&gt;  &lt;Rle&gt; | &lt;numeric&gt;\n  b     chr2   102-112      + |  0.888889\n  c     chr2   103-113      + |  0.777778\n  -------\n  seqinfo: 3 sequences from an unspecified genome; no seqlengths\n\n\nThe usual head() and tail() also work just fine.\n\nhead(gr,n=2)\n\nGRanges object with 2 ranges and 3 metadata columns:\n    seqnames    ranges strand |     score        GC        AT\n       &lt;Rle&gt; &lt;IRanges&gt;  &lt;Rle&gt; | &lt;integer&gt; &lt;numeric&gt; &lt;numeric&gt;\n  a     chr1   101-111      - |         1  1.000000  0.000000\n  b     chr2   102-112      + |         2  0.888889  0.111111\n  -------\n  seqinfo: 3 sequences from an unspecified genome; no seqlengths\n\ntail(gr,n=2)\n\nGRanges object with 2 ranges and 3 metadata columns:\n    seqnames    ranges strand |     score        GC        AT\n       &lt;Rle&gt; &lt;IRanges&gt;  &lt;Rle&gt; | &lt;integer&gt; &lt;numeric&gt; &lt;numeric&gt;\n  i     chr3   109-119      - |         9  0.111111  0.888889\n  j     chr3   110-120      - |        10  0.000000  1.000000\n  -------\n  seqinfo: 3 sequences from an unspecified genome; no seqlengths\n\n\n\n19.2.2 Interval operations on one GRanges object\n\n19.2.2.1 Intra-range methods\nThe methods described in this section work one-region-at-a-time and are, therefore, called “intra-region” methods. Methods that work across all regions are described below in the Inter-range methods section.\nThe GRanges class has accessors for the “ranges” part of the data. For example:\n\n## Make a smaller GRanges subset\ng &lt;- gr[1:3]\nstart(g) # to get start locations\n\n[1] 101 102 103\n\nend(g)   # to get end locations\n\n[1] 111 112 113\n\nwidth(g) # to get the \"widths\" of each range\n\n[1] 11 11 11\n\nrange(g) # to get the \"range\" for each sequence (min(start) through max(end))\n\nGRanges object with 2 ranges and 0 metadata columns:\n      seqnames    ranges strand\n         &lt;Rle&gt; &lt;IRanges&gt;  &lt;Rle&gt;\n  [1]     chr1   101-111      -\n  [2]     chr2   102-113      +\n  -------\n  seqinfo: 3 sequences from an unspecified genome; no seqlengths\n\n\nThe GRanges class also has many methods for manipulating the ranges. The methods can be classified as intra-range methods, inter-range methods, and between-range methods. Intra-range methods operate on each element of a GRanges object independent of the other ranges in the object. For example, the flank method can be used to recover regions flanking the set of ranges represented by the GRanges object. So to get a GRanges object containing the ranges that include the 10 bases upstream of the ranges:\n\nflank(g, 10)\n\nGRanges object with 3 ranges and 3 metadata columns:\n    seqnames    ranges strand |     score        GC        AT\n       &lt;Rle&gt; &lt;IRanges&gt;  &lt;Rle&gt; | &lt;integer&gt; &lt;numeric&gt; &lt;numeric&gt;\n  a     chr1   112-121      - |         1  1.000000  0.000000\n  b     chr2    92-101      + |         2  0.888889  0.111111\n  c     chr2    93-102      + |         3  0.777778  0.222222\n  -------\n  seqinfo: 3 sequences from an unspecified genome; no seqlengths\n\n\nNote how flank pays attention to “strand”. To get the flanking regions downstream of the ranges, we can do:\n\nflank(g, 10, start=FALSE)\n\nGRanges object with 3 ranges and 3 metadata columns:\n    seqnames    ranges strand |     score        GC        AT\n       &lt;Rle&gt; &lt;IRanges&gt;  &lt;Rle&gt; | &lt;integer&gt; &lt;numeric&gt; &lt;numeric&gt;\n  a     chr1    91-100      - |         1  1.000000  0.000000\n  b     chr2   113-122      + |         2  0.888889  0.111111\n  c     chr2   114-123      + |         3  0.777778  0.222222\n  -------\n  seqinfo: 3 sequences from an unspecified genome; no seqlengths\n\n\nOther examples of intra-range methods include resize and shift. The shift method will move the ranges by a specific number of base pairs, and the resize method will extend the ranges by a specified width.\n\nshift(g, 5)\n\nGRanges object with 3 ranges and 3 metadata columns:\n    seqnames    ranges strand |     score        GC        AT\n       &lt;Rle&gt; &lt;IRanges&gt;  &lt;Rle&gt; | &lt;integer&gt; &lt;numeric&gt; &lt;numeric&gt;\n  a     chr1   106-116      - |         1  1.000000  0.000000\n  b     chr2   107-117      + |         2  0.888889  0.111111\n  c     chr2   108-118      + |         3  0.777778  0.222222\n  -------\n  seqinfo: 3 sequences from an unspecified genome; no seqlengths\n\nresize(g, 30)\n\nGRanges object with 3 ranges and 3 metadata columns:\n    seqnames    ranges strand |     score        GC        AT\n       &lt;Rle&gt; &lt;IRanges&gt;  &lt;Rle&gt; | &lt;integer&gt; &lt;numeric&gt; &lt;numeric&gt;\n  a     chr1    82-111      - |         1  1.000000  0.000000\n  b     chr2   102-131      + |         2  0.888889  0.111111\n  c     chr2   103-132      + |         3  0.777778  0.222222\n  -------\n  seqinfo: 3 sequences from an unspecified genome; no seqlengths\n\n\nThe GenomicRanges help page ?\"intra-range-methods\" summarizes these methods.\n\n19.2.2.2 Inter-range methods\nInter-range methods involve comparisons between ranges in a single GRanges object. For instance, the reduce method will align the ranges and merge overlapping ranges to produce a simplified set.\n\nreduce(g)\n\nGRanges object with 2 ranges and 0 metadata columns:\n      seqnames    ranges strand\n         &lt;Rle&gt; &lt;IRanges&gt;  &lt;Rle&gt;\n  [1]     chr1   101-111      -\n  [2]     chr2   102-113      +\n  -------\n  seqinfo: 3 sequences from an unspecified genome; no seqlengths\n\n\nThe reduce method could, for example, be used to collapse individual overlapping coding exons into a single set of coding regions.\nSometimes one is interested in the gaps or the qualities of the gaps between the ranges represented by your GRanges object. The gaps method provides this information:\n\ngaps(g)\n\nGRanges object with 2 ranges and 0 metadata columns:\n      seqnames    ranges strand\n         &lt;Rle&gt; &lt;IRanges&gt;  &lt;Rle&gt;\n  [1]     chr1     1-100      -\n  [2]     chr2     1-101      +\n  -------\n  seqinfo: 3 sequences from an unspecified genome; no seqlengths\n\n\nIn this case, we have not specified the lengths of the chromosomes, so Bioconductor is making the assumption (incorrectly) that the chromosomes end at the largest location on each chromosome. We can correct this by setting the seqlengths correctly, but we can ignore that detail for now.\nThe disjoin method represents a GRanges object as a collection of non-overlapping ranges:\n\ndisjoin(g)\n\nGRanges object with 4 ranges and 0 metadata columns:\n      seqnames    ranges strand\n         &lt;Rle&gt; &lt;IRanges&gt;  &lt;Rle&gt;\n  [1]     chr1   101-111      -\n  [2]     chr2       102      +\n  [3]     chr2   103-112      +\n  [4]     chr2       113      +\n  -------\n  seqinfo: 3 sequences from an unspecified genome; no seqlengths\n\n\nThe coverage method quantifies the degree of overlap for all the ranges in a GRanges object.\n\ncoverage(g)\n\nRleList of length 3\n$chr1\ninteger-Rle of length 111 with 2 runs\n  Lengths: 100  11\n  Values :   0   1\n\n$chr2\ninteger-Rle of length 113 with 4 runs\n  Lengths: 101   1  10   1\n  Values :   0   1   2   1\n\n$chr3\ninteger-Rle of length 0 with 0 runs\n  Lengths: \n  Values : \n\n\nThe coverage is summarized as a list of coverages, one for each chromosome. The Rle class is used to store the values. Sometimes, one must convert these values to numeric using as.numeric. In many cases, this will happen automatically, though.\n\ncovg = coverage(g)\ncovg_chr2 = covg[['chr2']]\nplot(covg_chr2, type='l')\n\n\n\n\nSee the GenomicRanges help page ?\"intra-range-methods\" for more details.\n\n19.2.3 Set operations for GRanges objects\nBetween-range methods calculate relationships between different GRanges objects. Of central importance are findOverlaps and related operations; these are discussed below. Additional operations treat GRanges as mathematical sets of coordinates; union(g, g2) is the union of the coordinates in g and g2. Here are examples for calculating the union, the intersect and the asymmetric difference (using setdiff).\n\ng2 &lt;- head(gr, n=2)\nunion(g, g2)\n\nGRanges object with 2 ranges and 0 metadata columns:\n      seqnames    ranges strand\n         &lt;Rle&gt; &lt;IRanges&gt;  &lt;Rle&gt;\n  [1]     chr1   101-111      -\n  [2]     chr2   102-113      +\n  -------\n  seqinfo: 3 sequences from an unspecified genome; no seqlengths\n\nintersect(g, g2)\n\nGRanges object with 2 ranges and 0 metadata columns:\n      seqnames    ranges strand\n         &lt;Rle&gt; &lt;IRanges&gt;  &lt;Rle&gt;\n  [1]     chr1   101-111      -\n  [2]     chr2   102-112      +\n  -------\n  seqinfo: 3 sequences from an unspecified genome; no seqlengths\n\nsetdiff(g, g2)\n\nGRanges object with 1 range and 0 metadata columns:\n      seqnames    ranges strand\n         &lt;Rle&gt; &lt;IRanges&gt;  &lt;Rle&gt;\n  [1]     chr2       113      +\n  -------\n  seqinfo: 3 sequences from an unspecified genome; no seqlengths\n\n\nThere is extensive additional help available or by looking at the vignettes in at the GenomicRanges pages.\n\n?GRanges\n\nThere are also many possible methods that work with GRanges objects. To see a complete list (long), try:\n\nmethods(class=\"GRanges\")"
  },
  {
    "objectID": "ranges_and_signals.html#grangeslist",
    "href": "ranges_and_signals.html#grangeslist",
    "title": "\n19  Genomic ranges and features\n",
    "section": "\n19.3 GRangesList",
    "text": "19.3 GRangesList\nSome important genomic features, such as spliced transcripts that are are comprised of exons, are inherently compound structures. Such a feature makes much more sense when expressed as a compound object such as a GRangesList. If we thing of each transcript as a set of exons, each transcript would be summarized as a GRanges object. However, if we have multiple transcripts, we want to somehow keep them separate, with each transcript having its own exons. The GRangesList is then a list of GRanges objects that. Continuing with the transcripts thought, a GRangesList can contain all the transcripts and their exons; each transcript is an element in the list.\nWhenever genomic features consist of multiple ranges that are grouped by a parent feature, they can be represented as a GRangesList object. Consider the simple example of the two transcript GRangesList below created using the GRangesList constructor.\n\ngr1 &lt;- GRanges(\n    seqnames = \"chr1\", \n    ranges = IRanges(103, 106),\n    strand = \"+\", \n    score = 5L, GC = 0.45)\ngr2 &lt;- GRanges(\n    seqnames = c(\"chr1\", \"chr1\"),\n    ranges = IRanges(c(107, 113), width = 3),\n    strand = c(\"+\", \"-\"), \n    score = 3:4, GC = c(0.3, 0.5))\n\nThe gr1 and gr2 are each GRanges objects. We can combine them into a “named” GRangesList like so:\n\ngrl &lt;- GRangesList(\"txA\" = gr1, \"txB\" = gr2)\ngrl\n\nGRangesList object of length 2:\n$txA\nGRanges object with 1 range and 2 metadata columns:\n      seqnames    ranges strand |     score        GC\n         &lt;Rle&gt; &lt;IRanges&gt;  &lt;Rle&gt; | &lt;integer&gt; &lt;numeric&gt;\n  [1]     chr1   103-106      + |         5      0.45\n  -------\n  seqinfo: 1 sequence from an unspecified genome; no seqlengths\n\n$txB\nGRanges object with 2 ranges and 2 metadata columns:\n      seqnames    ranges strand |     score        GC\n         &lt;Rle&gt; &lt;IRanges&gt;  &lt;Rle&gt; | &lt;integer&gt; &lt;numeric&gt;\n  [1]     chr1   107-109      + |         3       0.3\n  [2]     chr1   113-115      - |         4       0.5\n  -------\n  seqinfo: 1 sequence from an unspecified genome; no seqlengths\n\n\nThe show method for a GRangesList object displays it as a named list of GRanges objects, where the names of this list are considered to be the names of the grouping feature. In the example above, the groups of individual exon ranges are represented as separate GRanges objects which are further organized into a list structure where each element name is a transcript name. Many other combinations of grouped and labeled GRanges objects are possible of course, but this example is a common arrangement.\nIn some cases, GRangesLists behave quite similarly to GRanges objects.\n\n19.3.1 Basic GRangesList accessors\nJust as with GRanges object, the components of the genomic coordinates within a GRangesList object can be extracted using simple accessor methods. Not surprisingly, the GRangesList objects have many of the same accessors as GRanges objects. The difference is that many of these methods return a list since the input is now essentially a list of GRanges objects. Here are a few examples:\n\nseqnames(grl)\n\nRleList of length 2\n$txA\nfactor-Rle of length 1 with 1 run\n  Lengths:    1\n  Values : chr1\nLevels(1): chr1\n\n$txB\nfactor-Rle of length 2 with 1 run\n  Lengths:    2\n  Values : chr1\nLevels(1): chr1\n\nranges(grl)\n\nIRangesList object of length 2:\n$txA\nIRanges object with 1 range and 0 metadata columns:\n          start       end     width\n      &lt;integer&gt; &lt;integer&gt; &lt;integer&gt;\n  [1]       103       106         4\n\n$txB\nIRanges object with 2 ranges and 0 metadata columns:\n          start       end     width\n      &lt;integer&gt; &lt;integer&gt; &lt;integer&gt;\n  [1]       107       109         3\n  [2]       113       115         3\n\nstrand(grl)\n\nRleList of length 2\n$txA\nfactor-Rle of length 1 with 1 run\n  Lengths: 1\n  Values : +\nLevels(3): + - *\n\n$txB\nfactor-Rle of length 2 with 2 runs\n  Lengths: 1 1\n  Values : + -\nLevels(3): + - *\n\n\nThe length and names methods will return the length or names of the list and the seqlengths method will return the set of sequence lengths.\n\nlength(grl)\n\n[1] 2\n\nnames(grl)\n\n[1] \"txA\" \"txB\"\n\nseqlengths(grl)\n\nchr1 \n  NA"
  },
  {
    "objectID": "ranges_and_signals.html#relationships-between-region-sets",
    "href": "ranges_and_signals.html#relationships-between-region-sets",
    "title": "\n19  Genomic ranges and features\n",
    "section": "\n19.4 Relationships between region sets",
    "text": "19.4 Relationships between region sets\nOne of the more powerful approaches to genomic data integration is to ask about the relationship between sets of genomic ranges. The key features of this process are to look at overlaps and distances to the nearest feature. These functionalities, combined with the operations like flank and resize, for instance, allow pretty useful analyses with relatively little code. In general, these operations are very fast, even on thousands to millions of regions.\n\n19.4.1 Overlaps\nThe findOverlaps method in the GenomicRanges package is a very useful function that allows users to identify overlaps between two sets of genomic ranges.\nHere’s how it works:\n\n\nInputs\n\nThe function requires two GRanges objects, referred to as query and subject.\n\n\n\nProcessing\n\nThe function then compares every range in the query object with every range in the subject object, looking for overlaps. An overlap is defined as any instance where the range in the query object intersects with a range in the subject object.\n\n\n\nOutput\n\nThe function returns a Hits (see ?Hits) object, which is a compact representation of the matrix of overlaps. Each entry in the Hits object corresponds to a pair of overlapping ranges, with the query index and the subject index.\n\n\n\nHere is an example of how you might use the findOverlaps function:\n\n# Create two GRanges objects\ngr1 &lt;- gr[1:4]\ngr2 &lt;- gr[3:8]\ngr1\n\nGRanges object with 4 ranges and 3 metadata columns:\n    seqnames    ranges strand |     score        GC        AT\n       &lt;Rle&gt; &lt;IRanges&gt;  &lt;Rle&gt; | &lt;integer&gt; &lt;numeric&gt; &lt;numeric&gt;\n  a     chr1   101-111      - |         1  1.000000  0.000000\n  b     chr2   102-112      + |         2  0.888889  0.111111\n  c     chr2   103-113      + |         3  0.777778  0.222222\n  d     chr2   104-114      * |         4  0.666667  0.333333\n  -------\n  seqinfo: 3 sequences from an unspecified genome; no seqlengths\n\ngr2\n\nGRanges object with 6 ranges and 3 metadata columns:\n    seqnames    ranges strand |     score        GC        AT\n       &lt;Rle&gt; &lt;IRanges&gt;  &lt;Rle&gt; | &lt;integer&gt; &lt;numeric&gt; &lt;numeric&gt;\n  c     chr2   103-113      + |         3  0.777778  0.222222\n  d     chr2   104-114      * |         4  0.666667  0.333333\n  e     chr1   105-115      * |         5  0.555556  0.444444\n  f     chr1   106-116      + |         6  0.444444  0.555556\n  g     chr3   107-117      + |         7  0.333333  0.666667\n  h     chr3   108-118      + |         8  0.222222  0.777778\n  -------\n  seqinfo: 3 sequences from an unspecified genome; no seqlengths\n\n\n\n# Find overlaps  \noverlaps &lt;- findOverlaps(query = gr1, subject = gr2)  \noverlaps\n\nHits object with 7 hits and 0 metadata columns:\n      queryHits subjectHits\n      &lt;integer&gt;   &lt;integer&gt;\n  [1]         1           3\n  [2]         2           1\n  [3]         2           2\n  [4]         3           1\n  [5]         3           2\n  [6]         4           1\n  [7]         4           2\n  -------\n  queryLength: 4 / subjectLength: 6\n\n\nIn the resulting overlaps object, each row corresponds to an overlapping pair of ranges, with the first column giving the index of the range in gr1 and the second column giving the index of the overlapping range in gr2.\nIf you are interested in only the queryHits or the subjectHits, there are accessors for those (ie., queryHits(overlaps)). To get the actual ranges that overlap, you can use the subjectHits or queryHits as an index into the original GRanges object.\nSpend some time looking at these results. Note how the strand comes into play when determining overlaps.\n\ngr1[queryHits(overlaps)]\n\nGRanges object with 7 ranges and 3 metadata columns:\n    seqnames    ranges strand |     score        GC        AT\n       &lt;Rle&gt; &lt;IRanges&gt;  &lt;Rle&gt; | &lt;integer&gt; &lt;numeric&gt; &lt;numeric&gt;\n  a     chr1   101-111      - |         1  1.000000  0.000000\n  b     chr2   102-112      + |         2  0.888889  0.111111\n  b     chr2   102-112      + |         2  0.888889  0.111111\n  c     chr2   103-113      + |         3  0.777778  0.222222\n  c     chr2   103-113      + |         3  0.777778  0.222222\n  d     chr2   104-114      * |         4  0.666667  0.333333\n  d     chr2   104-114      * |         4  0.666667  0.333333\n  -------\n  seqinfo: 3 sequences from an unspecified genome; no seqlengths\n\ngr2[subjectHits(overlaps)]\n\nGRanges object with 7 ranges and 3 metadata columns:\n    seqnames    ranges strand |     score        GC        AT\n       &lt;Rle&gt; &lt;IRanges&gt;  &lt;Rle&gt; | &lt;integer&gt; &lt;numeric&gt; &lt;numeric&gt;\n  e     chr1   105-115      * |         5  0.555556  0.444444\n  c     chr2   103-113      + |         3  0.777778  0.222222\n  d     chr2   104-114      * |         4  0.666667  0.333333\n  c     chr2   103-113      + |         3  0.777778  0.222222\n  d     chr2   104-114      * |         4  0.666667  0.333333\n  c     chr2   103-113      + |         3  0.777778  0.222222\n  d     chr2   104-114      * |         4  0.666667  0.333333\n  -------\n  seqinfo: 3 sequences from an unspecified genome; no seqlengths\n\n\nAs you might expect, the countOverlaps method counts the regions in the second GRanges that overlap with those that overlap with each element of the first.\n\ncountOverlaps(gr1, gr2)\n\na b c d \n1 2 2 2 \n\n\nThe subsetByOverlaps method simply subsets the query GRanges object to include only those that overlap the subject.\n\nsubsetByOverlaps(gr1, gr2)\n\nGRanges object with 4 ranges and 3 metadata columns:\n    seqnames    ranges strand |     score        GC        AT\n       &lt;Rle&gt; &lt;IRanges&gt;  &lt;Rle&gt; | &lt;integer&gt; &lt;numeric&gt; &lt;numeric&gt;\n  a     chr1   101-111      - |         1  1.000000  0.000000\n  b     chr2   102-112      + |         2  0.888889  0.111111\n  c     chr2   103-113      + |         3  0.777778  0.222222\n  d     chr2   104-114      * |         4  0.666667  0.333333\n  -------\n  seqinfo: 3 sequences from an unspecified genome; no seqlengths\n\n\nIn some cases, you may be interested in only one hit when doing overlaps. Note the select parameter. See the help for findOverlaps\n\nfindOverlaps(gr1, gr2, select=\"first\")\n\n[1] 3 1 1 1\n\nfindOverlaps(gr1, gr2, select=\"first\")\n\n[1] 3 1 1 1\n\n\nThe %over% logical operator allows us to do similar things to findOverlaps and subsetByOverlaps.\n\ngr2 %over% gr1\n\n[1]  TRUE  TRUE  TRUE FALSE FALSE FALSE\n\ngr1[gr1 %over% gr2]\n\nGRanges object with 4 ranges and 3 metadata columns:\n    seqnames    ranges strand |     score        GC        AT\n       &lt;Rle&gt; &lt;IRanges&gt;  &lt;Rle&gt; | &lt;integer&gt; &lt;numeric&gt; &lt;numeric&gt;\n  a     chr1   101-111      - |         1  1.000000  0.000000\n  b     chr2   102-112      + |         2  0.888889  0.111111\n  c     chr2   103-113      + |         3  0.777778  0.222222\n  d     chr2   104-114      * |         4  0.666667  0.333333\n  -------\n  seqinfo: 3 sequences from an unspecified genome; no seqlengths\n\n\n\n19.4.2 Nearest feature\nThere are a number of useful methods that find the nearest feature (region) in a second set for each element in the first set.\nWe can review our two GRanges toy objects:\n\ng\n\nGRanges object with 3 ranges and 3 metadata columns:\n    seqnames    ranges strand |     score        GC        AT\n       &lt;Rle&gt; &lt;IRanges&gt;  &lt;Rle&gt; | &lt;integer&gt; &lt;numeric&gt; &lt;numeric&gt;\n  a     chr1   101-111      - |         1  1.000000  0.000000\n  b     chr2   102-112      + |         2  0.888889  0.111111\n  c     chr2   103-113      + |         3  0.777778  0.222222\n  -------\n  seqinfo: 3 sequences from an unspecified genome; no seqlengths\n\ngr\n\nGRanges object with 10 ranges and 3 metadata columns:\n    seqnames    ranges strand |     score        GC        AT\n       &lt;Rle&gt; &lt;IRanges&gt;  &lt;Rle&gt; | &lt;integer&gt; &lt;numeric&gt; &lt;numeric&gt;\n  a     chr1   101-111      - |         1  1.000000  0.000000\n  b     chr2   102-112      + |         2  0.888889  0.111111\n  c     chr2   103-113      + |         3  0.777778  0.222222\n  d     chr2   104-114      * |         4  0.666667  0.333333\n  e     chr1   105-115      * |         5  0.555556  0.444444\n  f     chr1   106-116      + |         6  0.444444  0.555556\n  g     chr3   107-117      + |         7  0.333333  0.666667\n  h     chr3   108-118      + |         8  0.222222  0.777778\n  i     chr3   109-119      - |         9  0.111111  0.888889\n  j     chr3   110-120      - |        10  0.000000  1.000000\n  -------\n  seqinfo: 3 sequences from an unspecified genome; no seqlengths\n\n\n\nnearest: Performs conventional nearest neighbor finding. Returns an integer vector containing the index of the nearest neighbor range in subject for each range in x. If there is no nearest neighbor NA is returned. For details of the algorithm see the man page in the IRanges package (?nearest).\nprecede: For each range in x, precede returns the index of the range in subject that is directly preceded by the range in x. Overlapping ranges are excluded. NA is returned when there are no qualifying ranges in subject.\nfollow: The opposite of precede, follow returns the index of the range in subject that is directly followed by the range in x. Overlapping ranges are excluded. NA is returned when there are no qualifying ranges in subject.\n\nOrientation and strand for precede and follow: Orientation is 5’ to 3’, consistent with the direction of translation. Because positional numbering along a chromosome is from left to right and transcription takes place from 5’ to 3’, precede and follow can appear to have ‘opposite’ behavior on the + and - strand. Using positions 5 and 6 as an example, 5 precedes 6 on the + strand but follows 6 on the - strand.\nThe table below outlines the orientation when ranges on different strands are compared. In general, a feature on * is considered to belong to both strands. The single exception is when both x and subject are * in which case both are treated as +.\n       x  |  subject  |  orientation \n     -----+-----------+----------------\na)     +  |  +        |  ---&gt; \nb)     +  |  -        |  NA\nc)     +  |  *        |  ---&gt;\nd)     -  |  +        |  NA\ne)     -  |  -        |  &lt;---\nf)     -  |  *        |  &lt;---\ng)     *  |  +        |  ---&gt;\nh)     *  |  -        |  &lt;---\ni)     *  |  *        |  ---&gt;  (the only situation where * arbitrarily means +)\n\nres = nearest(g, gr)\nres\n\n[1] 5 4 4\n\n\nWhile nearest and friends give the index of the nearest feature, the distance to the nearest is sometimes also useful to have. The distanceToNearest method calculates the nearest feature as well as reporting the distance.\n\nres = distanceToNearest(g, gr)\nres\n\nHits object with 3 hits and 1 metadata column:\n      queryHits subjectHits |  distance\n      &lt;integer&gt;   &lt;integer&gt; | &lt;integer&gt;\n  [1]         1           5 |         0\n  [2]         2           4 |         0\n  [3]         3           4 |         0\n  -------\n  queryLength: 3 / subjectLength: 10"
  },
  {
    "objectID": "ranges_and_signals.html#plyranges",
    "href": "ranges_and_signals.html#plyranges",
    "title": "\n19  Genomic ranges and features\n",
    "section": "\n19.5 Plyranges",
    "text": "19.5 Plyranges\nPlyranges is a Bioconductor package that provides a grammar of genomic data manipulation. It’s a toolset for performing operations on genomic intervals (or ranges) and associated annotations in the R programming language. The package extends the functionality of the GenomicRanges package, another Bioconductor package designed to manage and manipulate genomic interval data. The plyranges package provides a grammar for manipulating genomic ranges (Lee, Cook, and Lawrence 2019). It is similar to the dplyr package for data frames.\nThe Plyranges package is designed to address several challenges that arise in the analysis of genomic data:\n\n\nManipulating Genomic Intervals\n\nGenomic intervals (e.g., gene locations, variant locations, etc.) are a fundamental data type in bioinformatics. Plyranges provides a simple and consistent set of operations for manipulating these intervals, such as finding overlaps, nearest neighbors, or shifting and resizing intervals.\n\n\n\nWorking with Genomic Annotations\n\nGenomic intervals often have associated annotations (e.g., gene names, variant alleles, etc.). Plyranges provides tools for manipulating these annotations along with their associated intervals.\n\n\n\nIntegration with the Tidyverse\n\nThe Tidyverse is a collection of R packages designed for data science. Plyranges uses a similar syntax and integrates well with these packages, making it easier for users familiar with the Tidyverse to work with genomic data.\n\n\n\nPerformance\n\nWorking with genomic data often involves large datasets. Plyranges is built on the Bioconductor ranges infrastructure and is designed to be efficient and performant, even when working with large genomic datasets.\n\n\n\n\nlibrary(plyranges)\n\n\n\n\n\n\n\n\n\n\nCategory\nVerb\nDescription\n\n\n\nAggregate\nsummarize()\nAggregate over column(s)\n\n\n\ndisjoin_ranges()\nAggregate column(s) over the union of end coordinates\n\n\n\nreduce_ranges()\nAggregate column(s) by merging overlapping and neighboring ranges\n\n\nModify (Unary)\nmutate()\nModifies any column\n\n\n\nselect()\nSelect columns\n\n\n\narrange()\nSort by columns\n\n\n\nstretch()\nExtend range by fixed amount\n\n\n\nshift_(direction)\nShift coordinates\n\n\n\nflank_(direction)\nGenerate flanking regions\n\n\n\n%intersection%\nRow-wise intersection\n\n\n\n%union%\nRow-wise union\n\n\n\ncompute_coverage\nCoverage over all ranges\n\n\nModify (Binary)\n%setdiff%\nRow-wise set difference\n\n\n\nbetween()\nRow-wise gap range\n\n\n\nspan()\nRow-wise spanning range\n\n\nMerge\njoin_overlap_*()\nMerge by overlapping ranges\n\n\n\njoin_nearest\nMerge by nearest neighbor ranges\n\n\n\njoin_follow\nMerge by following ranges\n\n\n\njoin_precedes\nMerge by preceding ranges\n\n\n\nunion_ranges\nRange-wise union\n\n\n\nintersect_ranges\nRange-wise intersect\n\n\n\nsetdiff_ranges\nRange-wise set difference\n\n\n\ncomplement_ranges\nRange-wise set complement\n\n\nOperate\nanchor_direction()\nFix coordinates at direction\n\n\n\ngroup_by()\nPartition by column(s)\n\n\n\ngroup_by_overlaps()\nPartition by overlaps\n\n\nRestrict\nfilter()\nSubset rows\n\n\n\nfilter_by_overlaps()\nSubset by overlap\n\n\n\nfilter_by_non_overlaps()\nSubset by no overlap\n\n\n\nTable 19.4: The verbs in the plyranges package."
  },
  {
    "objectID": "ranges_and_signals.html#gene-models",
    "href": "ranges_and_signals.html#gene-models",
    "title": "\n19  Genomic ranges and features\n",
    "section": "\n19.6 Gene models",
    "text": "19.6 Gene models\nThe TxDb package provides a convenient interface to gene models from a variety of sources. The TxDb.Hsapiens.UCSC.hg38.knownGene package provides access to the UCSC knownGene gene model for the hg19 build of the human genome.\n\n\nFigure 19.3: A graphical representation of range operations demonstrated on a gene model.\n\n\nlibrary(TxDb.Hsapiens.UCSC.hg38.knownGene)\ntxdb &lt;- TxDb.Hsapiens.UCSC.hg38.knownGene\n\nThe transcripts function returns a GRanges object with the transcripts for all genes in the database.\n\ntx &lt;- transcripts(txdb)"
  },
  {
    "objectID": "ranges_and_signals.html#session-info",
    "href": "ranges_and_signals.html#session-info",
    "title": "\n19  Genomic ranges and features\n",
    "section": "\n19.7 Session Info",
    "text": "19.7 Session Info\n\nsessionInfo()\n\nR version 4.3.0 (2023-04-21)\nPlatform: aarch64-apple-darwin20 (64-bit)\nRunning under: macOS Ventura 13.1\n\nMatrix products: default\nBLAS:   /Library/Frameworks/R.framework/Versions/4.3-arm64/Resources/lib/libRblas.0.dylib \nLAPACK: /Library/Frameworks/R.framework/Versions/4.3-arm64/Resources/lib/libRlapack.dylib;  LAPACK version 3.11.0\n\nlocale:\n[1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8\n\ntime zone: America/New_York\ntzcode source: internal\n\nattached base packages:\n[1] stats4    stats     graphics  grDevices utils     datasets  methods  \n[8] base     \n\nother attached packages:\n [1] TxDb.Hsapiens.UCSC.hg38.knownGene_3.17.0\n [2] GenomicFeatures_1.52.1                  \n [3] AnnotationDbi_1.62.2                    \n [4] Biobase_2.60.0                          \n [5] plyranges_1.19.0                        \n [6] GenomicRanges_1.52.0                    \n [7] GenomeInfoDb_1.36.1                     \n [8] IRanges_2.34.1                          \n [9] S4Vectors_0.38.1                        \n[10] BiocGenerics_0.46.0                     \n[11] knitr_1.43                              \n\nloaded via a namespace (and not attached):\n [1] KEGGREST_1.40.0             SummarizedExperiment_1.30.2\n [3] rjson_0.2.21                xfun_0.39                  \n [5] htmlwidgets_1.6.2           lattice_0.21-8             \n [7] vctrs_0.6.3                 tools_4.3.0                \n [9] bitops_1.0-7                generics_0.1.3             \n[11] curl_5.0.1                  parallel_4.3.0             \n[13] tibble_3.2.1                fansi_1.0.4                \n[15] RSQLite_2.3.1               blob_1.2.4                 \n[17] pkgconfig_2.0.3             Matrix_1.5-4.1             \n[19] dbplyr_2.3.2                lifecycle_1.0.3            \n[21] GenomeInfoDbData_1.2.10     stringr_1.5.0              \n[23] compiler_4.3.0              progress_1.2.2             \n[25] Rsamtools_2.16.0            Biostrings_2.68.1          \n[27] codetools_0.2-19            htmltools_0.5.5            \n[29] RCurl_1.98-1.12             yaml_2.3.7                 \n[31] pillar_1.9.0                crayon_1.5.2               \n[33] BiocParallel_1.34.2         DelayedArray_0.26.6        \n[35] cachem_1.0.8                tidyselect_1.2.0           \n[37] digest_0.6.31               stringi_1.7.12             \n[39] dplyr_1.1.2                 restfulr_0.0.15            \n[41] biomaRt_2.56.1              fastmap_1.1.1              \n[43] grid_4.3.0                  cli_3.6.1                  \n[45] magrittr_2.0.3              S4Arrays_1.0.4             \n[47] XML_3.99-0.14               utf8_1.2.3                 \n[49] rappdirs_0.3.3              filelock_1.0.2             \n[51] prettyunits_1.1.1           bit64_4.0.5                \n[53] rmarkdown_2.23              XVector_0.40.0             \n[55] httr_1.4.6                  matrixStats_1.0.0          \n[57] bit_4.0.5                   hms_1.1.3                  \n[59] png_0.1-8                   memoise_2.0.1              \n[61] evaluate_0.21               BiocIO_1.10.0              \n[63] BiocFileCache_2.8.0         rtracklayer_1.60.0         \n[65] rlang_1.1.1                 glue_1.6.2                 \n[67] DBI_1.1.3                   xml2_1.3.5                 \n[69] rstudioapi_0.14             jsonlite_1.8.7             \n[71] R6_2.5.1                    MatrixGenerics_1.12.2      \n[73] GenomicAlignments_1.36.0    zlibbioc_1.46.0            \n\n\n\n\n\n\nLawrence, Michael, Wolfgang Huber, Hervé Pagès, Patrick Aboyoun, Marc Carlson, Robert Gentleman, Martin T. Morgan, and Vincent J. Carey. 2013. “Software for Computing and Annotating Genomic Ranges.” PLoS Computational Biology 9 (8): e1003118. https://doi.org/10.1371/journal.pcbi.1003118.\n\n\nLee, Stuart, Dianne Cook, and Michael Lawrence. 2019. “Plyranges: A Grammar of Genomic Data Transformation.” Genome Biology 20 (1): 4. https://doi.org/10.1186/s13059-018-1597-8."
  },
  {
    "objectID": "atac.html",
    "href": "atac.html",
    "title": "\n20  ATAC-Seq with Bioconductor\n",
    "section": "",
    "text": "Overview"
  },
  {
    "objectID": "atac.html#pre-requisites",
    "href": "atac.html#pre-requisites",
    "title": "\n20  ATAC-Seq with Bioconductor\n",
    "section": "Pre-requisites",
    "text": "Pre-requisites\nThis workshop assumes:\n\nA working and up-to-date version of R\nBasic knowledge of R syntax\nFamiliarity with the GenomicRanges package and range manipulations\nFamiliarity with BAM files and their contents"
  },
  {
    "objectID": "atac.html#participation",
    "href": "atac.html#participation",
    "title": "\n20  ATAC-Seq with Bioconductor\n",
    "section": "Participation",
    "text": "Participation\nAfter a very brief review of ATAC-Seq and chromatin accessibility, students will work independently to follow this workflow. Additional materials are provided as links at the end of the workshop for those wanting deeper exposure. Additional materials include alignment from FASTQ files and peak calling."
  },
  {
    "objectID": "atac.html#r-bioconductor-packages-used",
    "href": "atac.html#r-bioconductor-packages-used",
    "title": "\n20  ATAC-Seq with Bioconductor\n",
    "section": "\nR / Bioconductor packages used",
    "text": "R / Bioconductor packages used\n\n\nRsamtools\n\n\nGenomicRanges\n\n\nGenomicFeatures\n\nGenomicAlignments\nrtracklayer\nheatmaps\nTxDb.Hsapiens.UCSC.hg19.knownGene"
  },
  {
    "objectID": "atac.html#time-outline",
    "href": "atac.html#time-outline",
    "title": "\n20  ATAC-Seq with Bioconductor\n",
    "section": "Time outline",
    "text": "Time outline\nAn example for a 45-minute workshop:\n\n\nActivity\nTime\n\n\n\nIntroduction\n15m\n\n\nIndependent work\n2-3hr\n\n\nAdditional exercises (optional, external)\nup to 12 hours"
  },
  {
    "objectID": "atac.html#learning-goals",
    "href": "atac.html#learning-goals",
    "title": "\n20  ATAC-Seq with Bioconductor\n",
    "section": "Learning goals",
    "text": "Learning goals\n\nDescribe how to import sequence alignments in BAM format into R\nRelate fragment size to genomic characteristics such as nucleosome occupancy and open chromatin.\nPerform basic alignment manipulations in R to enrich ATAC-seq data for chromatin characteristics.\nGain familiarity with the IGV genome browser and examining data in genomic context.\nVisualize summaries of genomic signal using profile plots and heatmaps."
  },
  {
    "objectID": "atac.html#learning-objectives",
    "href": "atac.html#learning-objectives",
    "title": "\n20  ATAC-Seq with Bioconductor\n",
    "section": "Learning objectives",
    "text": "Learning objectives\n\nLoad and save genomic data in BAM and BigWig formats [GenomicAlignments and rtracklayer].\nPerform basic QC plots from ATAC-Seq data.\nIsolate nucleosome-free and mononucleosome regions from ATAC-seq data.\nInstall and use IGV to visualize data in genomic context.\nCreate profile plots using the heatmaps package."
  },
  {
    "objectID": "atac.html#background",
    "href": "atac.html#background",
    "title": "\n20  ATAC-Seq with Bioconductor\n",
    "section": "\n20.1 Background",
    "text": "20.1 Background\nChromatin accessibility assays measure the extent to which DNA is open and accessible. Such assays now use high throughput sequencing as a quantitative readout. DNAse assays, first using microarrays(Crawford, Davis, et al. 2006) and then DNAse-Seq (Crawford, Holt, et al. 2006), requires a larger amount of DNA and is labor-indensive and has been largely supplanted by ATAC-Seq (Buenrostro et al. 2013).\nThe Assay for Transposase Accessible Chromatin with high-throughput sequencing (ATAC-seq) method maps chromatin accessibility genome-wide. This method quantifies DNA accessibility with a hyperactive Tn5 transposase that cuts and inserts sequencing adapters into regions of chromatin that are accessible. High throughput sequencing of fragments produced by the process map to regions of increased accessibility, transcription factor binding sites, and nucleosome positioning. The method is both fast and sensitive and can be used as a replacement for DNAse and MNase. A schematic of the protocol is given in Figure @ref(fig:protocol).\n\n\n\n\nSchematic overview of ATAC-Seq protocol. Figure from Wikipedia.\n\n\n\nAn early review of chromatin accessibility assays (Tsompana and Buck 2014) compares the use cases, pros and cons, and expected signals from each of the most common approaches (Figure @ref(fig:chromatinAssays)).\n\n\n\n\nChromatin accessibility methods, compared. Representative DNA fragments generated by each assay are shown, with end locations within chromatin defined by colored arrows. Bar diagrams represent data signal obtained from each assay across the entire region. The footprint created by a transcription factor (TF) is shown for ATAC-seq and DNase-seq experiments.\n\n\n\nThe first manuscript describing ATAC-Seq protocol and findings outlined how ATAC-Seq data “line up” with other datatypes such as ChIP-seq and DNAse-seq (Figure @ref(fig:greenleaf)). They also highlight how fragment length correlates with specific genomic regions and characteristics (Buenrostro et al. 2013, fig. 3).\n\n\n\n\nMultimodal chromatin comparisons. From (Buenrostro et al. 2013), Figure 4. (a) CTCF footprints observed in ATAC-seq and DNase-seq data, at a specific locus on chr1. (b) Aggregate ATAC-seq footprint for CTCF (motif shown) generated over binding sites within the genome (c) CTCF predicted binding probability inferred from ATAC-seq data, position weight matrix (PWM) scores for the CTCF motif, and evolutionary conservation (PhyloP). Right-most column is the CTCF ChIP-seq data (ENCODE) for this GM12878 cell line, demonstrating high concordance with predicted binding probability.\n\n\n\nBuenrostro et al. provide a detailed protocol for performing ATAC-Seq and quality control of results (Buenrostro et al. 2015). Updated and modified protocols that improve on signal-to-noise and reduce input DNA requirements have been described.\n\n20.1.1 Informatics overview\nATAC-Seq protocols typically utilize paired-end sequencing protocols. The reads are aligned to the respective genome using bowtie2, BWA, or other short-read aligner. The result, after appropriate manipulation, often using samtools, results in a BAM file. Among other details, the BAM format includes columns for:\n\nknitr::include_graphics('images/bam_shot.png')\n\n\n\nA BAM file in text form. The output of samtools view is the text format of the BAM file (called SAM format). Bioconductor and many other tools use BAM files for input. Note that BAM files also often include an index .bai file that enables random access into the file; one can read just a genomic region without having to read the entire file.\n\n\n\n\nsequence name (chr1)\nstart position (integer)\na CIGAR string that describes the alignment in a compact form\nthe sequence to which the pair aligns\nthe position to which the pair aligns\na bit flag field that describes multiple characteristics of the alignment\nthe sequence and quality string of the read\nadditional tags that tend to be aligner-specific\n\nDuplicate fragments (those with the same start and end position of other reads) are marked and likely discarded. Reads that fail to align “properly” are also often excluded from analysis. It is worth noting that most software packages allow simple “marking” of such reads and that there is usually no need to create a special BAM file before proceeding with downstream work.\nAfter alignment and BAM processing, the workflow can switch to Bioconductor.\n\n20.1.2 Working with sequencing data in Bioconductor\nThe Bioconductor project includes several infrastructure packages for dealing with ranges (sequence name, start, end, +/- strand) on sequences (Lawrence et al. 2013) as well as capabilities with working with Fastq files directly (Morgan et al. 2016).\n\nCommonly used Bioconductor and their high-level use cases.\n\nPackage\nUse cases\n\n\n\nRsamtools\nlow level access to FASTQ, VCF, SAM, BAM, BCF formats\n\n\nGenomicRanges\nContainer and methods for handling genomic reagions\n\n\nGenomicFeatures\nWork with transcript databases, gff, gtf and BED formats\n\n\nGenomicAlignments\nReader for BAM format\n\n\nrtracklayer\nimport and export multiple UCSC file formats including BigWig and Bed\n\n\n\nAs noted in the previous section, the output of an ATAC-Seq experiment is a BAM file. As paired-end sequencing is a commonly-applied approach for ATAC-Seq, the readGAlignmentPairs function is the appropriate method to use."
  },
  {
    "objectID": "atac.html#data-import-and-quality-control",
    "href": "atac.html#data-import-and-quality-control",
    "title": "\n20  ATAC-Seq with Bioconductor\n",
    "section": "\n20.2 Data import and quality control",
    "text": "20.2 Data import and quality control\n\nlibrary(GenomicAlignments)\n\nReading a paired-end BAM file looks a bit complicated, but the following code will:\n\nRead the included BAM file.\nInclude read pairs only (isPaired = TRUE)\nInclude properly paired reads (isProperPair = TRUE)\nInclude reads with mapping quality &gt;= 1\nAdd a couple of additional fields, mapq (mapping quality) and isize (insert size) to the default fields.\n\n\ngreenleaf = readGAlignmentPairs(\n  'data/Sorted_ATAC_21_22.bam',\n  param = ScanBamParam(\n    mapqFilter = 1, \n    flag = scanBamFlag(\n      isPaired = TRUE, \n      isProperPair = TRUE), \n    what = c(\"mapq\", \"isize\")))\n\nExercise: What is the class of greenleaf? Exercise: Use the GenomicAlignments::first() accessor to get the first read of the pair as a GAlignments object. Save the result as a variable called gl_first_read. Use the mcols accessor to find the “metadata columns” of gl_first_read. Exercise: How many read pairs map to each chromosome?\nWe can make plot of the number of reads mapping to each chromosome.\n\nlibrary(ggplot2)\nlibrary(dplyr)\nchromCounts = table(seqnames(greenleaf)) %&gt;% \n  data.frame() %&gt;%\n  dplyr::rename(chromosome=Var1, count = Freq)\n\nTo keep things small, the example BAM file includes only chromosomes 21 and 22.\n\nggplot(chromCounts, aes(x=chromosome, y=count)) + \n  geom_bar(stat='identity') +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1))\n\n\n\nReads per chromosome. In our example data, we are using only chromosomes 21 and 22.\n\n\n\nNormalizing by the chromosome length can yield the reads per megabase which should crudely be similar across all chromosomes.\n\nchromCounts = chromCounts %&gt;%\n  dplyr::mutate(readsPerMb = (count/(seqlengths(greenleaf)/1e6)))\n\nAnd show a plot. For two chromosomes, this is a little underwhelming.\n\nggplot(chromCounts, aes(x=chromosome, y=readsPerMb)) + \n  geom_bar(stat='identity') +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1)) + \n  theme_bw()\n\n\n\nRead counts normalized by chromosome length. This is not a particularly important plot, but it can be useful to see the relative contribution of each chromosome given its length.\n\n\n\n\n20.2.1 Coverage\nThe coverage method for genomic ranges calculates, for each base, the number of overlapping features. In the case of a BAM file from ATAC-Seq converted to a GAlignmentPairs object, the coverage gives us an idea of the extent to which reads pile up to form peaks.\n\ncvg = coverage(greenleaf)\nclass(cvg)\n\n[1] \"SimpleRleList\"\nattr(,\"package\")\n[1] \"IRanges\"\n\n\nThe coverage is returned as a SimpleRleList object. Using names can get us the names of the elements of the list.\n\nnames(cvg)\n\n [1] \"chr1\"  \"chr2\"  \"chr3\"  \"chr4\"  \"chr5\"  \"chr6\"  \"chr7\"  \"chr8\"  \"chr9\" \n[10] \"chr10\" \"chr11\" \"chr12\" \"chr13\" \"chr14\" \"chr15\" \"chr16\" \"chr17\" \"chr18\"\n[19] \"chr19\" \"chr20\" \"chr21\" \"chr22\" \"chrX\"  \"chrY\"  \"chrM\" \n\n\nThere is a name for each chromosome. Looking at the chr21 entry:\n\ncvg$chr21\n\ninteger-Rle of length 48129895 with 397462 runs\n  Lengths: 9411376      50      11      50 ...      36      14      28   10806\n  Values :       0       2       0       2 ...       1       2       1       0\n\n\nwe see that each chromosome is represented as an Rle, short for run-length-encoding. Simply put, since along the chromosome there are many repeated values, we can recode the long vector as a set of (length: value) pairs. For example, if the first 9,410,000 base pairs have 0 coverage, we encode that as (9,410,000: 0). Doing that across the chromosome can very significantly reduce the memory use for genomic coverage.\nThe following little function, plotCvgHistByChrom can plot a histogram of the coverage for a chromosome.\n\nplotCvgHistByChrom = function(cvg, chromosome) {\n  library(ggplot2)\n  cvgcounts = as.data.frame(table(cvg[[chromosome]]))\n  cvgcounts[,1] = as.numeric(as.character(cvgcounts[,1]))\n  colnames(cvgcounts) = c('Coverage', 'Count')\n  ggplot(cvgcounts, aes(x=Coverage, y=Count)) + \n    ggtitle(paste(\"Chromosome\",chromosome)) + \n    geom_point(alpha=0.5) + \n    geom_smooth(span=0.2) + \n    scale_y_log10() +\n    theme_bw() \n}\nfor(i in c('chr21', 'chr22')) {\n  print(plotCvgHistByChrom(cvg, i))\n}\n\n\n\n\n\n\n\n\n20.2.2 Fragment Lengths\nThe first ATAC-Seq manuscript (Buenrostro et al. 2013) highlighted the relationship between fragment length and nucleosomes (see Figure @ref{fig:flgreenleaf}).\n\nknitr::include_graphics('images/greenleaf_atac.png')\n\n\n\nRelationship between fragment length and nucleosome number.\n\n\n\nRemember that we loaded the example BAM file with insert sizes (isize). We can use that “column” to examine the fragment lengths (another name for insert size). Also, note that the insert size for the first read and the second are the same (absolute value). Here, we will use first.\n\nGenomicAlignments::first(greenleaf)\nmcols(GenomicAlignments::first(greenleaf))\nclass(mcols(GenomicAlignments::first(greenleaf)))\nhead(mcols(GenomicAlignments::first(greenleaf))$isize)\nfraglengths = abs(mcols(GenomicAlignments::first(greenleaf))$isize)\n\nWe can plot the fragment length density (histogram) using the density function.\n\nplot(density(fraglengths, bw=0.05), xlim=c(0,1000))\n\n\n\nFragment length histogram.\n\n\n\nExercise: Adjust the xlim, bw, and try log=\"y\" in the plot to highlight features present in figure \\(\\ref{fig:flgreenleaf}\\).\nAnd for fun, the ggplot2 version:\n\nlibrary(dplyr)\nlibrary(ggplot2)\nfragLenPlot &lt;- table(fraglengths) %&gt;% data.frame %&gt;% rename(InsertSize = fraglengths, \n    Count = Freq) %&gt;% mutate(InsertSize = as.numeric(as.vector(InsertSize)), \n    Count = as.numeric(as.vector(Count))) %&gt;% ggplot(aes(x = InsertSize, y = Count)) + \n    geom_line()\nprint(fragLenPlot + theme_bw() + lims(x=c(-1,250)))\n\n\n\n\nKnowing that the nucleosome-free regions will have insert sizes shorter than one nucleosome, we can isolate the read pairs that have that characteristic.\n\ngl_nf = greenleaf[mcols(GenomicAlignments::first(greenleaf))$isize&lt;100]\n\nAnd the mononucleosome reads will be between 187 and 250 base pairs for insert size/fragment length.\n\ngl_mn = greenleaf[mcols(GenomicAlignments::first(greenleaf))$isize&gt;187 & \n                  mcols(GenomicAlignments::first(greenleaf))$isize&lt;250  ]\n\nFinally, we expect nucleosome-free reads to be enriched near the TSS while mononucleosome reads should not be. We will use the heatmaps package to take a look at these two sets of reads with respect to the tss of the human genome.\n\nlibrary(TxDb.Hsapiens.UCSC.hg19.knownGene)\nproms = promoters(TxDb.Hsapiens.UCSC.hg19.knownGene, 250, 250)\nseqs = c('chr21','chr22')\nseqlevels(proms, pruning.mode='coarse') = seqs # only chromosome 21 and 22\n\nTake a look at the heatmaps package vignette to learn more about the heatmaps package capabilities.\n\nlibrary(heatmaps)\ngl_nf_hm = CoverageHeatmap(proms,coverage(gl_nf),coords=c(-250,250))\nlabel(gl_nf_hm) = \"NucFree\"\nscale(gl_nf_hm)=c(0,10)\nplotHeatmapMeta(gl_nf_hm)\n\n\n\nEnrichment of nucleosome free reads just upstream of the TSS.\n\n\n\n\ngl_mn_hm = CoverageHeatmap(proms,coverage(gl_mn),coords=c(-250,250))\nlabel(gl_mn_hm) = \"MonoNuc\"\nscale(gl_mn_hm)=c(0,10)\nplotHeatmapMeta(gl_mn_hm)\n\n\n\nDepletion of nucleosome free reads just upstream of the TSS.\n\n\n\n\nplotHeatmapList(list(gl_mn_hm, gl_nf_hm))\n\n\n\nComparison of signals at TSS. Mononucleosome data on the left, nucleosome-free on the right."
  },
  {
    "objectID": "atac.html#viewing-data-in-igv",
    "href": "atac.html#viewing-data-in-igv",
    "title": "\n20  ATAC-Seq with Bioconductor\n",
    "section": "\n20.3 Viewing data in IGV",
    "text": "20.3 Viewing data in IGV\nInstall IGV from here.\nWe export the greenleaf data as a BigWig file.\n\nlibrary(rtracklayer)\nexport.bw(coverage(greenleaf),'greenleaf.bw')\n\nExercise: In IGV, choose hg19. Then, load the greenleaf.bw file and explore chromosomes 21 and 22. Exercise: Export the nucleosome-free portion of the data as a BigWig file and examine that in IGV. Where do you expect to see the strongest signals?"
  },
  {
    "objectID": "atac.html#seqence-extraction",
    "href": "atac.html#seqence-extraction",
    "title": "\n20  ATAC-Seq with Bioconductor\n",
    "section": "\n20.4 Seqence extraction",
    "text": "20.4 Seqence extraction\n\n20.4.1 Sequences in Bioconductor\nThe BioStrings package provides a number of functions for working with sequences.\n\nlibrary(Biostrings)\n\nWe can create a DNAString object from a string of nucleotides.\n\ndna = DNAString('gcgctgctggatgcgaccgcgcatgcgagcgcgacctatccggaa')\ndna\n\n45-letter DNAString object\nseq: GCGCTGCTGGATGCGACCGCGCATGCGAGCGCGACCTATCCGGAA\n\n\nYou may not have known that DNA has a convention for including multiple possible bases. The entire possible alphabet for DNA is given by:\n\nBiostrings::alphabet(dna)\n\n [1] \"A\" \"C\" \"G\" \"T\" \"M\" \"R\" \"W\" \"S\" \"Y\" \"K\" \"V\" \"H\" \"D\" \"B\" \"N\" \"-\" \"+\" \".\"\n\n\nTry to create a DNAString object from a string of amino acids, for example. What happens?\nWe can reverse complement the sequence.\n\nreverseComplement(dna)\n\n45-letter DNAString object\nseq: TTCCGGATAGGTCGCGCTCGCATGCGCGGTCGCATCCAGCAGCGC\n\n\nWe can translate the sequence into amino acids.\n\ntranslate(dna)\n\n15-letter AAString object\nseq: ALLDATAHASATYPE\n\n\nSubsetting a DNAString object returns another DNAString object.\n\ndna[1:10]\n\n10-letter DNAString object\nseq: GCGCTGCTGG\n\n\nWe can extract the nucleotide frequencies (or amino acid frequencies) from a DNAStringSet object.\n\nalphabetFrequency(dna)\n\n A  C  G  T  M  R  W  S  Y  K  V  H  D  B  N  -  +  . \n 8 15 16  6  0  0  0  0  0  0  0  0  0  0  0  0  0  0 \n\n\n\n20.4.2 BSGenome(s)\nThe BSgenome package provides functionality to interact with genome sequences that come pre-packaged.\n\nlibrary(BSgenome)\n\nThe names of available genomes are below. They are all named BSgenome.&lt;species&gt;.&lt;source&gt;.&lt;assembly&gt; as in BSgenome.Hsapiens.UCSC.hg19, BSgenome.Mmusculus.UCSC.mm10, etc.\n\navailable.genomes()\n\n  [1] \"BSgenome.Alyrata.JGI.v1\"                           \n  [2] \"BSgenome.Amellifera.BeeBase.assembly4\"             \n  [3] \"BSgenome.Amellifera.NCBI.AmelHAv3.1\"               \n  [4] \"BSgenome.Amellifera.UCSC.apiMel2\"                  \n  [5] \"BSgenome.Amellifera.UCSC.apiMel2.masked\"           \n  [6] \"BSgenome.Aofficinalis.NCBI.V1\"                     \n  [7] \"BSgenome.Athaliana.TAIR.04232008\"                  \n  [8] \"BSgenome.Athaliana.TAIR.TAIR9\"                     \n  [9] \"BSgenome.Btaurus.UCSC.bosTau3\"                     \n [10] \"BSgenome.Btaurus.UCSC.bosTau3.masked\"              \n [11] \"BSgenome.Btaurus.UCSC.bosTau4\"                     \n [12] \"BSgenome.Btaurus.UCSC.bosTau4.masked\"              \n [13] \"BSgenome.Btaurus.UCSC.bosTau6\"                     \n [14] \"BSgenome.Btaurus.UCSC.bosTau6.masked\"              \n [15] \"BSgenome.Btaurus.UCSC.bosTau8\"                     \n [16] \"BSgenome.Btaurus.UCSC.bosTau9\"                     \n [17] \"BSgenome.Btaurus.UCSC.bosTau9.masked\"              \n [18] \"BSgenome.Carietinum.NCBI.v1\"                       \n [19] \"BSgenome.Celegans.UCSC.ce10\"                       \n [20] \"BSgenome.Celegans.UCSC.ce11\"                       \n [21] \"BSgenome.Celegans.UCSC.ce2\"                        \n [22] \"BSgenome.Celegans.UCSC.ce6\"                        \n [23] \"BSgenome.Cfamiliaris.UCSC.canFam2\"                 \n [24] \"BSgenome.Cfamiliaris.UCSC.canFam2.masked\"          \n [25] \"BSgenome.Cfamiliaris.UCSC.canFam3\"                 \n [26] \"BSgenome.Cfamiliaris.UCSC.canFam3.masked\"          \n [27] \"BSgenome.Cjacchus.UCSC.calJac3\"                    \n [28] \"BSgenome.Cjacchus.UCSC.calJac4\"                    \n [29] \"BSgenome.CneoformansVarGrubiiKN99.NCBI.ASM221672v1\"\n [30] \"BSgenome.Creinhardtii.JGI.v5.6\"                    \n [31] \"BSgenome.Dmelanogaster.UCSC.dm2\"                   \n [32] \"BSgenome.Dmelanogaster.UCSC.dm2.masked\"            \n [33] \"BSgenome.Dmelanogaster.UCSC.dm3\"                   \n [34] \"BSgenome.Dmelanogaster.UCSC.dm3.masked\"            \n [35] \"BSgenome.Dmelanogaster.UCSC.dm6\"                   \n [36] \"BSgenome.Drerio.UCSC.danRer10\"                     \n [37] \"BSgenome.Drerio.UCSC.danRer11\"                     \n [38] \"BSgenome.Drerio.UCSC.danRer5\"                      \n [39] \"BSgenome.Drerio.UCSC.danRer5.masked\"               \n [40] \"BSgenome.Drerio.UCSC.danRer6\"                      \n [41] \"BSgenome.Drerio.UCSC.danRer6.masked\"               \n [42] \"BSgenome.Drerio.UCSC.danRer7\"                      \n [43] \"BSgenome.Drerio.UCSC.danRer7.masked\"               \n [44] \"BSgenome.Dvirilis.Ensembl.dvircaf1\"                \n [45] \"BSgenome.Ecoli.NCBI.20080805\"                      \n [46] \"BSgenome.Gaculeatus.UCSC.gasAcu1\"                  \n [47] \"BSgenome.Gaculeatus.UCSC.gasAcu1.masked\"           \n [48] \"BSgenome.Ggallus.UCSC.galGal3\"                     \n [49] \"BSgenome.Ggallus.UCSC.galGal3.masked\"              \n [50] \"BSgenome.Ggallus.UCSC.galGal4\"                     \n [51] \"BSgenome.Ggallus.UCSC.galGal4.masked\"              \n [52] \"BSgenome.Ggallus.UCSC.galGal5\"                     \n [53] \"BSgenome.Ggallus.UCSC.galGal6\"                     \n [54] \"BSgenome.Gmax.NCBI.Gmv40\"                          \n [55] \"BSgenome.Hsapiens.1000genomes.hs37d5\"              \n [56] \"BSgenome.Hsapiens.NCBI.GRCh38\"                     \n [57] \"BSgenome.Hsapiens.NCBI.T2T.CHM13v2.0\"              \n [58] \"BSgenome.Hsapiens.UCSC.hg17\"                       \n [59] \"BSgenome.Hsapiens.UCSC.hg17.masked\"                \n [60] \"BSgenome.Hsapiens.UCSC.hg18\"                       \n [61] \"BSgenome.Hsapiens.UCSC.hg18.masked\"                \n [62] \"BSgenome.Hsapiens.UCSC.hg19\"                       \n [63] \"BSgenome.Hsapiens.UCSC.hg19.masked\"                \n [64] \"BSgenome.Hsapiens.UCSC.hg38\"                       \n [65] \"BSgenome.Hsapiens.UCSC.hg38.dbSNP151.major\"        \n [66] \"BSgenome.Hsapiens.UCSC.hg38.dbSNP151.minor\"        \n [67] \"BSgenome.Hsapiens.UCSC.hg38.masked\"                \n [68] \"BSgenome.Hsapiens.UCSC.hs1\"                        \n [69] \"BSgenome.Mdomestica.UCSC.monDom5\"                  \n [70] \"BSgenome.Mfascicularis.NCBI.5.0\"                   \n [71] \"BSgenome.Mfascicularis.NCBI.6.0\"                   \n [72] \"BSgenome.Mfuro.UCSC.musFur1\"                       \n [73] \"BSgenome.Mmulatta.UCSC.rheMac10\"                   \n [74] \"BSgenome.Mmulatta.UCSC.rheMac2\"                    \n [75] \"BSgenome.Mmulatta.UCSC.rheMac2.masked\"             \n [76] \"BSgenome.Mmulatta.UCSC.rheMac3\"                    \n [77] \"BSgenome.Mmulatta.UCSC.rheMac3.masked\"             \n [78] \"BSgenome.Mmulatta.UCSC.rheMac8\"                    \n [79] \"BSgenome.Mmusculus.UCSC.mm10\"                      \n [80] \"BSgenome.Mmusculus.UCSC.mm10.masked\"               \n [81] \"BSgenome.Mmusculus.UCSC.mm39\"                      \n [82] \"BSgenome.Mmusculus.UCSC.mm8\"                       \n [83] \"BSgenome.Mmusculus.UCSC.mm8.masked\"                \n [84] \"BSgenome.Mmusculus.UCSC.mm9\"                       \n [85] \"BSgenome.Mmusculus.UCSC.mm9.masked\"                \n [86] \"BSgenome.Osativa.MSU.MSU7\"                         \n [87] \"BSgenome.Ppaniscus.UCSC.panPan1\"                   \n [88] \"BSgenome.Ppaniscus.UCSC.panPan2\"                   \n [89] \"BSgenome.Ptroglodytes.UCSC.panTro2\"                \n [90] \"BSgenome.Ptroglodytes.UCSC.panTro2.masked\"         \n [91] \"BSgenome.Ptroglodytes.UCSC.panTro3\"                \n [92] \"BSgenome.Ptroglodytes.UCSC.panTro3.masked\"         \n [93] \"BSgenome.Ptroglodytes.UCSC.panTro5\"                \n [94] \"BSgenome.Ptroglodytes.UCSC.panTro6\"                \n [95] \"BSgenome.Rnorvegicus.UCSC.rn4\"                     \n [96] \"BSgenome.Rnorvegicus.UCSC.rn4.masked\"              \n [97] \"BSgenome.Rnorvegicus.UCSC.rn5\"                     \n [98] \"BSgenome.Rnorvegicus.UCSC.rn5.masked\"              \n [99] \"BSgenome.Rnorvegicus.UCSC.rn6\"                     \n[100] \"BSgenome.Rnorvegicus.UCSC.rn7\"                     \n[101] \"BSgenome.Scerevisiae.UCSC.sacCer1\"                 \n[102] \"BSgenome.Scerevisiae.UCSC.sacCer2\"                 \n[103] \"BSgenome.Scerevisiae.UCSC.sacCer3\"                 \n[104] \"BSgenome.Sscrofa.UCSC.susScr11\"                    \n[105] \"BSgenome.Sscrofa.UCSC.susScr3\"                     \n[106] \"BSgenome.Sscrofa.UCSC.susScr3.masked\"              \n[107] \"BSgenome.Tgondii.ToxoDB.7.0\"                       \n[108] \"BSgenome.Tguttata.UCSC.taeGut1\"                    \n[109] \"BSgenome.Tguttata.UCSC.taeGut1.masked\"             \n[110] \"BSgenome.Tguttata.UCSC.taeGut2\"                    \n[111] \"BSgenome.Vvinifera.URGI.IGGP12Xv0\"                 \n[112] \"BSgenome.Vvinifera.URGI.IGGP12Xv2\"                 \n[113] \"BSgenome.Vvinifera.URGI.IGGP8X\"                    \n\n\nWe can use the BSgenome.Hsapiens.UCSC.hg38 package to extract sequences from peaks.\n\nif(!require(BSgenome.Hsapiens.UCSC.hg38)){BiocManager::install('BSgenome.Hsapiens.UCSC.hg38')}\nlibrary(BSgenome.Hsapiens.UCSC.hg38)\n\nThe ENCODE project has thousands of datasets available for download. We’ll grab an arbitrary ATAC-seq dataset from here:\n\nhttps://www.encodeproject.org/files/ENCFF994UIF/@@download/ENCFF994UIF.bed.gz\n\n\nadrenal_atac &lt;- readr::read_tsv('https://www.encodeproject.org/files/ENCFF994UIF/@@download/ENCFF994UIF.bed.gz',\n                                col_names = FALSE)\ncolnames(adrenal_atac) &lt;- c('chrom','start','end','name','score','strand')\nhead(adrenal_atac)\n\n# A tibble: 6 × 10\n  chrom     start       end name        score strand    ``     ``     ``    ``\n  &lt;chr&gt;     &lt;dbl&gt;     &lt;dbl&gt; &lt;chr&gt;       &lt;dbl&gt; &lt;chr&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt;\n1 chr1  100037554 100039053 Peak_2668    1000 .      18.8  2883.  2879.    506\n2 chr1  100037554 100039053 Peak_40256   1000 .       5.92  434.   431.   1016\n3 chr1  100037554 100039053 Peak_57398   1000 .       4.71  278.   275.    811\n4 chr1  100037554 100039053 Peak_78482   1000 .       3.77  174.   172.   1287\n5 chr1  100046439 100046610 Peak_244701   146 .       2.75   14.7   12.9    83\n6 chr1  100060446 100060954 Peak_138710   597 .       5.32   59.7   57.7   275\n\n\nWe can convert the data to a GRanges object after ensureing that the column names are correct.\n\nadrenal_atac &lt;- GRanges(adrenal_atac)\nhead(adrenal_atac)\n\nGRanges object with 6 ranges and 6 metadata columns:\n      seqnames              ranges strand |        name     score      &lt;NA&gt;\n         &lt;Rle&gt;           &lt;IRanges&gt;  &lt;Rle&gt; | &lt;character&gt; &lt;numeric&gt; &lt;numeric&gt;\n  [1]     chr1 100037554-100039053      * |   Peak_2668      1000  18.84493\n  [2]     chr1 100037554-100039053      * |  Peak_40256      1000   5.91615\n  [3]     chr1 100037554-100039053      * |  Peak_57398      1000   4.70575\n  [4]     chr1 100037554-100039053      * |  Peak_78482      1000   3.76879\n  [5]     chr1 100046439-100046610      * | Peak_244701       146   2.74992\n  [6]     chr1 100060446-100060954      * | Peak_138710       597   5.32355\n           &lt;NA&gt;      &lt;NA&gt;      &lt;NA&gt;\n      &lt;numeric&gt; &lt;numeric&gt; &lt;numeric&gt;\n  [1] 2882.5295 2878.5525       506\n  [2]  433.6264  431.0128      1016\n  [3]  277.7893  275.3472       811\n  [4]  174.3991  172.1142      1287\n  [5]   14.6544   12.9433        83\n  [6]   59.7187   57.7244       275\n  -------\n  seqinfo: 24 sequences from an unspecified genome; no seqlengths\n\nlength(adrenal_atac)\n\n[1] 273730\n\n\nWe can extract the sequences from the peaks using the BSgenome.Hsapiens.UCSC.hg38 package.\n\nadrenal_atac_seqs &lt;- getSeq(BSgenome.Hsapiens.UCSC.hg38, adrenal_atac)\nhead(adrenal_atac_seqs)\n\nDNAStringSet object of length 6:\n    width seq\n[1]  1500 CAAAGGGTGACATCGGTCAGGTAAGGTATTTTTT...GATCGGAGGAGGACACTTTGAAGGGAAGAAAGG\n[2]  1500 CAAAGGGTGACATCGGTCAGGTAAGGTATTTTTT...GATCGGAGGAGGACACTTTGAAGGGAAGAAAGG\n[3]  1500 CAAAGGGTGACATCGGTCAGGTAAGGTATTTTTT...GATCGGAGGAGGACACTTTGAAGGGAAGAAAGG\n[4]  1500 CAAAGGGTGACATCGGTCAGGTAAGGTATTTTTT...GATCGGAGGAGGACACTTTGAAGGGAAGAAAGG\n[5]   172 TGGTCTTGAACTCCTGACCTCAGGTGATCCGCCC...GTTCCATATAAGTTTTAGGATTTAAAAGAAAAA\n[6]   509 TGCAAGTTGAGCAGTCTGGAGCTGGTATGGCAGT...CCATGGTGTTATCTACAAAGGCCCAAAGGTTTA\n\nlength(adrenal_atac_seqs)\n\n[1] 273730\n\n\nWe can then write the sequences to a FASTA file.\n\nwriteXStringSet(adrenal_atac_seqs, 'adrenal_atac_seqs.fa')"
  },
  {
    "objectID": "atac.html#common-peaks",
    "href": "atac.html#common-peaks",
    "title": "\n20  ATAC-Seq with Bioconductor\n",
    "section": "\n20.5 Common peaks",
    "text": "20.5 Common peaks\nLet’s grab another dataset from the ENCODE project. This time, we’ll use a dataset from the ENCODE Project.\n\ncardiac_atac &lt;- readr::read_tsv('https://www.encodeproject.org/files/ENCFF012BKZ/@@download/ENCFF012BKZ.bed.gz',\n                                col_names = FALSE)\ncolnames(cardiac_atac) &lt;- c('chrom','start','end','name','score','strand')\ncardiac_atac &lt;- GRanges(cardiac_atac)\nhead(cardiac_atac)\n\nGRanges object with 6 ranges and 6 metadata columns:\n      seqnames              ranges strand |        name     score      &lt;NA&gt;\n         &lt;Rle&gt;           &lt;IRanges&gt;  &lt;Rle&gt; | &lt;character&gt; &lt;numeric&gt; &lt;numeric&gt;\n  [1]     chr1 100006549-100007015      * | Peak_126876       195   4.37594\n  [2]     chr1 100006549-100007015      * | Peak_228935        62   2.54600\n  [3]     chr1 100023516-100023841      * |  Peak_53548       741   9.38837\n  [4]     chr1 100037565-100038984      * | Peak_140410       162   2.53385\n  [5]     chr1 100037565-100038984      * | Peak_243001        56   1.78235\n  [6]     chr1 100037565-100038984      * |   Peak_4079      1000  20.92354\n           &lt;NA&gt;      &lt;NA&gt;      &lt;NA&gt;\n      &lt;numeric&gt; &lt;numeric&gt; &lt;numeric&gt;\n  [1]  19.54719  17.50513       340\n  [2]   6.23524   4.47481       104\n  [3]  74.18277  71.72324       156\n  [4]  16.24701  14.25361      1006\n  [5]   5.63139   3.89871        82\n  [6] 768.94092 765.21680       489\n  -------\n  seqinfo: 24 sequences from an unspecified genome; no seqlengths\n\n\nUsing what we know about overlaps, we can look for peaks that are common between the two datasets.\n\nadrenal_over_cardiac &lt;- adrenal_atac[adrenal_atac %over% cardiac_atac]\n\nTo export the GRanges as a BED file, we can use the rtracklayer package.\n\nlibrary(rtracklayer)\nexport(adrenal_over_cardiac, 'adrenal_over_cardiac.bed', format='bed')\n\nOpen the BED file in IGV. What do you see? What do you expect to see?\n\nUse the GenomicRanges::intersect method to find the common regions shared between the two datasets.\nUse the GenomicRanges::setdiff method to find the regions that are unique to each dataset.\nUse the GenomicRanges::disjoin method to find all the disjoint regions between the two datasets.\nWhat is the relationship between the number of disjoint regions and the number of peaks in the intersection and setdiff?"
  },
  {
    "objectID": "atac.html#additional-work",
    "href": "atac.html#additional-work",
    "title": "\n20  ATAC-Seq with Bioconductor\n",
    "section": "\n20.6 Additional work",
    "text": "20.6 Additional work\nFor those working extensively on ATAC-Seq, there is a great workflow/tutorial available from Thomas Carrol:\nhttps://rockefelleruniversity.github.io/RU_ATAC_Workshop.html\nFeel free to work through it. In addition to the work above, there is also the ATACseqQC package vignette that offers more than just QC. At least a couple more packages are available in Bioconductor."
  },
  {
    "objectID": "atac.html#appendix",
    "href": "atac.html#appendix",
    "title": "\n20  ATAC-Seq with Bioconductor\n",
    "section": "Appendix",
    "text": "Appendix\nSession info\n\n\nR version 4.3.0 (2023-04-21)\nPlatform: aarch64-apple-darwin20 (64-bit)\nRunning under: macOS Ventura 13.1\n\nMatrix products: default\nBLAS:   /Library/Frameworks/R.framework/Versions/4.3-arm64/Resources/lib/libRblas.0.dylib \nLAPACK: /Library/Frameworks/R.framework/Versions/4.3-arm64/Resources/lib/libRlapack.dylib;  LAPACK version 3.11.0\n\nlocale:\n[1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8\n\ntime zone: America/New_York\ntzcode source: internal\n\nattached base packages:\n[1] stats4    stats     graphics  grDevices utils     datasets  methods  \n[8] base     \n\nother attached packages:\n [1] rtracklayer_1.60.0                     \n [2] heatmaps_1.24.0                        \n [3] TxDb.Hsapiens.UCSC.hg19.knownGene_3.2.2\n [4] GenomicFeatures_1.52.1                 \n [5] AnnotationDbi_1.62.2                   \n [6] dplyr_1.1.2                            \n [7] ggplot2_3.4.2                          \n [8] GenomicAlignments_1.36.0               \n [9] Rsamtools_2.16.0                       \n[10] Biostrings_2.68.1                      \n[11] XVector_0.40.0                         \n[12] SummarizedExperiment_1.30.2            \n[13] Biobase_2.60.0                         \n[14] MatrixGenerics_1.12.2                  \n[15] matrixStats_1.0.0                      \n[16] GenomicRanges_1.52.0                   \n[17] GenomeInfoDb_1.36.1                    \n[18] IRanges_2.34.1                         \n[19] S4Vectors_0.38.1                       \n[20] BiocGenerics_0.46.0                    \n[21] knitr_1.43                             \n\nloaded via a namespace (and not attached):\n [1] tidyselect_1.2.0        EBImage_4.42.0          blob_1.2.4             \n [4] filelock_1.0.2          bitops_1.0-7            fastmap_1.1.1          \n [7] RCurl_1.98-1.12         BiocFileCache_2.8.0     XML_3.99-0.14          \n[10] digest_0.6.31           lifecycle_1.0.3         KEGGREST_1.40.0        \n[13] RSQLite_2.3.1           magrittr_2.0.3          compiler_4.3.0         \n[16] rlang_1.1.1             progress_1.2.2          tools_4.3.0            \n[19] plotrix_3.8-2           utf8_1.2.3              yaml_2.3.7             \n[22] prettyunits_1.1.1       S4Arrays_1.0.4          htmlwidgets_1.6.2      \n[25] bit_4.0.5               curl_5.0.1              DelayedArray_0.26.6    \n[28] RColorBrewer_1.1-3      xml2_1.3.5              KernSmooth_2.23-21     \n[31] abind_1.4-5             BiocParallel_1.34.2     withr_2.5.0            \n[34] grid_4.3.0              fansi_1.0.4             colorspace_2.1-0       \n[37] scales_1.2.1            biomaRt_2.56.1          cli_3.6.1              \n[40] rmarkdown_2.23          crayon_1.5.2            generics_0.1.3         \n[43] rstudioapi_0.14         rjson_0.2.21            httr_1.4.6             \n[46] DBI_1.1.3               cachem_1.0.8            stringr_1.5.0          \n[49] zlibbioc_1.46.0         parallel_4.3.0          tiff_0.1-11            \n[52] restfulr_0.0.15         vctrs_0.6.3             Matrix_1.5-4.1         \n[55] jsonlite_1.8.7          fftwtools_0.9-11        hms_1.1.3              \n[58] bit64_4.0.5             jpeg_0.1-10             locfit_1.5-9.8         \n[61] glue_1.6.2              codetools_0.2-19        stringi_1.7.12         \n[64] gtable_0.3.3            BiocIO_1.10.0           munsell_0.5.0          \n[67] tibble_3.2.1            pillar_1.9.0            rappdirs_0.3.3         \n[70] htmltools_0.5.5         GenomeInfoDbData_1.2.10 R6_2.5.1               \n[73] dbplyr_2.3.2            evaluate_0.21           lattice_0.21-8         \n[76] png_0.1-8               memoise_2.0.1           xfun_0.39              \n[79] pkgconfig_2.0.3        \n\n\nMACS2\nThe MACS2 package is a commonly-used package for calling peaks. Installation and other details are available1.1 https://github.com/taoliu/MACS\npip install macs2\n\n\n\n\nBuenrostro, Jason D, Paul G Giresi, Lisa C Zaba, Howard Y Chang, and William J Greenleaf. 2013. “Transposition of Native Chromatin for Fast and Sensitive Epigenomic Profiling of Open Chromatin, DNA-binding Proteins and Nucleosome Position.” Nature Methods 10 (12): 1213–18. https://doi.org/10.1038/nmeth.2688.\n\n\nBuenrostro, Jason D, Beijing Wu, Howard Y Chang, and William J Greenleaf. 2015. “ATAC-seq: A Method for Assaying Chromatin Accessibility Genome-Wide.” Current Protocols in Molecular Biology / Edited by Frederick M. Ausubel ... [Et Al.] 109 (January): 21.29.1–9. https://doi.org/10.1002/0471142727.mb2129s109.\n\n\nCrawford, Gregory E, Sean Davis, Peter C Scacheri, Gabriel Renaud, Mohamad J Halawi, Michael R Erdos, Roland Green, Paul S Meltzer, Tyra G Wolfsberg, and Francis S Collins. 2006. “DNase-chip: A High-Resolution Method to Identify DNase I Hypersensitive Sites Using Tiled Microarrays.” Nature Methods 3 (7): 503–9. http://www.ncbi.nlm.nih.gov/pubmed/16791207?dopt=AbstractPlus.\n\n\nCrawford, Gregory E, Ingeborg E Holt, James Whittle, Bryn D Webb, Denise Tai, Sean Davis, Elliott H Margulies, et al. 2006. “Genome-Wide Mapping of DNase Hypersensitive Sites Using Massively Parallel Signature Sequencing (MPSS).” Genome Research 16 (1): 123–31. http://www.ncbi.nlm.nih.gov/pubmed/16344561?dopt=AbstractPlus.\n\n\nLawrence, Michael, Wolfgang Huber, Hervé Pagès, Patrick Aboyoun, Marc Carlson, Robert Gentleman, Martin T Morgan, and Vincent J Carey. 2013. “Software for Computing and Annotating Genomic Ranges.” PLoS Computational Biology 9 (8): e1003118. https://doi.org/10.1371/journal.pcbi.1003118.\n\n\nMorgan, Martin, Herve Pages, V Obenchain, and N Hayden. 2016. “Rsamtools: Binary Alignment (BAM), FASTA, Variant Call (BCF), and Tabix File Import.” R Package Version 1 (0): 677–89.\n\n\nTsompana, Maria, and Michael J Buck. 2014. “Chromatin Accessibility: A Window into the Genome.” Epigenetics & Chromatin 7 (1): 33. https://doi.org/10.1186/1756-8935-7-33."
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Bourgon, Richard, Robert Gentleman, and Wolfgang Huber. 2010.\n“Independent Filtering Increases Detection Power for\nHigh-Throughput Experiments.” Proceedings of the National\nAcademy of Sciences 107 (21): 9546–51. https://doi.org/10.1073/pnas.0914005107.\n\n\nBrouwer-Visser, Jurriaan, Wei-Yi Cheng, Anna Bauer-Mehren, Daniela\nMaisel, Katharina Lechner, Emilia Andersson, Joel T. Dudley, and\nFrancesca Milletti. 2018. “Regulatory T-Cell\nGenes Drive Altered\nImmune Microenvironment in Adult\nSolid Cancers and Allow for\nImmune Contextual Patient\nSubtyping.” Cancer Epidemiology, Biomarkers\n& Prevention 27 (1): 103–12. https://doi.org/10.1158/1055-9965.EPI-17-0461.\n\n\nBuenrostro, Jason D, Paul G Giresi, Lisa C Zaba, Howard Y Chang, and\nWilliam J Greenleaf. 2013. “Transposition of Native Chromatin for\nFast and Sensitive Epigenomic Profiling of Open Chromatin, DNA-binding Proteins and Nucleosome\nPosition.” Nature Methods 10 (12): 1213–18. https://doi.org/10.1038/nmeth.2688.\n\n\nBuenrostro, Jason D, Beijing Wu, Howard Y Chang, and William J\nGreenleaf. 2015. “ATAC-seq: A Method\nfor Assaying Chromatin Accessibility Genome-Wide.”\nCurrent Protocols in Molecular Biology / Edited by Frederick M.\nAusubel ... [Et Al.] 109 (January): 21.29.1–9. https://doi.org/10.1002/0471142727.mb2129s109.\n\n\nCenter, Pew Research. 2016. “Lifelong Learning and\nTechnology.” Pew Research Center: Internet,\nScience & Tech. https://www.pewresearch.org/internet/2016/03/22/lifelong-learning-and-technology/.\n\n\nCrawford, Gregory E, Sean Davis, Peter C Scacheri, Gabriel Renaud,\nMohamad J Halawi, Michael R Erdos, Roland Green, Paul S Meltzer, Tyra G\nWolfsberg, and Francis S Collins. 2006. “DNase-chip: A High-Resolution Method to Identify\nDNase I Hypersensitive Sites Using Tiled\nMicroarrays.” Nature Methods 3 (7): 503–9. http://www.ncbi.nlm.nih.gov/pubmed/16791207?dopt=AbstractPlus.\n\n\nCrawford, Gregory E, Ingeborg E Holt, James Whittle, Bryn D Webb, Denise\nTai, Sean Davis, Elliott H Margulies, et al. 2006. “Genome-Wide\nMapping of DNase Hypersensitive Sites Using Massively\nParallel Signature Sequencing (MPSS).” Genome\nResearch 16 (1): 123–31. http://www.ncbi.nlm.nih.gov/pubmed/16344561?dopt=AbstractPlus.\n\n\nDeRisi, J. L., V. R. Iyer, and P. O. Brown. 1997. “Exploring the\nMetabolic and Genetic Control of Gene Expression on a Genomic\nScale.” Science (New York, N.Y.) 278 (5338): 680–86. https://doi.org/10.1126/science.278.5338.680.\n\n\nGreener, Joe G., Shaun M. Kandathil, Lewis Moffat, and David T. Jones.\n2022. “A Guide to Machine Learning for Biologists.”\nNature Reviews Molecular Cell Biology 23 (1): 40–55. https://doi.org/10.1038/s41580-021-00407-0.\n\n\nKnowles, Malcolm S., Elwood F. Holton, and Richard A. Swanson. 2005.\nThe Adult Learner: The Definitive Classic in Adult Education and\nHuman Resource Development. 6th ed. Amsterdam ; Boston: Elsevier.\n\n\nLawrence, Michael, Wolfgang Huber, Hervé Pagès, Patrick Aboyoun, Marc\nCarlson, Robert Gentleman, Martin T. Morgan, and Vincent J. Carey.\n2013a. “Software for Computing and Annotating Genomic\nRanges.” PLoS Computational Biology 9 (8): e1003118. https://doi.org/10.1371/journal.pcbi.1003118.\n\n\nLawrence, Michael, Wolfgang Huber, Hervé Pagès, Patrick Aboyoun, Marc\nCarlson, Robert Gentleman, Martin T Morgan, and Vincent J Carey. 2013b.\n“Software for Computing and Annotating Genomic Ranges.”\nPLoS Computational Biology 9 (8): e1003118. https://doi.org/10.1371/journal.pcbi.1003118.\n\n\nLee, Stuart, Dianne Cook, and Michael Lawrence. 2019. “Plyranges:\nA Grammar of Genomic Data Transformation.” Genome\nBiology 20 (1): 4. https://doi.org/10.1186/s13059-018-1597-8.\n\n\nLibbrecht, Maxwell W., and William Stafford Noble. 2015. “Machine\nLearning Applications in Genetics and Genomics.” Nature\nReviews Genetics 16 (6): 321–32. https://doi.org/10.1038/nrg3920.\n\n\nMorgan, Martin, Herve Pages, V Obenchain, and N Hayden. 2016.\n“Rsamtools: Binary Alignment (BAM), FASTA, Variant Call (BCF), and\nTabix File Import.” R Package Version 1 (0): 677–89.\n\n\nStudent. 1908. “The Probable Error of a\nMean.” Biometrika 6 (1): 1–25. https://doi.org/10.2307/2331554.\n\n\nTsompana, Maria, and Michael J Buck. 2014. “Chromatin\nAccessibility: A Window into the Genome.” Epigenetics &\nChromatin 7 (1): 33. https://doi.org/10.1186/1756-8935-7-33."
  },
  {
    "objectID": "appendix.html#data-sets",
    "href": "appendix.html#data-sets",
    "title": "Appendix A — Appendix",
    "section": "\nA.1 Data Sets",
    "text": "A.1 Data Sets\n\nBRFSS subset\nALL clinical data\nALL expression data"
  },
  {
    "objectID": "appendix.html#swirl",
    "href": "appendix.html#swirl",
    "title": "Appendix A — Appendix",
    "section": "\nA.2 Swirl",
    "text": "A.2 Swirl\nThe following is from the swirl website.\n\nThe swirl R package makes it fun and easy to learn R programming and data science. If you are new to R, have no fear.\n\nTo get started, we need to install a new package into R.\n\ninstall.packages('swirl')\n\nOnce installed, we want to load it into the R workspace so we can use it.\n\nlibrary('swirl')\n\nFinally, to get going, start swirl and follow the instructions.\n\nswirl()"
  },
  {
    "objectID": "additional_resources.html",
    "href": "additional_resources.html",
    "title": "Appendix B — Additional resources",
    "section": "",
    "text": "Base R Cheat Sheet"
  },
  {
    "objectID": "license.html",
    "href": "license.html",
    "title": "",
    "section": "",
    "text": "Code\n\n\n\n\n    To the extent possible under law,  Sean Davis has waived all copyright and related or neighboring rights to Statistical analysis of functional genomics dataa. This work is published from:  United States."
  }
]